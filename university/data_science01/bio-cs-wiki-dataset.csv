url,title,summary,text,num_views,num_edits,categories,revision_id,topic
https://en.wikipedia.org/wiki/Geology,Geology,"Geology (from Ancient Greek  γῆ (gê) 'earth', and  λoγία (-logía) 'study of, discourse') is a branch of natural science concerned with Earth and other astronomical objects, the features or rocks of which it is composed, and the processes by which they change over time. Modern geology significantly overlaps all other Earth sciences, including hydrology, and so is treated as one major aspect of integrated Earth system science and planetary science.
Geology describes the structure of the Earth on and beneath its surface, and the processes that have shaped that structure. It also provides tools to determine the relative and absolute ages of rocks found in a given location, and also to describe the histories of those rocks. By combining these tools, geologists are able to chronicle the geological history of the Earth as a whole, and also to demonstrate the age of the Earth. Geology provides the primary evidence for plate tectonics, the evolutionary history of life, and the Earth's past climates.
Geologists broadly study the properties and processes of Earth and other terrestrial planets and  predominantly solid planetary bodies. Geologists use a wide variety of methods to understand the Earth's structure and evolution, including field work, rock description, geophysical techniques, chemical analysis, physical experiments, and numerical modelling. In practical terms, geology is important for mineral and hydrocarbon exploration and exploitation, evaluating water resources, understanding natural hazards, the remediation of environmental problems, and providing insights into past climate change. Geology is a major academic discipline, and it is central to geological engineering and plays an important role in geotechnical engineering.","Geology (from Ancient Greek  γῆ (gê) 'earth', and  λoγία (-logía) 'study of, discourse') is a branch of natural science concerned with Earth and other astronomical objects, the features or rocks of which it is composed, and the processes by which they change over time. Modern geology significantly overlaps all other Earth sciences, including hydrology, and so is treated as one major aspect of integrated Earth system science and planetary science.
Geology describes the structure of the Earth on and beneath its surface, and the processes that have shaped that structure. It also provides tools to determine the relative and absolute ages of rocks found in a given location, and also to describe the histories of those rocks. By combining these tools, geologists are able to chronicle the geological history of the Earth as a whole, and also to demonstrate the age of the Earth. Geology provides the primary evidence for plate tectonics, the evolutionary history of life, and the Earth's past climates.
Geologists broadly study the properties and processes of Earth and other terrestrial planets and  predominantly solid planetary bodies. Geologists use a wide variety of methods to understand the Earth's structure and evolution, including field work, rock description, geophysical techniques, chemical analysis, physical experiments, and numerical modelling. In practical terms, geology is important for mineral and hydrocarbon exploration and exploitation, evaluating water resources, understanding natural hazards, the remediation of environmental problems, and providing insights into past climate change. Geology is a major academic discipline, and it is central to geological engineering and plays an important role in geotechnical engineering.


== Geological material ==
The majority of geological data comes from research on solid Earth materials. Meteorites and other extraterrestrial natural materials are also studied by geological methods.


=== Mineral ===

Minerals are natural occurring elements and compounds with a definite homogeneous chemical composition and ordered atomic composition.
Each mineral has distinct physical properties, and there are many tests to determine each of them. The specimens can be tested for:
Luster: Quality of light reflected from the surface of a mineral. Examples are metallic, pearly, waxy, dull.
Color: Minerals are grouped by their color. Mostly diagnostic but impurities can change a mineral's color.
Streak: Performed by scratching the sample on a porcelain plate. The color of the streak can help name the mineral.
Hardness: The resistance of a mineral to scratching.
Breakage pattern: A mineral can either show fracture or cleavage, the former being breakage of uneven surfaces, and the latter a breakage along closely spaced parallel planes.
Specific gravity: the weight of a specific volume of a mineral.
Effervescence: Involves dripping hydrochloric acid on the mineral to test for fizzing.
Magnetism: Involves using a magnet to test for magnetism.
Taste: Minerals can have a distinctive taste, such as halite (which tastes like table salt).


=== Rock ===

A rock is any naturally occurring solid mass or aggregate of minerals or mineraloids. Most research in geology is associated with the study of rocks, as they provide the primary record of the majority of the geological history of the Earth. There are three major types of rock: igneous, sedimentary, and metamorphic. The rock cycle 
illustrates the relationships among them (see diagram).
When a rock solidifies or crystallizes from melt (magma or lava), it is an igneous rock. This rock can be weathered and eroded, then redeposited and lithified into a sedimentary rock. It can then be turned into a metamorphic rock by heat and pressure that change its mineral content, resulting in a characteristic fabric. All three types may melt again, and when this happens, new magma is formed, from which an igneous rock may once more solidify.
Organic matter, such as coal, bitumen, oil and natural gas, is linked mainly to organic-rich sedimentary rocks.

To study all three types of rock, geologists evaluate the minerals of which they are composed and their other physical properties, such as texture and fabric. 


=== Unlithified material ===
Geologists also study unlithified materials (referred to as superficial deposits) that lie above the bedrock. This study is often known as Quaternary geology, after the Quaternary period of geologic history, which is the most recent period of geologic time.


==== Magma ====

Magma is the original unlithified source of all igneous rocks. The active flow of molten rock is closely studied in volcanology, and igneous petrology aims to determine the history of igneous rocks from their original molten source to their final crystallization.


== Whole-Earth structure ==


=== Plate tectonics ===

In the 1960s, it was discovered that the Earth's lithosphere, which includes the crust and rigid uppermost portion of the upper mantle, is separated into tectonic plates that move across the plastically deforming, solid, upper mantle, which is called the asthenosphere. This theory is supported by several types of observations, including seafloor spreading and the global distribution of mountain terrain and seismicity.
There is an intimate coupling between the movement of the plates on the surface and the convection of the mantle (that is, the heat transfer caused by the slow movement of ductile mantle rock). Thus, oceanic plates and the adjoining mantle convection currents always move in the same direction – because the oceanic lithosphere is actually the rigid upper thermal boundary layer of the convecting mantle. This coupling between rigid plates moving on the surface of the Earth and the convecting mantle is called plate tectonics.

The development of plate tectonics has provided a physical basis for many observations of the solid Earth. Long linear regions of geological features are explained as plate boundaries.

For example:

Mid-ocean ridges, high regions on the seafloor where hydrothermal vents and volcanoes exist, are seen as divergent boundaries, where two plates move apart.
Arcs of volcanoes and earthquakes are theorized as convergent boundaries, where one plate subducts, or moves, under another.Transform boundaries, such as the San Andreas Fault system, resulted in widespread powerful earthquakes. Plate tectonics also has provided a mechanism for Alfred Wegener's theory of continental drift, in which the continents move across the surface of the Earth over geological time. They also provided a driving force for crustal deformation, and a new setting for the observations of structural geology. The power of the theory of plate tectonics lies in its ability to combine all of these observations into a single theory of how the lithosphere moves over the convecting mantle.


=== Earth structure ===

Advances in seismology, computer modeling, and mineralogy and crystallography at high temperatures and pressures give insights into the internal composition and structure of the Earth.
Seismologists can use the arrival times of seismic waves to image the interior of the Earth. Early advances in this field showed the existence of a liquid outer core (where shear waves were not able to propagate) and a dense solid inner core. These advances led to the development of a layered model of the Earth, with a crust and lithosphere on top, the mantle below (separated within itself by seismic discontinuities at 410 and 660 kilometers), and the outer core and inner core below that. More recently, seismologists have been able to create detailed images of wave speeds inside the earth in the same way a doctor images a body in a CT scan. These images have led to a much more detailed view of the interior of the Earth, and have replaced the simplified layered model with a much more dynamic model.
Mineralogists have been able to use the pressure and temperature data from the seismic and modeling studies alongside knowledge of the elemental composition of the Earth to reproduce these conditions in experimental settings and measure changes in crystal structure. These studies explain the chemical changes associated with the major seismic discontinuities in the mantle and show the crystallographic structures expected in the inner core of the Earth.


== Geological time ==

The geological time scale encompasses the history of the Earth. It is bracketed at the earliest by the dates of the first Solar System material at 4.567 Ga (or 4.567 billion years ago) and the formation of the Earth at
4.54 Ga
(4.54 billion years), which is the beginning of the informally recognized Hadean eon – a division of geological time. At the later end of the scale, it is marked by the present day (in the Holocene epoch).


=== Timescale of the Earth ===
The following five timelines show the geologic time scale to scale. The first shows the entire time from the formation of the Earth to the present, but this gives little space for the most recent eon. The second timeline shows an expanded view of the most recent eon. In a similar way, the most recent era is expanded in the third timeline, the most recent period is expanded in the fourth timeline, and the most recent epoch is expanded in the fifth timeline.


=== Important milestones on Earth ===

4.567 Ga (gigaannum: billion years ago): Solar system formation
4.54 Ga: Accretion, or formation, of Earth
c. 4 Ga: End of Late Heavy Bombardment, the first life
c. 3.5 Ga: Start of photosynthesis
c. 2.3 Ga: Oxygenated atmosphere, first snowball Earth
730–635 Ma (megaannum: million years ago): second snowball Earth
541 ± 0.3 Ma: Cambrian explosion – vast multiplication of hard-bodied life; first abundant fossils; start of the Paleozoic
c. 380 Ma: First vertebrate land animals
250 Ma: Permian-Triassic extinction – 90% of all land animals die; end of Paleozoic and beginning of Mesozoic
66 Ma: Cretaceous–Paleogene extinction – Dinosaurs die; end of Mesozoic and beginning of Cenozoic
c. 7 Ma: First hominins appear
3.9 Ma: First Australopithecus, direct ancestor to modern Homo sapiens, appear
200 ka (kiloannum: thousand years ago): First modern Homo sapiens appear in East Africa


=== Timescale of the Moon ===


=== Timescale of Mars ===


== Dating methods ==


=== Relative dating ===

Methods for relative dating were developed when geology first emerged as a natural science. Geologists still use the following principles today as a means to provide information about geological history and the timing of geological events.
The principle of uniformitarianism states that the geological processes observed in operation that modify the Earth's crust at present have worked in much the same way over geological time. A fundamental principle of geology advanced by the 18th-century Scottish physician and geologist James Hutton is that ""the present is the key to the past."" In Hutton's words: ""the past history of our globe must be explained by what can be seen to be happening now.""The principle of intrusive relationships concerns crosscutting intrusions. In geology, when an igneous intrusion cuts across a formation of sedimentary rock, it can be determined that the igneous intrusion is younger than the sedimentary rock. Different types of intrusions include stocks, laccoliths, batholiths, sills and dikes.
The principle of cross-cutting relationships pertains to the formation of faults and the age of the sequences through which they cut. Faults are younger than the rocks they cut; accordingly, if a fault is found that penetrates some formations but not those on top of it, then the formations that were cut are older than the fault, and the ones that are not cut must be younger than the fault. Finding the key bed in these situations may help determine whether the fault is a normal fault or a thrust fault.The principle of inclusions and components states that, with sedimentary rocks, if inclusions (or clasts) are found in a formation, then the inclusions must be older than the formation that contains them. For example, in sedimentary rocks, it is common for gravel from an older formation to be ripped up and included in a newer layer. A similar situation with igneous rocks occurs when xenoliths are found. These foreign bodies are picked up as magma or lava flows, and are incorporated, later to cool in the matrix. As a result, xenoliths are older than the rock that contains them.

The principle of original horizontality states that the deposition of sediments occurs as essentially horizontal beds. Observation of modern marine and non-marine sediments in a wide variety of environments supports this generalization (although cross-bedding is inclined, the overall orientation of cross-bedded units is horizontal).The principle of superposition states that a sedimentary rock layer in a tectonically undisturbed sequence is younger than the one beneath it and older than the one above it. Logically a younger layer cannot slip beneath a layer previously deposited. This principle allows sedimentary layers to be viewed as a form of the vertical timeline, a partial or complete record of the time elapsed from deposition of the lowest layer to deposition of the highest bed.The principle of faunal succession is based on the appearance of fossils in sedimentary rocks. As organisms exist during the same period throughout the world, their presence or (sometimes) absence provides a relative age of the formations where they appear. Based on principles that William Smith laid out almost a hundred years before the publication of Charles Darwin's theory of evolution, the principles of succession developed independently of evolutionary thought. The principle becomes quite complex, however, given the uncertainties of fossilization, localization of fossil types due to lateral changes in habitat (facies change in sedimentary strata), and that not all fossils formed globally at the same time.


=== Absolute dating ===

Geologists also use methods to determine the absolute age of rock samples and geological events. These dates are useful on their own and may also be used in conjunction with relative dating methods or to calibrate relative methods.At the beginning of the 20th century, advancement in geological science was facilitated by the ability to obtain accurate absolute dates to geological events using radioactive isotopes and other methods. This changed the understanding of geological time. Previously, geologists could only use fossils and stratigraphic correlation to date sections of rock relative to one another. With isotopic dates, it became possible to assign absolute ages to rock units, and these absolute dates could be applied to fossil sequences in which there was datable material, converting the old relative ages into new absolute ages.
For many geological applications, isotope ratios of radioactive elements are measured in minerals that give the amount of time that has passed since a rock passed through its particular closure temperature, the point at which different radiometric isotopes stop diffusing into and out of the crystal lattice. These are used in geochronologic and thermochronologic studies. Common methods include uranium–lead dating, potassium–argon dating, argon–argon dating and uranium–thorium dating. These methods are used for a variety of applications. Dating of lava and volcanic ash layers found within a stratigraphic sequence can provide absolute age data for sedimentary rock units that do not contain radioactive isotopes and calibrate relative dating techniques. These methods can also be used to determine ages of pluton emplacement.
Thermochemical techniques can be used to determine temperature profiles within the crust, the uplift of mountain ranges, and paleo-topography.
Fractionation of the lanthanide series elements is used to compute ages since rocks were removed from the mantle.
Other methods are used for more recent events. Optically stimulated luminescence and cosmogenic radionuclide dating are used to date surfaces and/or erosion rates. Dendrochronology can also be used for the dating of landscapes. Radiocarbon dating is used for geologically young materials containing organic carbon.


== Geological development of an area ==

The geology of an area changes through time as rock units are deposited and inserted, and deformational processes change their shapes and locations.
Rock units are first emplaced either by deposition onto the surface or intrusion into the overlying rock. Deposition can occur when sediments settle onto the surface of the Earth and later lithify into sedimentary rock, or when as volcanic material such as volcanic ash or lava flows blanket the surface. Igneous intrusions such as batholiths, laccoliths, dikes, and sills, push upwards into the overlying rock, and crystallize as they intrude.
After the initial sequence of rocks has been deposited, the rock units can be deformed and/or metamorphosed. Deformation typically occurs as a result of horizontal shortening, horizontal extension, or side-to-side (strike-slip) motion. These structural regimes broadly relate to convergent boundaries, divergent boundaries, and transform boundaries, respectively, between tectonic plates.
When rock units are placed under horizontal compression, they shorten and become thicker. Because rock units, other than muds, do not significantly change in volume, this is accomplished in two primary ways: through faulting and folding. In the shallow crust, where brittle deformation can occur, thrust faults form, which causes the deeper rock to move on top of the shallower rock. Because deeper rock is often older, as noted by the principle of superposition, this can result in older rocks moving on top of younger ones. Movement along faults can result in folding, either because the faults are not planar or because rock layers are dragged along, forming drag folds as slip occurs along the fault. Deeper in the Earth, rocks behave plastically and fold instead of faulting. These folds can either be those where the material in the center of the fold buckles upwards, creating ""antiforms"", or where it buckles downwards, creating ""synforms"". If the tops of the rock units within the folds remain pointing upwards, they are called anticlines and synclines, respectively. If some of the units in the fold are facing downward, the structure is called an overturned anticline or syncline, and if all of the rock units are overturned or the correct up-direction is unknown, they are simply called by the most general terms, antiforms, and synforms.

Even higher pressures and temperatures during horizontal shortening can cause both folding and metamorphism of the rocks. This metamorphism causes changes in the mineral composition of the rocks; creates a foliation, or planar surface, that is related to mineral growth under stress. This can remove signs of the original textures of the rocks, such as bedding in sedimentary rocks, flow features of lavas, and crystal patterns in crystalline rocks.
Extension causes the rock units as a whole to become longer and thinner. This is primarily accomplished through normal faulting and through the ductile stretching and thinning. Normal faults drop rock units that are higher below those that are lower. This typically results in younger units ending up below older units. Stretching of units can result in their thinning. In fact, at one location within the Maria Fold and Thrust Belt, the entire sedimentary sequence of the Grand Canyon appears over a length of less than a meter. Rocks at the depth to be ductilely stretched are often also metamorphosed. These stretched rocks can also pinch into lenses, known as boudins, after the French word for ""sausage"" because of their visual similarity.
Where rock units slide past one another, strike-slip faults develop in shallow regions, and become shear zones at deeper depths where the rocks deform ductilely.

The addition of new rock units, both depositionally and intrusively, often occurs during deformation. Faulting and other deformational processes result in the creation of topographic gradients, causing material on the rock unit that is increasing in elevation to be eroded by hillslopes and channels. These sediments are deposited on the rock unit that is going down. Continual motion along the fault maintains the topographic gradient in spite of the movement of sediment and continues to create accommodation space for the material to deposit. Deformational events are often also associated with volcanism and igneous activity. Volcanic ashes and lavas accumulate on the surface, and igneous intrusions enter from below. Dikes, long, planar igneous intrusions, enter along cracks, and therefore often form in large numbers in areas that are being actively deformed. This can result in the emplacement of dike swarms, such as those that are observable across the Canadian shield, or rings of dikes around the lava tube of a volcano.
All of these processes do not necessarily occur in a single environment and do not necessarily occur in a single order. The Hawaiian Islands, for example, consist almost entirely of layered basaltic lava flows. The sedimentary sequences of the mid-continental United States and the Grand Canyon in the southwestern United States contain almost-undeformed stacks of sedimentary rocks that have remained in place since Cambrian time. Other areas are much more geologically complex. In the southwestern United States, sedimentary, volcanic, and intrusive rocks have been metamorphosed, faulted, foliated, and folded. Even older rocks, such as the Acasta gneiss of the Slave craton in northwestern Canada, the oldest known rock in the world have been metamorphosed to the point where their origin is indiscernible without laboratory analysis. In addition, these processes can occur in stages. In many places, the Grand Canyon in the southwestern United States being a very visible example, the lower rock units were metamorphosed and deformed, and then deformation ended and the upper, undeformed units were deposited. Although any amount of rock emplacement and rock deformation can occur, and they can occur any number of times, these concepts provide a guide to understanding the geological history of an area.


== Methods of geology ==

Geologists use a number of fields, laboratory, and numerical modeling methods to decipher Earth history and to understand the processes that occur on and inside the Earth. In typical geological investigations, geologists use primary information related to petrology (the study of rocks), stratigraphy (the study of sedimentary layers), and structural geology (the study of positions of rock units and their deformation). In many cases, geologists also study modern soils, rivers, landscapes, and glaciers; investigate past and current life and biogeochemical pathways, and use geophysical methods to investigate the subsurface. Sub-specialities of geology may distinguish endogenous and exogenous geology.


=== Field methods ===

Geological field work varies depending on the task at hand. Typical fieldwork could consist of:

Geological mappingStructural mapping: identifying the locations of major rock units and the faults and folds that led to their placement there.
Stratigraphic mapping: pinpointing the locations of sedimentary facies (lithofacies and biofacies) or the mapping of isopachs of equal thickness of sedimentary rock
Surficial mapping: recording the locations of soils and surficial deposits
Surveying of topographic features
compilation of topographic maps
Work to understand change across landscapes, including:
Patterns of erosion and deposition
River-channel change through migration and avulsion
Hillslope processes
Subsurface mapping through geophysical methodsThese methods include:
Shallow seismic surveys
Ground-penetrating radar
Aeromagnetic surveys
Electrical resistivity tomography
They aid in:
Hydrocarbon exploration
Finding groundwater
Locating buried archaeological artifacts
High-resolution stratigraphy
Measuring and describing stratigraphic sections on the surface
Well drilling and logging
Biogeochemistry and geomicrobiologyCollecting samples to:
determine biochemical pathways
identify new species of organisms
identify new chemical compounds
and to use these discoveries to:
understand early life on Earth and how it functioned and metabolized
find important compounds for use in pharmaceuticals
Paleontology: excavation of fossil material
For research into past life and evolution
For museums and education
Collection of samples for geochronology and thermochronology
Glaciology: measurement of characteristics of glaciers and their motion


=== Petrology ===

In addition to identifying rocks in the field (lithology), petrologists identify rock samples in the laboratory. Two of the primary methods for identifying rocks in the laboratory are through optical microscopy and by using an electron microprobe. In an optical mineralogy analysis, petrologists analyze thin sections of rock samples using a petrographic microscope, where the minerals can be identified through their different properties in plane-polarized and cross-polarized light, including their birefringence, pleochroism, twinning, and interference properties with a conoscopic lens. In the electron microprobe, individual locations are analyzed for their exact chemical compositions and variation in composition within individual crystals. Stable and radioactive isotope studies provide insight into the geochemical evolution of rock units.
Petrologists can also use fluid inclusion data and perform high temperature and pressure physical experiments to understand the temperatures and pressures at which different mineral phases appear, and how they change through igneous and metamorphic processes. This research can be extrapolated to the field to understand metamorphic processes and the conditions of crystallization of igneous rocks. This work can also help to explain processes that occur within the Earth, such as subduction and magma chamber evolution.


=== Structural geology ===

Structural geologists use microscopic analysis of oriented thin sections of geological samples to observe the fabric within the rocks, which gives information about strain within the crystalline structure of the rocks. They also plot and combine measurements of geological structures to better understand the orientations of faults and folds to reconstruct the history of rock deformation in the area. In addition, they perform analog and numerical experiments of rock deformation in large and small settings.
The analysis of structures is often accomplished by plotting the orientations of various features onto stereonets. A stereonet is a stereographic projection of a sphere onto a plane, in which planes are projected as lines and lines are projected as points. These can be used to find the locations of fold axes, relationships between faults, and relationships between other geological structures.
Among the most well-known experiments in structural geology are those involving orogenic wedges, which are zones in which mountains are built along convergent tectonic plate boundaries. In the analog versions of these experiments, horizontal layers of sand are pulled along a lower surface into a back stop, which results in realistic-looking patterns of faulting and the growth of a critically tapered (all angles remain the same) orogenic wedge. Numerical models work in the same way as these analog models, though they are often more sophisticated and can include patterns of erosion and uplift in the mountain belt. This helps to show the relationship between erosion and the shape of a mountain range. These studies can also give useful information about pathways for metamorphism through pressure, temperature, space, and time.


=== Stratigraphy ===

In the laboratory, stratigraphers analyze samples of stratigraphic sections that can be returned from the field, such as those from drill cores. Stratigraphers also analyze data from geophysical surveys that show the locations of stratigraphic units in the subsurface. Geophysical data and well logs can be combined to produce a better view of the subsurface, and stratigraphers often use computer programs to do this in three dimensions. Stratigraphers can then use these data to reconstruct ancient processes occurring on the surface of the Earth, interpret past environments, and locate areas for water, coal, and hydrocarbon extraction.
In the laboratory, biostratigraphers analyze rock samples from outcrop and drill cores for the fossils found in them. These fossils help scientists to date the core and to understand the depositional environment in which the rock units formed. Geochronologists precisely date rocks within the stratigraphic section to provide better absolute bounds on the timing and rates of deposition.
Magnetic stratigraphers look for signs of magnetic reversals in igneous rock units within the drill cores. Other scientists perform stable-isotope studies on the rocks to gain information about past climate.


== Planetary geology ==

With the advent of space exploration in the twentieth century, geologists have begun to look at other planetary bodies in the same ways that have been developed to study the Earth. This new field of study is called planetary geology (sometimes known as astrogeology) and relies on known geological principles to study other bodies of the solar system. This is a major aspect of planetary science, and largely focuses on the terrestrial planets, icy moons, asteroids, comets, and meteorites. However, some planetary geophysicists study the giant planets and exoplanets.Although the Greek-language-origin prefix geo refers to Earth, ""geology"" is often used in conjunction with the names of other planetary bodies when describing their composition and internal processes: examples are ""the geology of Mars"" and ""Lunar geology"". Specialized terms such as selenology (studies of the Moon), areology (of Mars), etc., are also in use.
Although planetary geologists are interested in studying all aspects of other planets, a significant focus is to search for evidence of past or present life on other worlds. This has led to many missions whose primary or ancillary purpose is to examine planetary bodies for evidence of life. One of these is the Phoenix lander, which analyzed Martian polar soil for water, chemical, and mineralogical constituents related to biological processes.


== Applied geology ==


=== Economic geology ===

Economic geology is a branch of geology that deals with aspects of economic minerals that humankind uses to fulfill various needs. Economic minerals are those extracted profitably for various practical uses. Economic geologists help locate and manage the Earth's natural resources, such as petroleum and coal, as well as mineral resources, which include metals such as iron, copper, and uranium.


==== Mining geology ====

Mining geology consists of the extractions of mineral resources from the Earth. Some resources of economic interests include gemstones, metals such as gold and copper, and many minerals such as asbestos, perlite, mica, phosphates, zeolites, clay, pumice, quartz, and silica, as well as elements such as sulfur, chlorine, and helium.


==== Petroleum geology ====

Petroleum geologists study the locations of the subsurface of the Earth that can contain extractable hydrocarbons, especially petroleum and natural gas. Because many of these reservoirs are found in sedimentary basins, they study the formation of these basins, as well as their sedimentary and tectonic evolution and the present-day positions of the rock units.


=== Engineering geology ===

Engineering geology is the application of geological principles to engineering practice for the purpose of assuring that the geological factors affecting the location, design, construction, operation, and maintenance of engineering works are properly addressed. Engineering geology is distinct from geological engineering, particularly in North America.

In the field of civil engineering, geological principles and analyses are used in order to ascertain the mechanical principles of the material on which structures are built. This allows tunnels to be built without collapsing, bridges and skyscrapers to be built with sturdy foundations, and buildings to be built that will not settle in clay and mud.


=== Hydrology ===

Geology and geological principles can be applied to various environmental problems such as stream restoration, the restoration of brownfields, and the understanding of the interaction between natural habitat and the geological environment. Groundwater hydrology, or hydrogeology, is used to locate groundwater, which can often provide a ready supply of uncontaminated water and is especially important in arid regions, and to monitor the spread of contaminants in groundwater wells.


=== Paleoclimatology ===

Geologists also obtain data through stratigraphy, boreholes, core samples, and ice cores. Ice cores and sediment cores are used for paleoclimate reconstructions, which tell geologists about past and present temperature, precipitation, and sea level across the globe. These datasets are our primary source of information on global climate change outside of instrumental data.


=== Natural hazards ===

Geologists and geophysicists study natural hazards in order to enact safe building codes and warning systems that are used to prevent loss of property and life. Examples of important natural hazards that are pertinent to geology (as opposed those that are mainly or only pertinent to meteorology) are:


== History ==

The study of the physical material of the Earth dates back at least to ancient Greece when Theophrastus (372–287 BCE) wrote the work Peri Lithon (On Stones). During the Roman period, Pliny the Elder wrote in detail of the many minerals and metals, then in practical use – even correctly noting the origin of amber. Additionally, in the 4th century BCE Aristotle made critical observations of the slow rate of geological change. He observed the composition of the land and formulated a theory where the Earth changes at a slow rate and that these changes cannot be observed during one person's lifetime. Aristotle developed one of the first evidence-based concepts connected to the geological realm regarding the rate at which the Earth physically changes.Abu al-Rayhan al-Biruni (973–1048 CE) was one of the earliest Persian geologists, whose works included the earliest writings on the geology of India, hypothesizing that the Indian subcontinent was once a sea. Drawing from Greek and Indian scientific literature that were not destroyed by the Muslim conquests, the Persian scholar Ibn Sina (Avicenna, 981–1037) proposed detailed explanations for the formation of mountains, the origin of earthquakes, and other topics central to modern geology, which provided an essential foundation for the later development of the science. In China, the polymath Shen Kuo (1031–1095) formulated a hypothesis for the process of land formation: based on his observation of fossil animal shells in a geological stratum in a mountain hundreds of miles from the ocean, he inferred that the land was formed by the erosion of the mountains and by deposition of silt.Nicolas Steno (1638–1686) is credited with the law of superposition, the principle of original horizontality, and the principle of lateral continuity: three defining principles of stratigraphy.
The word geology was first used by Ulisse Aldrovandi in 1603, then by Jean-André Deluc in 1778 and introduced as a fixed term by Horace-Bénédict de Saussure in 1779. The word is derived from the Greek γῆ, gê, meaning ""earth"" and λόγος, logos, meaning ""speech"". But according to another source, the word ""geology"" comes from a Norwegian, Mikkel Pedersøn Escholt (1600–1699), who was a priest and scholar. Escholt first used the definition in his book titled, Geologia Norvegica (1657).William Smith (1769–1839) drew some of the first geological maps and began the process of ordering rock strata (layers) by examining the fossils contained in them.In 1763, Mikhail Lomonosov published his treatise On the Strata of Earth. His work was the first narrative of modern geology, based on the unity of processes in time and explanation of the Earth's past from the present.James Hutton (1726-1797) is often viewed as the first modern geologist. In 1785 he presented a paper entitled Theory of the Earth to the Royal Society of Edinburgh. In his paper, he explained his theory that the Earth must be much older than had previously been supposed to allow enough time for mountains to be eroded and for sediments to form new rocks at the bottom of the sea, which in turn were raised up to become dry land. Hutton published a two-volume version of his ideas in 1795.Followers of Hutton were known as Plutonists because they believed that some rocks were formed by vulcanism, which is the deposition of lava from volcanoes, as opposed to the Neptunists, led by Abraham Werner, who believed that all rocks had settled out of a large ocean whose level gradually dropped over time.
The first geological map of the U.S. was produced in 1809 by William Maclure. In 1807, Maclure commenced the self-imposed task of making a geological survey of the United States. Almost every state in the Union was traversed and mapped by him, the Allegheny Mountains being crossed and recrossed some 50 times. The results of his unaided labours were submitted to the American Philosophical Society in a memoir entitled Observations on the Geology of the United States explanatory of a Geological Map, and published in the Society's Transactions, together with the nation's first geological map. This antedates William Smith's geological map of England by six years, although it was constructed using a different classification of rocks.
Sir Charles Lyell (1797-1875) first published his famous book, Principles of Geology, in 1830. This book, which influenced the thought of Charles Darwin, successfully promoted the doctrine of uniformitarianism. This theory states that slow geological processes have occurred throughout the Earth's history and are still occurring today. In contrast, catastrophism is the theory that Earth's features formed in single, catastrophic events and remained unchanged thereafter. Though Hutton believed in uniformitarianism, the idea was not widely accepted at the time.
Much of 19th-century geology revolved around the question of the Earth's exact age. Estimates varied from a few hundred thousand to billions of years. By the early 20th century, radiometric dating allowed the Earth's age to be estimated at two billion years. The awareness of this vast amount of time opened the door to new theories about the processes that shaped the planet.
Some of the most significant advances in 20th-century geology have been the development of the theory of plate tectonics in the 1960s and the refinement of estimates of the planet's age. Plate tectonics theory arose from two separate geological observations: seafloor spreading and continental drift. The theory revolutionized the Earth sciences. Today the Earth is known to be approximately 4.5 billion years old.

		
		
		


== Fields or related disciplines ==


== See also ==


== References ==


== External links ==

One Geology: This interactive geological map of the world is an international initiative of the geological surveys around the globe. This groundbreaking project was launched in 2007 and contributed to the 'International Year of Planet Earth', becoming one of their flagship projects. 
Earth Science News, Maps, Dictionary, Articles, Jobs
American Geophysical Union
American Geosciences Institute
European Geosciences Union
Geological Society of America
Geological Society of London
Video-interviews with famous geologists
Geology OpenTextbook
Chronostratigraphy benchmarks",4234884,3290,"All articles with failed verification, Articles containing Ancient Greek (to 1453)-language text, Articles with BNF identifiers, Articles with EMU identifiers, Articles with FAST identifiers, Articles with GND identifiers, Articles with HDS identifiers, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with NARA identifiers, Articles with NDL identifiers, Articles with NKC identifiers, Articles with failed verification from September 2019, Articles with short description, CS1: long volume value, Commons category link is on Wikidata, Geology, Pages using multiple image with auto scaled images, Pages using the EasyTimeline extension, Short description is different from Wikidata, Webarchive template wayback links",1134347625,biology
https://en.wikipedia.org/wiki/Molecular_biology,Molecular biology,"Molecular biology  is the branch of biology that seeks to understand the molecular basis of biological activity in and between cells, including biomolecular synthesis, modification, mechanisms, and interactions. The study of chemical and physical structure of biological macromolecules is known as molecular biology.Molecular biology was first described as an approach focused on the underpinnings of biological phenomena - uncovering the structures of biological molecules as well as their interactions, and how these interactions explain observations of classical biology.In 1945 the term molecular biology was used by physicist William Astbury. In 1953 Francis Crick, James Watson,  Rosalind Franklin, and colleagues, working at Medical Research Council unit, Cavendish laboratory, Cambridge (now the MRC Laboratory of Molecular Biology), made a double helix model of DNA which changed the entire research scenario. They proposed the DNA structure based on previous research done by Rosalind Franklin and Maurice Wilkins. This research then lead to finding DNA material in other microorganisms, plants and animals.Molecular biology is not simply the study of biological molecules and their interactions; rather, it is also a collection of techniques developed since the field's genesis which have enabled scientists to learn about molecular processes. In this way it has both complemented and improved biochemistry and genetics as methods (of understanding nature) that began before its advent. One notable technique which has revolutionized the field is the polymerase chain reaction (PCR), which was developed in 1983. PCR is a reaction which amplifies small quantities of DNA, and it is used in many applications across scientific disciplines.The central dogma of molecular biology describes the process in which DNA is transcribed into RNA, which is then translated into protein.Molecular biology also plays a critical role in the understanding of structures, functions, and internal controls within individual cells, all of which can be used to efficiently target new drugs, diagnose disease, and better understand cell physiology. Some clinical research and medical therapies arising from molecular biology are covered under gene therapy whereas the use of molecular biology or molecular cell biology in medicine is now referred to as molecular medicine.","Molecular biology  is the branch of biology that seeks to understand the molecular basis of biological activity in and between cells, including biomolecular synthesis, modification, mechanisms, and interactions. The study of chemical and physical structure of biological macromolecules is known as molecular biology.Molecular biology was first described as an approach focused on the underpinnings of biological phenomena - uncovering the structures of biological molecules as well as their interactions, and how these interactions explain observations of classical biology.In 1945 the term molecular biology was used by physicist William Astbury. In 1953 Francis Crick, James Watson,  Rosalind Franklin, and colleagues, working at Medical Research Council unit, Cavendish laboratory, Cambridge (now the MRC Laboratory of Molecular Biology), made a double helix model of DNA which changed the entire research scenario. They proposed the DNA structure based on previous research done by Rosalind Franklin and Maurice Wilkins. This research then lead to finding DNA material in other microorganisms, plants and animals.Molecular biology is not simply the study of biological molecules and their interactions; rather, it is also a collection of techniques developed since the field's genesis which have enabled scientists to learn about molecular processes. In this way it has both complemented and improved biochemistry and genetics as methods (of understanding nature) that began before its advent. One notable technique which has revolutionized the field is the polymerase chain reaction (PCR), which was developed in 1983. PCR is a reaction which amplifies small quantities of DNA, and it is used in many applications across scientific disciplines.The central dogma of molecular biology describes the process in which DNA is transcribed into RNA, which is then translated into protein.Molecular biology also plays a critical role in the understanding of structures, functions, and internal controls within individual cells, all of which can be used to efficiently target new drugs, diagnose disease, and better understand cell physiology. Some clinical research and medical therapies arising from molecular biology are covered under gene therapy whereas the use of molecular biology or molecular cell biology in medicine is now referred to as molecular medicine.


== History of molecular biology ==

Molecular biology sits at the intersection of biochemistry and genetics; as these scientific disciplines emerged and evolved in the 20th century, it became clear that they both sought to determine the molecular mechanisms which underlie vital cellular functions. Advances in molecular biology have been closely related to the development of new technologies and their optimization. Molecular biology has been elucidated by the work of many scientists, and thus the history of the field depends on an understanding of these scientists and their experiments.
The field of genetics arose as an attempt to understand the molecular mechanisms of genetic inheritance and the structure of a gene. Gregor Mendel pioneered this work in 1866, when he first wrote the laws of genetic inheritance based on his studies of mating crosses in pea plants. One such law of genetic inheritance is the law of segregation, which states that diploid individuals with two alleles for a particular gene will pass one of these alleles to their offspring. Because of his critical work, the study of genetic inheritance is commonly referred to as Mendelian genetics.A major milestone in molecular biology was the discovery of the structure of DNA. This work began in 1869 by Friedrich Miescher, a Swiss biochemist who first proposed a structure called nuclein, which we now know to be (deoxyribonucleic acid), or DNA. He discovered this unique substance by studying the components of pus-filled bandages, and noting the unique properties of the ""phosphorus-containing substances"". Another notable contributor to the DNA model was Phoebus Levene, who proposed the ""polynucleotide model"" of DNA in 1919 as a result of his biochemical experiments on yeast. In 1950, Erwin Chargaff expanded on the work of Levene and elucidated a few critical properties of nucleic acids: first, the sequence of nucleic acids varies across species. Second, the total concentration of purines (adenine and guanine) is always equal to the total concentration of pyrimidines (cysteine and thymine). This is now known as Chargaff's rule. In 1953, James Watson and Francis Crick published the double helical structure of DNA, using the X-ray crystallography work done by Rosalind Franklin and Maurice Wilkins. Watson and Crick described the structure of DNA and conjectured about the implications of this unique structure for possible mechanisms of DNA replication.J. D. Watson and F. H. C. Crick were awarded Nobel prize in 1962, along with Maurice Wilkens, for proposing a model of the structure of DNA.In 1961, it was demonstrated that when a gene encodes a protein, three sequential bases of a gene's DNA specify each successive amino acid of the protein.  Thus the genetic code is a triplet code, where each triplet (called a codon) specifies a particular amino acid.  Furthermore, it was shown that the codons do not overlap with each other in the DNA sequence encoding a protein, and that each sequence is read from a fixed starting point.

During 1962–1964, through the use of conditional lethal mutants of a bacterial virus, fundamental advances were made in our understanding of the functions and interactions of the proteins employed in the machinery of DNA replication, DNA repair,  DNA recombination, and in the assembly of molecular structures.


== The F.Griffith experiment ==

In 1928, Fredrick Griffith, encountered a virulence property in pneumococcus bacteria, which was killing lab rats. According to Mendel, prevalent at that time, gene transfer could occur only from parent to daughter cells only. Griffith advanced another theory, stating that gene transfer occurring in member of same generation is known as horizontal gene transfer (HGT). This phenomenon is now referred to as genetic transformation.
Griffith addressed the Streptococcus pneumoniae bacteria, which had two different strains, one virulent and smooth and one avirulent and rough. The smooth strain had glistering appearance owing to the presence of a type of specific polysaccharide – a polymer of glucose and glucuronic acid capsule. Due to this polysaccharide layer of bacteria, a host's immune system cannot recognize the bacteria and it kills the host. The other, avirulent, rough strain lacks this polysaccharide capsule and has a dull, rough appearance.
Presence or absence of capsule in the  strain, is known to be genetically determined. Smooth and rough strains occur in several different type such as S-I, S-II, S-III, etc. and R-I, R-II, R-III, etc. respectively. All this subtypes of S and R bacteria differ with each other in antigen type they produce.


== Hershey and Chase experiment ==

Confirmation that DNA is the genetic material which is cause of infection came from Hershey and Chase experiment. They used E.coli and bacteriophage for the experiment. This experiment is also known as blender experiment, as kitchen blender was used as a major piece of apparatus. Alfred Hershey and Martha Chase demonstrated that the DNA injected by a phage particle into a bacterium contains all information required to synthesize progeny phage particles. They used radioactivity to tag the bacteriophage's protein coat with radioactive sulphur and DNA with radioactive phosphorus, into two different test tubes respectively. After mixing bacteriophage and E.coli into the test tube, the incubation period starts in which phage transforms the genetic material in the E.coli cells. Then the mixture is blended or agitated, which separates the phage from E.coli cells. The whole mixture is centrifuged and the pellet which contains E.coli cells was checked and the supernatant was discarded. The E.coli cells showed radioactive phosphorus, which indicated that the transformed material was DNA not the protein coat.
The transformed DNA gets attached to the DNA of E.coli and radioactivity is only seen onto the bacteriophage's DNA. This mutated DNA can be passed to the next generation and the theory of Transduction came into existence. Transduction is a process in which the bacterial DNA carry the fragment of bacteriophages and pass it on the next generation. This is also a type of horizontal gene transfer.


== Modern molecular biology ==
In the early 2020s, molecular biology entered a golden age defined by both vertical and horizontal technical development. Vertically, novel technologies are allowing for real-time monitoring of biological processes at the atomic level. Molecular biologists today have access to increasingly affordable sequencing data at increasingly higher depths, facilitating the development of novel genetic manipulation methods in new non-model organisms. Likewise, synthetic molecular biologists will drive the industrial production of small and macro molecules through the introduction of exogenous metabolic pathways in various prokaryotic and eukaryotic cell lines.Horizontally, sequencing data is becoming more affordable and used in many different scientific fields. This will drive the development of industries in developing nations and increase accessibility to individual researchers. Likewise, CRISPR-Cas9 gene editing experiments can now be conceived and implemented by individuals for under $10,000 in novel organisms, which will drive the development of industrial and medical applications 


== Relationship to other biological sciences ==

The following list describes a viewpoint on the interdisciplinary relationships between molecular biology and other related fields.
Molecular biology is the study of the molecular underpinnings of the biological phenomena, focusing on molecular synthesis, modification, mechanisms and interactions.
Biochemistry is the study of the chemical substances and vital processes occurring in living organisms. Biochemists focus heavily on the role, function, and structure of biomolecules such as proteins, lipids, carbohydrates and nucleic acids.
Genetics is the study of how genetic differences affect organisms. Genetics attempts to predict how mutations, individual genes and genetic interactions can affect the expression of a phenotypeWhile researchers practice techniques specific to molecular biology, it is common to combine these with methods from genetics and biochemistry. Much of molecular biology is quantitative, and recently a significant amount of work has been done using computer science techniques such as bioinformatics and computational biology. Molecular genetics, the study of gene structure and function, has been among the most prominent sub-fields of molecular biology since the early 2000s. Other branches of biology are informed by molecular biology, by either directly studying the interactions of molecules in their own right such as in cell biology and developmental biology, or indirectly, where molecular techniques are used to infer historical attributes of populations or species, as in fields in evolutionary biology such as population genetics and phylogenetics. There is also a long tradition of studying biomolecules ""from the ground up"", or molecularly, in biophysics.


== Techniques of molecular biology ==


=== Molecular cloning ===

Molecular cloning is used to isolate and then transfer a DNA sequence of interest into a plasmid vector. This recombinant DNA technology was first developed in the 1960s. In this technique, a DNA sequence coding for a protein of interest is cloned using polymerase chain reaction (PCR), and/or restriction enzymes, into a plasmid (expression vector). The plasmid vector usually has at least 3 distinctive features: an origin of replication, a multiple cloning site (MCS), and a selective marker (usually antibiotic resistance). Additionally, upstream of the MCS are the promoter regions and the transcription start site, which regulate the expression of cloned gene.
This plasmid can be inserted into either bacterial or animal cells. Introducing DNA into bacterial cells can be done by transformation via uptake of naked DNA, conjugation via cell-cell contact or by transduction via viral vector. Introducing DNA into eukaryotic cells, such as animal cells, by physical or chemical means is called transfection. Several different transfection techniques are available, such as calcium phosphate transfection, electroporation, microinjection and liposome transfection. The plasmid may be integrated into the genome, resulting in a stable transfection, or may remain independent of the genome and expressed temporarily, called a transient transfection.DNA coding for a protein of interest is now inside a cell, and the protein can now be expressed. A variety of systems, such as inducible promoters and specific cell-signaling factors, are available to help express the protein of interest at high levels. Large quantities of a protein can then be extracted from the bacterial or eukaryotic cell. The protein can be tested for enzymatic activity under a variety of situations, the protein may be crystallized so its tertiary structure can be studied, or, in the pharmaceutical industry, the activity of new drugs against the protein can be studied.


=== Polymerase chain reaction ===

Polymerase chain reaction (PCR) is an extremely versatile technique for copying DNA. In brief, PCR allows a specific DNA sequence to be copied or modified in predetermined ways. The reaction is extremely powerful and under perfect conditions could amplify one DNA molecule to become 1.07 billion molecules in less than two hours. PCR has many applications, including the study of gene expression, the detection of pathogenic microorganisms, the detection of genetic mutations, and the introduction of mutations to DNA. The PCR technique can be used to introduce restriction enzyme sites to ends of DNA molecules, or to mutate particular bases of DNA, the latter is a method referred to as site-directed mutagenesis. PCR can also be used to determine whether a particular DNA fragment is found in a cDNA library. PCR has many variations, like reverse transcription PCR (RT-PCR) for amplification of RNA, and, more recently, quantitative PCR which allow for quantitative measurement of DNA or RNA molecules.


=== Gel electrophoresis ===

Gel electrophoresis is a technique which separates molecules by their size using an agarose or polyacrylamide gel. This technique is one of the principal tools of molecular biology. The basic principle is that DNA fragments can be separated by applying an electric current across the gel - because the DNA backbone contains negatively charged phosphate groups, the DNA will migrate through the agarose gel towards the positive end of the current. Proteins can also be separated on the basis of size using an SDS-PAGE gel, or on the basis of size and their electric charge by using what is known as a 2D gel electrophoresis.


=== The Bradford Assay ===

The Bradford Assay is a molecular biology technique which enables the fast, accurate quantitation of protein molecules utilizing the unique properties of a dye called Coomassie Brilliant Blue G-250. Coomassie Blue undergoes a visible color shift from reddish-brown to bright blue upon binding to protein. In its unstable, cationic state, Coomassie Blue has a background wavelength of 465 nm and gives off a reddish-brown color. When Coomassie Blue binds to protein in an acidic solution, the background wavelength shifts to 595 nm and the dye gives off a bright blue color. Proteins in the assay bind Coomassie blue in about 2 minutes, and the protein-dye complex is stable for about an hour, although it's recommended that absorbance readings are taken within 5 to 20 minutes of reaction initiation. The concentration of protein in the Bradford assay can then be measured using a visible light spectrophotometer, and therefore does not require extensive equipment.This method was developed in 1975 by Marion M. Bradford, and has enabled significantly faster, more accurate protein quantitation compared to previous methods: the Lowry procedure and the biuret assay. Unlike the previous methods, the Bradford assay is not susceptible to interference by several non-protein molecules, including ethanol, sodium chloride, and magnesium chloride.  However, it is susceptible to influence by strong alkaline buffering agents, such as sodium dodecyl sulfate (SDS).


=== Macromolecule blotting and probing ===
The terms northern, western and eastern blotting are derived from what initially was a molecular biology joke that played on the term Southern blotting, after the technique described by Edwin Southern for the hybridisation of blotted DNA. Patricia Thomas, developer of the RNA blot which then became known as the northern blot, actually didn't use the term.


==== Southern blotting ====

Named after its inventor, biologist Edwin Southern, the Southern blot is a method for probing for the presence of a specific DNA sequence within a DNA sample. DNA samples before or after restriction enzyme (restriction endonuclease) digestion are separated by gel electrophoresis and then transferred to a membrane by blotting via capillary action. The membrane is then exposed to a labeled DNA probe that has a complement base sequence to the sequence on the DNA of interest. Southern blotting is less commonly used in laboratory science due to the capacity of other techniques, such as PCR, to detect specific DNA sequences from DNA samples. These blots are still used for some applications, however, such as measuring transgene copy number in transgenic mice or in the engineering of gene knockout embryonic stem cell lines.


==== Northern blotting ====

The northern blot is used to study the presence of specific RNA molecules as relative comparison among a set of different samples of RNA.  It is essentially a combination of denaturing RNA gel electrophoresis, and a blot.  In this process RNA is separated based on size and is then transferred to a membrane that is then probed with a labeled complement of a sequence of interest. The results may be visualized through a variety of ways depending on the label used; however, most result in the revelation of bands representing the sizes of the RNA detected in sample.  The intensity of these bands is related to the amount of the target RNA in the samples analyzed.  The procedure is commonly used to study when and how much gene expression is occurring by measuring how much of that RNA is present in different samples, assuming that no post-transcriptional regulation occurs and that the levels of mRNA reflect proportional levels of the corresponding protein being produced.  It is one of the most basic tools for determining at what time, and under what conditions, certain genes are expressed in living tissues.


==== Western blotting ====

A western blot is a technique by which specific proteins can be detected from a mixture of proteins. Western blots can be used to determine the size of isolated proteins, as well as to quantify their expression. In western blotting, proteins are first separated by size, in a thin gel sandwiched between two glass plates in a technique known as SDS-PAGE. The proteins in the gel are then transferred to a polyvinylidene fluoride (PVDF), nitrocellulose, nylon, or other support membrane. This membrane can then be probed with solutions of antibodies. Antibodies that specifically bind to the protein of interest can then be visualized by a variety of techniques, including colored products, chemiluminescence, or autoradiography. Often, the antibodies are labeled with enzymes. When a chemiluminescent substrate is exposed to the enzyme it allows detection. Using western blotting techniques allows not only detection but also quantitative analysis. Analogous methods to western blotting can be used to directly stain specific proteins in live cells or tissue sections.


==== Eastern blotting ====

The eastern blotting technique is used to detect post-translational modification of proteins. Proteins blotted on to the PVDF or nitrocellulose membrane are probed for modifications using specific substrates.


=== Microarrays ===

A DNA microarray is a collection of spots attached to a solid support such as a microscope slide where each spot contains one or more single-stranded DNA oligonucleotide fragments. Arrays make it possible to put down large quantities of very small (100 micrometre diameter) spots on a single slide. Each spot has a DNA fragment molecule that is complementary to a single DNA sequence. A variation of this technique allows the gene expression of an organism at a particular stage in development to be qualified (expression profiling). In this technique the RNA in a tissue is isolated and converted to labeled complementary DNA (cDNA). This cDNA is then hybridized to the fragments on the array and visualization of the hybridization can be done. Since multiple arrays can be made with exactly the same position of fragments, they are particularly useful for comparing the gene expression of two different tissues, such as a healthy and cancerous tissue. Also, one can measure what genes are expressed and how that expression changes with time or with other factors.
There are many different ways to fabricate microarrays; the most common are silicon chips, microscope slides with spots of ~100 micrometre diameter, custom arrays, and arrays with larger spots on porous membranes (macroarrays). There can be anywhere from 100 spots to more than 10,000 on a given array. Arrays can also be made with molecules other than DNA.


=== Allele-specific oligonucleotide ===

Allele-specific oligonucleotide (ASO) is a technique that allows detection of single base mutations without the need for PCR or gel electrophoresis. Short (20–25 nucleotides in length), labeled probes are exposed to the non-fragmented target DNA, hybridization occurs with high specificity due to the short length of the probes and even a single base change will hinder hybridization. The target DNA is then washed and the labeled probes that didn't hybridize are removed. The target DNA is then analyzed for the presence of the probe via radioactivity or fluorescence. In this experiment, as in most molecular biology techniques, a control must be used to ensure successful experimentation.In molecular biology, procedures and technologies are continually being developed and older technologies abandoned. For example, before the advent of DNA gel electrophoresis (agarose or polyacrylamide), the size of DNA molecules was typically determined by rate sedimentation in sucrose gradients, a slow and labor-intensive technique requiring expensive instrumentation; prior to sucrose gradients, viscometry was used. Aside from their historical interest, it is often worth knowing about older technology, as it is occasionally useful to solve another new problem for which the newer technique is inappropriate.


== See also ==


== References ==


== Further reading ==


== External links ==
 Media related to Molecular biology at Wikimedia Commons
Biochemistry and Molecular Biology at Curlie",2143110,1453,"Articles with BNE identifiers, Articles with BNF identifiers, Articles with Curlie links, Articles with FAST identifiers, Articles with GND identifiers, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with LNB identifiers, Articles with NDL identifiers, Articles with NKC identifiers, Articles with short description, CS1: long volume value, CS1 maint: url-status, Cell biology, Commons category link from Wikidata, Molecular biology, Pages containing links to subscription-or-libraries content, Short description is different from Wikidata",1124222343,biology
https://en.wikipedia.org/wiki/Family_(biology),Family (biology),"Family (Latin: familia, plural familiae) is one of the eight major hierarchical taxonomic ranks in Linnaean taxonomy. It is classified between order and genus. A family may be divided into subfamilies, which are intermediate ranks between the ranks of family and genus. The official family names are Latin in origin; however, popular names are often used: for example, walnut trees and hickory trees belong to the family Juglandaceae, but that family is commonly referred to as the ""walnut family"".
What belongs to a family—or if a described family should be recognized at all—are proposed and determined by practicing taxonomists. There are no hard rules for describing or recognizing a family, but in plants, they can be characterized on the basis of both vegetative and reproductive features of plant species. Taxonomists often take different positions about descriptions, and there may be no broad consensus across the scientific community for some time. The publishing of new data and opinions often enables adjustments and consensus.

","Family (Latin: familia, plural familiae) is one of the eight major hierarchical taxonomic ranks in Linnaean taxonomy. It is classified between order and genus. A family may be divided into subfamilies, which are intermediate ranks between the ranks of family and genus. The official family names are Latin in origin; however, popular names are often used: for example, walnut trees and hickory trees belong to the family Juglandaceae, but that family is commonly referred to as the ""walnut family"".
What belongs to a family—or if a described family should be recognized at all—are proposed and determined by practicing taxonomists. There are no hard rules for describing or recognizing a family, but in plants, they can be characterized on the basis of both vegetative and reproductive features of plant species. Taxonomists often take different positions about descriptions, and there may be no broad consensus across the scientific community for some time. The publishing of new data and opinions often enables adjustments and consensus.


== Nomenclature ==
The naming of families is codified by various international bodies using the following suffixes:

In fungal, algal, and botanical nomenclature, the family names of plants, fungi, and algae end with the suffix ""-aceae"", except for a small number of historic but widely used names including Compositae and Gramineae.
In zoological nomenclature, the family names of animals end with the suffix ""-idae"".


== History ==
The taxonomic term familia was first used by French botanist Pierre Magnol in his Prodromus historiae generalis plantarum, in quo familiae plantarum per tabulas disponuntur (1689) where he called the seventy-six groups of plants he recognised in his tables families (familiae). The concept of rank at that time was not yet settled, and in the preface to the Prodromus Magnol spoke of uniting his families into larger genera, which is far from how the term is used today.
Carl Linnaeus used the word familia in his Philosophia botanica (1751) to denote major groups of plants: trees, herbs, ferns, palms, and so on. He used this term only in the morphological section of the book, discussing the vegetative and generative organs of plants.
Subsequently, in French botanical publications, from Michel Adanson's Familles naturelles des plantes (1763) and until the end of the 19th century, the word famille was used as a French equivalent of the Latin ordo (or ordo naturalis).
In zoology, the family as a rank intermediate between order and genus was introduced by Pierre André Latreille in his Précis des caractères génériques des insectes, disposés dans un ordre naturel (1796). He used families (some of them were not named) in some but not in all his orders of ""insects"" (which then included all arthropods).
In nineteenth-century works such as the Prodromus of Augustin Pyramus de Candolle and the Genera Plantarum of George Bentham and Joseph Dalton Hooker this word ordo was used for what now is given the rank of family.


== Uses ==
Families can be used for evolutionary, palaeontological and genetic studies because they are more stable than lower taxonomic levels such as genera and species.


== See also ==
Systematics, the study of the diversity of living organisms
Cladistics, the classification of organisms by their order of branching in an evolutionary tree
Phylogenetics, the study of evolutionary relatedness among various groups of organisms
Taxonomy
Virus classification
List of Anuran families
List of Testudines families
List of fish families
List of families of spiders


== References ==


== Bibliography ==",2773053,921,"Articles containing French-language text, Articles containing Latin-language text, Articles with GND identifiers, Articles with short description, CS1: Julian–Gregorian uncertainty, CS1 errors: generic name, Families (biology), Short description is different from Wikidata, Use dmy dates from September 2020",1133677852,biology
https://en.wikipedia.org/wiki/Taxonomy_(biology),Taxonomy (biology),"In biology, taxonomy (from Ancient Greek  τάξις (taxis) 'arrangement', and  -νομία (-nomia) 'method') is the scientific study of naming, defining (circumscribing) and classifying groups of biological organisms based on shared characteristics. Organisms are grouped into taxa (singular: taxon) and these groups are given a taxonomic rank; groups of a given rank can be aggregated to form a more inclusive group of higher rank, thus creating a taxonomic hierarchy. The principal ranks in modern use are domain, kingdom, phylum (division is sometimes used in botany in place of phylum), class, order, family, genus, and species. The Swedish botanist Carl Linnaeus is regarded as the founder of the current system of taxonomy, as he developed a ranked system known as Linnaean taxonomy for categorizing organisms and binomial nomenclature for naming organisms.
With advances in the theory, data and analytical technology of biological systematics, the Linnaean system has transformed into a system of modern biological classification intended to reflect the evolutionary relationships among organisms, both living and extinct.","In biology, taxonomy (from Ancient Greek  τάξις (taxis) 'arrangement', and  -νομία (-nomia) 'method') is the scientific study of naming, defining (circumscribing) and classifying groups of biological organisms based on shared characteristics. Organisms are grouped into taxa (singular: taxon) and these groups are given a taxonomic rank; groups of a given rank can be aggregated to form a more inclusive group of higher rank, thus creating a taxonomic hierarchy. The principal ranks in modern use are domain, kingdom, phylum (division is sometimes used in botany in place of phylum), class, order, family, genus, and species. The Swedish botanist Carl Linnaeus is regarded as the founder of the current system of taxonomy, as he developed a ranked system known as Linnaean taxonomy for categorizing organisms and binomial nomenclature for naming organisms.
With advances in the theory, data and analytical technology of biological systematics, the Linnaean system has transformed into a system of modern biological classification intended to reflect the evolutionary relationships among organisms, both living and extinct.


== Definition ==
The exact definition of taxonomy varies from source to source, but the core of the discipline remains: the conception, naming, and classification of groups of organisms. As points of reference, recent definitions of taxonomy are presented below:

Theory and practice of grouping individuals into species, arranging species into larger groups, and giving those groups names, thus producing a classification.
A field of science (and major component of systematics) that encompasses description, identification, nomenclature, and classification
The science of classification, in biology the arrangement of organisms into a classification
""The science of classification as applied to living organisms, including the study of means of formation of species, etc.""
""The analysis of an organism's characteristics for the purpose of classification""
""Systematics studies phylogeny to provide a pattern that can be translated into the classification and names of the more inclusive field of taxonomy"" (listed as a desirable but unusual definition)The varied definitions either place taxonomy as a sub-area of systematics (definition 2), invert that relationship (definition 6), or appear to consider the two terms synonymous. There is some disagreement as to whether biological nomenclature is considered a part of taxonomy (definitions 1 and 2), or a part of systematics outside taxonomy. For example, definition 6 is paired with the following definition of systematics that places nomenclature outside taxonomy:
Systematics: ""The study of the identification, taxonomy, and nomenclature of organisms, including the classification of living things with regard to their natural relationships and the study of variation and the evolution of taxa"".In 1970 Michener et al. defined ""systematic biology"" and ""taxonomy"" (terms that are often confused and used interchangeably) in relationship to one another as follows:

Systematic biology (hereafter called simply systematics) is the field that (a) provides scientific names for organisms, (b) describes them, (c) preserves collections of them, (d) provides classifications for the organisms, keys for their identification, and data on their distributions, (e) investigates their evolutionary histories, and (f) considers their environmental adaptations. This is a field with a long history that in recent years has experienced a notable renaissance, principally with respect to theoretical content. Part of the theoretical material has to do with evolutionary areas (topics e and f above), the rest relates especially to the problem of classification. Taxonomy is that part of Systematics concerned with topics (a) to (d) above.

A whole set of terms including taxonomy, systematic biology, systematics, biosystematics, scientific classification, biological classification, and phylogenetics have at times had overlapping meanings – sometimes the same, sometimes slightly different, but always related and intersecting. The broadest meaning of ""taxonomy"" is used here. The term itself was introduced in 1813 by de Candolle, in his Théorie élémentaire de la botanique. John Lindley provided an early definition of systematics in 1830, although he wrote of ""systematic botany"" rather than using the term ""systematics"". Europeans tend to use the terms ""systematics"" and ""biosystematics"" for the study of biodiversity as a whole, whereas North Americans tend to use ""taxonomy"" more frequently. However, taxonomy, and in particular alpha taxonomy, is more specifically the identification, description, and naming (i.e. nomenclature) of organisms,
while ""classification"" focuses on placing organisms within hierarchical groups that show their relationships to other organisms.


=== Monograph and taxonomic revision ===
A taxonomic revision or taxonomic review is a novel analysis of the variation patterns in a particular taxon. This analysis may be executed on the basis of any combination of the various available kinds of characters, such as morphological, anatomical, palynological, biochemical and genetic. A monograph or complete revision is a revision that is comprehensive for a taxon for the information given at a particular time, and for the entire world. Other (partial) revisions may be restricted in the sense that they may only use some of the available character sets or have a limited spatial scope. A revision results in a conformation of or new insights in the relationships between the subtaxa within the taxon under study, which may lead to a change in the classification of these subtaxa, the identification of new subtaxa, or the merger of previous subtaxa.


=== Taxonomic characters ===
Taxonomic characters are the taxonomic attributes that can be used to provide the evidence from which relationships (the phylogeny) between taxa  are inferred. Kinds of taxonomic characters include:


=== Alpha and beta taxonomy ===

The term ""alpha taxonomy"" is primarily used today to refer to the discipline of finding, describing, and naming taxa, particularly species. In earlier literature, the term had a different meaning, referring to morphological taxonomy, and the products of research through the end of the 19th century.
William Bertram Turrill introduced the term ""alpha taxonomy"" in a series of papers published in 1935 and 1937 in which he discussed the philosophy and possible future directions of the discipline of taxonomy. ... there is an increasing desire amongst taxonomists to consider their problems from wider viewpoints, to investigate the possibilities of closer co-operation with their cytological, ecological and genetics colleagues and to acknowledge that some revision or expansion, perhaps of a drastic nature, of their aims and methods, may be desirable ... Turrill (1935) has suggested that while accepting the older invaluable taxonomy, based on structure, and conveniently designated ""alpha"", it is possible to glimpse a far-distant taxonomy built upon as wide a basis of morphological and physiological facts as possible, and one in which ""place is found for all observational and experimental data relating, even if indirectly, to the constitution, subdivision, origin, and behaviour of species and other taxonomic groups"". Ideals can, it may be said, never be completely realized. They have, however, a great value of acting as permanent stimulants, and if we have some, even vague, ideal of an ""omega"" taxonomy we may progress a little way down the Greek alphabet. Some of us please ourselves by thinking we are now groping in a ""beta"" taxonomy.
Turrill thus explicitly excludes from alpha taxonomy various areas of study that he includes within taxonomy as a whole, such as ecology, physiology, genetics, and cytology. He further excludes phylogenetic reconstruction from alpha taxonomy.
Later authors have used the term in a different sense, to mean the delimitation of species (not subspecies or taxa of other ranks), using whatever investigative techniques are available, and including sophisticated computational or laboratory techniques. Thus, Ernst Mayr in 1968 defined ""beta taxonomy"" as the classification of ranks higher than species.An understanding of the biological meaning of variation and of the evolutionary origin of groups of related species is even more important for the second stage of taxonomic activity, the sorting of species into groups of relatives (""taxa"") and their arrangement in a hierarchy of higher categories. This activity is what the term classification denotes; it is also referred to as ""beta taxonomy"".


=== Microtaxonomy and macrotaxonomy ===

How species should be defined in a particular group of organisms gives rise to practical and theoretical problems that are referred to as the species problem. The scientific work of deciding how to define species has been called microtaxonomy. By extension, macrotaxonomy is the study of groups at the higher taxonomic ranks subgenus and above.


== History ==
While some descriptions of taxonomic history attempt to date taxonomy to ancient civilizations, a truly scientific attempt to classify organisms did not occur until the 18th century. Earlier works were primarily descriptive and focused on plants that were useful in agriculture or medicine. There are a number of stages in this scientific thinking. Early taxonomy was based on arbitrary criteria, the so-called ""artificial systems"", including Linnaeus's system of sexual classification for plants (Linnaeus's 1735 classification of animals was entitled ""Systema Naturae"" (""the System of Nature""), implying that he, at least, believed that it was more than an ""artificial system""). Later came systems based on a more complete consideration of the characteristics of taxa, referred to as ""natural systems"", such as those of de Jussieu (1789), de Candolle (1813) and Bentham and Hooker (1862–1863). These classifications described empirical patterns and were pre-evolutionary in thinking. The publication of Charles Darwin's On the Origin of Species (1859) led to a new explanation for classifications, based on evolutionary relationships. This was the concept of phyletic systems, from 1883 onwards. This approach was typified by those of Eichler (1883) and Engler (1886–1892). The advent of cladistic methodology in the 1970s led to classifications based on the sole criterion of monophyly, supported by the presence of synapomorphies. Since then, the evidentiary basis has been expanded with data from molecular genetics that for the most part complements traditional morphology.


=== Pre-Linnaean ===


==== Early taxonomists ====
Naming and classifying human surroundings likely begun with the onset of language. Distinguishing poisonous plants from edible plants is integral to the survival of human communities. Medicinal plant illustrations show up in Egyptian wall paintings from c. 1500 BC, indicating that the uses of different species were understood and that a basic taxonomy was in place.


==== Ancient times ====

Organisms were first classified by Aristotle (Greece, 384–322 BC) during his stay on the Island of Lesbos. He classified beings by their parts, or in modern terms attributes, such as having live birth, having four legs, laying eggs, having blood, or being warm-bodied. He divided all living things into two groups: plants and animals. Some of his groups of animals, such as Anhaima (animals without blood, translated as invertebrates) and Enhaima (animals with blood, roughly the vertebrates), as well as groups like the sharks and cetaceans, are still commonly used today. His student Theophrastus (Greece, 370–285 BC) carried on this tradition, mentioning some 500 plants and their uses in his Historia Plantarum. Again, several plant groups currently still recognized can be traced back to Theophrastus, such as Cornus, Crocus, and Narcissus.


==== Medieval ====
Taxonomy in the Middle Ages was largely based on the Aristotelian system, with additions concerning the philosophical and existential order of creatures. This included concepts such as the great chain of being in the Western scholastic tradition, again deriving ultimately from Aristotle. The Aristotelian system did not classify plants or fungi, due to the lack of microscopes at the time, as his ideas were based on arranging the complete world in a single continuum, as per the scala naturae (the Natural Ladder). This, as well, was taken into consideration in the great chain of being. Advances were made by scholars such as Procopius, Timotheos of Gaza, Demetrios Pepagomenos, and Thomas Aquinas. Medieval thinkers used abstract philosophical and logical categorizations more suited to abstract philosophy than to pragmatic taxonomy.


==== Renaissance and early modern ====
During the Renaissance and the Age of Enlightenment, categorizing organisms became more prevalent,
and taxonomic works became ambitious enough to replace the ancient texts. This is sometimes credited to the development of sophisticated optical lenses, which allowed the morphology of organisms to be studied in much greater detail. One of the earliest authors to take advantage of this leap in technology was the Italian physician Andrea Cesalpino (1519–1603), who has been called ""the first taxonomist"". His magnum opus De Plantis came out in 1583, and described more than 1500 plant species. Two large plant families that he first recognized are still in use today: the Asteraceae and Brassicaceae. Then in the 17th century John Ray (England, 1627–1705) wrote many important taxonomic works. Arguably his greatest accomplishment was Methodus Plantarum Nova (1682), in which he published details of over 18,000 plant species. At the time, his classifications were perhaps the most complex yet produced by any taxonomist, as he based his taxa on many combined characters. The next major taxonomic works were produced by Joseph Pitton de Tournefort (France, 1656–1708). His work from 1700, Institutiones Rei Herbariae, included more than 9000 species in 698 genera, which directly influenced Linnaeus, as it was the text he used as a young student.


=== Linnaean era ===

The Swedish botanist Carl Linnaeus (1707–1778) ushered in a new era of taxonomy. With his major works Systema Naturae 1st Edition in 1735, Species Plantarum in 1753, and Systema Naturae 10th Edition, he revolutionized modern taxonomy. His works implemented a standardized binomial naming system for animal and plant species, which proved to be an elegant solution to a chaotic and disorganized taxonomic literature. He not only introduced the standard of class, order, genus, and species, but also made it possible to identify plants and animals from his book, by using the smaller parts of the flower. Thus the Linnaean system was born, and is still used in essentially the same way today as it was in the 18th century. Currently, plant and animal taxonomists regard Linnaeus' work as the ""starting point"" for valid names (at 1753 and 1758 respectively). Names published before these dates are referred to as ""pre-Linnaean"", and not considered valid (with the exception of spiders published in Svenska Spindlar). Even taxonomic names published by Linnaeus himself before these dates are considered pre-Linnaean.


== Modern system of classification ==

A pattern of groups nested within groups was specified by Linnaeus' classifications of plants and animals, and these patterns began to be represented as dendrograms of the animal and plant kingdoms toward the end of the 18th century, well before Charles Darwin's On the Origin of Species was published. The pattern of the ""Natural System"" did not entail a generating process, such as evolution, but may have implied it, inspiring early transmutationist thinkers. Among early works exploring the idea of a transmutation of species were Erasmus Darwin's (Charles Darwin's grandfather's) 1796 Zoönomia and Jean-Baptiste Lamarck's Philosophie Zoologique of 1809. The idea was popularized in the Anglophone world by the speculative but widely read Vestiges of the Natural History of Creation, published anonymously by Robert Chambers in 1844.With Darwin's theory, a general acceptance quickly appeared that a classification should reflect the Darwinian principle of common descent. Tree of life representations became popular in scientific works, with known fossil groups incorporated. One of the first modern groups tied to fossil ancestors was birds. Using the then newly discovered fossils of Archaeopteryx and Hesperornis, Thomas Henry Huxley pronounced that they had evolved from dinosaurs, a group formally named by Richard Owen in 1842. The resulting description, that of dinosaurs ""giving rise to"" or being ""the ancestors of"" birds, is the essential hallmark of evolutionary taxonomic thinking. As more and more fossil groups were found and recognized in the late 19th and early 20th centuries, palaeontologists worked to understand the history of animals through the ages by linking together known groups. With the modern evolutionary synthesis of the early 1940s, an essentially modern understanding of the evolution of the major groups was in place. As evolutionary taxonomy is based on Linnaean taxonomic ranks, the two terms are largely interchangeable in modern use.The cladistic method has emerged since the 1960s. In 1958, Julian Huxley used the term clade. Later, in 1960, Cain and Harrison introduced the term cladistic. The salient feature is arranging taxa in a hierarchical evolutionary tree, with the desideratum that all named taxa are monophyletic. A taxon is called monophyletic if it includes all the descendants of an ancestral form. Groups that have descendant groups removed from them are termed paraphyletic, while groups representing more than one branch from the tree of life are called polyphyletic. Monophyletic groups are recognized and diagnosed on the basis of synapomorphies, shared derived character states.Cladistic classifications are compatible with traditional Linnean taxonomy and the Codes of Zoological and Botanical nomenclature. An alternative system of nomenclature, the International Code of Phylogenetic Nomenclature or PhyloCode has been proposed, whose intent is to regulate the formal naming of clades. Linnaean ranks will be optional under the PhyloCode, which is intended to coexist with the current, rank-based codes. It remains to be seen whether the systematic community will adopt the PhyloCode or reject it in favor of the current systems of nomenclature that have been employed (and modified as needed) for over 250 years.


=== Kingdoms and domains ===

Well before discovery of Carl Linnaeus (Botanist) plants and animals were considered separate Kingdoms. Linnaeus used this as the top rank, dividing the physical world into the vegetable, animal and mineral kingdoms. As advances in microscopy made the classification of microorganisms possible, the number of kingdoms increased, five- and six-kingdom systems being the most common.
Domains are a relatively new grouping. First proposed in 1977, Carl Woese's three-domain system was not generally accepted until later. One main characteristic of the three-domain method is the separation of Archaea and Bacteria, previously grouped into the single kingdom Bacteria (a kingdom also sometimes called Monera), with the Eukaryota for all organisms whose cells contain a nucleus. A small number of scientists include a sixth kingdom, Archaea, but do not accept the domain method.Thomas Cavalier-Smith, who published extensively on the classification of protists, in 2002 proposed that the Neomura, the clade that groups together the Archaea and Eucarya, would have evolved from Bacteria, more precisely from Actinomycetota. His 2004 classification treated the archaeobacteria as part of a subkingdom of the kingdom Bacteria, i.e., he rejected the three-domain system entirely. Stefan Luketa in 2012 proposed a five ""dominion"" system, adding Prionobiota (acellular and without nucleic acid) and Virusobiota (acellular but with nucleic acid) to the traditional three domains.


=== Recent comprehensive classifications ===
Partial classifications exist for many individual groups of organisms and are revised and replaced as new information becomes available; however, comprehensive, published treatments of most or all life are rarer; recent examples are that of Adl et al., 2012 and 2019, which covers eukaryotes only with an emphasis on protists, and Ruggiero et al., 2015, covering both eukaryotes and prokaryotes to the rank of Order, although both exclude fossil representatives. A separate compilation (Ruggiero, 2014) covers extant taxa to the rank of Family. Other, database-driven treatments include the Encyclopedia of Life, the Global Biodiversity Information Facility, the NCBI taxonomy database, the Interim Register of Marine and Nonmarine Genera, the Open Tree of Life, and the Catalogue of Life. The Paleobiology Database is a resource for fossils.


== Application ==
Biological taxonomy is a sub-discipline of biology, and is generally practiced by biologists known as ""taxonomists"", though enthusiastic naturalists are also frequently involved in the publication of new taxa. Because taxonomy aims to describe and organize life, the work conducted by taxonomists is essential for the study of biodiversity and the resulting field of conservation biology.


=== Classifying organisms ===

Biological classification is a critical component of the taxonomic process. As a result, it informs the user as to what the relatives of the taxon are hypothesized to be. Biological classification uses taxonomic ranks, including among others (in order from most inclusive to least inclusive): Domain, Kingdom, Phylum, Class, Order, Family, Genus, Species, and Strain.


=== Taxonomic descriptions ===

The ""definition"" of a taxon is encapsulated by its description or its diagnosis or by both combined. There are no set rules governing the definition of taxa, but the naming and publication of new taxa is governed by sets of rules. In zoology, the nomenclature for the more commonly used ranks (superfamily to subspecies), is regulated by the International Code of Zoological Nomenclature (ICZN Code). In the fields of phycology, mycology, and botany, the naming of taxa is governed by the International Code of Nomenclature for algae, fungi, and plants (ICN).The initial description of a taxon involves five main requirements:
The taxon must be given a name based on the 26 letters of the Latin alphabet (a binomial for new species, or uninomial for other ranks).
The name must be unique (i.e. not a homonym).
The description must be based on at least one name-bearing type specimen.
It should include statements about appropriate attributes either to describe (define) the taxon or to differentiate it from other taxa (the diagnosis, ICZN Code, Article 13.1.1, ICN, Article 38, which may or may not be based on morphology). Both codes deliberately separate defining the content of a taxon (its circumscription) from defining its name.
These first four requirements must be published in a work that is obtainable in numerous identical copies, as a permanent scientific record.However, often much more information is included, like the geographic range of the taxon, ecological notes, chemistry, behavior, etc. How researchers arrive at their taxa varies: depending on the available data, and resources, methods vary from simple quantitative or qualitative comparisons of striking features, to elaborate computer analyses of large amounts of DNA sequence data.


=== Author citation ===

An ""authority"" may be placed after a scientific name. The authority is the name of the scientist or scientists who first validly published the name. For example, in 1758 Linnaeus gave the Asian elephant the scientific name Elephas maximus, so the name is sometimes written as ""Elephas maximus Linnaeus, 1758"". The names of authors are frequently abbreviated: the abbreviation L., for Linnaeus, is commonly used. In botany, there is, in fact, a regulated list of standard abbreviations (see list of botanists by author abbreviation). The system for assigning authorities differs slightly between botany and zoology. However, it is standard that if the genus of a species has been changed since the original description, the original authority's name is placed in parentheses.


== Phenetics ==

In phenetics, also known as taximetrics, or numerical taxonomy, organisms are classified based on overall similarity, regardless of their phylogeny or evolutionary relationships. It results in a measure of hypergeometric ""distance"" between taxa. Phenetic methods have become relatively rare in modern times, largely superseded by cladistic analyses, as phenetic methods do not distinguish shared ancestral (or plesiomorphic) traits from shared derived (or apomorphic) traits. However, certain phenetic methods, such as neighbor joining, have persisted, as rapid estimators of relationship when more advanced methods (such as Bayesian inference) are too computationally expensive.


== Databases ==

Modern taxonomy uses database technologies to search and catalogue classifications and their documentation. While there is no commonly used database, there are comprehensive databases such as the Catalogue of Life, which attempts to list every documented species. The catalogue listed 1.64 million species for all kingdoms as of April 2016, claiming coverage of more than three quarters of the estimated species known to modern science.


== See also ==


== Notes ==


== References ==


== Bibliography ==


== External links ==
What is taxonomy? at the Natural History Museum London
Taxonomy at NCBI the National Center for Biotechnology Information
Taxonomy at UniProt the Universal Protein Resource
ITIS the Integrated Taxonomic Information System
CETaF the Consortium of European Taxonomic Facilities
Wikispecies free species directory
Biological classification. Archived 13 August 2020 at the Wayback Machine",9881050,3868,"All articles lacking reliable references, All articles with unsourced statements, Articles lacking reliable references from April 2017, Articles with short description, Articles with unsourced statements from May 2018, Biological classification, Biological nomenclature, CS1: long volume value, CS1 Latin-language sources (la), CS1 Swedish-language sources (sv), Good articles, Pages using div col with small parameter, Short description matches Wikidata, Taxonomy (biology), Use dmy dates from March 2022, Webarchive template wayback links, Wikipedia articles needing page number citations from July 2019, Wikipedia indefinitely semi-protected pages",1133363688,biology
https://en.wikipedia.org/wiki/Cell_(biology),Cell (biology),"The cell is the basic structural and functional unit of life forms. Every cell consists of a cytoplasm enclosed within a membrane, and contains many biomolecules such as proteins, DNA and RNA, as well as many small molecules of nutrients and metabolites.  The term comes from the Latin word cellula meaning 'small room'.Cells can acquire specified function and carry out various tasks within the cell such as replication, DNA repair, protein synthesis, and motility. Cells are capable of specialization and mobility within the cell. Most cells are measured in micrometers due to their small size.
Most plant and animal cells are only visible under a light microscope, with dimensions between 1 and 100 micrometres. Electron microscopy gives a much higher resolution showing greatly detailed cell structure. Organisms can be classified as unicellular (consisting of a single cell such as bacteria) or multicellular (including plants and animals). Most unicellular organisms are classed as microorganisms. The number of cells in plants and animals varies from species to species; it has been approximated that the human body contains an estimated 37 trillion (3.72×1013) cells. The brain accounts for around 80 billion of these cells.The study of cells and how they work has led to many other studies in related areas of biology, including: discovery of DNA, cancer systems biology, aging and developmental biology.
Cell biology is the study of cells, which were discovered by Robert Hooke in 1665, who named them for their resemblance to cells inhabited by Christian monks in a monastery. Cell theory, first developed in 1839 by Matthias Jakob Schleiden and Theodor Schwann, states that all organisms are composed of one or more cells, that cells are the fundamental unit of structure and function in all living organisms, and that all cells come from pre-existing cells. Cells emerged on Earth about 4 billion years ago.

","The cell is the basic structural and functional unit of life forms. Every cell consists of a cytoplasm enclosed within a membrane, and contains many biomolecules such as proteins, DNA and RNA, as well as many small molecules of nutrients and metabolites.  The term comes from the Latin word cellula meaning 'small room'.Cells can acquire specified function and carry out various tasks within the cell such as replication, DNA repair, protein synthesis, and motility. Cells are capable of specialization and mobility within the cell. Most cells are measured in micrometers due to their small size.
Most plant and animal cells are only visible under a light microscope, with dimensions between 1 and 100 micrometres. Electron microscopy gives a much higher resolution showing greatly detailed cell structure. Organisms can be classified as unicellular (consisting of a single cell such as bacteria) or multicellular (including plants and animals). Most unicellular organisms are classed as microorganisms. The number of cells in plants and animals varies from species to species; it has been approximated that the human body contains an estimated 37 trillion (3.72×1013) cells. The brain accounts for around 80 billion of these cells.The study of cells and how they work has led to many other studies in related areas of biology, including: discovery of DNA, cancer systems biology, aging and developmental biology.
Cell biology is the study of cells, which were discovered by Robert Hooke in 1665, who named them for their resemblance to cells inhabited by Christian monks in a monastery. Cell theory, first developed in 1839 by Matthias Jakob Schleiden and Theodor Schwann, states that all organisms are composed of one or more cells, that cells are the fundamental unit of structure and function in all living organisms, and that all cells come from pre-existing cells. Cells emerged on Earth about 4 billion years ago.


== Cell types ==
Cells are of two types: eukaryotic, which contain a nucleus, and prokaryotic cells, which do not have a nucleus, but a nucleoid region is still present. Prokaryotes are single-celled organisms, while eukaryotes may be either single-celled or multicellular.


=== Prokaryotic cells ===

Prokaryotes include bacteria and archaea, two of the three domains of life. Prokaryotic cells were the first form of life on Earth, characterized by having vital biological processes including cell signaling. They are simpler and smaller than eukaryotic cells, and lack a nucleus, and other membrane-bound organelles. The DNA of a prokaryotic cell consists of a single circular chromosome that is in direct contact with the cytoplasm. The nuclear region in the cytoplasm is called the nucleoid. Most prokaryotes are the smallest of all organisms ranging from 0.5 to 2.0 μm in diameter.A prokaryotic cell has three regions:

Enclosing the cell is the cell envelope – generally consisting of a plasma membrane covered by a cell wall which, for some bacteria, may be further covered by a third layer called a capsule. Though most prokaryotes have both a cell membrane and a cell wall, there are exceptions such as Mycoplasma (bacteria) and Thermoplasma (archaea) which only possess the cell membrane layer. The envelope gives rigidity to the cell and separates the interior of the cell from its environment, serving as a protective filter. The cell wall consists of peptidoglycan in bacteria and acts as an additional barrier against exterior forces. It also prevents the cell from expanding and bursting (cytolysis) from osmotic pressure due to a hypotonic environment. Some eukaryotic cells (plant cells and fungal cells) also have a cell wall.
Inside the cell is the cytoplasmic region that contains the genome (DNA), ribosomes and various sorts of inclusions. The genetic material is freely found in the cytoplasm. Prokaryotes can carry extrachromosomal DNA elements called plasmids, which are usually circular. Linear bacterial plasmids have been identified in several species of spirochete bacteria, including members of the genus Borrelia notably Borrelia burgdorferi, which causes Lyme disease. Though not forming a nucleus, the DNA is condensed in a nucleoid. Plasmids encode additional genes, such as antibiotic resistance genes.
On the outside, flagella and pili project from the cell's surface. These are structures (not present in all prokaryotes) made of proteins that facilitate movement and communication between cells.


=== Eukaryotic cells ===

Plants, animals, fungi, slime moulds, protozoa, and algae are all eukaryotic. These cells are about fifteen times wider than a typical prokaryote and can be as much as a thousand times greater in volume. The main distinguishing feature of eukaryotes as compared to prokaryotes is compartmentalization: the presence of membrane-bound organelles (compartments) in which specific activities take place. Most important among these is a cell nucleus, an organelle that houses the cell's DNA. This nucleus gives the eukaryote its name, which means ""true kernel (nucleus)"". Some of the other differences are:

The plasma membrane resembles that of prokaryotes in function, with minor differences in the setup. Cell walls may or may not be present.
The eukaryotic DNA is organized in one or more linear molecules, called chromosomes, which are associated with histone proteins. All chromosomal DNA is stored in the cell nucleus, separated from the cytoplasm by a membrane. Some eukaryotic organelles such as mitochondria also contain some DNA.
Many eukaryotic cells are ciliated with primary cilia. Primary cilia play important roles in chemosensation, mechanosensation, and thermosensation. Each cilium may thus be ""viewed as a sensory cellular antennae that coordinates a large number of cellular signaling pathways, sometimes coupling the signaling to ciliary motility or alternatively to cell division and differentiation.""
Motile eukaryotes can move using motile cilia or flagella. Motile cells are absent in conifers and flowering plants. Eukaryotic flagella are more complex than those of prokaryotes.


== Cell shapes ==
Cell shape, also called cell morphology, has been hypothesized to form from the arrangement and movement of the cytoskeleton. Many advancements in the study of cell morphology come from studying simple bacteria such as Staphylococcus aureus,  E. coli,  and B. subtilis. Different cell shapes have been found and described, but how and why cells form different shapes is still widely unknown. Some cell shapes that have been identified include rods, cocci and spirochaetes. Cocci have a circular shape, bacilli have an elongated rod-like shape, and spirochaetes have a spiral shape. Many other shapes have also been determined. 


== Subcellular components ==
All cells, whether prokaryotic or eukaryotic, have a membrane that envelops the cell, regulates what moves in and out (selectively permeable), and maintains the electric potential of the cell. Inside the membrane, the cytoplasm takes up most of the cell's volume. Except red blood cells, which lack a cell nucleus and most organelles to accommodate maximum space for hemoglobin, all cells possess DNA, the hereditary material of genes, and RNA, containing the information necessary to build various proteins such as enzymes, the cell's primary machinery. There are also other kinds of biomolecules in cells. This article lists these primary cellular components, then briefly describes their function.


=== Cell membrane ===

The cell membrane, or plasma membrane, is a selectively permeable biological membrane that surrounds the cytoplasm of a cell. In animals, the plasma membrane is the outer boundary of the cell, while in plants and prokaryotes it is usually covered by a cell wall. This membrane serves to separate and protect a cell from its surrounding environment and is made mostly from a double layer of phospholipids, which are amphiphilic (partly hydrophobic and partly hydrophilic). Hence, the layer is called a phospholipid bilayer, or sometimes a fluid mosaic membrane. Embedded within this membrane is a macromolecular structure called the porosome the universal secretory portal in cells and a variety of protein molecules that act as channels and pumps that move different molecules into and out of the cell. The membrane is semi-permeable, and selectively permeable, in that it can either let a substance (molecule or ion) pass through freely, pass through to a limited extent or not pass through at all. Cell surface membranes also contain receptor proteins that allow cells to detect external signaling molecules such as hormones.


=== Cytoskeleton ===

The cytoskeleton acts to organize and maintain the cell's shape; anchors organelles in place; helps during endocytosis, the uptake of external materials by a cell, and cytokinesis, the separation of daughter cells after cell division; and moves parts of the cell in processes of growth and mobility. The eukaryotic cytoskeleton is composed of microtubules, intermediate filaments and microfilaments. In the cytoskeleton of a neuron the intermediate filaments are known as neurofilaments. There are a great number of proteins associated with them, each controlling a cell's structure by directing, bundling, and aligning filaments. The prokaryotic cytoskeleton is less well-studied but is involved in the maintenance of cell shape, polarity and cytokinesis. The subunit protein of microfilaments is a small, monomeric protein called actin. The subunit of microtubules is a dimeric molecule called tubulin. Intermediate filaments are heteropolymers whose subunits vary among the cell types in different tissues. Some of the subunit proteins of intermediate filaments include vimentin, desmin, lamin (lamins A, B and C), keratin (multiple acidic and basic keratins), and neurofilament proteins (NF–L, NF–M).


=== Genetic material ===

Two different kinds of genetic material exist: deoxyribonucleic acid (DNA) and ribonucleic acid (RNA). Cells use DNA for their long-term information storage. The biological information contained in an organism is encoded in its DNA sequence. RNA is used for information transport (e.g., mRNA) and enzymatic functions (e.g., ribosomal RNA). Transfer RNA (tRNA) molecules are used to add amino acids during protein translation.
Prokaryotic genetic material is organized in a simple circular bacterial chromosome in the nucleoid region of the cytoplasm. Eukaryotic genetic material is divided into different, linear molecules called chromosomes inside a discrete nucleus, usually with additional genetic material in some organelles like mitochondria and chloroplasts (see endosymbiotic theory).
A human cell has genetic material contained in the cell nucleus (the nuclear genome) and in the mitochondria (the mitochondrial genome). In humans, the nuclear genome is divided into 46 linear DNA molecules called chromosomes, including 22 homologous chromosome pairs and a pair of sex chromosomes. The mitochondrial genome is a circular DNA molecule distinct from nuclear DNA. Although the mitochondrial DNA is very small compared to nuclear chromosomes, it codes for 13 proteins involved in mitochondrial energy production and specific tRNAs.
Foreign genetic material (most commonly DNA) can also be artificially introduced into the cell by a process called transfection. This can be transient, if the DNA is not inserted into the cell's genome, or stable, if it is. Certain viruses also insert their genetic material into the genome.


=== Organelles ===

Organelles are parts of the cell that are adapted and/or specialized for carrying out one or more vital functions, analogous to the organs of the human body (such as the heart, lung, and kidney, with each organ performing a different function). Both eukaryotic and prokaryotic cells have organelles, but prokaryotic organelles are generally simpler and are not membrane-bound.
There are several types of organelles in a cell. Some (such as the nucleus and Golgi apparatus) are typically solitary, while others (such as mitochondria, chloroplasts, peroxisomes and lysosomes) can be numerous (hundreds to thousands). The cytosol is the gelatinous fluid that fills the cell and surrounds the organelles.


==== Eukaryotic ====

Cell nucleus: A cell's information center, the cell nucleus is the most conspicuous organelle found in a eukaryotic cell. It houses the cell's chromosomes, and is the place where almost all DNA replication and RNA synthesis (transcription) occur. The nucleus is spherical and separated from the cytoplasm by a double membrane called the nuclear envelope, space between these two membrane is called perinuclear space. The nuclear envelope isolates and protects a cell's DNA from various molecules that could accidentally damage its structure or interfere with its processing. During processing, DNA is transcribed, or copied into a special RNA, called messenger RNA (mRNA). This mRNA is then transported out of the nucleus, where it is translated into a specific protein molecule. The nucleolus is a specialized region within the nucleus where ribosome subunits are assembled. In prokaryotes, DNA processing takes place in the cytoplasm.
Mitochondria and chloroplasts: generate energy for the cell. Mitochondria are self-replicating double membrane-bound organelles that occur in various numbers, shapes, and sizes in the cytoplasm of all eukaryotic cells. Respiration occurs in the cell mitochondria, which generate the cell's energy by oxidative phosphorylation, using oxygen to release energy stored in cellular nutrients (typically pertaining to glucose) to generate ATP(aerobic respiration). Mitochondria multiply by binary fission, like prokaryotes. Chloroplasts can only be found in plants and algae, and they capture the sun's energy to make carbohydrates through photosynthesis.
Endoplasmic reticulum: The endoplasmic reticulum (ER) is a transport network for molecules targeted for certain modifications and specific destinations, as compared to molecules that float freely in the cytoplasm. The ER has two forms: the rough ER, which has ribosomes on its surface that secrete proteins into the ER, and the smooth ER, which lacks ribosomes. The smooth ER plays a role in calcium sequestration and release and also helps in synthesis of lipid.
Golgi apparatus: The primary function of the Golgi apparatus is to process and package the macromolecules such as proteins and lipids that are synthesized by the cell.
Lysosomes and peroxisomes: Lysosomes contain digestive enzymes (acid hydrolases). They digest excess or worn-out organelles, food particles, and engulfed viruses or bacteria. Peroxisomes have enzymes that rid the cell of toxic peroxides, Lysosomes are optimally active at acidic pH. The cell could not house these destructive enzymes if they were not contained in a membrane-bound system.
Centrosome: the cytoskeleton organizer: The centrosome produces the microtubules of a cell – a key component of the cytoskeleton. It directs the transport through the ER and the Golgi apparatus. Centrosomes are composed of two centrioles which lie perpendicular to each other in which each has an organization like a cartwheel, which separate during cell division and help in the formation of the mitotic spindle. A single centrosome is present in the animal cells. They are also found in some fungi and algae cells.
Vacuoles: Vacuoles sequester waste products and in plant cells store water. They are often described as liquid filled spaces and are surrounded by a membrane. Some cells, most notably Amoeba, have contractile vacuoles, which can pump water out of the cell if there is too much water. The vacuoles of plant cells and fungal cells are usually larger than those of animal cells. Vacuoles of plant cells are surrounded by tonoplast which helps in transport of ions and other substances against concentration gradients.


==== Eukaryotic and prokaryotic ====
Ribosomes: The ribosome is a large complex of RNA and protein molecules. They each consist of two subunits, and act as an assembly line where RNA from the nucleus is used to synthesise proteins from amino acids. Ribosomes can be found either floating freely or bound to a membrane (the rough endoplasmatic reticulum in eukaryotes, or the cell membrane in prokaryotes).Plastids: Plastid are  membrane-bound organelle generally found in plant cells and euglenoids and contain specific pigments, thus affecting the colour of the plant and organism. And these pigments also helps in food storage and tapping of light energy. There are three types of plastids based upon the specific pigments. Chloroplasts(contains chlorophyll and some carotenoid pigments which helps in the tapping of light energy during photosynthesis), Chromoplasts(contains fat-soluble carotenoid pigments like orange carotene and yellow xanthophylls which helps in synthesis and storage), Leucoplasts(are non-pigmented plastids and helps in storage of nutrients).


== Structures outside the cell membrane ==
Many cells also have structures which exist wholly or partially outside the cell membrane. These structures are notable because they are not protected from the external environment by the semipermeable cell membrane. In order to assemble these structures, their components must be carried across the cell membrane by export processes.


=== Cell wall ===

Many types of prokaryotic and eukaryotic cells have a cell wall. The cell wall acts to protect the cell mechanically and chemically from its environment, and is an additional layer of protection to the cell membrane. Different types of cell have cell walls made up of different materials; plant cell walls are primarily made up of cellulose, fungi cell walls are made up of chitin and bacteria cell walls are made up of peptidoglycan.


=== Prokaryotic ===


==== Capsule ====
A gelatinous capsule is present in some bacteria outside the cell membrane and cell wall. The capsule may be polysaccharide as in pneumococci, meningococci or polypeptide as Bacillus anthracis or hyaluronic acid as in streptococci.
Capsules are not marked by normal staining protocols and can be detected by India ink or methyl blue; which allows for higher contrast between the cells for observation.: 87 


==== Flagella ====
Flagella are organelles for cellular mobility. The bacterial flagellum stretches from cytoplasm through the cell membrane(s) and extrudes through the cell wall. They are long and thick thread-like appendages, protein in nature. A different type of flagellum is found in archaea and a different type is found in eukaryotes.


==== Fimbriae ====
A fimbria (plural fimbriae also known as a pilus, plural pili) is a short, thin, hair-like filament found on the surface of bacteria. Fimbriae are formed of a protein called pilin (antigenic) and are responsible for the attachment of bacteria to specific receptors on human cells (cell adhesion). There are special types of pili involved in bacterial conjugation.


== Cellular processes ==


=== Replication ===

Cell division involves a single cell (called a mother cell) dividing into two daughter cells. This leads to growth in multicellular organisms (the growth of tissue) and to procreation (vegetative reproduction) in unicellular organisms. Prokaryotic cells divide by binary fission, while eukaryotic cells usually undergo a process of nuclear division, called mitosis, followed by division of the cell, called cytokinesis. A diploid cell may also undergo meiosis to produce haploid cells, usually four. Haploid cells serve as gametes in multicellular organisms, fusing to form new diploid cells.
DNA replication, or the process of duplicating a cell's genome, always happens when a cell divides through mitosis or binary fission. This occurs during the S phase of the cell cycle.
In meiosis, the DNA is replicated only once, while the cell divides twice. DNA replication only occurs before meiosis I. DNA replication does not occur when the cells divide the second time, in meiosis II. Replication, like all cellular activities, requires specialized proteins for carrying out the job.


=== DNA repair ===

In general, cells of all organisms contain enzyme systems that scan their DNA for DNA damage and carry out repair processes when damage is detected. Diverse repair processes have evolved in organisms ranging from bacteria to humans. The widespread prevalence of these repair processes indicates the importance of maintaining cellular DNA in an undamaged state in order to avoid cell death or errors of replication due to damage that could lead to mutation. E. coli bacteria are a well-studied example of a cellular organism with diverse well-defined DNA repair processes. These include: (1) nucleotide excision repair, (2) DNA mismatch repair, (3) non-homologous end joining of double-strand breaks, (4) recombinational repair and (5) light-dependent repair (photoreactivation).


=== Growth and metabolism ===

Between successive cell divisions, cells grow through the functioning of cellular metabolism. Cell metabolism is the process by which individual cells process nutrient molecules. Metabolism has two distinct divisions: catabolism, in which the cell breaks down complex molecules to produce energy and reducing power, and anabolism, in which the cell uses energy and reducing power to construct complex molecules and perform other biological functions.
Complex sugars consumed by the organism can be broken down into simpler sugar molecules called monosaccharides such as glucose. Once inside the cell, glucose is broken down to make adenosine triphosphate (ATP), a molecule that possesses readily available energy, through two different pathways.


=== Protein synthesis ===

Cells are capable of synthesizing new proteins, which are essential for the modulation and maintenance of cellular activities. This process involves the formation of new protein molecules from amino acid building blocks based on information encoded in DNA/RNA. Protein synthesis generally consists of two major steps: transcription and translation.
Transcription is the process where genetic information in DNA is used to produce a complementary RNA strand. This RNA strand is then processed to give messenger RNA (mRNA), which is free to migrate through the cell. mRNA molecules bind to protein-RNA complexes called ribosomes located in the cytosol, where they are translated into polypeptide sequences. The ribosome mediates the formation of a polypeptide sequence based on the mRNA sequence. The mRNA sequence directly relates to the polypeptide sequence by binding to transfer RNA (tRNA) adapter molecules in binding pockets within the ribosome. The new polypeptide then folds into a functional three-dimensional protein molecule.


=== Motility ===

Unicellular organisms can move in order to find food or escape predators. Common mechanisms of motion include flagella and cilia.
In multicellular organisms, cells can move during processes such as wound healing, the immune response and cancer metastasis. For example, in wound healing in animals, white blood cells move to the wound site to kill the microorganisms that cause infection. Cell motility involves many receptors, crosslinking, bundling, binding, adhesion, motor and other proteins. The process is divided into three steps – protrusion of the leading edge of the cell, adhesion of the leading edge and de-adhesion at the cell body and rear, and cytoskeletal contraction to pull the cell forward. Each step is driven by physical forces generated by unique segments of the cytoskeleton.


==== Navigation, control and communication ====

In August 2020, scientists described one way cells – in particular cells of a slime mold and mouse pancreatic cancer–derived cells – are able to navigate efficiently through a body and identify the best routes through complex mazes: generating gradients after breaking down diffused chemoattractants which enable them to sense upcoming maze junctions before reaching them, including around corners.


== Multicellularity ==


=== Cell specialization/differentiation ===

Multicellular organisms are organisms that consist of more than one cell, in contrast to single-celled organisms.In complex multicellular organisms, cells specialize into different cell types that are adapted to particular functions. In mammals, major cell types include skin cells, muscle cells, neurons, blood cells, fibroblasts, stem cells, and others. Cell types differ both in appearance and function, yet are genetically identical. Cells are able to be of the same genotype but of different cell type due to the differential expression of the genes they contain.
Most distinct cell types arise from a single totipotent cell, called a zygote, that differentiates into hundreds of different cell types during the course of development. Differentiation of cells is driven by different environmental cues (such as cell–cell interaction) and intrinsic differences (such as those caused by the uneven distribution of molecules during division).


=== Origin of multicellularity ===

Multicellularity has evolved independently at least 25 times, including in some prokaryotes, like cyanobacteria, myxobacteria, actinomycetes, Magnetoglobus multicellularis, or Methanosarcina. However, complex multicellular organisms evolved only in six eukaryotic groups: animals, fungi, brown algae, red algae, green algae, and plants. It evolved repeatedly for plants (Chloroplastida), once or twice for animals, once for brown algae, and perhaps several times for fungi, slime molds, and red algae. Multicellularity may have evolved from colonies of interdependent organisms, from cellularization, or from organisms in symbiotic relationships.
The first evidence of multicellularity is from cyanobacteria-like organisms that lived between 3 and 3.5 billion years ago. Other early fossils of multicellular organisms include the contested Grypania spiralis and the fossils of the black shales of the Palaeoproterozoic Francevillian Group Fossil B Formation in Gabon.The evolution of multicellularity from unicellular ancestors has been replicated in the laboratory, in evolution experiments using predation as the selective pressure.


== Origins ==

The origin of cells has to do with the origin of life, which began the history of life on Earth.


=== Origin of the first cell ===

There are several theories about the origin of small molecules that led to life on the early Earth. They may have been carried to Earth on meteorites (see Murchison meteorite), created at deep-sea vents, or synthesized by lightning in a reducing atmosphere (see Miller–Urey experiment). There is little experimental data defining what the first self-replicating forms were. RNA is thought to be the earliest self-replicating molecule, as it is capable of both storing genetic information and catalyzing chemical reactions (see RNA world hypothesis), but some other entity with the potential to self-replicate could have preceded RNA, such as clay or peptide nucleic acid.Cells emerged at least 3.5 billion years ago. The current belief is that these cells were heterotrophs. The early cell membranes were probably more simple and permeable than modern ones, with only a single fatty acid chain per lipid. Lipids are known to spontaneously form bilayered vesicles in water, and could have preceded RNA, but the first cell membranes could also have been produced by catalytic RNA, or even have required structural proteins before they could form.


=== Origin of eukaryotic cells ===

The eukaryotic cell seems to have evolved from a symbiotic community of prokaryotic cells. DNA-bearing organelles like the mitochondria and the chloroplasts are descended from ancient symbiotic oxygen-breathing Alphaproteobacteria and ""Cyanobacteria"", respectively, which were endosymbiosed by an ancestral archaean prokaryote.
There is still considerable debate about whether organelles like the hydrogenosome predated the origin of mitochondria, or vice versa: see the hydrogen hypothesis for the origin of eukaryotic cells.


== History of research ==

1632–1723: Antonie van Leeuwenhoek taught himself to make lenses, constructed basic optical microscopes and drew protozoa, such as Vorticella from rain water, and bacteria from his own mouth.
1665: Robert Hooke discovered cells in cork, then in living plant tissue using an early compound microscope. He coined the term cell (from Latin cellula, meaning ""small room"") in his book Micrographia (1665).
1839: Theodor Schwann and Matthias Jakob Schleiden elucidated the principle that plants and animals are made of cells, concluding that cells are a common unit of structure and development, and thus founding the cell theory.
1855: Rudolf Virchow stated that new cells come from pre-existing cells by cell division (omnis cellula ex cellula).
1859: The belief that life forms can occur spontaneously (generatio spontanea) was contradicted by Louis Pasteur (1822–1895) (although Francesco Redi had performed an experiment in 1668 that suggested the same conclusion).
1931: Ernst Ruska built the first transmission electron microscope (TEM) at the University of Berlin. By 1935, he had built an EM with twice the resolution of a light microscope, revealing previously unresolvable organelles.
1953: Based on Rosalind Franklin's work, Watson and Crick made their first announcement on the double helix structure of DNA.
1981: Lynn Margulis published Symbiosis in Cell Evolution detailing the endosymbiotic theory.


== See also ==


== References ==


== Further reading ==


== External links ==

MBInfo – Descriptions on Cellular Functions and Processes
Inside the Cell Archived 2017-07-20 at the Wayback Machine – a science education booklet by National Institutes of Health, in PDF and ePub.
Cell Biology in ""The Biology Project"" of University of Arizona.
Centre of the Cell online
The Image & Video Library of The American Society for Cell Biology Archived 2011-06-10 at the Wayback Machine, a collection of peer-reviewed still images, video clips and digital books that illustrate the structure, function and biology of the cell.
WormWeb.org: Interactive Visualization of the C. elegans Cell lineage – Visualize the entire cell lineage tree of the nematode C. elegans",9210652,4701,"1665 in science, Articles containing Latin-language text, Articles with BNE identifiers, Articles with BNF identifiers, Articles with GND identifiers, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with NDL identifiers, Articles with NKC identifiers, Articles with short description, Cell anatomy, Cell biology, Commons category link is on Wikidata, Short description is different from Wikidata, Webarchive template wayback links, Wikipedia articles incorporating text from the United States National Library of Medicine, Wikipedia indefinitely move-protected pages, Wikipedia indefinitely semi-protected pages",1133996848,biology
https://en.wikipedia.org/wiki/Domain_(biology),Domain (biology),"In biological taxonomy, a domain ( or ) (Latin: regio), also dominion, superkingdom, realm, or empire, is the highest taxonomic rank of all organisms taken together. It was introduced in the three-domain system of taxonomy devised by Carl Woese, Otto Kandler and Mark Wheelis in 1990.According to the domain system, the tree of life consists of either three domains such as Archaea, Bacteria, and Eukarya, or two domains consisting of Archaea and Bacteria, with Eukarya included in Archaea. The first two are all prokaryotes, single-celled microorganisms without a membrane-bound nucleus. All organisms that have a cell nucleus and other membrane-bound organelles are included in Eukarya.
Non-cellular life is not included in this system. Alternatives to the three-domain system include the earlier two-empire system (with the empires Prokaryota and Eukaryota), and the eocyte hypothesis (with two domains of Bacteria and Archaea, with Eukarya included as a branch of Archaea).","In biological taxonomy, a domain ( or ) (Latin: regio), also dominion, superkingdom, realm, or empire, is the highest taxonomic rank of all organisms taken together. It was introduced in the three-domain system of taxonomy devised by Carl Woese, Otto Kandler and Mark Wheelis in 1990.According to the domain system, the tree of life consists of either three domains such as Archaea, Bacteria, and Eukarya, or two domains consisting of Archaea and Bacteria, with Eukarya included in Archaea. The first two are all prokaryotes, single-celled microorganisms without a membrane-bound nucleus. All organisms that have a cell nucleus and other membrane-bound organelles are included in Eukarya.
Non-cellular life is not included in this system. Alternatives to the three-domain system include the earlier two-empire system (with the empires Prokaryota and Eukaryota), and the eocyte hypothesis (with two domains of Bacteria and Archaea, with Eukarya included as a branch of Archaea).


== Terminology ==
The term domain was proposed by Carl Woese, Otto Kandler, and Mark Wheelis (1990) in a three-domain system. This term represents a synonym for the category of dominion (Lat. dominium), introduced by Moore in 1974.


== Development of the Domain System ==
Carolus Linnaeus made the classification of domain popular in the famous taxonomy system he created in the middle of the eighteenth century. This system was further improved by the studies of Charles Darwin later on but failed to properly classify the domain, Bacteria, due to it having very few observable features to compare to the other domains.Carl Woese made a revolutionary breakthrough when, in 1977, he compared the nucleotide sequences of the 16s ribosomal RNA and discovered that the rank, domain, contained three branches; not two like scientists had previously thought. Initially, due to their physical similarities, Archaea and Bacteria were classified together and called ""archaebacteria"". However, scientists now know that these two domains are hardly similar and are internally wildly different.


== Characteristics of the three domains ==

Each of these three domains contains unique ribosomal RNA. This forms the basis of the three-domain system. While the presence of a nuclear membrane differentiates the Eukarya from the Archaea and Bacteria, both of which lack a nuclear envelope, the Archaea and Bacteria are distinct from each other differences in the biochemistry of their cell membranes and RNA markers.


=== Archaea ===

Archaea are  prokaryotic cells, typically characterized by membrane lipids that are branched hydrocarbon chains attached to glycerol by ether linkages. The presence of these ether linkages in Archaea adds to their ability to withstand extreme temperatures and highly acidic conditions, but many archaea live in mild environments. Halophiles, organisms that thrive in highly salty environments, and hyperthermophiles, organisms that thrive in extremely hot environments, are examples of Archaea.Archaea evolved many cell sizes, but all are relatively small. Their size ranges from 0.1 μm to 15 μm diameter and up to 200 μm long. They are about the size of bacteria, or similar in size to the mitochondria found in eukaryotic cells. Members of the genus Thermoplasma are the smallest of the Archaea.


=== Bacteria ===

Cyanobacteria and mycoplasmas are two examples of bacteria.
Even though bacteria are prokaryotic cells just like Archaea, their cell membranes are instead made of phospholipid bilayers. 
Bacteria cell membranes are distinct from Archean membranes: They characteristically have none of the ether linkages that Archaea have. Internally, bacteria have different RNA structures in their ribosomes, hence they are grouped into a different category. In the two- and three-domain systems, this puts them into a separate domain.
There is a great deal of diversity in the domain Bacteria. That diversity is further confounded by exchange of genes between different bacterial lineages. The occurrence of duplicate genes between otherwise distantly-related bacteria makes it nearly impossible to distinguish bacterial species, or count the bacterial species on the Earth, or to organize them into a tree-like structure (unless the structure includes cross-connections between branches, making it a ""network"" instead of a ""tree"").


=== Eukarya ===

Members of the domain Eukarya – called eukaryotes – have membrane-bound organelles (including a nucleus containing genetic material) and are represented by five kingdoms: Plantae, Protozoa, Animalia, Chromista, and Fungi.


== Exclusion of viruses and prions ==

The three-domain system does not include any form of non-cellular life. Stefan Luketa proposed a five-domain system in 2012, adding Prionobiota (acellular and without nucleic acid) and Virusobiota (acellular but with nucleic acid) to the traditional three domains.


== Alternative classifications ==
Alternative classifications of life include:

The two-empire system or superdomain system, proposed by Mayr (1998), with top-level groupings of Prokaryota (or Monera) and Eukaryota.
The eocyte hypothesis, proposed by Lake et al. (1984), which posits two domains: Bacteria and Archaea, with Eukaryota included as a subordinate clade branching from Archaea.


== See also ==
Biological dark matter
Neomura, which is the two domains of life of Archaea and Eukaryota
Phylogenetics
Protein structure
Realm (virology), an equivalent rank for non-cellular life
Systematics


== References ==


== External links ==
Learn Biology: Classification-Domains on YouTube",2315088,783,"Articles with short description, Domains (biology), Short description is different from Wikidata, Species, Use dmy dates from April 2016",1125372452,biology
https://en.wikipedia.org/wiki/Evolutionary_biology,Evolutionary biology,"Evolutionary biology is the subfield of biology that studies the evolutionary processes (natural selection, common descent, speciation) that produced the diversity of life on Earth. It is also defined as the study of the history of life forms on Earth. Evolution holds that all species are related and gradually change over generations. In a population, the genetic variations affect the phenotypes (physical characteristics) of an organism. These changes in the phenotypes will be an advantage to some organisms, which will then be passed onto their offspring. Some examples of evolution in species over many generations are the peppered moth and flightless birds. In the 1930s, the discipline of evolutionary biology emerged through what Julian Huxley called the modern synthesis of understanding, from previously unrelated fields of biological research, such as genetics and ecology, systematics, and paleontology.
The investigational range of current research has widened to encompass the genetic architecture of adaptation, molecular evolution, and the different forces that contribute to evolution, such as sexual selection, genetic drift, and biogeography. Moreover, the newer field of evolutionary developmental biology (""evo-devo"") investigates how embryogenesis (the development of the embryo) is controlled, thus yielding a wider synthesis that integrates developmental biology with the fields of study covered by the earlier evolutionary synthesis.

","Evolutionary biology is the subfield of biology that studies the evolutionary processes (natural selection, common descent, speciation) that produced the diversity of life on Earth. It is also defined as the study of the history of life forms on Earth. Evolution holds that all species are related and gradually change over generations. In a population, the genetic variations affect the phenotypes (physical characteristics) of an organism. These changes in the phenotypes will be an advantage to some organisms, which will then be passed onto their offspring. Some examples of evolution in species over many generations are the peppered moth and flightless birds. In the 1930s, the discipline of evolutionary biology emerged through what Julian Huxley called the modern synthesis of understanding, from previously unrelated fields of biological research, such as genetics and ecology, systematics, and paleontology.
The investigational range of current research has widened to encompass the genetic architecture of adaptation, molecular evolution, and the different forces that contribute to evolution, such as sexual selection, genetic drift, and biogeography. Moreover, the newer field of evolutionary developmental biology (""evo-devo"") investigates how embryogenesis (the development of the embryo) is controlled, thus yielding a wider synthesis that integrates developmental biology with the fields of study covered by the earlier evolutionary synthesis.


== Subfields ==

Evolution is the central unifying concept in biology. Biology can be divided into various ways. One way is by the level of biological organization, from molecular to cell, organism to population. Another way is by perceived taxonomic group, with fields such as zoology, botany, and microbiology, reflecting what was once seen as the major divisions of life.
A third way is by approaches, such as field biology, theoretical biology, experimental evolution, and paleontology. These alternative ways of dividing up the subject have been combined with evolutionary biology to create subfields like evolutionary ecology and evolutionary developmental biology.
More recently, the merge between biological science and applied sciences gave birth to new fields that are extensions of evolutionary biology, including evolutionary robotics, engineering, algorithms, economics, and architecture. The basic mechanisms of evolution are applied directly or indirectly to come up with novel designs or solve problems that are difficult to solve otherwise. The research generated in these applied fields, contribute towards progress, especially from work on evolution in computer science and engineering fields such as mechanical engineering.


== Different types of evolution ==


=== Adaptive evolution ===
Adaptive evolution relates to evolutionary changes that happen due to the changes in the environment, this makes the organism suitable to its habitat. This change increases the chances of survival and reproduction of the organism (this can be referred to as an organism's fitness). For example, Darwin's Finches on Galapagos island developed different shaped beaks in order to survive for a long time. Adaptive evolution can also be convergent evolution if two distantly related species live in similar environments facing similar pressures.


=== Convergent evolution ===
Convergent evolution is the process in which related or distantly related organisms evolve similar characteristics independently. This type of evolution creates analogous structures which have a similar function, structure, or form between the two species. For example, sharks and dolphins look alike but they are not related. Likewise, birds, flying insects, and bats all have the ability to fly, but they are not related to each other. These similar traits tend to evolve from having similar environmental pressures.


=== Divergent evolution ===
Divergent evolution is the process of speciation. This can happen in several ways:

Allopatric speciation is when species are separated by a physical barrier that separates the population into two groups. evolutionary mechanisms such as genetic drift and natural selection can then act independently on each population.Peripatric speciation is a type of allopatric speciation that occurs when one of the new populations is considerably smaller than the other initial population. This leads to the founder's effect and the population can have different allele frequencies and phenotypes than the original population. These small populations are also more likely to see effects from genetic drift.Parapatric speciation is allopatric speciation but occurs when the species diverge without a physical barrier separating the population. This tends to occur when a population of a species is incredibly large and occupies a vast environment.Sympatric speciation is when a new species or subspecies sprouts from the original population while still occupying the same small environment, and without any physical barriers separating them from members of their original population. There is scientific debate as to whether sympatric speciation actually exists.Artificial speciation is when scientists purposefully cause new species to emerge to use in laboratory procedures.


=== Coevolution ===
The influence of two closely associated species is known as coevolution. When two or more species evolve in company with each other, one species adapts to changes in other species. This type of evolution often happens in species that have symbiotic relationships. For example, predator-prey coevolution, this is the most common type of co-evolution. In this, the predator must evolve to become a more effective hunter because there is a selective pressure on the prey to steer clear of capture. The prey in turn need to develop better survival strategies. The Red Queen hypothesis is an example of predator-prey interations. The relationship between pollinating insects like bees and flowering plants, herbivores and plants, are also some common examples of diffuse or guild coevolution.


== Mechanism: The process of evolution ==
The mechanisms of evolution focus mainly on mutation, genetic drift, gene flow, non-random mating, and natural selection.
Mutation: Mutation is a change in the DNA sequence inside a gene or a chromosome of an organism. Most mutations are deleterious, or neutral; i.e. they can neither harm nor benefit, but can also be beneficial sometimes.
Genetic drift: Genetic drift is a variational process, it happens as a result of the sampling errors from one generation to another generation where a random event that happens by chance in nature changes or influences allele frequency within a population. It has a much stronger effect on small populations than large ones.
Gene flow: Gene flow is the transfer of genetic material from the gene pool of one population to another. In a population, migration occurs from one species to another, resulting in the change of allele frequency.
Natural selection: The survival and reproductive rate of a species depends on the adaptability of the species to their environment. This process is called natural selection. Some species with certain traits in a population have higher survival and reproductive rate than others (fitness), and they pass on these genetic features to their offsprings.


== Evolutionary developmental biology ==

In evolutionary developmental biology scientists look at how the different processes in development play a role in how a specific organism reaches its current body plan. The genetic regulation of ontogeny and the phylogenetic process is what allows for this kind of understanding of biology to be possible. By looking at different processes during development, and going through the evolutionary tree, one can determine at which point a specific structure came about. For example, the three germ layers can be observed to not be present in cnidarians and ctenophores, which instead present in worms, being more or less developed depending on the kind of worm itself. Other structures like the development of Hox genes and sensory organs such as eyes can also be traced with this practice.


== Phylogenic Trees ==

Phylogenic Trees are representations of genetic lineage. They are figures that show how related species are to one another. They formed by analyzing the physical traits as well as the similarities of the DNA between species. Then by using a molecular clock scientists can estimate when the species diverged. An example of a phylogeny would be the tree of life.


== Homologs ==
Genes that have shared ancestry are homologs. If a speciation event occurs and one gene ends up in two different species the genes are now orthologous. If a gene is duplicated within the a singular species then it is a paralog. A molecular clock can be used to estimate when these events occurred.


== History ==

The idea of evolution by natural selection was proposed by Charles Darwin in 1859, but evolutionary biology, as an academic discipline in its own right, emerged during the period of the modern synthesis in the 1930s and 1940s. It was not until the 1980s that many universities had departments of evolutionary biology. In the United States, many universities have created departments of molecular and cell biology or ecology and evolutionary biology, in place of the older departments of botany and zoology. Palaeontology is often grouped with earth science.
Microbiology too is becoming an evolutionary discipline now that microbial physiology and genomics are better understood. The quick generation time of bacteria and viruses such as bacteriophages makes it possible to explore evolutionary questions.
Many biologists have contributed to shaping the modern discipline of evolutionary biology. Theodosius Dobzhansky and E. B. Ford established an empirical research programme. Ronald Fisher, Sewall Wright, and J. B. S. Haldane created a sound theoretical framework. Ernst Mayr in systematics, George Gaylord Simpson in paleontology and G. Ledyard Stebbins in botany helped to form the modern synthesis.
James Crow, Richard Lewontin, Dan Hartl, Marcus Feldman, and Brian Charlesworth trained a generation of evolutionary biologists.


== Current research topics ==
Current research in evolutionary biology covers diverse topics and incorporates ideas from diverse areas, such as molecular genetics and computer science.
First, some fields of evolutionary research try to explain phenomena that were poorly accounted for in the modern evolutionary synthesis. These include speciation, the evolution of sexual reproduction, the evolution of cooperation, the evolution of ageing, and evolvability.Second, some evolutionary biologists ask the most straightforward evolutionary question: ""what happened and when?"".  This includes fields such as paleobiology, where paleobiologists and evolutionary biologists, including Thomas Halliday and Anjali Goswami, studied the evolution of early mammals going far back in time during the Mesozoic and Cenozoic eras (between 299 million to 12,000 years ago). Other fields related to generic exploration of evolution (""what happened and when?"" ) include systematics and phylogenetics.
Third, the modern evolutionary synthesis was devised at a time when nobody understood the molecular basis of genes. Today, evolutionary biologists try to determine the genetic architecture of interesting evolutionary phenomena such as adaptation and speciation. They seek answers to questions such as how many genes are involved, how large are the effects of each gene, how interdependent are the effects of different genes, what do the genes do, and what changes happen to them (e.g., point mutations vs. gene duplication or even genome duplication). They try to reconcile the high heritability seen in twin studies with the difficulty in finding which genes are responsible for this heritability using genome-wide association studies.One challenge in studying genetic architecture is that the classical population genetics that catalysed the modern evolutionary synthesis must be updated to take into account modern molecular knowledge. This requires a great deal of mathematical development to relate DNA sequence data to evolutionary theory as part of a theory of molecular evolution. For example, biologists try to infer which genes have been under strong selection by detecting selective sweeps.Fourth, the modern evolutionary synthesis involved agreement about which forces contribute to evolution, but not about their relative importance. Current research seeks to determine this. Evolutionary forces include natural selection, sexual selection, genetic drift, genetic draft, developmental constraints, mutation bias and biogeography.
This evolutionary approach is key to much current research in organismal biology and ecology, such as life history theory. Annotation of genes and their function relies heavily on comparative approaches. The field of evolutionary developmental biology (""evo-devo"") investigates how developmental processes work, and compares them in different organisms to determine how they evolved.
Many physicians do not have enough background in evolutionary biology, making it difficult to use it in modern medicine. However, there are efforts to gain a deeper understanding of disease through evolutionary medicine and to develop evolutionary therapies.


== Drug resistance today ==
Evolution plays a role in resistance of drugs; for example, how HIV becomes resistant to medications and the body's immune system. The mutation of resistance of HIV is due to the natural selection of the survivors and their offspring. The few HIV that survive the immune system reproduced and had offspring that were also resistant to the immune system.  Drug resistance also causes many problems for patients such as a worsening sickness or the sickness can mutate into something that can no longer be cured with medication. Without the proper medicine, a sickness can be the death of a patient. If their body has resistance to a certain number of drugs, then the right medicine will be harder and harder to find. Not completing the prescribed full course of antibiotic is also an example of resistance that will cause the bacteria against which the antibiotic is being taken to evolve and continue to spread in the body. When the full dosage of the medication does not enter the body and perform its proper job, the bacteria that survive the initial dosage will continue to reproduce. This can make for another bout of sickness later on that will be more difficult to cure because the bacteria involved will be resistant to the first medication used. Taking the full course of medicine that is prescribed is a vital step in avoiding antibiotic resistance.
Individuals with chronic illnesses, especially those that can recur throughout a lifetime, are at greater risk of antibiotic resistance than others. This is because overuse of a drug or too high of a dosage can cause a patient's immune system to weaken and the illness will evolve and grow stronger. For example, cancer patients will need a stronger and stronger dosage of medication because of their low functioning immune system.


== Journals ==
Some scientific journals specialise exclusively in evolutionary biology as a whole, including the journals Evolution, Journal of Evolutionary Biology, and BMC Evolutionary Biology. Some journals cover sub-specialties within evolutionary biology, such as the journals Systematic Biology, Molecular Biology and Evolution and its sister journal Genome Biology and Evolution, and Cladistics.
Other journals combine aspects of evolutionary biology with other related fields. For example, Molecular Ecology, Proceedings of the Royal Society of London Series B, The American Naturalist and Theoretical Population Biology have overlap with ecology and other aspects of organismal biology. Overlap with ecology is also prominent in the review journals Trends in Ecology and Evolution and Annual Review of Ecology, Evolution, and Systematics. The journals Genetics and PLoS Genetics overlap with molecular genetics questions that are not obviously evolutionary in nature.


== See also ==


== References ==


== External links ==
 Media related to Evolutionary biology at Wikimedia Commons
Evolution And Paleobotany at Britannica",1028819,862,"Articles with GND identifiers, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with NKC identifiers, Articles with short description, Commons category link from Wikidata, Evolutionary biology, Philosophy of biology, Short description matches Wikidata, Use British English from March 2015, Use dmy dates from December 2022",1134337348,biology
https://en.wikipedia.org/wiki/Order_(biology),Order (biology),"Order (Latin: ordo) is one of the eight major hierarchical taxonomic ranks in Linnaean taxonomy. It is classified between family and  class.  In biological classification, the order is a taxonomic rank used in the classification of organisms and recognized by the nomenclature codes. An immediately higher rank, superorder, is sometimes added directly above order, with suborder directly beneath order. An order can also be defined as a group of related families.
What does and does not belong to each order is determined by a taxonomist, as is whether a particular order should be recognized at all. Often there is no exact agreement, with different taxonomists each taking a different position. There are no hard rules that a taxonomist needs to follow in describing or recognizing an order. Some taxa are accepted almost universally, while others are recognized only rarely.The name of an order is usually written with a capital letter. For some groups of organisms, their orders may follow consistent naming schemes. Orders of plants, fungi, and algae use the suffix -ales (e.g. Dictyotales). Orders of birds and fishes use the Latin suffix -(i)formes meaning 'having the form of' (e.g. Passeriformes), but orders of mammals and invertebrates are not so consistent (e.g. Artiodactyla, Actiniaria, Primates).","Order (Latin: ordo) is one of the eight major hierarchical taxonomic ranks in Linnaean taxonomy. It is classified between family and  class.  In biological classification, the order is a taxonomic rank used in the classification of organisms and recognized by the nomenclature codes. An immediately higher rank, superorder, is sometimes added directly above order, with suborder directly beneath order. An order can also be defined as a group of related families.
What does and does not belong to each order is determined by a taxonomist, as is whether a particular order should be recognized at all. Often there is no exact agreement, with different taxonomists each taking a different position. There are no hard rules that a taxonomist needs to follow in describing or recognizing an order. Some taxa are accepted almost universally, while others are recognized only rarely.The name of an order is usually written with a capital letter. For some groups of organisms, their orders may follow consistent naming schemes. Orders of plants, fungi, and algae use the suffix -ales (e.g. Dictyotales). Orders of birds and fishes use the Latin suffix -(i)formes meaning 'having the form of' (e.g. Passeriformes), but orders of mammals and invertebrates are not so consistent (e.g. Artiodactyla, Actiniaria, Primates).


== Hierarchy of ranks ==


=== Zoology ===
For some clades covered by the International Code of Zoological Nomenclature, several additional classifications are sometimes used, although not all of these are officially recognized.

In their 1997 classification of mammals, McKenna and Bell used two extra levels between superorder and order: grandorder and mirorder. Michael Novacek (1986) inserted them at the same position. Michael Benton (2005) inserted them between superorder and magnorder instead. This position was adopted by Systema Naturae 2000 and others.


=== Botany ===
In botany, the ranks of subclass and suborder are secondary ranks pre-defined as respectively above and below the rank of order. Any number of further ranks can be used as long as they are clearly defined.The superorder rank is commonly used, with the ending -anae that was initiated by Armen Takhtajan's publications from 1966 onwards.


== History ==
The order as a distinct rank of biological classification having its own distinctive name (and not just called a higher genus (genus summum)) was first introduced by the German botanist Augustus Quirinus Rivinus in his classification of plants that appeared in a series of treatises in the 1690s. Carl Linnaeus was the first to apply it consistently to the division of all three kingdoms of nature (then minerals, plants, and animals) in his Systema Naturae (1735, 1st. Ed.).


=== Botany ===

For plants, Linnaeus' orders in the Systema Naturae and the Species Plantarum were strictly artificial, introduced to subdivide the artificial classes into more comprehensible smaller groups.  When the word ordo was first consistently used for natural units of plants, in 19th-century works such as the Prodromus Systematis Naturalis Regni Vegetabilis of Augustin Pyramus de Candolle and the Genera Plantarum of Bentham & Hooker, it indicated taxa that are now given the rank of family. (See ordo naturalis, 'natural order'.)
In French botanical publications, from Michel Adanson's Familles naturelles des plantes (1763) and until the end of the 19th century, the word famille (plural: familles) was used as a French equivalent for this Latin ordo. This equivalence was explicitly stated in the Alphonse Pyramus de Candolle's Lois de la nomenclature botanique (1868), the precursor of the currently used International Code of Nomenclature for algae, fungi, and plants.
In the first international Rules of botanical nomenclature from the International Botanical Congress of 1905, the word family (familia) was assigned to the rank indicated by the French famille, while order (ordo) was reserved for a higher rank, for what in the 19th century had often been named a cohors (plural cohortes).
Some of the plant families still retain the names of Linnaean ""natural orders"" or even the names of pre-Linnaean natural groups recognized by Linnaeus as orders in his natural classification (e.g. Palmae or Labiatae). Such names are known as descriptive family names.


=== Zoology ===
In zoology, the Linnaean orders were used more consistently. That is, the orders in the zoology part of the Systema Naturae refer to natural groups. Some of his ordinal names are still in use, e.g. Lepidoptera (moths and butterflies) and Diptera (flies, mosquitoes, midges, and gnats).


=== Virology ===
In virology, the International Committee on Taxonomy of Viruses's virus classification includes fifteen taxa to be applied for viruses, viroids and satellite nucleic acids: realm, subrealm, kingdom, subkingdom, phylum, subphylum, class, subclass, order, suborder, family, subfamily, genus, subgenus, and species. There are currently fourteen viral orders, each ending in the suffix -virales.


== See also ==
Biological classification
Cladistics
Phylogenetics
Taxonomic rank
Systematics
Taxonomy
Virus classification


== References ==


== Works cited ==
McNeill, J.; Barrie, F.R.; Buck, W.R.; Demoulin, V.; Greuter, W.; Hawksworth, D.L.; Herendeen, P.S.; Knapp, S.; Marhold, K.; Prado, J.; Prud'homme Van Reine, W.F.; Smith, G.F.; Wiersema, J.H.; Turland, N.J. (2012). International Code of Nomenclature for algae, fungi, and plants (Melbourne Code) adopted by the Eighteenth International Botanical Congress Melbourne, Australia, July 2011. Regnum Vegetabile. Vol. 154. A.R.G. Gantner Verlag KG. ISBN 978-3-87429-425-6.",1656132,699,"All articles needing additional references, All articles with unsourced statements, Articles containing French-language text, Articles containing Latin-language text, Articles needing additional references from May 2008, Articles with GND identifiers, Articles with short description, Articles with unsourced statements from May 2014, Bacterial nomenclature, Botanical nomenclature, CS1 Latin-language sources (la), Orders (biology), Plant taxonomy, Short description is different from Wikidata, Zoological nomenclature",1134321842,biology
https://en.wikipedia.org/wiki/Cell_biology,Cell biology,"Cell biology (also cellular biology or cytology) is a branch of biology that studies the structure, function, and behavior of cells. All living organisms are made of cells. A cell is the basic unit of life that is responsible for the living and functioning of organisms. Cell biology is the study of structural and functional units of cells. Cell biology encompasses both prokaryotic and eukaryotic cells and has many subtopics which may include the study of cell metabolism, cell communication, cell cycle, biochemistry, and cell composition. The study of cells is performed using several microscopy techniques, cell culture, and cell fractionation. These have allowed for and are currently being used for discoveries and research pertaining to how cells function, ultimately giving insight into understanding larger organisms. Knowing the components of cells and how cells work is fundamental to all biological sciences while also being essential for research in biomedical fields such as cancer, and other diseases. Research in cell biology is interconnected to other fields such as genetics, molecular genetics, molecular biology, medical microbiology, immunology, and cytochemistry.","Cell biology (also cellular biology or cytology) is a branch of biology that studies the structure, function, and behavior of cells. All living organisms are made of cells. A cell is the basic unit of life that is responsible for the living and functioning of organisms. Cell biology is the study of structural and functional units of cells. Cell biology encompasses both prokaryotic and eukaryotic cells and has many subtopics which may include the study of cell metabolism, cell communication, cell cycle, biochemistry, and cell composition. The study of cells is performed using several microscopy techniques, cell culture, and cell fractionation. These have allowed for and are currently being used for discoveries and research pertaining to how cells function, ultimately giving insight into understanding larger organisms. Knowing the components of cells and how cells work is fundamental to all biological sciences while also being essential for research in biomedical fields such as cancer, and other diseases. Research in cell biology is interconnected to other fields such as genetics, molecular genetics, molecular biology, medical microbiology, immunology, and cytochemistry.


== History ==
Cells were first seen in 17th century Europe with the invention of the compound microscope. In 1665, Robert Hooke termed the building block of all living organisms as ""cells"" (published in Micrographia) after looking at a piece of cork and observing a cell-like structure, however, the cells were dead and gave no indication to the actual overall components of a cell. A few years later, in 1674, Anton Van Leeuwenhoek was the first to analyze live cells in his examination of algae. All of this preceded the cell theory which states that all living things are made up of cells and that cells are the functional and structural unit of organisms. This was ultimately concluded by plant scientist, Matthias Schleiden and animal scientist Theodor Schwann in 1838, who viewed live cells in plant and animal tissue, respectively. 19 years later, Rudolf Virchow further contributed to the cell theory, adding that all cells come from the division of pre-existing cells. Viruses are not considered in cell biology – they lack the characteristics of a living cell, and instead are studied in the microbiology subclass of virology.


== Techniques ==
Cell biology research looks at different ways to culture and manipulate cells outside of a living body to further research in human anatomy and physiology, and to derive medications. The techniques by which cells are studied have evolved. Due to advancements in microscopy, techniques and technology have allowed scientists to hold a better understanding of the structure and function of cells. Many techniques commonly used to study cell biology are listed below:
Cell culture: Utilizes rapidly growing cells on media which allows for a large amount of a specific cell type and an efficient way to study cells. Cell culture is one of the major tools used in cellular and molecular biology, providing excellent model systems for studying the normal physiology and biochemistry of cells (e.g., metabolic studies, aging), the effects of drugs and toxic compounds on the cells, and mutagenesis and carcinogenesis. It is also used in drug screening and development, and large scale manufacturing of biological compounds (e.g., vaccines, therapeutic proteins).
Fluorescence microscopy: Fluorescent markers such as GFP, are used to label a specific component of the cell. Afterwards, a certain light wavelength is used to excite the fluorescent marker which can then be visualized.
Phase-contrast microscopy: Uses the optical aspect of light to represent the solid, liquid, and gas-phase changes as brightness differences.
Confocal microscopy: Combines fluorescence microscopy with imaging by focusing light and snap shooting instances to form a 3-D image.
Transmission electron microscopy: Involves metal staining and the passing of electrons through the cells, which will be deflected upon interaction with metal. This ultimately forms an image of the components being studied.
Cytometry: The cells are placed in the machine which uses a beam to scatter the cells based on different aspects and can therefore separate them based on size and content. Cells may also be tagged with GFP-fluorescence and can be separated that way as well.
Cell fractionation: This process requires breaking up the cell using high temperature or sonification followed by centrifugation to separate the parts of the cell allowing for them to be studied separately.


== Cell types ==

There are two fundamental classifications of cells: prokaryotic and eukaryotic. Prokaryotic cells are distinguished from eukaryotic cells by the absence of a cell nucleus or other membrane-bound organelle. Prokaryotic cells are much smaller than eukaryotic cells, making them the smallest form of life. Prokaryotic cells include Bacteria and Archaea, and lack an enclosed cell nucleus.  Eukaryotic cells are found in plants, animals, fungi, and protists. They range from 10 to 100 μm in diameter, and their DNA is contained within a membrane-bound nucleus. Eukaryotes are organisms containing eukaryotic cells. The four eukaryotic kingdoms are Animalia, Plantae, Fungi, and Protista.They both reproduce through binary fission. Bacteria, the most prominent type, have several different shapes, although most are spherical or rod-shaped. Bacteria can be classed as either gram-positive or gram-negative depending on the cell wall composition. Gram-positive bacteria have a thicker peptidoglycan layer than gram-negative bacteria. Bacterial structural features include a flagellum that helps the cell to move, ribosomes for the translation of RNA to protein, and a nucleoid that holds all the genetic material in a circular structure. There are many processes that occur in prokaryotic cells that allow them to survive. In prokaryotes, mRNA synthesis is initiated at a promoter sequence on the DNA template comprising two consensus sequences that recruit RNA polymerase. The prokaryotic polymerase consists of a core enzyme of four protein subunits and a σ protein that assists only with initiation. For instance, in a process termed conjugation, the fertility factor allows the bacteria to possess a pilus which allows it to transmit DNA to another bacteria which lacks the F factor, permitting the transmittance of resistance allowing it to survive in certain environments.


== Structure and function ==


=== Structure of eukaryotic cells ===

Eukaryotic cells are composed of the following organelles:

Nucleus: The nucleus of the cell functions as the genome and genetic information storage for the cell, containing all the DNA organized in the form of chromosomes. It is surrounded by a nuclear envelope, which includes nuclear pores allowing for the transportation of proteins between the inside and outside of the nucleus. This is also the site for replication of DNA as well as transcription of DNA to RNA. Afterwards, the RNA is modified and transported out to the cytosol to be translated to protein.
Nucleolus: This structure is within the nucleus, usually dense and spherical in shape. It is the site of ribosomal RNA (rRNA) synthesis, which is needed for ribosomal assembly.
Endoplasmic reticulum (ER): This functions to synthesize, store, and secrete proteins to the Golgi apparatus. Structurally, the endoplasmic reticulum is a network of membranes found throughout the cell and connected to the nucleus. The membranes are slightly different from cell to cell and a cell's function determines the size and structure of the ER.
Mitochondria: Commonly known as the powerhouse of the cell is a double membrane bound cell organelle. This functions for the production of energy or ATP within the cell. Specifically, this is the place where the Krebs cycle or TCA cycle for the production of NADH and FADH occurs. Afterwards, these products are used within the electron transport chain (ETC) and oxidative phosphorylation for the final production of ATP.
Golgi apparatus: This functions to further process, package, and secrete the proteins to their destination. The proteins contain a signal sequence that allows the Golgi apparatus to recognize and direct it to the correct place. Golgi apparatus also produce glycoproteins and glycolipids.
Lysosome: The lysosome functions to degrade material brought in from the outside of the cell or old organelles. This contains many acid hydrolases, proteases, nucleases, and lipases, which break down the various molecules. Autophagy is the process of degradation through lysosomes which occurs when a vesicle buds off from the ER and engulfs the material, then, attaches and fuses with the lysosome to allow the material to be degraded.
Ribosomes: Functions to translate RNA to protein. it serves as a site of protein synthesis.
Cytoskeleton: Cytoskeleton is a structure that helps to maintain the shape and general organization of the cytoplasm. It anchors organelles within the cells and makes up the structure and stability of the cell. The cytoskeleton is composed of three principal types of protein filaments: actin filaments, intermediate filaments, and microtubules, which are held together and linked to subcellular organelles and the plasma membrane by a variety of accessory proteins.
Cell membrane: The cell membrane can be described as a phospholipid bilayer and is also consisted of lipids and proteins. Because the inside of the bilayer is hydrophobic and in order for molecules to participate in reactions within the cell, they need to be able to cross this membrane layer to get into the cell via osmotic pressure, diffusion, concentration gradients, and membrane channels.
Centrioles: Function to produce spindle fibers which are used to separate chromosomes during cell division.Eukaryotic cells may also be composed of the following molecular components:

Chromatin: This makes up chromosomes and is a mixture of DNA with various proteins.
Cilia: They help to propel substances and can also be used for sensory purposes.


=== Cell metabolism ===
Cell metabolism is necessary for the production of energy for the cell and therefore its survival and includes many pathways. For cellular respiration, once glucose is available, glycolysis occurs within the cytosol of the cell to produce pyruvate. Pyruvate undergoes decarboxylation using the multi-enzyme complex to form acetyl coA which can readily be used in the TCA cycle to produce NADH and FADH2. These products are involved in the electron transport chain to ultimately form a proton gradient across the inner mitochondrial membrane. This gradient can then drive the production of ATP and H2O during oxidative phosphorylation. Metabolism in plant cells includes photosynthesis which is simply the exact opposite of respiration as it ultimately produces molecules of glucose.


=== Cell signaling ===

Cell signaling or cell communication is important for cell regulation and for cells to process information from the environment and respond accordingly. Signaling can occur through direct cell contact or endocrine, paracrine, and autocrine signaling. Direct cell-cell contact is when a receptor on a cell binds a molecule that is attached to the membrane of another cell. Endocrine signaling occurs through molecules secreted into the bloodstream. Paracrine signaling uses molecules diffusing between two cells to communicate. Autocrine is a cell sending a signal to itself by secreting a molecule that binds to a receptor on its surface. Forms of communication can be through:

Ion channels: Can be of different types such as voltage or ligand gated ion channels. They allow for the outflow and inflow of molecules and ions.
G-protein coupled receptor (GPCR): Is widely recognized to contain seven transmembrane domains. The ligand binds on the extracellular domain and once the ligand binds, this signals a guanine exchange factor to convert GDP to GTP and activate the G-α subunit. G-α can target other proteins such as adenyl cyclase or phospholipase C, which ultimately produce secondary messengers such as cAMP, Ip3, DAG, and calcium. These secondary messengers function to amplify signals and can target ion channels or other enzymes. One example for amplification of a signal is cAMP binding to and activating PKA by removing the regulatory subunits and releasing the catalytic subunit. The catalytic subunit has a nuclear localization sequence which prompts it to go into the nucleus and phosphorylate other proteins to either repress or activate gene activity.
Receptor tyrosine kinases: Bind growth factors, further promoting the tyrosine on the intracellular portion of the protein to cross phosphorylate. The phosphorylated tyrosine becomes a landing pad for proteins containing an SH2 domain allowing for the activation of Ras and the involvement of the MAP kinase pathway.


== Growth and development ==


=== Eukaryotic cell cycle ===

Cells are the foundation of all organisms and are the fundamental units of life. The growth and development of cells are essential for the maintenance of the host and survival of the organism. For this process, the cell goes through the steps of the cell cycle and development which involves cell growth, DNA replication, cell division, regeneration, and cell death.
The cell cycle is divided into four distinct phases: G1, S, G2, and M. The G phase – which is the cell growth phase – makes up approximately 95% of the cycle. The proliferation of cells is instigated by progenitors. All cells start out in an identical form and can essentially become any type of cells. Cell signaling such as induction can influence nearby cells to determinate the type of cell it will become. Moreover, this allows cells of the same type to aggregate and form tissues, then organs, and ultimately systems. The G1, G2, and S phase (DNA replication, damage and repair) are considered to be the interphase portion of the cycle, while the M phase (mitosis) is the cell division portion of the cycle. Mitosis is composed of many stages which include, prophase, metaphase, anaphase, telophase, and cytokinesis, respectively. The ultimate result of mitosis is the formation of two identical daughter cells.
The cell cycle is regulated in cell cycle checkpoints, by a series of signaling factors and complexes such as cyclins, cyclin-dependent kinase, and p53. When the cell has completed its growth process and if it is found to be damaged or altered, it undergoes cell death, either by apoptosis or necrosis, to eliminate the threat it can cause to the organism's survival.


=== Cell mortality, cell lineage immortality ===
The ancestry of each present day cell presumably traces back, in an unbroken lineage for over 3 billion years to the origin of life. It is not actually cells that are immortal but multi-generational cell lineages.  The immortality of a cell lineage depends on the maintenance of cell division potential.  This potential may be lost in any particular lineage because of cell damage, terminal differentiation as occurs in nerve cells, or programmed cell death (apoptosis) during development.  Maintenance of cell division potential over successive generations depends on the avoidance and the accurate repair of cellular damage, particularly DNA damage.  In sexual organisms, continuity of the germline depends on the effectiveness of processes for avoiding DNA damage and repairing those DNA damages that do occur.  Sexual processes in eukaryotes, as well as in prokaryotes, provide an opportunity for effective repair of DNA damages in the germ line by homologous recombination.


=== Cell cycle phases ===
The cell cycle is a four-stage process that a cell goes through as it develops and divides. It includes Gap 1 (G1), synthesis (S), Gap 2 (G2), and mitosis (M).The cell either restarts the cycle from G1 or leaves the cycle through G0 after completing the cycle. The cell can progress from G0 through terminal differentiation.
The interphase refers to the phases of the cell cycle that occur between one mitosis and the next, and includes G1, S, and G2.


==== G1 phase ====
The size of the cell grows.
The contents of cells are replicated.


==== S phase ====
Replication of DNA
The cell replicates each of the 46 chromosomes (23 pairs).


==== G2 phase ====
The cell multiplies.
In preparation for cell division, organelles and proteins form.


==== M phase ====
After mitosis, cytokinesis occurs (cell separation)
Formation of two daughter cells that are identical


==== G0 phase ====
These cells leave G1 and enter G0, a resting stage. A cell in G0 is doing its job without actively preparing to divide.


== Pathology ==

The scientific branch that studies and diagnoses diseases on the cellular level is called cytopathology. Cytopathology is generally used on samples of free cells or tissue fragments, in contrast to the pathology branch of histopathology, which studies whole tissues. Cytopathology is commonly used to investigate diseases involving a wide range of body sites, often to aid in the diagnosis of cancer but also in the diagnosis of some infectious diseases and other inflammatory conditions. For example, a common application of cytopathology is the Pap smear, a screening test used to detect cervical cancer, and precancerous cervical lesions that may lead to cervical cancer.


== Cell cycle checkpoints and DNA damage repair system ==
The cell cycle is composed of a number of well-ordered, consecutive stages that result in cellular division. The fact that cells do not begin the next stage until the last one is finished, is a significant element of cell cycle regulation. Cell cycle checkpoints are characteristics that constitute an excellent monitoring strategy for accurate cell cycle and divisions. Cdks, associated cyclin counterparts, protein kinases, and phosphatases regulate cell growth and division from one stage to another. The cell cycle is controlled by the temporal activation of Cdks, which is governed by cyclin partner interaction, phosphorylation by particular protein kinases, and de-phosphorylation by Cdc25 family phosphatases. In response to DNA damage, a cell's DNA repair reaction is a cascade of signaling pathways that leads to checkpoint engagement, regulates, the repairing mechanism in DNA, cell cycle alterations, and apoptosis. Numerous biochemical structures, as well as processes that detect damage in DNA, are ATM and ATR, which induce the DNA repair checkpointsThe cell cycle is a sequence of activities in which cell organelles are duplicated and subsequently separated into daughter cells with precision. There are major events that happen during a cell cycle. The processes that happen in the cell cycle include cell development, replication and segregation of chromosomes.  The cell cycle checkpoints are surveillance systems that keep track of the cell cycle's integrity, accuracy, and chronology. Each checkpoint serves as an alternative cell cycle endpoint, wherein the cell's parameters are examined and only when desirable characteristics are fulfilled does the cell cycle advance through the distinct steps.The cell cycle's goal is to precisely copy each organism's DNA and afterwards equally split the cell and its components between the two new cells. Four main stages occur in the eukaryotes. In G1, the cell is usually active and continues to grow rapidly, while in G2, the cell growth continues while protein molecules become ready for separation. These are not dormant times; they are when cells gain mass, integrate growth factor receptors, establish a replicated genome, and prepare for chromosome segregation. DNA replication is restricted to a separate Synthesis in eukaryotes, which is also known as the S-phase. During mitosis, which is also known as the M-phase, the segregation of the chromosomes occur.  DNA, like every other molecule, is capable of undergoing a wide range of chemical reactions. Modifications in DNA's sequence, on the other hand, have a considerably bigger impact than modifications in other cellular constituents like RNAs or proteins because DNA acts as a permanent copy of the cell genome. When erroneous nucleotides are incorporated during DNA replication, mutations can occur. The majority of DNA damage is fixed by removing the defective bases and then re-synthesizing the excised area. On the other hand, some DNA lesions can be mended by reversing the damage, which may be a more effective method of coping with common types of DNA damage. Only a few forms of DNA damage are mended in this fashion, including pyrimidine dimers caused by ultraviolet (UV) light changed by the insertion of methyl or ethyl groups at the purine ring's O6 position.


== Mitochondrial membrane dynamics ==
Mitochondria are commonly referred to as the cell's ""powerhouses"" because of their capacity to effectively produce ATP which is essential to maintain cellular homeostasis and metabolism. Moreover, researchers have gained a better knowledge of mitochondria's significance in cell biology because of the discovery of cell signaling pathways by mitochondria which are crucial platforms for cell function regulation such as apoptosis. Its physiological adaptability is strongly linked to the cell mitochondrial channel's ongoing reconfiguration through a range of mechanisms known as mitochondrial membrane dynamics, which include endomembrane fusion and fragmentation (separation) as well as ultrastructural membrane remodeling. As a result, mitochondrial dynamics regulate and frequently choreograph not only metabolic but also complicated cell signaling processes such as cell pluripotent stem cells, proliferation, maturation, aging, and mortality. Mutually, post-translational alterations of mitochondrial apparatus and the development of transmembrane contact sites among mitochondria and other structures, which both have the potential to link signals from diverse routes that affect mitochondrial membrane dynamics substantially,  Mitochondria are wrapped by two membranes: an inner mitochondrial membrane (IMM) and an outer mitochondrial membrane (OMM), each with a distinctive function and structure, which parallels their dual role as cellular powerhouses and signaling organelles. The inner mitochondrial membrane divides the mitochondrial lumen into two parts: the inner border membrane, which runs parallel to the OMM, and the cristae, which are deeply twisted, multinucleated invaginations that give room for surface area enlargement and house the mitochondrial respiration apparatus. The outer mitochondrial membrane, on the other hand, is soft and permeable. It, therefore, acts as a foundation for cell signaling pathways to congregate, be deciphered, and be transported into mitochondria. Furthermore, the OMM connects to other cellular organelles, such as the endoplasmic reticulum (ER), lysosomes, endosomes, and the plasma membrane. Mitochondria play a wide range of roles in cell biology, which is reflected in their morphological diversity. Ever since the beginning of the mitochondrial study, it has been well documented that mitochondria can have a variety of forms, with both their general and ultra-structural morphology varying greatly among cells, during the cell cycle, and in response to metabolic or cellular cues. Mitochondria can exist as independent organelles or as part of larger systems; they can also be unequally distributed in the cytosol through regulated mitochondrial transport and placement to meet the cell's localized energy requirements. Mitochondrial dynamics refers to the adaptive and variable aspect of mitochondria, including their shape and subcellular distribution.


== Autophagy ==

Autophagy is a self-degradative mechanism that regulates energy sources during growth and reaction to dietary stress. Autophagy also cleans up after itself, clearing aggregated proteins, cleaning damaged structures including mitochondria and endoplasmic reticulum and eradicating intracellular infections. Additionally, autophagy has antiviral and antibacterial roles within the cell, and it is involved at the beginning of distinctive and adaptive immune responses to viral and bacterial contamination. Some viruses include virulence proteins that prevent autophagy, while others utilize autophagy elements for intracellular development or cellular splitting.  Macro autophagy, micro autophagy, and chaperon-mediated autophagy are the three basic types of autophagy. When macro autophagy is triggered, an exclusion membrane incorporates a section of the cytoplasm, generating the autophagosome, a distinctive double-membraned organelle. The autophagosome then joins the lysosome to create an autolysosome, with lysosomal enzymes degrading the components. In micro autophagy, the lysosome or vacuole engulfs a piece of the cytoplasm by invaginating or protruding the lysosomal membrane to enclose the cytosol or organelles. The chaperone-mediated autophagy (CMA) protein quality assurance by digesting oxidized and altered proteins under stressful circumstances and supplying amino acids through protein denaturation. Autophagy is the primary intrinsic degradative system for peptides, fats, carbohydrates, and other cellular structures. In both physiologic and stressful situations, this cellular progression is vital for upholding the correct cellular balance. Autophagy instability leads to a variety of illness symptoms, including inflammation, biochemical disturbances, aging, and neurodegenerative, due to its involvement in controlling cell integrity. The modification of the autophagy-lysosomal networks is a typical hallmark of many neurological and muscular illnesses. As a result, autophagy has been identified as a potential strategy for the prevention and treatment of various disorders. Many of these disorders are prevented or improved by consuming polyphenol in the meal. As a result, natural compounds with the ability to modify the autophagy mechanism are seen as a potential therapeutic option. The creation of the double membrane (phagophore), which would be known as nucleation, is the first step in macro-autophagy. The phagophore approach indicates dysregulated polypeptides or defective organelles that come from the cell membrane, Golgi apparatus, endoplasmic reticulum, and mitochondria. With the conclusion of the autophagocyte, the phagophore's enlargement comes to an end. The auto-phagosome combines with the lysosomal vesicles to formulate an auto-lysosome that degrades the encapsulated substances, referred to as phagocytosis.


== Notable cell biologists ==


== See also ==
The American Society for Cell Biology
Cell biophysics
Cell disruption
Cell physiology
Cellular adaptation
Cellular microbiology
Institute of Molecular and Cell Biology (disambiguation)
Meiomitosis
Organoid
Outline of cell biology


== Notes ==


== References ==
Penner-Hahn, James E. (2013). ""Chapter 2. Technologies for Detecting Metals in Single Cells. Section 4. Intrinsic X-Ray Fluorescence"".  In Bani, Lucia (ed.). Metallomics and the Cell. Metal Ions in Life Sciences. Vol. 12. Springer. pp. 15–40. doi:10.1007/978-94-007-5561-1_2. ISBN 978-94-007-5560-4. PMID 23595669.electronic-book ISBN 978-94-007-5561-1 ISSN 1559-0836electronic-ISSN 1868-0402
Cell and Molecular Biology by Karp 5th Ed., ISBN 0-471-46580-1
 This article incorporates public domain material from Science Primer. NCBI. Archived from the original on 8 December 2009.


== External links ==

 Media related to Cell biology at Wikimedia Commons
Cell Biology at Curlie
Aging Cell
""Francis Harry Compton Crick (1916-2004)"" by A. Andrei at the Embryo Project Encyclopedia
""Biology Resource By Professor Lin.""",2671431,1536,"All articles with unsourced statements, Articles with BNF identifiers, Articles with Curlie links, Articles with GND identifiers, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with NDL identifiers, Articles with NKC identifiers, Articles with short description, Articles with unsourced statements from July 2022, CS1 errors: missing periodical, Cell biology, Commons category link is on Wikidata, Molecular biology, Short description is different from Wikidata, Use dmy dates from April 2019, Wikipedia articles incorporating a citation from the 1911 Encyclopaedia Britannica with Wikisource reference, Wikipedia articles incorporating text from the United States National Library of Medicine",1133003691,biology
https://en.wikipedia.org/wiki/Organ_(biology),Organ (biology),"In biology, an organ is a collection of tissues joined in a structural unit to serve a common function. In the hierarchy of life, an organ lies between tissue and an organ system. Tissues are formed from same type cells to act together in a function. Tissues of different types combine to form an organ which has a specific function. The intestinal wall for example is formed by epithelial tissue and smooth muscle tissue. Two or more organs working together in the execution of a specific body function form an organ system, also called a biological system or body system.
An organ's tissues can be broadly categorized as parenchyma, the functional tissue, and stroma, the structural tissue with supportive, connective, or ancillary functions. For example, the gland's tissue that makes the hormones is the parenchyma, whereas the stroma includes the nerves that innervate the parenchyma, the blood vessels that oxygenate and nourish it and carry away its metabolic wastes, and the connective tissues that provide a suitable place for it to be situated and anchored. The main tissues that make up an organ tend to have common embryologic origins, such as arising from the same germ layer. Organs exist in most multicellular organisms. In single-celled organisms such as bacteria, the functional analogue of an organ is known as an organelle. In plants, there are three main organs.In the study of anatomy, viscera (singular viscus) refers to the internal organs of the abdominal, thoracic, and pelvic cavities. The abdominal organs may be classified as solid organs, or hollow organs. The solid organs are the liver, pancreas, spleen, kidneys, and adrenal glands. The hollow organs of the abdomen are the stomach, intestines, gallbladder, bladder, and rectum. In the thoracic cavity the heart is a hollow, muscular organ.The number of organs in any organism depends on the definition used. By one widely adopted definition, 79 organs have been identified in the human body.","In biology, an organ is a collection of tissues joined in a structural unit to serve a common function. In the hierarchy of life, an organ lies between tissue and an organ system. Tissues are formed from same type cells to act together in a function. Tissues of different types combine to form an organ which has a specific function. The intestinal wall for example is formed by epithelial tissue and smooth muscle tissue. Two or more organs working together in the execution of a specific body function form an organ system, also called a biological system or body system.
An organ's tissues can be broadly categorized as parenchyma, the functional tissue, and stroma, the structural tissue with supportive, connective, or ancillary functions. For example, the gland's tissue that makes the hormones is the parenchyma, whereas the stroma includes the nerves that innervate the parenchyma, the blood vessels that oxygenate and nourish it and carry away its metabolic wastes, and the connective tissues that provide a suitable place for it to be situated and anchored. The main tissues that make up an organ tend to have common embryologic origins, such as arising from the same germ layer. Organs exist in most multicellular organisms. In single-celled organisms such as bacteria, the functional analogue of an organ is known as an organelle. In plants, there are three main organs.In the study of anatomy, viscera (singular viscus) refers to the internal organs of the abdominal, thoracic, and pelvic cavities. The abdominal organs may be classified as solid organs, or hollow organs. The solid organs are the liver, pancreas, spleen, kidneys, and adrenal glands. The hollow organs of the abdomen are the stomach, intestines, gallbladder, bladder, and rectum. In the thoracic cavity the heart is a hollow, muscular organ.The number of organs in any organism depends on the definition used. By one widely adopted definition, 79 organs have been identified in the human body.


== Animals ==

Except for placozoans, multicellular animals including humans have a variety of organ systems. These specific systems are widely studied in human anatomy. The functions of these organ systems often share significant overlap. For instance, the nervous and endocrine system both operate via a shared organ, the hypothalamus. For this reason, the two systems are combined and studied as the neuroendocrine system. The same is true for the musculoskeletal system because of the relationship between the muscular and skeletal systems.

Cardiovascular system: pumping and channeling blood to and from the body and lungs with heart, blood and blood vessels.
Digestive system: digestion and processing food with salivary glands, esophagus, stomach, liver, gallbladder, pancreas, intestines, colon, rectum and anus.
Endocrine system: communication within the body using hormones made by endocrine glands such as the hypothalamus, pituitary gland, pineal body or pineal gland, thyroid, parathyroids and adrenals, i.e., adrenal glands.
Excretory system: kidneys, ureters, bladder and urethra involved in fluid balance, electrolyte balance and excretion of urine.
Lymphatic system: structures involved in the transfer of lymph between tissues and the blood stream, the lymph and the nodes and vessels that transport it including the immune system: defending against disease-causing agents with leukocytes, tonsils, adenoids, thymus and spleen.
Integumentary system: skin, hair and nails of mammals. Also scales of fish, reptiles, and birds, and feathers of birds.
Muscular system: movement with muscles.
Nervous system: collecting, transferring and processing information with brain, spinal cord and nerves.
Reproductive system: the sex organs, such as ovaries, fallopian tubes, uterus, vulva, vagina, testes, vas deferens, seminal vesicles, prostate and penis.
Respiratory system: the organs used for breathing, the pharynx, larynx, trachea, bronchi, lungs and diaphragm.
Skeletal system: structural support and protection with bones, cartilage, ligaments and tendons.


=== Viscera ===
In the study of anatomy, viscera (singular viscus) refers to the internal organs of the abdominal, thoracic, and pelvic cavities. The abdominal organs may be classed as solid organs, or hollow organs. The solid organs are the liver, pancreas, spleen, kidneys, and adrenal glands. The hollow organs are the stomach, intestines, gallbladder, bladder, and rectum.  In the thoracic cavity the heart is a hollow, muscular organ. Splanchnology is the study of the viscera. The term ""visceral"" is contrasted with the term ""parietal"", meaning ""of or relating to the wall of a body part, organ or cavity"" The two terms are often used in describing a membrane or piece of connective tissue, referring to the opposing sides.


=== Origin and evolution ===

The organ level of organisation in animals can be first detected in flatworms and the more derived phyla, i.e. the bilaterians. The less-advanced taxa (i.e. Placozoa, Porifera, Ctenophora and Cnidaria) do not show consolidation of their tissues into organs.
More complex animals are composed of different organs, which have evolved over time. For example, the liver and heart evolved in the chordates about 550-500 million years ago, while the gut and brain are even more ancient, arising in the ancestor of vertebrates, insects, molluscs, and worms about 700-650 million years ago.
Given the ancient origin of most vertebrate organs, researchers have looked for model systems, where organs have evolved more recently, and ideally have evolved multiple times independently. An outstanding model for this kind of research is the placenta, which has evolved more than 100 times independently in vertebrates, has evolved relatively recently in some lineages, and exists in intermediate forms in extant taxa. Studies on the evolution of the placenta have identified a variety of genetic and physiological processes that contribute to the origin and evolution of organs, these include the re-purposing of existing animal tissues, the acquisition of new functional properties by these tissues, and novel interactions of distinct tissue types.


== Plants ==

The study of plant organs is covered in plant morphology. Organs of plants can be divided into vegetative and reproductive. Vegetative plant organs include roots, stems, and leaves. The reproductive organs are variable. In flowering plants, they are represented by the flower, seed and fruit. In conifers, the organ that bears the reproductive structures is called a cone. In other divisions (phyla) of plants, the reproductive organs are called strobili, in Lycopodiophyta, or simply gametophores in mosses. Common organ system designations in plants include the differentiation of shoot and root. All parts of the plant above ground (in non-epiphytes), including the functionally distinct leaf and flower organs, may be classified together as the shoot organ system.The vegetative organs are essential for maintaining the life of a plant. While there can be 11 organ systems in animals, there are far fewer in plants, where some perform the vital functions, such as photosynthesis, while the reproductive organs are essential in reproduction. However, if there is asexual vegetative reproduction, the vegetative organs are those that create the new generation of plants (see clonal colony).


== Society and culture ==
Many societies have a system for organ donation, in which a living or deceased donor's organ are transplanted into a person with a failing organ. The transplantation of larger solid organs often requires immunosuppression to prevent organ rejection or graft-versus-host disease.
There is considerable interest throughout the world in creating laboratory-grown or artificial organs.


=== Organ transplants ===
Beginning in the 20th century organ transplants began to take place as scientists knew more about the anatomy of organs. These came later in time as procedures were often dangerous and difficult.  Both the source and method of obtaining the organ to transplant are major ethical issues to consider, and because organs as resources for transplant are always more limited than demand for them, various notions of justice, including distributive justice, are developed in the ethical analysis.  This situation continues as long as transplantation relies upon organ donors rather than technological innovation, testing, and industrial manufacturing.


== History ==

The English word ""organ"" dates back to the twelfth century and refers to any musical instrument. By the late 14th century, the musical term's meaning had narrowed to refer specifically to the keyboard-based instrument. At the same time, a second meaning arose, in reference to a ""body part adapted to a certain function"".Plant organs are made from tissue composed of different types of tissue. The three tissue types are ground, vascular, and dermal. When three or more organs are present, it is called an organ system.The adjective visceral, also splanchnic, is used for anything pertaining to the internal organs. Historically, viscera of animals were examined by Roman pagan priests like the haruspices or the augurs in order to divine the future by their shape, dimensions or other factors. This practice remains an important ritual in some remote, tribal societies.
The term ""visceral"" is contrasted with the term ""parietal"", meaning ""of or relating to the wall of a body part, organ or cavity"" The two terms are often used in describing a membrane or piece of connective tissue, referring to the opposing sides.


=== Antiquity ===
Aristotle used the word frequently in his philosophy, both to describe the organs of plants or animals (e.g. the roots of a tree, the heart or liver of an animal), and to describe more abstract ""parts"" of an interconnected whole (e.g. his logical works, taken as a whole, are referred to as the Organon).Some alchemists (e.g. Paracelsus) adopted the Hermetic Qabalah assignment between the seven vital organs and the seven classical planets as follows:


== See also ==
Organoid
Organ-on-a-chip


== References ==


== External links ==
 Media related to Organs (anatomy) at Wikimedia Commons",1224820,2016,"All articles to be expanded, All articles with unsourced statements, Articles to be expanded from February 2018, Articles using small message boxes, Articles with BNF identifiers, Articles with GND identifiers, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with short description, Articles with unsourced statements from August 2021, Articles with unsourced statements from February 2018, Articles with unsourced statements from May 2021, Commons category link is on Wikidata, Levels of organization (Biology), Organ systems, Organs (anatomy), Short description matches Wikidata, Wikipedia articles needing page number citations from May 2015",1132774654,biology
https://en.wikipedia.org/wiki/Hybrid_(biology),Hybrid (biology),"In biology, a hybrid is the offspring resulting from combining the qualities of two organisms of different breeds, varieties, species or genera through sexual reproduction. Generally, it means that each cell has genetic material from two different organisms, whereas an individual where some cells are derived from a different organism is called a chimera. Hybrids are not always intermediates between their parents (such as in blending inheritance), but can show hybrid vigor, sometimes growing larger or taller than either parent. The concept of a hybrid is interpreted differently in animal and plant breeding, where there is interest in the individual parentage. In genetics, attention is focused on the numbers of chromosomes. In taxonomy, a key question is how closely related the parent species are.
Species are reproductively isolated by strong barriers to hybridisation, which include genetic and morphological differences, differing times of fertility, mating behaviors and cues, and physiological rejection of sperm cells or the developing embryo. Some act before fertilization and others after it. Similar barriers exist in plants, with differences in flowering times, pollen vectors, inhibition of pollen tube growth, somatoplastic sterility, cytoplasmic-genic male sterility and the structure of the chromosomes. A few animal species and many plant species, however, are the result of hybrid speciation, including important crop plants such as wheat, where the number of chromosomes has been doubled.
Human impact on the environment has resulted in an increase in the interbreeding between regional species, and the proliferation of introduced species worldwide has also resulted in an increase in hybridisation. This genetic mixing may threaten many species with extinction, while genetic erosion from monoculture in crop plants may be damaging the gene pools of many species for future breeding.  A form of often intentional human-mediated hybridisation is the crossing of wild and domesticated species. This is common in both traditional horticulture and modern agriculture; many commercially useful fruits, flowers, garden herbs, and trees have been produced by hybridisation. One such flower, Oenothera lamarckiana, was central to early genetics research into mutationism and polyploidy.  It is also more occasionally done in the livestock and pet trades; some well-known wild × domestic hybrids are beefalo and wolfdogs.  Human selective breeding of domesticated animals and plants has resulted in the development of distinct breeds (usually called cultivars in reference to plants); crossbreeds between them (without any wild stock) are sometimes also imprecisely referred to as ""hybrids"".
Hybrid humans existed in prehistory. For example, Neanderthals and anatomically modern humans are thought to have interbred as recently as 40,000 years ago.
Mythological hybrids appear in human culture in forms as diverse as the Minotaur, blends of animals, humans and mythical beasts such as centaurs and sphinxes, and the Nephilim of the Biblical apocrypha described as the wicked sons of fallen angels and attractive women.","In biology, a hybrid is the offspring resulting from combining the qualities of two organisms of different breeds, varieties, species or genera through sexual reproduction. Generally, it means that each cell has genetic material from two different organisms, whereas an individual where some cells are derived from a different organism is called a chimera. Hybrids are not always intermediates between their parents (such as in blending inheritance), but can show hybrid vigor, sometimes growing larger or taller than either parent. The concept of a hybrid is interpreted differently in animal and plant breeding, where there is interest in the individual parentage. In genetics, attention is focused on the numbers of chromosomes. In taxonomy, a key question is how closely related the parent species are.
Species are reproductively isolated by strong barriers to hybridisation, which include genetic and morphological differences, differing times of fertility, mating behaviors and cues, and physiological rejection of sperm cells or the developing embryo. Some act before fertilization and others after it. Similar barriers exist in plants, with differences in flowering times, pollen vectors, inhibition of pollen tube growth, somatoplastic sterility, cytoplasmic-genic male sterility and the structure of the chromosomes. A few animal species and many plant species, however, are the result of hybrid speciation, including important crop plants such as wheat, where the number of chromosomes has been doubled.
Human impact on the environment has resulted in an increase in the interbreeding between regional species, and the proliferation of introduced species worldwide has also resulted in an increase in hybridisation. This genetic mixing may threaten many species with extinction, while genetic erosion from monoculture in crop plants may be damaging the gene pools of many species for future breeding.  A form of often intentional human-mediated hybridisation is the crossing of wild and domesticated species. This is common in both traditional horticulture and modern agriculture; many commercially useful fruits, flowers, garden herbs, and trees have been produced by hybridisation. One such flower, Oenothera lamarckiana, was central to early genetics research into mutationism and polyploidy.  It is also more occasionally done in the livestock and pet trades; some well-known wild × domestic hybrids are beefalo and wolfdogs.  Human selective breeding of domesticated animals and plants has resulted in the development of distinct breeds (usually called cultivars in reference to plants); crossbreeds between them (without any wild stock) are sometimes also imprecisely referred to as ""hybrids"".
Hybrid humans existed in prehistory. For example, Neanderthals and anatomically modern humans are thought to have interbred as recently as 40,000 years ago.
Mythological hybrids appear in human culture in forms as diverse as the Minotaur, blends of animals, humans and mythical beasts such as centaurs and sphinxes, and the Nephilim of the Biblical apocrypha described as the wicked sons of fallen angels and attractive women.


== Etymology ==

The term hybrid is derived from Latin hybrida, used for crosses such as of a tame sow and a wild boar. The term came into popular use in English in the 19th century, though examples of its use have been found from the early 17th century. Conspicuous hybrids are popularly named with portmanteau words, starting in the 1920s with the breeding of tiger–lion hybrids (liger and tigon).


== As seen by different disciplines ==


=== Animal and plant breeding ===
From the point of view of animal and plant breeders, there are several kinds of hybrid formed from crosses within a species, such as between different breeds.
Single cross hybrids result from the cross between two true-breeding organisms which produces an F1 hybrid (first filial generation). The cross between two different homozygous lines produces an F1 hybrid that is heterozygous; having two alleles, one contributed by each parent and typically one is dominant and the other recessive. Typically, the F1 generation is also phenotypically homogeneous, producing offspring that are all similar to each other.
Double cross hybrids result from the cross between two different F1 hybrids (i.e., there are four unrelated grandparents).
Three-way cross hybrids result from the cross between an F1 hybrid and an inbred line. Triple cross hybrids result from the crossing of two different three-way cross hybrids. Top cross (or ""topcross"") hybrids result from the crossing of a top quality or pure-bred male and a lower quality female, intended to improve the quality of the offspring, on average.Population hybrids result from the crossing of plants or animals in one population with those of another population. These include interspecific hybrids or crosses between different breeds.In horticulture, the term stable hybrid is used to describe an annual plant that, if grown and bred in a small monoculture free of external pollen (e.g., an air-filtered greenhouse) produces offspring that are ""true to type"" with respect to phenotype; i.e., a true-breeding organism.


=== Biogeography ===

Hybridisation can occur in the hybrid zones where the geographical ranges of species, subspecies, or distinct genetic lineages overlap. For example, the butterfly Limenitis arthemis has two major subspecies in North America, L. a. arthemis (the white admiral) and L. a. astyanax (the red-spotted purple). The white admiral has a bright, white band on its wings, while the red-spotted purple has cooler blue-green shades. Hybridisation occurs between a narrow area across New England, southern Ontario, and the Great Lakes, the ""suture region"". It is at these regions that the subspecies were formed. Other hybrid zones have formed between described species of plants and animals.


=== Genetics ===

From the point of view of genetics, several different kinds of hybrid can be distinguished.
A genetic hybrid carries two different alleles of the same gene, where for instance one allele may code for a lighter coat colour than the other. A structural hybrid results from the fusion of gametes that have differing structure in at least one chromosome, as a result of structural abnormalities. A numerical hybrid results from the fusion of gametes having different haploid numbers of chromosomes. A permanent hybrid results when only the heterozygous genotype occurs, as in Oenothera lamarckiana, because all homozygous combinations are lethal. In the early history of genetics, Hugo de Vries supposed these were caused by mutation.


=== Taxonomy ===

From the point of view of taxonomy, hybrids differ according to their parentage.
Hybrids between different subspecies (such as between the Dog and Eurasian wolf) are called intra-specific hybrids. Interspecific hybrids are the offspring from interspecies mating; these sometimes result in hybrid speciation. Intergeneric hybrids result from matings between different genera, such as between sheep and goats. Interfamilial hybrids, such as between chickens and guineafowl or pheasants, are reliably described but extremely rare. Interordinal hybrids (between different orders) are few, but have been engineered between the sea urchin Strongylocentrotus purpuratus (female) and the sand dollar Dendraster excentricus (male).


== Biology ==


=== Expression of parental traits ===

When two distinct types of organisms breed with each other, the resulting hybrids typically have intermediate traits (e.g., one plant parent has red flowers, the other has white, and the hybrid, pink flowers). Commonly, hybrids also combine traits seen only separately in one parent or the other (e.g., a bird hybrid might combine the yellow head of one parent with the orange belly of the other).


=== Mechanisms of reproductive isolation ===

Interspecific hybrids are bred by mating individuals from two species, normally from within the same genus. The offspring display traits and characteristics of both parents, but are often sterile, preventing gene flow between the species. Sterility is often attributed to the different number of chromosomes between the two species. For example, donkeys have 62 chromosomes, horses have 64 chromosomes, and mules or hinnies have 63 chromosomes. Mules, hinnies, and other normally sterile interspecific hybrids cannot produce viable gametes, because differences in chromosome structure prevent appropriate pairing and segregation during meiosis, meiosis is disrupted, and viable sperm and eggs are not formed. However, fertility in female mules has been reported with a donkey as the father.A variety of mechanisms limit the success of hybridisation, including the large genetic difference between most species. Barriers include morphological differences, differing times of fertility, mating behaviors and cues, and physiological rejection of sperm cells or the developing embryo. Some act before fertilization; others after it.In plants, some barriers to hybridisation include blooming period differences, different pollinator vectors, inhibition of pollen tube growth, somatoplastic sterility, cytoplasmic-genic male sterility and structural differences of the chromosomes.


=== Speciation ===

A few animal species are the result of hybridization. The Lonicera fly is a natural hybrid. The American red wolf appears to be a hybrid of the gray wolf and the coyote, although its taxonomic status has been a subject of controversy. The European edible frog is a semi-permanent hybrid between pool frogs and marsh frogs; its population requires the continued presence of at least one of the parent species. Cave paintings indicate that the European bison is a natural hybrid of the aurochs and the steppe bison.Plant hybridization is more commonplace compared to animal hybridization. Many crop species are hybrids, including notably the polyploid wheats: some have four sets of chromosomes (tetraploid) or six (hexaploid), while other wheat species have (like most eukaryotic organisms) two sets (diploid), so hybridization events likely involved the doubling of chromosome sets, causing immediate genetic isolation.Hybridization may be important in speciation in some plant groups. However, homoploid hybrid speciation (not increasing the number of sets of chromosomes) may be rare: by 1997, only 8 natural examples had been fully described. Experimental studies suggest that hybridization offers a rapid route to speciation, a prediction confirmed by the fact that early generation hybrids and ancient hybrid species have matching genomes, meaning that once hybridization has occurred, the new hybrid genome can remain stable.Many hybrid zones are known where the ranges of two species meet, and hybrids are continually produced in great numbers. These hybrid zones are useful as biological model systems for studying the mechanisms of speciation. Recently DNA analysis of a bear shot by a hunter in the North West Territories confirmed the existence of naturally-occurring and fertile grizzly–polar bear hybrids.


=== Hybrid vigour ===

Hybridization between reproductively isolated species often results in hybrid offspring with lower fitness than either parental. However, hybrids are not, as might be expected, always intermediate between their parents (as if there were blending inheritance), but are sometimes stronger or perform better than either parental lineage or variety, a phenomenon called heterosis, hybrid vigour, or heterozygote advantage. This is most common with plant hybrids. A transgressive phenotype is a phenotype that displays more extreme characteristics than either of the parent lines. Plant breeders use several techniques to produce hybrids, including line breeding and the formation of complex hybrids. An economically important example is hybrid maize (corn), which provides a considerable seed yield advantage over open pollinated varieties. Hybrid seed dominates the commercial maize seed market in the United States, Canada and many other major maize-producing countries.In a hybrid, any trait that falls outside the range of parental variation (and is thus not simply intermediate between its parents) is considered heterotic. Positive heterosis produces more robust hybrids, they might be stronger or bigger; while the term negative heterosis refers to weaker or smaller hybrids. Heterosis is common in both animal and plant hybrids. For example, hybrids between a lion and a tigress (""ligers"") are much larger than either of the two progenitors, while ""tigons"" (lioness × tiger) are smaller. Similarly, the hybrids between the common pheasant (Phasianus colchicus) and domestic fowl (Gallus gallus) are larger than either of their parents, as are those produced between the common pheasant and hen golden pheasant (Chrysolophus pictus). Spurs are absent in hybrids of the former type, although present in both parents.


== Human influence ==


=== Anthropogenic hybridization ===
Hybridization is greatly influenced by human impact on the environment, through effects such as habitat fragmentation and species introductions. Such impacts make it difficult to conserve the genetics of populations undergoing introgressive hybridization. Humans have introduced species worldwide to environments for a long time, both intentionally for purposes such as biological control, and unintentionally, as with accidental escapes of individuals. Introductions can drastically affect populations, including through hybridization.


=== Management ===

There is a kind of continuum with three semi-distinct categories dealing with anthropogenic hybridization: hybridization without introgression, hybridization with widespread introgression (backcrossing with one of the parent species), and hybrid swarms (highly variable populations with much interbreeding as well as backcrossing with the parent species). Depending on where a population falls along this continuum, the management plans for that population will change. Hybridization is currently an area of great discussion within wildlife management and habitat management. Global climate change is creating other changes such as difference in population distributions which are indirect causes for an increase in anthropogenic hybridization.Conservationists disagree on when is the proper time to give up on a population that is becoming a hybrid swarm, or to try and save the still existing pure individuals. Once a population becomes a complete mixture, the goal becomes to conserve those hybrids to avoid their loss. Conservationists treat each case on its merits, depending on detecting hybrids within the population. It is nearly impossible to formulate a uniform hybridization policy, because hybridization can occur beneficially when it occurs ""naturally"", and when hybrid swarms are the only remaining evidence of prior species, they need to be conserved as well.


=== Genetic mixing and extinction ===

Regionally developed ecotypes can be threatened with extinction when new alleles or genes are introduced that alter that ecotype. This is sometimes called genetic mixing. Hybridization and introgression, which can happen in natural and hybrid populations, of new genetic material can lead to the replacement of local genotypes if the hybrids are more fit and have breeding advantages over the indigenous ecotype or species. These hybridization events can result from the introduction of non-native genotypes by humans or through habitat modification, bringing previously isolated species into contact. Genetic mixing can be especially detrimental for rare species in isolated habitats, ultimately affecting the population to such a degree that none of the originally genetically distinct population remains.


=== Effect on biodiversity and food security ===

In agriculture and animal husbandry, the Green Revolution's use of conventional hybridization increased yields by breeding ""high-yielding varieties"". The replacement of locally indigenous breeds, compounded with unintentional cross-pollination and crossbreeding (genetic mixing), has reduced the gene pools of various wild and indigenous breeds resulting in the loss of genetic diversity. Since the indigenous breeds are often well-adapted to local extremes in climate and have immunity to local pathogens, this can be a significant genetic erosion of the gene pool for future breeding. Therefore, commercial plant geneticists strive to breed ""widely adapted"" cultivars to counteract this tendency.


== Different taxa ==


=== In animals ===


==== Mammals ====
Familiar examples of equid hybrids are the mule, a cross between a female horse and a male donkey, and the hinny, a cross between a female donkey and a male horse. Pairs of complementary types like the mule and hinny are called reciprocal hybrids. Polar bears and brown bears are another case of a hybridizing species pairs, and introgression among non-sister species of bears appears to have shaped the Ursidae family tree. Among many other mammal crosses are hybrid camels, crosses between a bactrian camel and a dromedary. There are many examples of felid hybrids, including the liger. The oldest known animal hybrid bred by humans is the kunga equid hybrid produced as a draft animal and status symbol 4500 years ago to ancient Syria.The first known instance of hybrid speciation in marine mammals was discovered in 2014. The clymene dolphin (Stenella clymene) is a hybrid of two Atlantic species, the spinner and striped dolphins. In 2019, scientists confirmed that a skull found 30 years earlier was a hybrid between the beluga whale and narwhal; dubbed the narluga.


==== Birds ====

Cagebird breeders sometimes breed bird hybrids known as mules between species of finch, such as goldfinch × canary.


==== Amphibians ====
Among amphibians, Japanese giant salamanders and Chinese giant salamanders have created hybrids that threaten the survival of Japanese giant salamanders because of competition for similar resources in Japan.


==== Fish ====
Among fish, a group of about fifty natural hybrids between Australian blacktip shark and the larger common blacktip shark was found by Australia's eastern coast in 2012.Russian sturgeon and American paddlefish were hybridized in captivity when sperm from the paddlefish and eggs from the sturgeon were combined, unexpectedly resulting in viable offspring. This hybrid is called a sturddlefish.


==== Cephalochordates ====
The two genera Asymmetron and Branchiostoma are able to produce viable hybrid offspring, even if none have lived into adulthood so far, despite the parents' common ancestor living tens of millions of years ago.


==== Insects ====
Among insects, so-called killer bees were accidentally created during an attempt to breed a strain of bees that would both produce more honey and be better adapted to tropical conditions. It was done by crossing a European honey bee and an African bee.The Colias eurytheme and C. philodice butterflies have retained enough genetic compatibility to produce viable hybrid offspring. Hybrid speciation may have produced the diverse Heliconius butterflies, but that is disputed.The two closely related harvester ant species Pogonomyrmex barbatus and Pogonomyrmex rugosus have evolved to depend on hybridization. When a queen fertilize her eggs with sperm from males of her own species, the offspring is always new queens. And when she fertilize the eggs with sperm from males of the other species, the offspring is always sterile worker ants (and because ants are haplodiploid, unfertilized eggs become males). Without mating with males of the other species, the queens are unable to produce workers, and will fail to establish a colony of their own.

		
		


=== In plants ===

Plant species hybridize more readily than animal species, and the resulting hybrids are fertile more often. Many plant species are the result of hybridization, combined with polyploidy, which duplicates the chromosomes. Chromosome duplication allows orderly meiosis and so viable seed can be produced.Plant hybrids are generally given names that include an ""×"" (not in italics), such as Platanus × acerifolia for the London plane, a natural hybrid of P. orientalis (oriental plane) and P. occidentalis (American sycamore). The parent's names may be kept in their entirety, as seen in Prunus persica × Prunus americana, with the female parent's name given first, or if not known, the parent's names given alphabetically.Plant species that are genetically compatible may not hybridize in nature for various reasons, including geographical isolation, differences in flowering period, or differences in pollinators. Species that are brought together by humans in gardens may hybridize naturally, or hybridization can be facilitated by human efforts, such as altered flowering period or artificial pollination. Hybrids are sometimes created by humans to produce improved plants that have some of the characteristics of each of the parent species. Much work is now being done with hybrids between crops and their wild relatives to improve disease-resistance or climate resilience for both agricultural and horticultural crops.Some crop plants are hybrids from different genera (intergeneric hybrids), such as Triticale, × Triticosecale, a wheat–rye hybrid. Most modern and ancient wheat breeds are themselves hybrids; bread wheat, Triticum aestivum, is a hexaploid hybrid of three wild grasses. Several commercial fruits including loganberry (Rubus × loganobaccus) and grapefruit (Citrus × paradisi) are hybrids, as are garden herbs such as peppermint (Mentha × piperita), and trees such as the London plane (Platanus × acerifolia). Among many natural plant hybrids is Iris albicans, a sterile hybrid that spreads by rhizome division, and Oenothera lamarckiana, a flower that was the subject of important experiments by Hugo de Vries that produced an understanding of polyploidy.

		
Sterility in a non-polyploid hybrid is often a result of chromosome number; if parents are of differing chromosome pair number, the offspring will have an odd number of chromosomes, which leaves them unable to produce chromosomally-balanced gametes. While that is undesirable in a crop such as wheat, for which growing a crop that produces no seeds would be pointless, it is an attractive attribute in some fruits. Triploid bananas and watermelons are intentionally bred because they produce no seeds and are also parthenocarpic.


=== In humans ===

There is evidence of hybridisation between modern humans and other species of the genus Homo. In 2010, the Neanderthal genome project showed that 1–4% of DNA from all people living today, apart from most Sub-Saharan Africans, is of Neanderthal heritage. Analyzing the genomes of 600 Europeans and East Asians found that combining them covered 20% of the Neanderthal genome that is in the modern human population. Ancient human populations lived and interbred with Neanderthals, Denisovans, and at least one other extinct Homo species. Thus, Neanderthal and Denisovan DNA has been incorporated into human DNA by introgression.In 1998, a complete prehistorical skeleton found in Portugal, the Lapedo child, had features of both anatomically modern humans and Neanderthals. Some ancient human skulls with especially large nasal cavities and unusually shaped braincases represent human-Neanderthal hybrids. A 37,000- to 42,000-year-old human jawbone found in Romania's Oase cave contains traces of Neanderthal ancestry from only four to six generations earlier. All genes from Neanderthals in the current human population are descended from Neanderthal fathers and human mothers.


== Mythology ==

Folk tales and myths sometimes contain mythological hybrids; the Minotaur was the offspring of a human, Pasiphaë, and a white bull. More often, they are composites of the physical attributes of two or more kinds of animals, mythical beasts, and humans, with no suggestion that they are the result of interbreeding, as in the centaur (man/horse), chimera (goat/lion/snake), hippocamp (fish/horse), and sphinx (woman/lion). The Old Testament mentions a first generation of half-human hybrid giants, the Nephilim, while the apocryphal Book of Enoch describes the Nephilim as the wicked sons of fallen angels and attractive women.


== See also ==


== Notes ==


== References ==


== External links ==

Artificial Hybridisation Archived 8 March 2021 at the Wayback Machine – Artificial Hybridisation in orchids
Domestic Fowl Hybrids
Hybridisation in animals Evolution Revolution: Two Species Become One, Study Says (nationalgeographic.com)
Scientists Create Butterfly Hybrid – Creation of new species through hybridization was thought to be common only in plants, and rare in animals.
What is a human admixed embryo?",2730658,2343,"All articles with dead external links, All articles with unsourced statements, Articles containing Latin-language text, Articles with EMU identifiers, Articles with GND identifiers, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with NDL identifiers, Articles with dead external links from January 2020, Articles with permanently dead external links, Articles with short description, Articles with unsourced statements from September 2019, Biology terminology, Botanical nomenclature, Breeding, CS1: long volume value, Commons category link is on Wikidata, Evolutionary biology, Good articles, Hybrid organisms, Hybridisation (biology), Population genetics, Short description matches Wikidata, Use dmy dates from June 2020, Webarchive template wayback links",1131612637,biology
https://en.wikipedia.org/wiki/Computational_biology,Computational biology,"Computational biology refers to the use of data analysis, mathematical modeling and computational simulations to understand biological systems and relationships. An intersection of computer science, biology, and big data, the field also has foundations in applied mathematics, chemistry, and genetics. It differs from biological computing, a subfield of computer engineering which uses bioengineering to build computers.

","Computational biology refers to the use of data analysis, mathematical modeling and computational simulations to understand biological systems and relationships. An intersection of computer science, biology, and big data, the field also has foundations in applied mathematics, chemistry, and genetics. It differs from biological computing, a subfield of computer engineering which uses bioengineering to build computers.


== History ==
Bioinformatics, the analysis of informatics processes in biological systems, began in the early 1970s. At this time, research in artificial intelligence was using network models of the human brain in order to generate new algorithms. This use of biological data pushed biological researchers to use computers to evaluate and compare large data sets in their own field.By 1982, researchers shared information via punch cards. The amount of data grew exponentially by the end of the 1980s, requiring new computational methods for quickly interpreting relevant information.Perhaps the best-known example of computational biology, the Human Genome Project, officially began in 1990. By 2003, the project had mapped around 85% of the human genome, satisfying its initial goals. Work continued, however, and by 2021 level ""complete genome"" was reached with only 0.3% remaining bases covered by potential issues. The missing Y chromosome was added in January 2022.
Since the late 1990s, computational biology has become an important part of biology, leading to numerous subfields. Today, the International Society for Computational Biology recognizes 21 different 'Communities of Special Interest', each representing a slice of the larger field. In addition to helping sequence the human genome, computational biology has helped create accurate models of the human brain, map the 3D structure of genomes, and model biological systems.


== Applications ==


=== Anatomy ===

Computational anatomy is the study of anatomical shape and form at the visible or gross anatomical 
  
    
      
        50
        −
        100
        μ
      
    
    {\displaystyle 50-100\mu }
   scale of morphology. It involves the development of computational mathematical and data-analytical methods for modeling and simulating biological structures. It focuses on the anatomical structures being imaged, rather than the medical imaging devices. Due to the availability of dense 3D measurements via technologies such as magnetic resonance imaging, computational anatomy has emerged as a subfield of medical imaging and bioengineering for extracting anatomical coordinate systems at the morpheme scale in 3D.
The original formulation of computational anatomy is as a generative model of shape and form from exemplars acted upon via transformations. The diffeomorphism group is used to study different coordinate systems via coordinate transformations as generated via the Lagrangian and Eulerian velocities of flow from one anatomical configuration in 
  
    
      
        
          
            
              R
            
          
          
            3
          
        
      
    
    {\displaystyle {\mathbb {R} }^{3}}
   to another. It relates with shape statistics and morphometrics, with the distinction that diffeomorphisms are used to map coordinate systems, whose study is known as diffeomorphometry.


=== Data and modeling ===

Mathematical biology is the use of mathematical models of living organisms to examine the systems that govern structure, development, and behavior in biological systems. This entails a more theoretical approach to problems, rather than its more empirically-minded counterpart of experimental biology. Mathematical biology draws on discrete mathematics, topology (also useful for computational modeling), Bayesian statistics, linear algebra and Boolean algebra.These mathematical approaches have enabled the creation of databases and other methods for storing, retrieving, and analyzing biological data, a field known as bioinformatics. Usually, this process involves genetics and analyzing genes.
Gathering and analyzing large datasets have made room for growing research fields such as data mining, and computational biomodeling, which refers to building computer models and visual simulations of biological systems. This allows researchers to predict how such systems will react to different environments, which is useful for determining if a system can ""maintain their state and functions against external and internal perturbations"". While current techniques focus on small biological systems, researchers are working on approaches that will allow for larger networks to be analyzed and modeled. A majority of researchers believe this will be essential in developing modern medical approaches to creating new drugs and gene therapy. A useful modeling approach is to use Petri nets via tools such as esyN.Along similar lines, until recent decades theoretical ecology has largely dealt with analytic models that were detached from the statistical models used by empirical ecologists. However, computational methods have aided in developing ecological theory via simulation of ecological systems, in addition to increasing application of methods from computational statistics in ecological analyses.


=== Systems Biology ===

Systems biology consists of computing the interactions between various biological systems ranging from the cellular level to entire populations with the goal of discovering emergent properties. This process usually involves networking cell signaling and metabolic pathways. Systems biology often uses computational techniques from biological modeling and graph theory to study these complex interactions at cellular levels.


=== Evolutionary biology ===

Computational biology has assisted evolutionary biology by:

Using DNA data to reconstruct the tree of life with computational phylogenetics
Fitting population genetics models (either forward time or backward time) to DNA data to make inferences about demographic or selective history
Building population genetics models of evolutionary systems from first principles in order to predict what is likely to evolve


=== Genomics ===

Computational genomics is the study of the genomes of cells and organisms. The Human Genome Project is one example of computational genomics. This project looks to sequence the entire human genome into a set of data. Once fully implemented, this could allow for doctors to analyze the genome of an individual patient.  This opens the possibility of personalized medicine, prescribing treatments based on an individual's pre-existing genetic patterns. Researchers are looking to sequence the genomes of animals, plants, bacteria, and all other types of life.One of the main ways that genomes are compared is by sequence homology. Homology is the study of biological structures and nucleotide sequences in different organisms that come from a common ancestor. Research suggests that between 80 and 90% of genes in newly sequenced prokaryotic genomes can be identified this way.Sequence alignment is another process for comparing and detecting similarities between biological sequences or genes. Sequence alignment is useful in a number of bioinformatics applications, such as computing the longest common subsequence of two genes or comparing variants of certain diseases.An untouched project in computational genomics is the analysis of intergenic regions, which comprise roughly 97% of the human genome. Researchers are working to understand the functions of non-coding regions of the human genome through the development of computational and statistical methods and via large consortia projects such as ENCODE and the Roadmap Epigenomics Project.
Understanding how individual genes contribute to the biology of an organism at the molecular, cellular, and organism levels is known as gene ontology. The Gene Ontology Consortium's mission is to develop an up-to-date, comprehensive, computational model of biological systems, from the molecular level to larger pathways, cellular, and organism-level systems. The Gene Ontology resource provides a computational representation of current scientific knowledge about the functions of genes (or, more properly, the protein and non-coding RNA molecules produced by genes) from many different organisms, from humans to bacteria.3D genomics is a subsection in computational biology that focuses on the organization and interaction of genes within a eukaryotic cell. One method used to gather 3D genomic data is through Genome Architecture Mapping (GAM). GAM measures 3D distances of chromatin and DNA in the genome by combining cryosectioning, the process of cutting a strip from the nucleus to examine the DNA, with laser microdissection. A nuclear profile is simply this strip or slice that is taken from the nucleus. Each nuclear profile contains genomic windows, which are certain sequences of nucleotides - the base unit of DNA. GAM captures a genome network of complex, multi enhancer chromatin contacts throughout a cell.


=== Neuroscience ===

Computational neuroscience is the study of brain function in terms of the information processing properties of the nervous system. A subset of neuroscience, it looks to model the brain to examine specific aspects of the neurological system. Models of the brain include:

Realistic Brain Models: These models look to represent every aspect of the brain, including as much detail at the cellular level as possible. Realistic models provide the most information about the brain, but also have the largest margin for error. More variables in a brain model create the possibility for more error to occur. These models do not account for parts of the cellular structure that scientists do not know about. Realistic brain models are the most computationally heavy and the most expensive to implement.
Simplifying Brain Models:  These models look to limit the scope of a model in order to assess a specific physical property of the neurological system. This allows for the intensive computational problems to be solved, and reduces the amount of potential error from a realistic brain model.It is the work of computational neuroscientists to improve the algorithms and data structures currently used to increase the speed of such calculations.
Computational neuropsychiatry is an emerging field that uses mathematical and computer-assisted modeling of brain mechanisms involved in mental disorders. Several initiatives have demonstrated that computational modeling is an important contribution to understand neuronal circuits that could generate mental functions and dysfunctions.


=== Pharmacology ===

Computational pharmacology is ""the study of the effects of genomic data to find links between specific genotypes and diseases and then screening drug data"". The pharmaceutical industry requires a shift in methods to analyze drug data. Pharmacologists were able to use Microsoft Excel to compare chemical and genomic data related to the effectiveness of drugs. However, the industry has reached what is referred to as the Excel barricade. This arises from the limited number of cells accessible on a spreadsheet. This development led to the need for computational pharmacology. Scientists and researchers develop computational methods to analyze these massive data sets. This allows for an efficient comparison between the notable data points and allows for more accurate drugs to be developed.Analysts project that if major medications fail due to patents, that computational biology will be necessary to replace current drugs on the market. Doctoral students in computational biology are being encouraged to pursue careers in industry rather than take Post-Doctoral positions. This is a direct result of major pharmaceutical companies needing more qualified analysts of the large data sets required for producing new drugs.Similarly, computational oncology aims to determine the future mutations in cancer through algorithmic approaches. Research in this field has led to the use of high-throughput measurement that millions of data points using robotics and other sensing devices. This data is collected from DNA, RNA, and other biological structures. Areas of focus include determining the characteristics of tumors, analyzing molecules that are deterministic in causing cancer, and understanding how the human genome relates to the causation of tumors and cancer.


== Techniques ==
Computational biologists use a wide range of software and algorithms to carry out their research.


=== Unsupervised Learning ===
Unsupervised learning is a type of algorithm that finds patterns in unlabeled data. One example is k-means clustering, which aims to partition n data points into k clusters, in which each data point belongs to the cluster with the nearest mean. Another version is the k-medoids algorithm, which, when selecting a cluster center or cluster centroid, will pick one of its data points in the set, and not just an average of the cluster.

The algorithm follows these steps:

Randomly select k distinct data points. These are the initial clusters.
Measure the distance between each point and each of the 'k' clusters. (This is the distance of the points from each point k).
Assign each point to the nearest cluster.
Find the center of each cluster (medoid).
Repeat until the clusters no longer change.
Assess the quality of the clustering by adding up the variation within each cluster.
Repeat the processes with different values of k.
Pick the best value for 'k' by finding the ""elbow"" in the plot of which k value has the lowest variance.One example of this in biology is used in the 3D mapping of a genome. Information of a mouse's HIST1 region of chromosome 13 is gathered from Gene Expression Omnibus. This information contains data on which nuclear profiles show up in certain genomic regions. With this information, the Jaccard distance can be used to find a normalized distance between all the loci.


=== Graph Analytics ===
Graph analytics, or network analysis, is the study of graphs that represent connections between different objects. Graphs can represent all kinds of networks in biology such as Protein-protein interaction networks, regulatory networks, Metabolic and biochemical networks and much more. There are many ways to analyze these networks. One of which is looking at Centrality in graphs. Finding centrality in graphs assigns nodes rankings to their popularity or centrality in the graph. This can be useful in finding what nodes are most important. This can be very useful in biology in many ways. For example, if we were to have data on the activity of genes in a given time period, we can use degree centrality to see what genes are most active throughout the network, or what genes interact with others the most throughout the network. This can help us understand what roles certain genes play in the network.
There are many ways to calculate centrality in graphs all of which can give different kinds of information on centrality. Finding centralities in biology can be applied in many different circumstances, some of which are gene regulatory, protein interaction and metabolic networks.


=== Supervised Learning ===
Supervised learning is a type of algorithm that learns from labeled data and learns how to assign labels to future data that is unlabeled. In biology supervised learning can be helpful when we have data that we know how to categorize and we would like to categorize more data into those categories.A common supervised learning algorithm is the random forest, which uses numerous decision trees to train a model to classify a dataset. Forming the basis of the random forest, a decision tree is a structure which aims to classify, or label, some set of data using certain known features of that data. A practical biological example of this would be taking an individual's genetic data and predicting whether or not that individual is predisposed to develop a certain disease or cancer. At each internal node the algorithm checks the dataset for exactly one feature, a specific gene in the previous example, and then branches left or right based on the result. Then at each leaf node, the decision tree assigns a class label to the dataset. So in practice, the algorithm walks a specific root-to-leaf path based on the input dataset through the decision tree, which results in the classification of that dataset. Commonly, decision trees have target variables that take on discrete values, like yes/no, in which case it is referred to as a classification tree, but if the target variable is continuous then it is called a regression tree. To construct a decision tree, it must first be trained using a training set to identify which features are the best predictors of the target variable.


=== Open source software ===
Open source software provides a platform for computational biology where everyone can access and benefit from software developed in research. PLOS cites four main reasons for the use of open source software:

Reproducibility: This allows for researchers to use the exact methods used to calculate the relations between biological data.
Faster development: developers and researchers do not have to reinvent existing code for minor tasks. Instead they can use pre-existing programs to save time on the development and implementation of larger projects.
Increased quality: Having input from multiple researchers studying the same topic provides a layer of assurance that errors will not be in the code.
Long-term availability: Open source programs are not tied to any businesses or patents. This allows for them to be posted to multiple web pages and ensure that they are available in the future.


== Research ==
There are several large conferences that are concerned with computational biology. Some notable examples are Intelligent Systems for Molecular Biology, European Conference on Computational Biology and Research in Computational Molecular Biology.
There are also numerous journals dedicated to computational biology. Some notable examples include Journal of Computational Biology and PLOS Computational Biology, a peer-reviewed open access journal that has many notable research projects in the field of computational biology. They provide reviews on software, tutorials for open source software, and display information on upcoming computational biology conferences.


== Related fields ==
Computational biology, bioinformatics and mathematical biology are all interdisciplinary approaches to the life sciences that draw from quantitative disciplines such as mathematics and information science. The NIH describes computational/mathematical biology as the use of computational/mathematical approaches to address theoretical and experimental questions in biology and, by contrast, bioinformatics as the application of information science to understand complex life-sciences data.Specifically, the NIH defines

Computational biology: The development and application of data-analytical and theoretical methods, mathematical modeling and computational simulation techniques to the study of biological, behavioral, and social systems.
Bioinformatics: Research, development, or application of computational tools and approaches for expanding the use of biological, medical, behavioral or health data, including those to acquire, store, organize, archive, analyze, or visualize such data.
While each field is distinct, there may be significant overlap at their interface, so much so that to many, bioinformatics and computational biology are terms that are used interchangeably.
The terms computational biology and evolutionary computation have a similar name, but are not to be confused. Unlike computational biology, evolutionary computation is not concerned with modeling and analyzing biological data. It instead creates algorithms based on the ideas of evolution across species. Sometimes referred to as genetic algorithms, the research of this field can be applied to computational biology. While evolutionary computation is not inherently a part of computational biology, computational evolutionary biology is a subfield of it.


== See also ==


== References ==


== External links ==
bioinformatics.org",925265,480,"All articles with unsourced statements, Articles with short description, Articles with unsourced statements from January 2022, Articles with unsourced statements from November 2021, Bioinformatics, Computational biology, Computational fields of study, Short description is different from Wikidata",1131620059,biology
https://en.wikipedia.org/wiki/Kingdom_(biology),Kingdom (biology),"In biology, a kingdom is the second highest taxonomic rank, just below domain. Kingdoms are divided into smaller groups called phyla. 
Traditionally, some textbooks from the United States and Canada used a system of six kingdoms (Animalia, Plantae, Fungi, Protista, Archaea/Archaebacteria, and Bacteria/Eubacteria) while textbooks in Great Britain, India, Greece, Brazil and other countries use five kingdoms only (Animalia, Plantae, Fungi, Protista and Monera). 
Some recent classifications based on modern cladistics have explicitly abandoned the term kingdom, noting that some traditional kingdoms are not monophyletic, meaning that they do not consist of all the descendants of a common ancestor. The terms flora (for plants), fauna (for animals), and, in the 21st century, funga (for fungi) are also used for life present in a particular region or time.","In biology, a kingdom is the second highest taxonomic rank, just below domain. Kingdoms are divided into smaller groups called phyla. 
Traditionally, some textbooks from the United States and Canada used a system of six kingdoms (Animalia, Plantae, Fungi, Protista, Archaea/Archaebacteria, and Bacteria/Eubacteria) while textbooks in Great Britain, India, Greece, Brazil and other countries use five kingdoms only (Animalia, Plantae, Fungi, Protista and Monera). 
Some recent classifications based on modern cladistics have explicitly abandoned the term kingdom, noting that some traditional kingdoms are not monophyletic, meaning that they do not consist of all the descendants of a common ancestor. The terms flora (for plants), fauna (for animals), and, in the 21st century, funga (for fungi) are also used for life present in a particular region or time.


== Definition and associated terms ==
When Carl Linnaeus introduced the rank-based system of nomenclature into biology in 1735, the highest rank was given the name ""kingdom"" and was followed by four other main or principal ranks: class, order, genus and species. Later two further main ranks were introduced, making the sequence kingdom, phylum or division, class, order, family, genus and species. In 1990, the rank of domain was introduced above kingdom.Prefixes can be added so subkingdom (subregnum) and infrakingdom (also known as infraregnum) are the two ranks immediately below kingdom. Superkingdom may be considered as an equivalent of domain or empire or as an independent rank between kingdom and domain or subdomain. In some classification systems the additional rank branch (Latin: ramus) can be inserted between subkingdom and infrakingdom, e.g., Protostomia and Deuterostomia in the classification of Cavalier-Smith.


== History ==


=== Two kingdoms of life ===

The classification of living things into animals and plants is an ancient one. Aristotle (384–322 BC) classified animal species in his History of Animals, while his pupil Theophrastus (c. 371–c. 287 BC) wrote a parallel work, the Historia Plantarum, on plants.Carl Linnaeus (1707–1778) laid the foundations for modern biological nomenclature, now regulated by the Nomenclature Codes, in 1735. He distinguished two kingdoms of living things: Regnum Animale ('animal kingdom') and Regnum Vegetabile ('vegetable kingdom', for plants). Linnaeus also included minerals in his classification system, placing them in a third kingdom, Regnum Lapideum.


=== Three kingdoms of life ===

In 1674, Antonie van Leeuwenhoek, often called the ""father of microscopy"", sent the Royal Society of London a copy of his first observations of microscopic single-celled organisms. Until then, the existence of such microscopic organisms was entirely unknown. Despite this, Linnaeus did not include any microscopic creatures in his original taxonomy.
At first, microscopic organisms were classified within the animal and plant kingdoms. However, by the mid–19th century, it had become clear to many that ""the existing dichotomy of the plant and animal kingdoms [had become] rapidly blurred at its boundaries and outmoded"".In 1860 John Hogg proposed the Protoctista, a third kingdom of life composed of ""all the lower creatures, or the primary organic beings""; he retained Regnum Lapideum as a fourth kingdom of minerals. In 1866, Ernst Haeckel also proposed a third kingdom of life, the Protista, for ""neutral organisms"" or ""the kingdom of primitive forms"", which were neither animal nor plant; he did not include the Regnum Lapideum in his scheme. Haeckel revised the content of this kingdom a number of times before settling on a division based on whether organisms were unicellular (Protista) or multicellular (animals and plants).


=== Four kingdoms ===
The development of microscopy revealed important distinctions between those organisms whose cells do not have a distinct nucleus (prokaryotes) and organisms whose cells do have a distinct nucleus (eukaryotes). In 1937 Édouard Chatton introduced the terms ""prokaryote"" and ""eukaryote"" to differentiate these organisms.In 1938, Herbert F. Copeland proposed a four-kingdom classification by creating the novel Kingdom Monera of prokaryotic organisms; as a revised phylum Monera of the Protista, it included organisms now classified as Bacteria and Archaea. Ernst Haeckel, in his 1904 book The Wonders of Life, had placed the blue-green algae (or Phycochromacea) in Monera; this would gradually gain acceptance, and the blue-green algae would become classified as bacteria in the phylum Cyanobacteria.In the 1960s, Roger Stanier and C. B. van Niel promoted and popularized Édouard Chatton's earlier work, particularly in their paper of 1962, ""The Concept of a Bacterium""; this created, for the first time, a rank above kingdom—a superkingdom or empire—with the two-empire system of prokaryotes and eukaryotes. The two-empire system would later be expanded to the three-domain system of Archaea, Bacteria, and Eukaryota.


=== Five kingdoms ===

The differences between fungi and other organisms regarded as plants had long been recognised by some; Haeckel had moved the fungi out of Plantae into Protista after his original classification, but was largely ignored in this separation by scientists of his time. Robert Whittaker recognized an additional kingdom for the Fungi. The resulting five-kingdom system, proposed in 1969 by Whittaker, has become a popular standard and with some refinement is still used in many works and forms the basis for new multi-kingdom systems. It is based mainly upon differences in nutrition; his Plantae were mostly multicellular autotrophs, his Animalia multicellular heterotrophs, and his Fungi multicellular saprotrophs.
The remaining two kingdoms, Protista and Monera, included unicellular and simple cellular colonies. The five kingdom system may be combined with the two empire system. In the Whittaker system, Plantae included some algae. In other systems, such as Lynn Margulis's system of five kingdoms, the plants included just the land plants (Embryophyta), and Protoctista has a broader definition.Following publication of Whittaker's system, the five-kingdom model began to be commonly used in high school biology textbooks. But despite the development from two kingdoms to five among most scientists, some authors as late as 1975 continued to employ a traditional two-kingdom system of animals and plants, dividing the plant kingdom into subkingdoms Prokaryota (bacteria and cyanobacteria), Mycota (fungi and supposed relatives), and Chlorota (algae and land plants).


=== Six kingdoms ===
In 1977, Carl Woese and colleagues proposed the fundamental subdivision of the prokaryotes into the Eubacteria (later called the Bacteria) and Archaebacteria (later called the Archaea), based on ribosomal RNA structure; this would later lead to the proposal of three ""domains"" of life, of Bacteria, Archaea, and Eukaryota. Combined with the five-kingdom model, this created a six-kingdom model, where the kingdom Monera is replaced by the kingdoms Bacteria and Archaea. This six-kingdom model is commonly used in recent US high school biology textbooks, but has received criticism for compromising the current scientific consensus. But the division of prokaryotes into two kingdoms remains in use with the recent seven kingdoms scheme of Thomas Cavalier-Smith, although it primarily differs in that Protista is replaced by Protozoa and Chromista.


=== Eight kingdoms ===
Thomas Cavalier-Smith supported the consensus at that time, that the difference between Eubacteria and Archaebacteria was so great (particularly considering the genetic distance of ribosomal genes) that the prokaryotes needed to be separated into two different kingdoms. He then divided Eubacteria into two subkingdoms: Negibacteria (Gram negative bacteria) and Posibacteria (Gram positive bacteria). Technological advances in electron microscopy allowed the separation of the Chromista from the Plantae kingdom. Indeed, the chloroplast of the chromists is located in the lumen of the endoplasmic reticulum instead of in the cytosol. Moreover, only chromists contain chlorophyll c. Since then, many non-photosynthetic phyla of protists, thought to have secondarily lost their chloroplasts, were integrated into the kingdom Chromista.
Finally, some protists lacking mitochondria were discovered. As mitochondria were known to be the result of the endosymbiosis of a proteobacterium, it was thought that these amitochondriate eukaryotes were primitively so, marking an important step in eukaryogenesis. As a result, these amitochondriate protists were separated from the protist kingdom, giving rise to the, at the same time, superkingdom and kingdom Archezoa. This superkingdom was opposed to the Metakaryota superkingdom, grouping together the five other eukaryotic kingdoms (Animalia, Protozoa, Fungi, Plantae and Chromista). This was known as the Archezoa hypothesis, which has since been abandoned; later schemes did not include the Archezoa–Metakaryota divide.
‡ No longer recognized by taxonomists.


=== Six kingdoms (1998) ===
In 1998, Cavalier-Smith published a six-kingdom model, which has been revised in subsequent papers. The version published in 2009 is shown below. Cavalier-Smith no longer accepted the importance of the fundamental Eubacteria–Archaebacteria divide put forward by Woese and others and supported by recent research. The kingdom Bacteria (sole kingdom of empire Prokaryota) was subdivided into two sub-kingdoms according to their membrane topologies: Unibacteria and Negibacteria. Unibacteria was divided into phyla Archaebacteria and Posibacteria; the bimembranous-unimembranous transition was thought to be far more fundamental than the long branch of genetic distance of Archaebacteria, viewed as having no particular biological significance.
Cavalier-Smith does not accept the requirement for taxa to be monophyletic (""holophyletic"" in his terminology) to be valid. He defines Prokaryota, Bacteria, Negibacteria, Unibacteria, and Posibacteria as valid paraphyla (therefore ""monophyletic"" in the sense he uses this term) taxa, marking important innovations of biological significance (in regard of the concept of biological niche).
In the same way, his paraphyletic kingdom Protozoa includes the ancestors of Animalia, Fungi, Plantae, and Chromista. The advances of phylogenetic studies allowed Cavalier-Smith to realize that all the phyla thought to be archezoans (i.e. primitively amitochondriate eukaryotes) had in fact secondarily lost their mitochondria, typically by transforming them into new organelles: Hydrogenosomes. This means that all living eukaryotes are in fact metakaryotes, according to the significance of the term given by Cavalier-Smith. Some of the members of the defunct kingdom Archezoa, like the phylum Microsporidia, were reclassified into kingdom Fungi. Others were reclassified in kingdom Protozoa, like Metamonada which is now part of infrakingdom Excavata.
Because Cavalier-Smith allows paraphyly, the diagram below is an 'organization chart', not an 'ancestor chart', and does not represent an evolutionary tree.


=== Seven kingdoms ===
Cavalier-Smith and his collaborators revised their classification in 2015. In this scheme they introduced two superkingdoms of Prokaryota and Eukaryota and seven kingdoms. Prokaryota have two kingdoms: Bacteria and Archaea. (This was based on the consensus in the Taxonomic Outline of Bacteria and Archaea, and the Catalogue of Life). The Eukaryota have five kingdoms: Protozoa, Chromista, Plantae, Fungi, and Animalia. In this classification a protist is any of the eukaryotic unicellular organisms.


=== Summary ===
The kingdom-level classification of life is still widely employed as a useful way of grouping organisms, notwithstanding some problems with this approach:

Kingdoms such as Protozoa represent grades rather than clades, and so are rejected by phylogenetic classification systems.
The most recent research does not support the classification of the eukaryotes into any of the standard systems. As of April 2010, no set of kingdoms is sufficiently supported by research to attain widespread acceptance. In 2009, Andrew Roger and Alastair Simpson emphasized the need for diligence in analyzing new discoveries: ""With the current pace of change in our understanding of the eukaryote tree of life, we should proceed with caution.""


== Beyond traditional kingdoms ==

While the concept of kingdoms continues to be used by some taxonomists, there has been a movement away from traditional kingdoms, as they are no longer seen as providing a cladistic classification, where there is emphasis in arranging organisms into natural groups.


=== Three domains of life ===

From around the mid-1970s onwards, there was an increasing emphasis on comparisons of genes at the molecular level (initially ribosomal RNA genes) as the primary factor in classification; genetic similarity was stressed over outward appearances and behavior. Taxonomic ranks, including kingdoms, were to be groups of organisms with a common ancestor, whether monophyletic (all descendants of a common ancestor) or paraphyletic (only some descendants of a common ancestor).Based on such RNA studies, Carl Woese thought life could be divided into three large divisions and referred to them as the ""three primary kingdom"" model or ""urkingdom"" model.In 1990, the name ""domain"" was proposed for the highest rank. This term represents a synonym for the category of dominion (lat. dominium), introduced by Moore in 1974. Unlike Moore, Woese et al. (1990) did not suggest a Latin term for this category, which represents a further argument supporting the accurately introduced term dominion.Woese divided the prokaryotes (previously classified as the Kingdom Monera) into two groups, called Eubacteria and Archaebacteria, stressing that there was as much genetic difference between these two groups as between either of them and all eukaryotes.

According to genetic data, although eukaryote groups such as plants, fungi, and animals may look different, they are more closely related to each other than they are to either the Eubacteria or Archaea. It was also found that the eukaryotes are more closely related to the Archaea than they are to the Eubacteria. Although the primacy of the Eubacteria-Archaea divide has been questioned, it has been upheld by subsequent research. There is no consensus on how many kingdoms exist in the classification scheme proposed by Woese.


=== Eukaryotic supergroups ===

In 2004, a review article by Simpson and Roger noted that the Protista were ""a grab-bag for all eukaryotes that are not animals, plants or fungi"". They held that only monophyletic groups should be accepted as formal ranks in a classification and that – while this approach had been impractical previously (necessitating ""literally dozens of eukaryotic 'kingdoms'"") – it had now become possible to divide the eukaryotes into ""just a few major groups that are probably all monophyletic"".On this basis, the diagram opposite (redrawn from their article) showed the real ""kingdoms"" (their quotation marks) of the eukaryotes. A classification which followed this approach was produced in 2005 for the International Society of Protistologists, by a committee which ""worked in collaboration with specialists from many societies"". It divided the eukaryotes into the same six ""supergroups"". The published classification deliberately did not use formal taxonomic ranks, including that of ""kingdom"".

In this system the multicellular animals (Metazoa) are descended from the same ancestor as both the unicellular choanoflagellates and the fungi which form the Opisthokonta. Plants are thought to be more distantly related to animals and fungi.

However, in the same year as the International Society of Protistologists' classification was published (2005), doubts were being expressed as to whether some of these supergroups were monophyletic, particularly the Chromalveolata, and a review in 2006 noted the lack of evidence for several of the six proposed supergroups.
As of 2010, there is widespread agreement that the Rhizaria belong with the Stramenopiles and the Alveolata, in a clade dubbed the SAR supergroup, so that Rhizaria is not one of the main eukaryote groups. Beyond this, there does not appear to be a consensus. Rogozin et al. in 2009 noted that ""The deep phylogeny of eukaryotes is an extremely difficult and controversial problem."" As of December 2010, there appears to be a consensus that the six supergroup model proposed in 2005 does not reflect the true phylogeny of the eukaryotes and hence how they should be classified, although there is no agreement as to the model which should replace it.


=== Comparison of top level classification ===

Some authors have added non-cellular life to their classifications. This can create a ""superdomain"" called ""Acytota"", also called ""Aphanobionta"", of non-cellular life; with the other superdomain being ""cytota"" or cellular life. The eocyte hypothesis proposes that the eukaryotes emerged from a phylum within the archaea called the Thermoproteota (formerly known as eocytes or Crenarchaeota).


== Viruses ==
The International Committee on Taxonomy of Viruses uses the taxonomic rank ""kingdom"" for the classification of viruses (with the suffix -virae); but this is beneath the top level classifications of realm and subrealm.There is ongoing debate as to whether viruses can be included in the tree of life. The arguments against include the fact that they are obligate intracellular parasites that lack metabolism and are not capable of replication outside of a host cell. Another argument is that their placement in the tree would be problematic, since it is suspected that viruses have arisen multiple times, and they have a penchant for harvesting nucleotide sequences from their hosts.
On the other hand, arguments favor their inclusion.
One comes from the discovery of unusually large and complex viruses, such as Mimivirus, that possess typical cellular genes.


== See also ==
Cladistics
Phylogenetics
Systematics
Taxonomy


== Notes ==


== References ==


== Further reading ==
Pelentier, B. (2007-2015). Empire Biota: a comprehensive taxonomy, [1]. [Historical overview.]
Peter H. Raven and Helena Curtis (1970), Biology of Plants, New York: Worth Publishers. [Early presentation of five-kingdom system.]


== External links ==
A Brief History of the Kingdoms of Life at Earthling Nature
The five kingdom concept
Whittaker's classification",5150030,3568,"All articles containing potentially dated statements, All articles with unsourced statements, Articles containing potentially dated statements from 2010, Articles containing potentially dated statements from April 2010, Articles containing potentially dated statements from December 2010, Articles with short description, Articles with unsourced statements from January 2017, Articles with unsourced statements from October 2019, Biology terminology, CS1: long volume value, Kingdoms (biology), Short description matches Wikidata, Species",1134164062,biology
https://en.wikipedia.org/wiki/Conservation_biology,Conservation biology,"Conservation biology is the study of the conservation of nature and of Earth's biodiversity with the aim of protecting species, their habitats, and ecosystems from excessive rates of extinction and the erosion of biotic interactions. It is an interdisciplinary subject drawing on natural and social sciences, and the practice of natural resource management.: 478 The conservation ethic is based on the findings of conservation biology.","Conservation biology is the study of the conservation of nature and of Earth's biodiversity with the aim of protecting species, their habitats, and ecosystems from excessive rates of extinction and the erosion of biotic interactions. It is an interdisciplinary subject drawing on natural and social sciences, and the practice of natural resource management.: 478 The conservation ethic is based on the findings of conservation biology.


== Origins ==

The term conservation biology and its conception as a new field originated with the convening of ""The First International Conference on Research in Conservation Biology"" held at the University of California, San Diego in La Jolla, California, in 1978 led by American biologists Bruce A. Wilcox and Michael E. Soulé with a group of leading university and zoo researchers and conservationists including Kurt Benirschke, Sir Otto Frankel, Thomas Lovejoy, and Jared Diamond. The meeting was prompted due to concern over tropical deforestation, disappearing species, and eroding genetic diversity within species. The conference and proceedings that resulted sought to initiate the bridging of a gap between theory in ecology and evolutionary genetics on the one hand and conservation policy and practice on the other.Conservation biology and the concept of biological diversity (biodiversity) emerged together, helping crystallize the modern era of conservation science and policy.  The inherent multidisciplinary basis for conservation biology has led to new subdisciplines including conservation social science, conservation behavior and conservation physiology. It stimulated further development of conservation genetics which Otto Frankel had originated first but is now often considered a subdiscipline as well.


== Description ==
The rapid decline of established biological systems around the world means that conservation biology is often referred to as a ""Discipline with a deadline"". Conservation biology is tied closely to ecology in researching the population ecology (dispersal, migration, demographics, effective population size, inbreeding depression, and minimum population viability) of rare or endangered species. Conservation biology is concerned with phenomena that affect the maintenance, loss, and restoration of biodiversity and the science of sustaining evolutionary processes that engender genetic, population, species, and ecosystem diversity. The concern stems from estimates suggesting that up to 50% of all species on the planet will disappear within the next 50 years, which has contributed to poverty, starvation, and will reset the course of evolution on this planet.Conservation biologists research and educate on the trends and process of biodiversity loss, species extinctions, and the negative effect these are having on our capabilities to sustain the well-being of human society. Conservation biologists work in the field and office, in government, universities, non-profit organizations and industry. The topics of their research are diverse, because this is an interdisciplinary network with professional alliances in the biological as well as social sciences. Those dedicated to the cause and profession advocate for a global response to the current biodiversity crisis based on morals, ethics, and scientific reason. Organizations and citizens are responding to the biodiversity crisis through conservation action plans that direct research, monitoring, and education programs that engage concerns at local through global scales. There is increasing recognition that conservation is not just about what is achieved but how it is done.  A ""conservation acrostic"" has been created to emphasize that point where C = co-produced, O = open, N = nimble, S = solutions-oriented, E = empowering, R = relational, V = values-based, A = actionable, T = transdisciplinary, I = inclusive, O = optimistic, and N = nurturing 


== History ==


=== Natural resource conservation ===
Conscious efforts to conserve and protect global biodiversity are a recent phenomenon. Natural resource conservation, however, has a history that extends prior to the age of conservation. Resource ethics grew out of necessity through direct relations with nature. Regulation or communal restraint became necessary to prevent selfish motives from taking more than could be locally sustained, therefore compromising the long-term supply for the rest of the community. This social dilemma with respect to natural resource management is often called the ""Tragedy of the Commons"".From this principle, conservation biologists can trace communal resource based ethics throughout cultures as a solution to communal resource conflict. For example, the Alaskan Tlingit peoples and the Haida of the Pacific Northwest had resource boundaries, rules, and restrictions among clans with respect to the fishing of sockeye salmon. These rules were guided by clan elders who knew lifelong details of each river and stream they managed. There are numerous examples in history where cultures have followed rules, rituals, and organized practice with respect to communal natural resource management.The Mauryan emperor Ashoka around 250 B.C. issued edicts restricting the slaughter of animals and certain kinds of birds, as well as opened veterinary clinics.
Conservation ethics are also found in early religious and philosophical writings. There are examples in the Tao, Shinto, Hindu, Islamic and Buddhist traditions. In Greek philosophy, Plato lamented about pasture land degradation: ""What is left now is, so to say, the skeleton of a body wasted by disease; the rich, soft soil has been carried off and only the bare framework of the district left."" In the bible, through Moses, God commanded to let the land rest from cultivation every seventh year. Before the 18th century, however, much of European culture considered it a pagan view to admire nature. Wilderness was denigrated while agricultural development was praised. However, as early as AD 680 a wildlife sanctuary was founded on the Farne Islands by St Cuthbert in response to his religious beliefs.


=== Early naturalists ===

Natural history was a major preoccupation in the 18th century, with grand expeditions and the opening of popular public displays in Europe and North America. By 1900 there were 150 natural history museums in Germany, 250 in Great Britain, 250 in the United States, and 300 in France. Preservationist or conservationist sentiments are a development of the late 18th to early 20th centuries.
Before Charles Darwin set sail on HMS Beagle, most people in the world, including Darwin, believed in special creation and that all species were unchanged. George-Louis Leclerc was one of the first naturalist that questioned this belief. He proposed in his 44 volume natural history book that species evolve due to environmental influences. Erasmus Darwin was also a naturalist who also suggested that species evolved. Erasmus Darwin noted that some species have vestigial structures which are anatomical structures that have no apparent function in the species currently but would have been useful for the species' ancestors. The thinking of these early 18th century naturalists helped to change the mindset and thinking of the early 19th century naturalists.
By the early 19th century biogeography was ignited through the efforts of Alexander von Humboldt, Charles Lyell and Charles Darwin. The 19th-century fascination with natural history engendered a fervor to be the first to collect rare specimens with the goal of doing so before they became extinct by other such collectors. Although the work of many 18th and 19th century naturalists were to inspire nature enthusiasts and conservation organizations, their writings, by modern standards, showed insensitivity towards conservation as they would kill hundreds of specimens for their collections.


=== Conservation movement ===

The modern roots of conservation biology can be found in the late 18th-century Enlightenment period particularly in England and Scotland. A number of thinkers, among them notably Lord Monboddo, described the importance of ""preserving nature""; much of this early emphasis had its origins in Christian theology.
Scientific conservation principles were first practically applied to the forests of British India. The conservation ethic that began to evolve included three core principles: that human activity damaged the environment, that there was a civic duty to maintain the environment for future generations, and that scientific, empirically based methods should be applied to ensure this duty was carried out. Sir James Ranald Martin was prominent in promoting this ideology, publishing many medico-topographical reports that demonstrated the scale of damage wrought through large-scale deforestation and desiccation, and lobbying extensively for the institutionalization of forest conservation activities in British India through the establishment of Forest Departments.The Madras Board of Revenue started local conservation efforts in 1842, headed by Alexander Gibson, a professional botanist who systematically adopted a forest conservation program based on scientific principles. This was the first case of state conservation management of forests in the world. Governor-General Lord Dalhousie introduced the first permanent and large-scale forest conservation program in the world in 1855, a model that soon spread to other colonies, as well the United States, where Yellowstone National Park was opened in 1872 as the world's first national park.The term conservation came into widespread use in the late 19th century and referred to the management, mainly for economic reasons, of such natural resources as timber, fish, game, topsoil, pastureland, and minerals. In addition it referred to the preservation of forests (forestry), wildlife (wildlife refuge), parkland, wilderness, and watersheds. This period also saw the passage of the first conservation legislation and the establishment of the first nature conservation societies. The Sea Birds Preservation Act of 1869 was passed in Britain as the first nature protection law in the world after extensive lobbying from the Association for the Protection of Seabirds and the respected ornithologist Alfred Newton. Newton was also instrumental in the passage of the first Game laws from 1872, which protected animals during their breeding season so as to prevent the stock from being brought close to extinction.One of the first conservation societies was the Royal Society for the Protection of Birds, founded in 1889 in Manchester as a protest group campaigning against the use of great crested grebe and kittiwake skins and feathers in fur clothing. Originally known as ""the Plumage League"", the group gained popularity and eventually amalgamated with the Fur and Feather League in Croydon, and formed the RSPB. The National Trust formed in 1895 with the manifesto to ""...promote the permanent preservation, for the benefit of the nation, of lands, ... to preserve (so far practicable) their natural aspect."" In May 1912, a month after the Titanic sank, banker and expert naturalist Charles Rothschild held a meeting at the Natural History Museum in London to discuss his idea for a new organisation to save the best places for wildlife in the British Isles. This meeting led to the formation of the Society for the Promotion of Nature Reserves, which later became the Wildlife Trusts.
In the United States, the Forest Reserve Act of 1891 gave the President power to set aside forest reserves from the land in the public domain. John Muir founded the Sierra Club in 1892, and the New York Zoological Society was set up in 1895. A series of national forests and preserves were established by Theodore Roosevelt from 1901 to 1909. The 1916 National Parks Act, included a 'use without impairment' clause, sought by John Muir, which eventually resulted in the removal of a proposal to build a dam in Dinosaur National Monument in 1959.

In the 20th century, Canadian civil servants, including Charles Gordon Hewitt and James Harkin spearheaded the movement toward wildlife conservation.In the 21st century professional conservation officiers begun to collaborate with indigenous communities for protecting wildlife in Canada.


=== Global conservation efforts ===
In the mid-20th century, efforts arose to target individual species for conservation, notably efforts in big cat conservation in South America led by the New York Zoological Society. In the early 20th century the New York Zoological Society was instrumental in developing concepts of establishing preserves for particular species and conducting the necessary conservation studies to determine the suitability of locations that are most appropriate as conservation priorities; the work of Henry Fairfield Osborn Jr., Carl E. Akeley, Archie Carr and his son Archie Carr III is notable in this era. Akeley for example, having led expeditions to the Virunga Mountains and observed the mountain gorilla in the wild, became convinced that the species and the area were conservation priorities. He was instrumental in persuading Albert I of Belgium to act in defense of the mountain gorilla and establish Albert National Park (since renamed Virunga National Park) in what is now Democratic Republic of Congo.By the 1970s, led primarily by work in the United States under the Endangered Species Act along with the Species at Risk Act (SARA) of Canada, Biodiversity Action Plans developed in Australia, Sweden, the United Kingdom, hundreds of species specific protection plans ensued. Notably the United Nations acted to conserve sites of outstanding cultural or natural importance to the common heritage of mankind. The programme was adopted by the General Conference of UNESCO in 1972. As of 2006, a total of 830 sites are listed: 644 cultural, 162 natural. The first country to pursue aggressive biological conservation through national legislation was the United States, which passed back to back legislation in the Endangered Species Act (1966) and National Environmental Policy Act (1970), which together injected major funding and protection measures to large-scale habitat protection and threatened species research. Other conservation developments, however, have taken hold throughout the world. India, for example, passed the Wildlife Protection Act of 1972.In 1980, a significant development was the emergence of the urban conservation movement. A local organization was established in Birmingham, UK, a development followed in rapid succession in cities across the UK, then overseas. Although perceived as a grassroots movement, its early development was driven by academic research into urban wildlife. Initially perceived as radical, the movement's view of conservation being inextricably linked with other human activity has now become mainstream in conservation thought. Considerable research effort is now directed at urban conservation biology. The Society for Conservation Biology originated in 1985.: 2 By 1992, most of the countries of the world had become committed to the principles of conservation of biological diversity with the Convention on Biological Diversity; subsequently many countries began programmes of Biodiversity Action Plans to identify and conserve threatened species within their borders, as well as protect associated habitats. The late 1990s saw increasing professionalism in the sector, with the maturing of organisations such as the Institute of Ecology and Environmental Management and the Society for the Environment.
Since 2000, the concept of landscape scale conservation has risen to prominence, with less emphasis being given to single-species or even single-habitat focused actions. Instead an ecosystem approach is advocated by most mainstream conservationists, although concerns have been expressed by those working to protect some high-profile species.
Ecology has clarified the workings of the biosphere; i.e., the complex interrelationships among humans, other species, and the physical environment. The burgeoning human population and associated agriculture, industry, and the ensuing pollution, have demonstrated how easily ecological relationships can be disrupted.
The last word in ignorance is the man who says of an animal or plant: ""What good is it?"" If the land mechanism as a whole is good, then every part is good, whether we understand it or not. If the biota, in the course of aeons, has built something we like but do not understand, then who but a fool would discard seemingly useless parts? To keep every cog and wheel is the first precaution of intelligent tinkering.


== Concepts and foundations ==


=== Measuring extinction rates ===

Extinction rates are measured in a variety of ways. Conservation biologists measure and apply statistical measures of fossil records, rates of habitat loss, and a multitude of other variables such as loss of biodiversity as a function of the rate of habitat loss and site occupancy to obtain such estimates. The Theory of Island Biogeography is possibly the most significant contribution toward the scientific understanding of both the process and how to measure the rate of species extinction. The current background extinction rate is estimated to be one species every few years. Actual extinction rates are estimated to be orders of magnitudes higher.The measure of ongoing species loss is made more complex by the fact that most of the Earth's species have not been described or evaluated. Estimates vary greatly on how many species actually exist (estimated range: 3,600,000-111,700,000) to how many have received a species binomial (estimated range: 1.5-8 million). Less than 1% of all species that have been described beyond simply noting its existence. From these figures, the IUCN reports that 23% of vertebrates, 5% of invertebrates and 70% of plants that have been evaluated are designated as endangered or threatened. Better knowledge is being constructed by The Plant List for actual numbers of species.


=== Systematic conservation planning ===
Systematic conservation planning is an effective way to seek and identify efficient and effective types of reserve design to capture or sustain the highest priority biodiversity values and to work with communities in support of local ecosystems. Margules and Pressey identify six interlinked stages in the systematic planning approach:
Compile data on the biodiversity of the planning region
Identify conservation goals for the planning region
Review existing conservation areas
Select additional conservation areas
Implement conservation actions
Maintain the required values of conservation areasConservation biologists regularly prepare detailed conservation plans for grant proposals or to effectively coordinate their plan of action and to identify best management practices (e.g.). Systematic strategies generally employ the services of Geographic Information Systems to assist in the decision-making process. The SLOSS debate is often considered in planning.


=== Conservation physiology: a mechanistic approach to conservation ===
Conservation physiology was defined by Steven J. Cooke and colleagues as: ""An integrative scientific discipline applying physiological concepts, tools, and knowledge to characterizing biological diversity and its ecological implications; understanding and predicting how organisms, populations, and ecosystems respond to environmental change and stressors; and solving conservation problems across the broad range of taxa (i.e. including microbes, plants, and animals). Physiology is considered in the broadest possible terms to include functional and mechanistic responses at all scales, and conservation includes the development and refinement of strategies to rebuild populations, restore ecosystems, inform conservation policy, generate decision-support tools, and manage natural resources.""  Conservation physiology is particularly relevant to practitioners in that it has the potential to generate cause-and-effect relationships and reveal the factors that contribute to population declines.


=== Conservation biology as a profession ===
The Society for Conservation Biology is a global community of conservation professionals dedicated to advancing the science and practice of conserving biodiversity. Conservation biology as a discipline reaches beyond biology, into subjects such as philosophy, law, economics, humanities, arts, anthropology, and education. Within biology, conservation genetics and evolution are immense fields unto themselves, but these disciplines are of prime importance to the practice and profession of conservation biology.
Conservationists introduce bias when they support policies using qualitative description, such as habitat degradation, or healthy ecosystems. Conservation biologists advocate for reasoned and sensible management of natural resources and do so with a disclosed combination of science, reason, logic, and values in their conservation management plans. This sort of advocacy is similar to the medical profession advocating for healthy lifestyle options, both are beneficial to human well-being yet remain scientific in their approach.
There is a movement in conservation biology suggesting a new form of leadership is needed to mobilize conservation biology into a more effective discipline that is able to communicate the full scope of the problem to society at large. The movement proposes an adaptive leadership approach that parallels an adaptive management approach. The concept is based on a new philosophy or leadership theory steering away from historical notions of power, authority, and dominance. Adaptive conservation leadership is reflective and more equitable as it applies to any member of society who can mobilize others toward meaningful change using communication techniques that are inspiring, purposeful, and collegial. Adaptive conservation leadership and mentoring programs are being implemented by conservation biologists through organizations such as the Aldo Leopold Leadership Program.


=== Approaches ===
Conservation may be classified as either in-situ conservation, which is protecting an endangered species in its natural habitat, or ex-situ conservation, which occurs outside the natural habitat. In-situ conservation involves protecting or restoring the habitat. Ex-situ conservation, on the other hand, involves protection outside of an organism's natural habitat, such as on reservations or in gene banks, in circumstances where viable populations may not be present in the natural habitat.
Also, non-interference may be used, which is termed a preservationist method. Preservationists advocate for giving areas of nature and species a protected existence that halts interference from the humans. In this regard, conservationists differ from preservationists in the social dimension, as conservation biology engages society and seeks equitable solutions for both society and ecosystems. Some preservationists emphasize the potential of biodiversity in a world without humans.


=== Ecological monitoring in conservation ===
Ecological monitoring is the systematic collection of data relevant to the ecology of a species or habitat at repeating intervals with defined methods. Long-term monitoring for environmental and ecological metrics is an important part of any successful conservation initiative. Unfortunately, long-term data for many species and habitats is not available in many cases. A lack of historical data on species populations, habitats, and ecosystems means that any current or future conservation work will have to make assumptions to determine if the work is having any effect on the population or ecosystem health. Ecological monitoring can provide early warning signals of deleterious effects (from human activities or natural changes in an environment) on an ecosystem and its species. In order for signs of negative trends in ecosystem or species health to be detected, monitoring methods must be carried out at appropriate time intervals, and the metric must be able to capture the trend of the population or habitat as a whole.
Long-term monitoring can include the continued measuring of many biological, ecological, and environmental metrics including annual breeding success, population size estimates, water quality, biodiversity (which can be measured in many way, i.e. Shannon Index), and many other methods. When determining which metrics to monitor for a conservation project, it is important to understand how an ecosystem functions and what role different species and abiotic factors have within the system. It is important to have a precise reason for why ecological monitoring is implemented; within the context of conservation, this reasoning is often to track changes before, during, or after conservation measures are put in place to help a species or habitat recover from degradation and/or maintain integrity.Another benefit of ecological monitoring is the hard evidence it provides scientists to use for advising policy makers and funding bodies about conservation efforts. Not only is ecological monitoring data important for convincing politicians, funders, and the public why a conservation program is important to implement, but also to keep them convinced that a program should be continued to be supported.There is plenty of debate on how conservation resources can be used most efficiently; even within ecological monitoring, there is debate on which metrics that money, time and personnel should be dedicated to for the best chance of making a positive impact. One specific general discussion topic is whether monitoring should happen where there is little human impact (to understand a system that hasn't been degraded by humans), where there is human impact (so the effects from humans can be investigated), or where there is data deserts and little is known about the habitats' and communities' response to human perturbations.The concept of bioindicators / indicator species can be applied to ecological monitoring as a way to investigate how pollution is affecting an ecosystem. Species like amphibians and birds are highly susceptible to pollutants in their environment due to their behaviours and physiological features  that cause them to absorb pollutants at a faster rate than other species. Amphibians spend parts of their time in the water and on land, making them susceptible to changes in both environments. They also have very permeable skin that allows them to breath and intake water, which means they also take any air or water-soluble pollutants in as well. Birds often cover a wide range in habitat types annually, and also generally revisit the same nesting site each year. This makes it easier for researchers to track ecological effects at both an individual and a population level for the species.Having a long-term ecological monitoring program should be a priority for all conservation projects, protected areas, and regions where environmental harm mitigation is used.


=== Ethics and values ===

Conservation biologists are interdisciplinary researchers that practice ethics in the biological and social sciences. Chan states that conservationists must advocate for biodiversity and can do so in a scientifically ethical manner by not promoting simultaneous advocacy against other competing values.
A conservationist may be inspired by the resource conservation ethic,: 15  which seeks to identify what measures will deliver ""the greatest good for the greatest number of people for the longest time."": 13  In contrast, some conservation biologists argue that nature has an intrinsic value that is independent of anthropocentric usefulness or utilitarianism.: 3, 12, 16–17  Aldo Leopold was a classical thinker and writer on such conservation ethics whose philosophy, ethics and writings are still valued and revisited by modern conservation biologists.: 16–17 


=== Conservation priorities ===

The International Union for Conservation of Nature (IUCN) has organized a global assortment of scientists and research stations across the planet to monitor the changing state of nature in an effort to tackle the extinction crisis. The IUCN provides annual updates on the status of species conservation through its Red List. The IUCN Red List serves as an international conservation tool to identify those species most in need of conservation attention and by providing a global index on the status of biodiversity. More than the dramatic rates of species loss, however, conservation scientists note that the sixth mass extinction is a biodiversity crisis requiring far more action than a priority focus on rare, endemic or endangered species. Concerns for biodiversity loss covers a broader conservation mandate that looks at ecological processes, such as migration, and a holistic examination of biodiversity at levels beyond the species, including genetic, population and ecosystem diversity. Extensive, systematic, and rapid rates of biodiversity loss threatens the sustained well-being of humanity by limiting supply of ecosystem services that are otherwise regenerated by the complex and evolving holistic network of genetic and ecosystem diversity. While the conservation status of species is employed extensively in conservation management, some scientists highlight that it is the common species that are the primary source of exploitation and habitat alteration by humanity. Moreover, common species are often undervalued despite their role as the primary source of ecosystem services.While most in the community of conservation science ""stress the importance"" of sustaining biodiversity, there is debate on how to prioritize genes, species, or ecosystems, which are all components of biodiversity (e.g. Bowen, 1999). While the predominant approach to date has been to focus efforts on endangered species by conserving biodiversity hotspots, some scientists (e.g) and conservation organizations, such as the Nature Conservancy, argue that it is more cost-effective, logical, and socially relevant to invest in biodiversity coldspots. The costs of discovering, naming, and mapping out the distribution of every species, they argue, is an ill-advised conservation venture. They reason it is better to understand the significance of the ecological roles of species.Biodiversity hotspots and coldspots are a way of recognizing that the spatial concentration of genes, species, and ecosystems is not uniformly distributed on the Earth's surface. For example, ""... 44% of all species of vascular plants and 35% of all species in four vertebrate groups are confined to 25 hotspots comprising only 1.4% of the land surface of the Earth.""Those arguing in favor of setting priorities for coldspots point out that there are other measures to consider beyond biodiversity. They point out that emphasizing hotspots downplays the importance of the social and ecological connections to vast areas of the Earth's ecosystems where biomass, not biodiversity, reigns supreme. It is estimated that 36% of the Earth's surface, encompassing 38.9% of the worlds vertebrates, lacks the endemic species to qualify as biodiversity hotspot. Moreover, measures show that maximizing protections for biodiversity does not capture ecosystem services any better than targeting randomly chosen regions. Population level biodiversity (mostly in coldspots) are disappearing at a rate that is ten times that at the species level. The level of importance in addressing biomass versus endemism as a concern for conservation biology is highlighted in literature measuring the level of threat to global ecosystem carbon stocks that do not necessarily reside in areas of endemism. A hotspot priority approach would not invest so heavily in places such as steppes, the Serengeti, the Arctic, or taiga. These areas contribute a great abundance of population (not species) level biodiversity and ecosystem services, including cultural value and planetary nutrient cycling.

Those in favor of the hotspot approach point out that species are irreplaceable components of the global ecosystem, they are concentrated in places that are most threatened, and should therefore receive maximal strategic protections. The IUCN Red List categories, which appear on Wikipedia species articles, is an example of the hotspot conservation approach in action; species that are not rare or endemic are listed the least concern and their Wikipedia articles tend to be ranked low on the importance scale. This is a hotspot approach because the priority is set to target species level concerns over population level or biomass. Species richness and genetic biodiversity contributes to and engenders ecosystem stability, ecosystem processes, evolutionary adaptability, and biomass. Both sides agree, however, that conserving biodiversity is necessary to reduce the extinction rate and identify an inherent value in nature; the debate hinges on how to prioritize limited conservation resources in the most cost-effective way.


=== Economic values and natural capital ===

Conservation biologists have started to collaborate with leading global economists to determine how to measure the wealth and services of nature and to make these values apparent in global market transactions. This system of accounting is called natural capital and would, for example, register the value of an ecosystem before it is cleared to make way for development. The WWF publishes its Living Planet Report and provides a global index of biodiversity by monitoring approximately 5,000 populations in 1,686 species of vertebrate (mammals, birds, fish, reptiles, and amphibians) and report on the trends in much the same way that the stock market is tracked.This method of measuring the global economic benefit of nature has been endorsed by the G8+5 leaders and the European Commission. Nature sustains many ecosystem services that benefit humanity. Many of the Earth's ecosystem services are public goods without a market and therefore no price or value. When the stock market registers a financial crisis, traders on Wall Street are not in the business of trading stocks for much of the planet's living natural capital stored in ecosystems. There is no natural stock market with investment portfolios into sea horses, amphibians, insects, and other creatures that provide a sustainable supply of ecosystem services that are valuable to society. The ecological footprint of society has exceeded the bio-regenerative capacity limits of the planet's ecosystems by about 30 percent, which is the same percentage of vertebrate populations that have registered decline from 1970 through 2005.

The inherent natural economy plays an essential role in sustaining humanity, including the regulation of global atmospheric chemistry, pollinating crops, pest control, cycling soil nutrients, purifying our water supply, supplying medicines and health benefits, and unquantifiable quality of life improvements. There is a relationship, a correlation, between markets and natural capital, and social income inequity and biodiversity loss. This means that there are greater rates of biodiversity loss in places where the inequity of wealth is greatestAlthough a direct market comparison of natural capital is likely insufficient in terms of human value, one measure of ecosystem services suggests the contribution amounts to trillions of dollars yearly. For example, one segment of North American forests has been assigned an annual value of 250 billion dollars; as another example, honey-bee pollination is estimated to provide between 10 and 18 billion dollars of value yearly. The value of ecosystem services on one New Zealand island has been imputed to be as great as the GDP of that region. This planetary wealth is being lost at an incredible rate as the demands of human society is exceeding the bio-regenerative capacity of the Earth. While biodiversity and ecosystems are resilient, the danger of losing them is that humans cannot recreate many ecosystem functions through technological innovation.


=== Strategic species concepts ===


==== Keystone species ====

Some species, called a keystone species form a central supporting hub unique to their ecosystem. The loss of such a species results in a collapse in ecosystem function, as well as the loss of coexisting species. Keystone species are usually predators due to their ability to control the population of prey in their ecosystem. The importance of a keystone species was shown by the extinction of the Steller's sea cow (Hydrodamalis gigas) through its interaction with sea otters, sea urchins, and kelp. Kelp beds grow and form nurseries in shallow waters to shelter creatures that support the food chain. Sea urchins feed on kelp, while sea otters feed on sea urchins. With the rapid decline of sea otters due to overhunting, sea urchin populations grazed unrestricted on the kelp beds and the ecosystem collapsed. Left unchecked, the urchins destroyed the shallow water kelp communities that supported the Steller's sea cow's diet and hastened their demise. The sea otter was thought to be a keystone species because the coexistence of many ecological associates in the kelp beds relied upon otters for their survival. However this was later questioned by Turvey and Risley, who showed that hunting alone would have driven the Steller's sea cow extinct.


==== Indicator species ====

An indicator species has a narrow set of ecological requirements, therefore they become useful targets for observing the health of an ecosystem. Some animals, such as amphibians with their semi-permeable skin and linkages to wetlands, have an acute sensitivity to environmental harm and thus may serve as a miner's canary. Indicator species are monitored in an effort to capture environmental degradation through pollution or some other link to proximate human activities. Monitoring an indicator species is a measure to determine if there is a significant environmental impact that can serve to advise or modify practice, such as through different forest silviculture treatments and management scenarios, or to measure the degree of harm that a pesticide may impart on the health of an ecosystem.
Government regulators, consultants, or NGOs regularly monitor indicator species, however, there are limitations coupled with many practical considerations that must be followed for the approach to be effective. It is generally recommended that multiple indicators (genes, populations, species, communities, and landscape) be monitored for effective conservation measurement that prevents harm to the complex, and often unpredictable, response from ecosystem dynamics (Noss, 1997: 88–89 ).


==== Umbrella and flagship species ====

An example of an umbrella species is the monarch butterfly, because of its lengthy migrations and aesthetic value. The monarch migrates across North America, covering multiple ecosystems and so requires a large area to exist. Any protections afforded to the monarch butterfly will at the same time umbrella many other species and habitats. An umbrella species is often used as flagship species, which are species, such as the giant panda, the blue whale, the tiger, the mountain gorilla and the monarch butterfly, that capture the public's attention and attract support for conservation measures. Paradoxically, however, conservation bias towards flagship species sometimes threatens other species of chief concern.


== Context and trends ==
Conservation biologists study trends and process from the paleontological past to the ecological present as they gain an understanding of the context related to species extinction. It is generally accepted that there have been five major global mass extinctions that register in Earth's history. These include: the Ordovician (440 mya), Devonian (370 mya), Permian–Triassic (245 mya), Triassic–Jurassic (200 mya), and Cretaceous–Paleogene extinction event (66 mya) extinction spasms. Within the last 10,000 years, human influence over the Earth's ecosystems has been so extensive that scientists have difficulty estimating the number of species lost; that is to say the rates of deforestation, reef destruction, wetland draining and other human acts are proceeding much faster than human assessment of species. The latest Living Planet Report by the World Wide Fund for Nature estimates that we have exceeded the bio-regenerative capacity of the planet, requiring 1.6 Earths to support the demands placed on our natural resources.


=== Holocene extinction ===

Conservation biologists are dealing with and have published evidence from all corners of the planet indicating that humanity may be causing the sixth and fastest planetary extinction event. It has been suggested that an unprecedented number of species is becoming extinct in what is known as the Holocene extinction event. The global extinction rate may be approximately 1,000 times higher than the natural background extinction rate. It is estimated that two-thirds of all mammal genera and one-half of all mammal species weighing at least 44 kilograms (97 lb) have gone extinct in the last 50,000 years. The Global Amphibian Assessment reports that amphibians are declining on a global scale faster than any other vertebrate group, with over 32% of all surviving species being threatened with extinction. The surviving populations are in continual decline in 43% of those that are threatened. Since the mid-1980s the actual rates of extinction have exceeded 211 times rates measured from the fossil record. However, ""The current amphibian extinction rate may range from 25,039 to 45,474 times the background extinction rate for amphibians."" The global extinction trend occurs in every major vertebrate group that is being monitored. For example, 23% of all mammals and 12% of all birds are Red Listed by the International Union for Conservation of Nature (IUCN), meaning they too are threatened with extinction. Even though extinction is natural, the decline in species is happening at such an incredible rate that evolution can simply not match, therefore, leading to the greatest continual mass extinction on Earth. Humans have dominated the planet and our high consumption of resources, along with the pollution generated is affecting the environments in which other species live. There are a wide variety of species that humans are working to protect such as the Hawaiian Crow and the Whooping Crane of Texas. People can also take action on preserving species by advocating and voting for global and national policies that improve climate, under the concepts of climate mitigation and climate restoration. The Earth's oceans demand particular attention as climate change continues to alter pH levels, making it uninhabitable for organisms with shells which dissolve as a result.


=== Status of oceans and reefs ===

Global assessments of coral reefs of the world continue to report drastic and rapid rates of decline. By 2000, 27% of the world's coral reef ecosystems had effectively collapsed. The largest period of decline occurred in a dramatic ""bleaching"" event in 1998, where approximately 16% of all the coral reefs in the world disappeared in less than a year. Coral bleaching is caused by a mixture of environmental stresses, including increases in ocean temperatures and acidity, causing both the release of symbiotic algae and death of corals. Decline and extinction risk in coral reef biodiversity has risen dramatically in the past ten years. The loss of coral reefs, which are predicted to go extinct in the next century, threatens the balance of global biodiversity, will have huge economic impacts, and endangers food security for hundreds of millions of people.  Conservation biology plays an important role in international agreements covering the world's oceans (and other issues pertaining to biodiversity).

The oceans are threatened by acidification due to an increase in CO2 levels. This is a most serious threat to societies relying heavily upon oceanic natural resources. A concern is that the majority of all marine species will not be able to evolve or acclimate in response to the changes in the ocean chemistry.The prospects of averting mass extinction seems unlikely when ""90% of all of the large (average approximately ≥50 kg), open ocean tuna, billfishes, and sharks in the ocean"" are reportedly gone. Given the scientific review of current trends, the ocean is predicted to have few surviving multi-cellular organisms with only microbes left to dominate marine ecosystems.


=== Groups other than vertebrates ===
Serious concerns also being raised about taxonomic groups that do not receive the same degree of social attention or attract funds as the vertebrates. These include fungal (including lichen-forming species), invertebrate (particularly insect) and plant communities where the vast majority of biodiversity is represented. Conservation of fungi and conservation of insects, in particular, are both of pivotal importance for conservation biology. As mycorrhizal symbionts, and as decomposers and recyclers, fungi are essential for sustainability of forests. The value of insects in the biosphere is enormous because they outnumber all other living groups in measure of species richness. The greatest bulk of biomass on land is found in plants, which is sustained by insect relations. This great ecological value of insects is countered by a society that often reacts negatively toward these aesthetically 'unpleasant' creatures.One area of concern in the insect world that has caught the public eye is the mysterious case of missing honey bees (Apis mellifera). Honey bees provide an indispensable ecological services through their acts of pollination supporting a huge variety of agriculture crops. The use of honey and wax have become vastly used throughout the world. The sudden disappearance of bees leaving empty hives or colony collapse disorder (CCD) is not uncommon. However, in 16-month period from 2006 through 2007, 29% of 577 beekeepers across the United States reported CCD losses in up to 76% of their colonies. This sudden demographic loss in bee numbers is placing a strain on the agricultural sector. The cause behind the massive declines is puzzling scientists. Pests, pesticides, and global warming are all being considered as possible causes.Another highlight that links conservation biology to insects, forests, and climate change is the mountain pine beetle (Dendroctonus ponderosae) epidemic of British Columbia, Canada, which has infested 470,000 km2 (180,000 sq mi) of forested land since 1999. An action plan has been prepared by the Government of British Columbia to address this problem.
This impact [pine beetle epidemic] converted the forest from a small net carbon sink to a large net carbon source both during and immediately after the outbreak. In the worst year, the impacts resulting from the beetle outbreak in British Columbia were equivalent to 75% of the average annual direct forest fire emissions from all of Canada during 1959–1999.


=== Conservation biology of parasites ===

A large proportion of parasite species are threatened by extinction. A few of them are being eradicated as pests of humans or domestic animals; however, most of them are harmless. Threats include the decline or fragmentation of host populations, or the extinction of host species.


=== Threats to biodiversity ===

Today, many threats to biodiversity exist. An acronym that can be used to express the top threats of present-day H.I.P.P.O stands for Habitat Loss, Invasive Species, Pollution, Human Population, and Overharvesting. The primary threats to biodiversity are habitat destruction (such as deforestation, agricultural expansion, urban development), and overexploitation (such as wildlife trade). Habitat fragmentation also poses challenges, because the global network of protected areas only covers 11.5% of the Earth's surface.  A significant consequence of fragmentation and lack of linked protected areas is the reduction of animal migration on a global scale. Considering that billions of tonnes of biomass are responsible for nutrient cycling across the earth, the reduction of migration is a serious matter for conservation biology.

However, human activities need not necessarily cause irreparable harm to the biosphere. With conservation management and planning for biodiversity at all levels, from genes to ecosystems, there are examples where humans mutually coexist in a sustainable way with nature. Even with the current threats to biodiversity there are ways we can improve the current condition and start anew.
Many of the threats to biodiversity, including disease and climate change, are reaching inside borders of protected areas, leaving them 'not-so protected' (e.g. Yellowstone National Park). Climate change, for example, is often cited as a serious threat in this regard, because there is a feedback loop between species extinction and the release of carbon dioxide into the atmosphere. Ecosystems store and cycle large amounts of carbon which regulates global conditions. In present day, there have been major climate shifts with temperature changes making survival of some species difficult. The effects of global warming add a catastrophic threat toward a mass extinction of global biological diversity. Conservationists have claimed that not all the species can be saved, and they have to decide which their efforts should be used to protect. This concept is known as the Conservation Triage. The extinction threat is estimated to range from 15 to 37 percent of all species by 2050, or 50 percent of all species over the next 50 years. The current extinction rate is 100–100,000 times more rapid today than the last several billion years.


== See also ==


== References ==


== Further reading ==
Scientific literature

Bowen, Brian W. (1999). ""Preserving genes, species, or ecosystems? Healing the fractured foundations of conservation policy"". Molecular Ecology. 8 (s1): S5–S10. doi:10.1046/j.1365-294X.1999.00798.x. PMID 10703547. S2CID 33096004.
Brooks T. M.; Mittermeier R. A.; Gerlach J.; Hoffmann M.; Lamoreux J. F.; Mittermeier C. G.; Pilgrim J. D.; Rodrigues A. S. L. (2006). ""Global Biodiversity Conservation Priorities"". Science. 313 (5783): 58–61. Bibcode:2006Sci...313...58B. doi:10.1126/science.1127609. PMID 16825561. S2CID 5133902.
Kareiva P.; Marvier M. (2003). ""Conserving Biodiversity Coldspots"" (PDF). American Scientist. 91 (4): 344–351. doi:10.1511/2003.4.344. Archived from the original (PDF) on September 6, 2006.
Manlik, Oliver. (2019). ""The Importance of Reproduction for the Conservation of Slow-Growing Animal Populations"". Reproductive Sciences in Animal Conservation. Advances in Experimental Medicine and Biology. Advances in Experimental Medicine and Biology. 1200: 13–39. doi:10.1007/978-3-030-23633-5_2. ISBN 978-3-030-23633-5. PMID 31471793. S2CID 201756810.
McCallum M. L. (2008). ""Amphibian Decline or Extinction? Current Declines Dwarf Background Extinction Rate"" (PDF). Journal of Herpetology. 41 (3): 483–491. doi:10.1670/0022-1511(2007)41[483:ADOECD]2.0.CO;2. S2CID 30162903. Archived from the original (PDF) on 2008-12-17.
Myers, Norman; Mittermeier, Russell A.; Mittermeier, Cristina G.; da Fonseca, Gustavo A. B.; Kent, Jennifer (2000). ""Biodiversity hotspots for conservation priorities"". Nature. 403 (6772): 853–8. Bibcode:2000Natur.403..853M. doi:10.1038/35002501. PMID 10706275. S2CID 4414279.
Brooks T. M.; Mittermeier R. A.; Gerlach J.; Hoffmann M.; Lamoreux J. F.; Mittermeier C. G.; Pilgrim J. D.; Rodrigues A. S. L. (2006). ""Global Biodiversity Conservation Priorities"". Science. 313 (5783): 58–61. Bibcode:2006Sci...313...58B. doi:10.1126/science.1127609. PMID 16825561. S2CID 5133902.
Kareiva P.; Marvier M. (2003). ""Conserving Biodiversity Coldspots"" (PDF). American Scientist. 91 (4): 344–351. doi:10.1511/2003.4.344. Archived from the original (PDF) on September 6, 2006.
Mccallum, Malcolm L.; Bury, Gwendolyn W. (2013). ""Google search patterns suggest declining interest in the environment"". Biodiversity and Conservation. 22 (6–7): 1355–67. doi:10.1007/s10531-013-0476-6. S2CID 15593201.
Myers, Norman; Mittermeier, Russell A.; Mittermeier, Cristina G.; da Fonseca, Gustavo A. B.; Kent, Jennifer (2000). ""Biodiversity hotspots for conservation priorities"". Nature. 403 (6772): 853–8. Bibcode:2000Natur.403..853M. doi:10.1038/35002501. PMID 10706275. S2CID 4414279.
Wake, D. B.; Vredenburg, V. T. (2008). ""Are we in the midst of the sixth mass extinction? A view from the world of amphibians"". Proceedings of the National Academy of Sciences. 105 (Suppl 1): 11466–73. Bibcode:2008PNAS..10511466W. doi:10.1073/pnas.0801921105. PMC 2556420. PMID 18695221.Textbooks

Groom, Martha J.; Meffe, Gary K.; Carroll, C. Ronald. (2006). Principles of Conservation Biology. Sunderland, Mass: Sinauer Associates. ISBN 978-0-87893-597-0.
Norse, Elliott A.; Crowder, Larry B., eds. (2005). Marine conservation biology: the science of maintaining the sea's biodiversity. Washington, DC: Island Press. ISBN 978-1-55963-662-9.
Primack, Richard B. (2004). A primer of Conservation Biology. Sunderland, Mass: Sinauer Associates. ISBN 978-0-87893-728-8.
Primack, Richard B. (2006). Essentials of Conservation Biology. Sunderland, Mass: Sinauer Associates. ISBN 978-0-87893-720-2.
Wilcox, Bruce A.; Soulé, Michael E.; Soulé, Michael E. (1980). Conservation Biology: an evolutionary-ecological perspective. Sunderland, Mass: Sinauer Associates. ISBN 978-0-87893-800-1.
Kleiman, Devra G.; Thompson, Katerina V.; Baer, Charlotte Kirk (2010). Wild Mammals in Captivity. Chicago, Illinois: University of Chicago Press. ISBN 978-0-226-44009-5.
Scheldeman, X.; van Zonneveld, M. (2010). Training Manual on Spatial Analysis of Plant Diversity and Distribution. Bioversity International. Archived from the original on 2011-09-27.
Sodhi, Navjot S.; Ehrlich, Paul R. (2010). Conservation biology for all. Oxford University Press. A free textbook for download.
Sutherland, W.;  et al. (2015).  Sutherland, William J; Dicks, Lynn V; Ockendon, Nancy; Smith, Rebecca K (eds.). What Works in Conservation. Open Book Publishers. doi:10.11647/OBP.0060. ISBN 978-1-78374-157-1. A free textbook for download.General non-fiction

Christy, Bryan (2008). The Lizard King: The true crimes and passions of the world's greatest reptile smugglers. New York: Twelve. ISBN 978-0-446-58095-3.
Nijhuis, Michelle (July 23, 2012). ""Conservationists use triage to determine which species to save and not: Like battlefield medics, conservationists are being forced to explicitly apply triage to determine which creatures to save and which to let go"". Scientific American. Retrieved 2017-05-07.Periodicals

Animal Conservation [2]
Biological Conservation
Conservation [3], a quarterly magazine of the Society for Conservation Biology
Conservation and Society [4]
Conservation Biology, a peer-reviewed journal of the Society for Conservation Biology
Conservation Letters
Diversity and Distributions
Ecology and SocietyTraining manuals

White, James Emery; Kapoor-Vijay, Promila (1992). Conservation biology: a training manual for biological diversity and genetic resources. London: Commonwealth Science Council, Commonwealth Secretariat. ISBN 978-0-85092-392-6.


== External links ==
Conservation Biology Institute (CBI)
United Nations Environment Programme - World Conservation Monitoring Centre (UNEP-WCMC)
The Center for Biodiversity and Conservation - (American Museum of Natural History)
Sarkar, Sahotra. ""Conservation Biology"".  In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.
Dictionary of the History of Ideas
Conservationevidence.com - Free access to conservation studies",939452,1601,"All accuracy disputes, All articles with dead external links, All articles with failed verification, All articles with incomplete citations, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with NKC identifiers, Articles with dead external links from February 2020, Articles with disputed statements from July 2012, Articles with failed verification from July 2012, Articles with incomplete citations from October 2016, Articles with permanently dead external links, Articles with short description, Biology terminology, CS1: long volume value, CS1 errors: bare URL, CS1 errors: missing title, CS1 maint: bot: original URL status unknown, Conservation biology, Environmental conservation, Habitat, Landscape ecology, Philosophy of biology, Short description matches Wikidata, Webarchive template wayback links, Wikipedia articles needing page number citations from October 2016",1133668847,biology
https://en.wikipedia.org/wiki/Systems_biology,Systems biology,"Systems biology is the computational and mathematical analysis and modeling of complex biological systems. It is a biology-based interdisciplinary field of study that focuses on complex interactions within biological systems, using a holistic approach (holism instead of the more traditional reductionism) to biological research.Particularly from the year 2000 onwards, the concept has been used widely in biology in a variety of contexts. The Human Genome Project is an example of applied systems thinking in biology which has led to new, collaborative ways of working on problems in the biological field of genetics. One of the aims of systems biology is to model and discover emergent properties, properties of cells, tissues and organisms functioning as a system whose theoretical description is only possible using techniques of systems biology. These typically involve metabolic networks or cell signaling networks.","Systems biology is the computational and mathematical analysis and modeling of complex biological systems. It is a biology-based interdisciplinary field of study that focuses on complex interactions within biological systems, using a holistic approach (holism instead of the more traditional reductionism) to biological research.Particularly from the year 2000 onwards, the concept has been used widely in biology in a variety of contexts. The Human Genome Project is an example of applied systems thinking in biology which has led to new, collaborative ways of working on problems in the biological field of genetics. One of the aims of systems biology is to model and discover emergent properties, properties of cells, tissues and organisms functioning as a system whose theoretical description is only possible using techniques of systems biology. These typically involve metabolic networks or cell signaling networks.


== Overview ==
Systems biology can be considered from a number of different aspects.
As a field of study, particularly, the study of the interactions between the components of biological systems, and how these interactions give rise to the function and behavior of that system (for example, the enzymes and metabolites in a metabolic pathway or the heart beats).As a paradigm, systems biology is usually defined in antithesis to the so-called reductionist paradigm (biological organisation), although it is consistent with the scientific method. The distinction between the two paradigms is referred to in these quotations: ""the reductionist approach has successfully identified most of the components and many of the interactions but, unfortunately, offers no convincing concepts or methods to understand how system properties emerge ... the pluralism of causes and effects in biological networks is better addressed by observing, through quantitative measures, multiple components simultaneously and by rigorous data integration with mathematical models."" (Sauer et al.) ""Systems biology ... is about putting together rather than taking apart, integration rather than reduction. It requires that we develop ways of thinking about integration that are as rigorous as our reductionist programmes, but different. ... It means changing our philosophy, in the full sense of the term."" (Denis Noble)As a series of operational protocols used for performing research, namely a cycle composed of theory, analytic or computational modelling to propose specific testable hypotheses about a biological system, experimental validation, and then using the newly acquired quantitative description of cells or cell processes to refine the computational model or theory. Since the objective is a model of the interactions in a system, the experimental techniques that most suit systems biology are those that are system-wide and attempt to be as complete as possible. Therefore, transcriptomics, metabolomics, proteomics and high-throughput techniques are used to collect quantitative data for the construction and validation of models.As the application of dynamical systems theory to molecular biology. Indeed, the focus on the dynamics of the studied systems is the main conceptual difference between systems biology and bioinformatics.As a socioscientific phenomenon defined by the strategy of pursuing integration of complex data about the interactions in biological systems from diverse experimental sources using interdisciplinary tools and personnel.


== History ==
Systems biology was begun as a new field of science around 2000, when the Institute for Systems Biology was established in Seattle in an effort to lure ""computational"" type people who it was felt were not attracted to the academic settings of the university. The institute did not have a clear definition of what the field actually was: roughly bringing together people from diverse fields to use computers to holistically study biology in new ways. A Department of Systems Biology at Harvard Medical School was launched in 2003.  In 2006 it was predicted that the buzz generated by the ""very fashionable"" new concept would cause all the major universities to need a systems biology department, thus that there would be careers available for graduates with a modicum of ability in computer programming and biology. In 2006 the National Science Foundation put forward a challenge to build a mathematical model of the whole cell. In 2012 the first whole-cell model of Mycoplasma genitalium was achieved by the Karr Laboratory at the Mount Sinai School of Medicine in New York. The whole-cell model is able to predict viability of M. genitalium cells in response to genetic mutations.An earlier precursor of systems biology, as a distinct discipline, may have been by systems theorist Mihajlo Mesarovic in 1966 with an international symposium at the Case Institute of Technology in Cleveland, Ohio, titled Systems Theory and Biology. Mesarovic predicted that perhaps in the future there would be such as thing as ""systems biology"". Other early precursors that focused on the view that biology should be analyzed as a system, rather than a simple collection of parts, were Metabolic Control Analysis, developed by Henrik Kacser and Jim Burns later thoroughly revised, and Reinhart Heinrich and Tom Rapoport, and Biochemical Systems Theory developed by Michael SavageauAccording to Robert Rosen in the 1960s, holistic biology had become passé by the early 20th century, as more empirical science dominated by molecular chemistry had become popular. Echoing him forty years later in 2006 Kling writes that the success of molecular biology throughout the 20th century had suppressed holistic computational methods. By 2011 the National Institutes of Health had made grant money available to support over ten systems biology centers in the United States, but by 2012 Hunter writes that systems biology had not lived up to the hype, having promised more than it achieved, which had caused it to become a somewhat minor field with few practical applications. Nonetheless, proponents hoped that it might once prove more useful in the future.

An important milestone in the development of systems biology has become the international project Physiome.


== Associated disciplines ==

According to the interpretation of systems biology as using large data sets using interdisciplinary tools, a typical application is metabolomics, which is the complete set of all the metabolic products, metabolites, in the system at the organism, cell, or tissue level.Items that may be a computer database include: phenomics, organismal variation in phenotype as it changes during its life span; genomics, organismal deoxyribonucleic acid (DNA) sequence, including intra-organismal cell specific variation. (i.e., telomere length variation); epigenomics/epigenetics, organismal and corresponding cell specific transcriptomic regulating factors not empirically coded in the genomic sequence. (i.e., DNA methylation, Histone acetylation and deacetylation, etc.); transcriptomics, organismal, tissue or whole cell gene expression measurements by DNA microarrays or serial analysis of gene expression; interferomics, organismal, tissue, or cell-level transcript correcting factors (i.e., RNA interference), proteomics, organismal, tissue, or cell level measurements of proteins and peptides via two-dimensional gel electrophoresis, mass spectrometry or multi-dimensional protein identification techniques (advanced HPLC systems coupled with mass spectrometry). Sub disciplines include phosphoproteomics, glycoproteomics and other methods to detect chemically modified proteins; glycomics, organismal, tissue, or cell-level measurements of carbohydrates;  lipidomics, organismal, tissue, or cell level measurements of lipids.The molecular interactions within the cell are also studied, this is called interactomics. A discipline in this field of study is protein-protein interactions, although interactomics includes the interactions of other molecules. Neuroelectrodynamics, where the computer's or a brain's computing function as a dynamic system is studied along with its (bio)physical mechanisms; and fluxomics, measurements of the rates of metabolic reactions in a biological system (cell, tissue, or organism).In approaching a systems biology problem there are two main approaches. These are the top down and bottom up approach. The top down approach takes as much of the system into account as possible and relies largely on experimental results. The RNA-Seq technique is an example of an experimental top down approach. Conversely, the bottom up approach is used to create detailed models while also incorporating experimental data. An example of the bottom up approach is the use of circuit models to describe a simple gene network.Various technologies utilized to capture dynamic changes in mRNA, proteins, and post-translational modifications. Mechanobiology, forces and physical properties at all scales, their interplay with other regulatory mechanisms; biosemiotics, analysis of the system of sign relations of an organism or other biosystems; Physiomics, a systematic study of physiome in biology.
Cancer systems biology is an example of the systems biology approach, which can be distinguished by the specific object of study (tumorigenesis and treatment of cancer). It works with the specific data  (patient samples, high-throughput data with particular attention to characterizing cancer genome in patient tumour samples) and tools (immortalized cancer cell lines, mouse models of tumorigenesis, xenograft models, high-throughput sequencing methods, siRNA-based gene knocking down high-throughput screenings, computational modeling of the consequences of somatic mutations and genome instability). The long-term objective of the systems biology of cancer is ability to better diagnose cancer, classify it and better predict the outcome of a suggested treatment, which is a basis for personalized cancer medicine and virtual cancer patient in more distant prospective. Significant efforts in computational systems biology of cancer have been made in creating realistic multi-scale in silico models of various tumours.The systems biology approach often involves the development of mechanistic models, such as the reconstruction of dynamic systems from the quantitative properties of their elementary building blocks. For instance, a cellular network can be modelled mathematically using methods coming from chemical kinetics and control theory. Due to the large number of parameters, variables and constraints in cellular networks, numerical and computational techniques are often used (e.g., flux balance analysis).


== Bioinformatics and data analysis ==
Other aspects of computer science, informatics, and statistics are also used in systems biology. These include new forms of computational models, such as the use of process calculi to model biological processes (notable approaches include stochastic π-calculus, BioAmbients, Beta Binders, BioPEPA, and Brane calculus) and constraint-based modeling; integration of information from the literature, using techniques of information extraction and text mining; development of online databases and repositories for sharing data and models, approaches to database integration and software interoperability via loose coupling of software, websites and databases, or commercial suits; network-based approaches for analyzing high dimensional genomic data sets. For example, weighted correlation network analysis is often used for identifying clusters (referred to as modules), modeling the relationship between clusters, calculating fuzzy measures of cluster (module) membership, identifying intramodular hubs, and for studying cluster preservation in other data sets; pathway-based methods for omics data analysis, e.g. approaches to identify and score pathways with differential activity of their gene, protein, or metabolite members. Much of the analysis of genomic data sets also include identifying correlations. Additionally, as much of the information comes from different fields, the development of syntactically and semantically sound ways of representing biological models is needed.


== Creating biological models ==

Researchers begin by choosing a biological pathway and diagramming all of the protein interactions. After determining all of the interactions of the proteins, mass action kinetics is utilized to describe the speed of the reactions in the system. Mass action kinetics will provide differential equations to model the biological system as a mathematical model in which experiments can determine the parameter values to use in the differential equations. These parameter values will be the reaction rates of each proteins interaction in the system. This model determines the behavior of certain proteins in biological systems and bring new insight to the specific activities of individual proteins. Sometimes it is not possible to gather all reaction rates of a system. Unknown reaction rates are determined by simulating the model of known parameters and target behavior which provides possible parameter values.The use of constraint-based reconstruction and analysis (COBRA) methods has become popular among systems biologists to simulate and predict the metabolic phenotypes, using genome-scale models. One of the methods is the flux balance analysis (FBA) approach, by which one can study the biochemical networks and analyze the flow of metabolites through a particular metabolic network, by maximizing the object of interest.


== See also ==


== References ==


== Further reading ==
Klipp, Edda; Liebermeister, Wolfram; Wierling, Christoph; Kowald, Axel (2016). Systems Biology - A Textbook, 2nd edition. Wiley. ISBN 978-3-527-33636-4.
Asfar S. Azmi, ed. (2012). Systems Biology in Cancer Research and Drug Discovery. ISBN 978-94-007-4819-4.
Kitano, Hiroaki (15 October 2001). Foundations of Systems Biology. MIT Press. ISBN 978-0-262-11266-6.
Werner, Eric (29 March 2007). ""All systems go"". Nature. 446 (7135): 493–494. Bibcode:2007Natur.446..493W. doi:10.1038/446493a. provides a comparative review of three books:
Alon, Uri (7 July 2006). An Introduction to Systems Biology: Design Principles of Biological Circuits. Chapman & Hall. ISBN 978-1-58488-642-6.
Kaneko, Kunihiko (15 September 2006). Life: An Introduction to Complex Systems Biology. Springer-Verlag. Bibcode:2006lics.book.....K. ISBN 978-3-540-32666-3.
Palsson, Bernhard O. (16 January 2006). Systems Biology: Properties of Reconstructed Networks. Cambridge University Press. ISBN 978-0-521-85903-5.
Werner Dubitzky; Olaf Wolkenhauer; Hiroki Yokota; Kwan-Hyun Cho, eds. (13 August 2013). Encyclopedia of Systems Biology. Springer-Verlag. ISBN 978-1-4419-9864-4.


== External links ==

 Media related to Systems biology at Wikimedia Commons
Biological Systems in bio-physics-wiki",786795,1253,"All articles with style issues, All articles with unsourced statements, Articles with BNF identifiers, Articles with GND identifiers, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with NDL identifiers, Articles with short description, Articles with unsourced statements from December 2020, Articles with unsourced statements from October 2019, Bioinformatics, CS1 errors: missing periodical, Commons category link is on Wikidata, Computational fields of study, Short description matches Wikidata, Systems biology, Wikipedia articles with style issues from December 2022",1129297987,biology
https://en.wikipedia.org/wiki/Marine_biology,Marine biology,"Marine biology is the scientific study of the biology of marine life, organisms in the sea.  Given that in biology many phyla, families and genera have some species that live in the sea and others that live on land, marine biology classifies species based on the environment rather than on taxonomy.
A large proportion of all life on Earth lives in the ocean. The exact size of this large proportion is unknown, since many ocean species are still to be discovered. The ocean is a complex three-dimensional world covering approximately 71% of the Earth's surface. The habitats studied in marine biology include everything from the tiny layers of surface water in which organisms and abiotic items may be trapped in surface tension between the ocean and atmosphere, to the depths of the oceanic trenches, sometimes 10,000 meters or more beneath the surface of the ocean. Specific habitats include estuaries, coral reefs, kelp forests, seagrass meadows, the surrounds of seamounts and thermal vents, tidepools, muddy, sandy and rocky bottoms, and the open ocean (pelagic) zone, where solid objects are rare and the surface of the water is the only visible boundary. The organisms studied range from microscopic phytoplankton and zooplankton to huge cetaceans (whales) 25–32 meters (82–105 feet) in length. Marine ecology is the study of how marine organisms interact with each other and the environment.
Marine life is a vast resource, providing food, medicine, and raw materials, in addition to helping to support recreation and tourism all over the world. At a fundamental level, marine life helps determine the very nature of our planet. Marine organisms contribute significantly to the oxygen cycle, and are involved in the regulation of the Earth's climate. Shorelines are in part shaped and protected by marine life, and some marine organisms even help create new land.Many species are economically important to humans, including both finfish and shellfish. It is also becoming understood that the well-being of marine organisms and other organisms are linked in fundamental ways. The human body of knowledge regarding the relationship between life in the sea and important cycles is rapidly growing, with new discoveries being made nearly every day. These cycles include those of matter (such as the carbon cycle) and of air (such as Earth's respiration, and movement of energy through ecosystems including the ocean). Large areas beneath the ocean surface still remain effectively unexplored.","Marine biology is the scientific study of the biology of marine life, organisms in the sea.  Given that in biology many phyla, families and genera have some species that live in the sea and others that live on land, marine biology classifies species based on the environment rather than on taxonomy.
A large proportion of all life on Earth lives in the ocean. The exact size of this large proportion is unknown, since many ocean species are still to be discovered. The ocean is a complex three-dimensional world covering approximately 71% of the Earth's surface. The habitats studied in marine biology include everything from the tiny layers of surface water in which organisms and abiotic items may be trapped in surface tension between the ocean and atmosphere, to the depths of the oceanic trenches, sometimes 10,000 meters or more beneath the surface of the ocean. Specific habitats include estuaries, coral reefs, kelp forests, seagrass meadows, the surrounds of seamounts and thermal vents, tidepools, muddy, sandy and rocky bottoms, and the open ocean (pelagic) zone, where solid objects are rare and the surface of the water is the only visible boundary. The organisms studied range from microscopic phytoplankton and zooplankton to huge cetaceans (whales) 25–32 meters (82–105 feet) in length. Marine ecology is the study of how marine organisms interact with each other and the environment.
Marine life is a vast resource, providing food, medicine, and raw materials, in addition to helping to support recreation and tourism all over the world. At a fundamental level, marine life helps determine the very nature of our planet. Marine organisms contribute significantly to the oxygen cycle, and are involved in the regulation of the Earth's climate. Shorelines are in part shaped and protected by marine life, and some marine organisms even help create new land.Many species are economically important to humans, including both finfish and shellfish. It is also becoming understood that the well-being of marine organisms and other organisms are linked in fundamental ways. The human body of knowledge regarding the relationship between life in the sea and important cycles is rapidly growing, with new discoveries being made nearly every day. These cycles include those of matter (such as the carbon cycle) and of air (such as Earth's respiration, and movement of energy through ecosystems including the ocean). Large areas beneath the ocean surface still remain effectively unexplored.


== Biological oceanography ==

Marine biology can be contrasted with biological oceanography. Marine life is a field of study both in marine biology and in biological oceanography. Biological oceanography is the study of how organisms affect and are affected by the physics, chemistry, and geology of the oceanographic system. Biological oceanography mostly focuses on the microorganisms within the ocean; looking at how they are affected by their environment and how that affects larger marine creatures and their ecosystem. Biological oceanography is similar to marine biology, but it studies ocean life from a different perspective. Biological oceanography takes a bottom up approach in terms of the food web, while marine biology studies the ocean from a top down perspective. Biological oceanography mainly focuses on the ecosystem of the ocean with an emphasis on plankton: their diversity (morphology, nutritional sources, motility, and metabolism); their productivity and how that plays a role in the global carbon cycle; and their distribution (predation and life cycle). Biological oceanography also investigates the role of microbes in food webs, and how humans impact the ecosystems in the oceans.


== Marine habitats ==

Marine habitats can be divided into coastal and open ocean habitats. Coastal habitats are found in the area that extends from the shoreline to the edge of the continental shelf. Most marine life is found in coastal habitats, even though the shelf area occupies only seven percent of the total ocean area. Open ocean habitats are found in the deep ocean beyond the edge of the continental shelf. Alternatively, marine habitats can be divided into pelagic and demersal habitats. Pelagic habitats are found near the surface or in the open water column, away from the bottom of the ocean and affected by ocean currents, while demersal habitats are near or on the bottom. Marine habitats can be modified by their inhabitants. Some marine organisms, like corals, kelp and sea grasses, are ecosystem engineers which reshape the marine environment to the point where they create further habitat for other organisms.


=== Intertidal and near shore ===

Intertidal zones, the areas that are close to the shore, are constantly being exposed and covered by the ocean's tides. A huge array of life can be found within this zone. Shore habitats span from the upper intertidal zones to the area where land vegetation takes prominence. It can be underwater anywhere from daily to very infrequently. Many species here are scavengers, living off of sea life that is washed up on the shore. Many land animals also make much use of the shore and intertidal habitats. A subgroup of organisms in this habitat bores and grinds exposed rock through the process of bioerosion.


=== Estuaries ===

Estuaries are also near shore and influenced by the tides. An estuary is a partially enclosed coastal body of water with one or more rivers or streams flowing into it and with a free connection to the open sea. Estuaries form a transition zone between freshwater river environments and saltwater maritime environments. They are subject both to marine influences—such as tides, waves, and the influx of saline water—and to riverine influences—such as flows of fresh water and sediment. The shifting flows of both sea water and fresh water provide high levels of nutrients both in the water column and in sediment, making estuaries among the most productive natural habitats in the world.


=== Reefs ===

Reefs comprise some of the densest and most diverse habitats in the world. The best-known types of reefs are tropical coral reefs which exist in most tropical waters; however, reefs can also exist in cold water. Reefs are built up by corals and other calcium-depositing animals, usually on top of a rocky outcrop on the ocean floor. Reefs can also grow on other surfaces, which has made it possible to create artificial reefs. Coral reefs also support a huge community of life, including the corals themselves, their symbiotic zooxanthellae, tropical fish and many other organisms.

Much attention in marine biology is focused on coral reefs and the El Niño weather phenomenon. In 1998, coral reefs experienced the most severe mass bleaching events on record, when vast expanses of reefs across the world died because sea surface temperatures rose well above normal. Some reefs are recovering, but scientists say that between 50% and 70% of the world's coral reefs are now endangered and predict that global warming could exacerbate this trend.


=== Open ocean ===

The open ocean is relatively unproductive because of a lack of nutrients, yet because it is so vast, in total it produces the most primary productivity. The open ocean is separated into different zones, and the different zones each have different ecologies. Zones which vary according to their depth include the epipelagic, mesopelagic, bathypelagic, abyssopelagic, and hadopelagic zones. Zones which vary by the amount of light they receive include the photic and aphotic zones. Much of the aphotic zone's energy is supplied by the open ocean in the form of detritus.


=== Deep sea and trenches ===

The deepest recorded oceanic trench measured to date is the Mariana Trench, near the Philippines, in the Pacific Ocean at 10,924 m (35,840 ft). At such depths, water pressure is extreme and there is no sunlight, but some life still exists. A white flatfish, a shrimp and a jellyfish were seen by the American crew of the bathyscaphe Trieste when it dove to the bottom in 1960. In general, the deep sea is considered to start at the aphotic zone, the point where sunlight loses its power of transference through the water. Many life forms that live at these depths have the ability to create their own light known as bio-luminescence. Marine life also flourishes around seamounts that rise from the depths, where fish and other sea life congregate to spawn and feed. Hydrothermal vents along the mid-ocean ridge spreading centers act as oases, as do their opposites, cold seeps. Such places support unique biomes and many new microbes and other lifeforms have been discovered at these locations.


== Marine life ==

In biology many phyla, families and genera have some species that live in the sea and others that live on land. Marine biology classifies species based on the environment rather than on taxonomy. For this reason marine biology encompasses not only organisms that live only in a marine environment, but also other organisms whose lives revolve around the sea. 


=== Microscopic life ===

As inhabitants of the largest environment on Earth, microbial marine systems drive changes in every global system.  Microbes are responsible for virtually all the photosynthesis that occurs in the ocean, as well as the cycling of carbon, nitrogen, phosphorus and other nutrients and trace elements.Microscopic life undersea is incredibly diverse and still poorly understood. For example, the role of viruses in marine ecosystems is barely being explored even in the beginning of the 21st century.The role of phytoplankton is better understood due to their critical position as the most numerous primary producers on Earth. Phytoplankton are categorized into cyanobacteria (also called blue-green algae/bacteria), various types of algae (red, green, brown, and yellow-green), diatoms, dinoflagellates, euglenoids, coccolithophorids, cryptomonads, chrysophytes, chlorophytes, prasinophytes, and silicoflagellates.
Zooplankton tend to be somewhat larger, and not all are microscopic. Many Protozoa are zooplankton, including dinoflagellates, zooflagellates, foraminiferans, and radiolarians. Some of these (such as dinoflagellates) are also phytoplankton; the distinction between plants and animals often breaks down in very small organisms. Other zooplankton include cnidarians, ctenophores, chaetognaths, molluscs, arthropods, urochordates, and annelids such as polychaetes. Many larger animals begin their life as zooplankton before they become large enough to take their familiar forms. Two examples are fish larvae and sea stars (also called starfish).


=== Plants and algae ===

Microscopic algae and plants provide important habitats for life, sometimes acting as hiding places for larval forms of larger fish and foraging places for invertebrates.
Algal life is widespread and very diverse under the ocean. Microscopic photosynthetic algae contribute a larger proportion of the world's photosynthetic output than all the terrestrial forests combined. Most of the niche occupied by sub plants on land is actually occupied by macroscopic algae in the ocean, such as Sargassum and kelp, which are commonly known as seaweeds that create kelp forests.
Plants that survive in the sea are often found in shallow waters, such as the seagrasses (examples of which are eelgrass, Zostera, and turtle grass, Thalassia). These plants have adapted to the high salinity of the ocean environment. The intertidal zone is also a good place to find plant life in the sea, where mangroves or cordgrass or beach grass might grow. 


=== Invertebrates ===

As on land, invertebrates make up a huge portion of all life in the sea. Invertebrate sea life includes Cnidaria such as jellyfish and sea anemones; Ctenophora; sea worms including the phyla Platyhelminthes, Nemertea, Annelida, Sipuncula, Echiura, Chaetognatha, and Phoronida; Mollusca including shellfish, squid, octopus; Arthropoda including Chelicerata and Crustacea; Porifera; Bryozoa; Echinodermata including starfish; and Urochordata including sea squirts or tunicates. Invertebrates have no backbone. There are over a million species.


=== Fungi ===

Over 10,000 species of fungi are known from marine environments. These are parasitic on marine algae or animals, or are saprobes on algae, corals, protozoan cysts, sea grasses, wood and other substrata, and can also be found in sea foam. Spores of many species have special appendages which facilitate attachment to the substratum. A very diverse range of unusual secondary metabolites is produced by marine fungi.


=== Vertebrates ===


==== Fish ====

A reported 33,400 species of fish, including bony and cartilaginous fish, had been described by 2016, more than all other vertebrates combined. About 60% of fish species live in saltwater.


==== Reptiles ====

Reptiles which inhabit or frequent the sea include sea turtles, sea snakes, terrapins, the marine iguana, and the saltwater crocodile. Most extant marine reptiles, except for some sea snakes, are oviparous and need to return to land to lay their eggs. Thus most species, excepting sea turtles, spend most of their lives on or near land rather than in the ocean. Despite their marine adaptations, most sea snakes prefer shallow waters nearby land, around islands, especially waters that are somewhat sheltered, as well as near estuaries. Some extinct marine reptiles, such as ichthyosaurs, evolved to be viviparous and had no requirement to return to land.


==== Birds ====

Birds adapted to living in the marine environment are often called seabirds. Examples include albatross, penguins, gannets, and auks. Although they spend most of their lives in the ocean, species such as gulls can often be found thousands of miles inland.


==== Mammals ====

There are five main types of marine mammals, namely cetaceans (toothed whales and baleen whales); sirenians such as manatees;  pinnipeds including seals and the walrus; sea otters; and the 
polar bear. All are air-breathing, and while some such as the sperm whale can dive for prolonged periods, all must return to the surface to breathe.


== Subfields ==
The marine ecosystem is large, and thus there are many sub-fields of marine biology. Most involve studying specializations of particular animal groups, such as phycology, invertebrate zoology and ichthyology. Other subfields study the physical effects of continual immersion in sea water and the ocean in general, adaptation to a salty environment, and the effects of changing various oceanic properties on marine life. A subfield of marine biology studies the relationships between oceans and ocean life, and global warming and environmental issues (such as carbon dioxide displacement). Recent marine biotechnology has focused largely on marine biomolecules, especially proteins, that may have uses in medicine or engineering. Marine environments are the home to many exotic biological materials that may inspire biomimetic materials.


=== Related fields ===
Marine biology is a branch of biology. It is closely linked to oceanography, especially biological oceanography, and may be regarded as a sub-field of marine science. It also encompasses many ideas from ecology. Fisheries science and marine conservation can be considered partial offshoots of marine biology (as well as environmental studies). Marine Chemistry, Physical oceanography and Atmospheric sciences are closely related to this field.


== Distribution factors ==
An active research topic in marine biology is to discover and map the life cycles of various species and where they spend their time. Technologies that aid in this discovery include pop-up satellite archival tags, acoustic tags, and a variety of other data loggers. Marine biologists study how the ocean currents, tides and many other oceanic factors affect ocean life forms, including their growth, distribution and well-being. This has only recently become technically feasible with advances in GPS and newer underwater visual devices.Most ocean life breeds in specific places, nests or not in others, spends time as juveniles in still others, and in maturity in yet others. Scientists know little about where many species spend different parts of their life cycles especially in the infant and juvenile years. For example, it is still largely unknown where juvenile sea turtles and some year-1 sharks travel. Recent advances in underwater tracking devices are illuminating what we know about marine organisms that live at great Ocean depths. The information that pop-up satellite archival tags give aids in certain time of the year fishing closures and development of a marine protected area. This data is important to both scientists and fishermen because they are discovering that by restricting commercial fishing in one small area they can have a large impact in maintaining a healthy fish population in a much larger area.


== History ==

The study of marine biology dates back to Aristotle (384–322 BC), who made many observations of life in the sea around Lesbos, laying the foundation for many future discoveries. In 1768, Samuel Gottlieb Gmelin (1744–1774) published the Historia Fucorum, the first work dedicated to marine algae and the first book on marine biology to use the new binomial nomenclature of Linnaeus. It included elaborate illustrations of seaweed and marine algae on folded leaves. The British naturalist Edward Forbes (1815–1854) is generally regarded as the founder of the science of marine biology. The pace of oceanographic and marine biology studies quickly accelerated during the course of the 19th century.

The observations made in the first studies of marine biology fueled the age of discovery and exploration that followed. During this time, a vast amount of knowledge was gained about the life that exists in the oceans of the world. Many voyages contributed significantly to this pool of knowledge. Among the most significant were the voyages of HMS Beagle where Charles Darwin came up with his theories of evolution and on the formation of coral reefs. Another important expedition was undertaken by HMS Challenger, where findings were made of unexpectedly high species diversity among fauna stimulating much theorizing by population ecologists on how such varieties of life could be maintained in what was thought to be such a hostile environment. This era was important for the history of marine biology but naturalists were still limited in their studies because they lacked technology that would allow them to adequately examine species that lived in deep parts of the oceans.
The creation of marine laboratories was important because it allowed marine biologists to conduct research and process their specimens from expeditions. The oldest marine laboratory in the world, Station biologique de Roscoff, was established in Concarneau, France founded by the College of France in 1859. In the United States, the Scripps Institution of Oceanography dates back to 1903, while the prominent Woods Hole Oceanographic Institute was founded in 1930. The development of technology such as sound navigation ranging, scuba diving gear, submersibles and remotely operated vehicles allowed marine biologists to discover and explore life in deep oceans that was once thought to not exist.


== See also ==


=== Lists ===


== References ==


== Further references ==
Morrissey J and Sumich J (2011) Introduction to the Biology of Marine Life Jones & Bartlett Publishers. ISBN 9780763781606.
Mladenov, Philip V., Marine Biology: A Very Short Introduction, 2nd edn (Oxford, 2020; online edn, Very Short Introductions online, Feb. 2020), http://dx.doi.org/10.1093/actrade/9780198841715.001.0001, accessed 21 Jun. 2020.


== External links ==

Smithsonian Ocean Portal
Marine Conservation Society
Marine biology at Curlie
Marine Ecology - an evolutionary perspective
Free special issue: Marine Biology in Time and Space
Creatures of the deep ocean – National Geographic documentary, 2010.
Exploris
Freshwater and Marine Image Bank - From the University of Washington Library
Marine Training Portal - Portal grouping training initiatives in the field of Marine Biology",1504308,4504,"All articles with dead external links, Articles with BNF identifiers, Articles with Curlie links, Articles with GND identifiers, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with NARA identifiers, Articles with NKC identifiers, Articles with dead external links from January 2018, Articles with dead external links from October 2022, Articles with permanently dead external links, Articles with short description, Biological oceanography, CS1 maint: multiple names: authors list, Commons category link from Wikidata, Fisheries science, Marine biology, Oceanographical terminology, Pages using multiple image with auto scaled images, Short description is different from Wikidata, Webarchive template other archives, Webarchive template wayback links",1132340583,biology
https://en.wikipedia.org/wiki/Synthetic_biology,Synthetic biology,,"
== Definition ==
Synthetic biology (SynBio) is a multidisciplinary field of science that focuses on living systems and organisms, and it applies engineering principles to develop new biological  parts, devices, and systems or to redesign existing systems found in nature.It is a branch of science that encompasses a broad range of methodologies from various disciplines, such as biotechnology, biomaterials, material science/engineering, genetic engineering, molecular biology, molecular engineering, systems biology, membrane science, biophysics, chemical and biological engineering, electrical and computer engineering, control engineering and evolutionary biology.
It includes designing and constructing biological modules, biological systems, and biological machines or, re-design of existing biological systems for useful purposes.Additionally, it is the branch of science that focuses on the new abilities of engineering into existing organisms to redesign them for useful purposes.In order to produce predictable and robust systems with novel functionalities that do not already exist in nature, it is also necessary to apply the engineering paradigm of systems design to biological systems. According to The European Commission, this possibly involves a molecular assembler based on biomolecular systems such as the ribosome.


== History ==

1910: First identifiable use of the term ""synthetic biology"" in Stéphane Leduc's publication Théorie physico-chimique de la vie et générations spontanées. He also noted this term in another publication, La Biologie Synthétique in 1912.1961: Jacob and Monod postulate cellular regulation by molecular networks from their study of the lac operon in E. coli and envisioned the ability to assemble new systems from molecular components.1973: First molecular cloning and amplification of DNA in a plasmid is published in P.N.A.S. by Cohen, Boyer et al. constituting the dawn of synthetic biology.1978: Arber, Nathans and Smith win the Nobel Prize in Physiology or Medicine for the discovery of restriction enzymes, leading Szybalski to offer an editorial comment in the journal Gene:

The work on restriction nucleases not only permits us easily to construct recombinant DNA molecules and to analyze individual genes, but also has led us into the new era of synthetic biology where not only existing genes are described and analyzed but also new gene arrangements can be constructed and evaluated.
1988: First DNA amplification by the polymerase chain reaction (PCR) using a thermostable DNA polymerase is published in Science by Mullis et al. This obviated adding new DNA polymerase after each PCR cycle, thus greatly simplifying DNA mutagenesis and assembly.
2000: Two papers in Nature report synthetic biological circuits, a genetic toggle switch and a biological clock, by combining genes within E. coli cells.2003: The most widely used standardized DNA parts, BioBrick plasmids, are invented by Tom Knight. These parts will become central to the international Genetically Engineered Machine competition (iGEM) founded at MIT in the following year.

2003: Researchers engineer an artemisinin precursor pathway in E. coli.2004: First international conference for synthetic biology, Synthetic Biology 1.0 (SB1.0) is held at the Massachusetts Institute of Technology, USA.
2005: Researchers develop a light-sensing circuit in E. coli. Another group designs circuits capable of multicellular pattern formation.2006: Researchers engineer a synthetic circuit that promotes bacterial invasion of tumour cells.2010: Researchers publish in Science the first synthetic bacterial genome, called M. mycoides JCVI-syn1.0. The genome is made from chemically-synthesized DNA using yeast recombination.
2011: Functional synthetic chromosome arms are engineered in yeast.2012: Charpentier and Doudna labs publish in Science the programming of CRISPR-Cas9 bacterial immunity for targeting DNA cleavage. This technology greatly simplified and expanded eukaryotic gene editing.
2019: Scientists at ETH Zurich report the creation of the first bacterial genome, named Caulobacter ethensis-2.0, made entirely by a computer, although a related viable form of C. ethensis-2.0 does not yet exist.2019: Researchers report the production of a new synthetic (possibly artificial) form of viable life, a variant of the bacteria Escherichia coli, by reducing the natural number of 64 codons in the bacterial genome to 59 codons instead, in order to encode 20 amino acids.2020: Scientist created the first xenobot, programmable synthetic organism derived from frog cells and designed by AI.2021: Scientist reported that xenobots are able to self-replicate by gathering loose cells in the environment and then forming new xenobot.


== Perspectives ==
It is a field whose scope is expanding in terms of systems integration, engineered organisms and practical findings.Engineers view biology as a technology (in other words, a given system includes biotechnology or its biological engineering) Synthetic biology includes the broad redefinition and expansion of biotechnology, with the ultimate goals of being able to design and build engineered live biological systems that process information, manipulate chemicals, fabricate materials and structures, produce energy, provide food, and maintain and enhance human health, as well as advance fundamental knowledge of biological systems (see Biomedical Engineering) and our environment.Researchers and companies working in synthetic biology are using nature's power to solve issues in agriculture, manufacturing, and medicine.Studies in synthetic biology can be subdivided into broad classifications according to the approach they take to the problem at hand: standardization of biological parts, biomolecular engineering, genome engineering, metabolic engineering.Synthetic biology has traditionally been divided into two different approaches: top down and bottom up.

The top down approach involves using metabolic and genetic engineering techniques to impart new functions to living cells.
The bottom up approach involves creating new biological systems in vitro by bringing together 'non-living' biomolecular components, often with the aim of constructing an artificial cell.Two broad categories of synthetic biologists exist. To replicate emergent behaviours from natural biology and build artificial life, unnatural chemicals are used. The other looks for interchangeable components from biological systems to put together and create systems that don't work naturally. In either case, a synthetic objective compels researchers to venture into new area in order to engage and resolve issues that cannot be readily resolved by analysis. Due to this, new paradigms are driven to arise in ways that analysis cannot easily do. In addition to equipments that oscillate, creep, and play tic-tac-toe, synthetic biology has produced diagnostic instruments that enhance the treatment of patients with infectious diseases.Due to more powerful genetic engineering capabilities and decreased DNA synthesis and sequencing costs, the field of synthetic biology is rapidly growing. In 2016, more than 350 companies across 40 countries were actively engaged in synthetic biology applications; all these companies had an estimated net worth of $3.9 billion in the global market. Synthetic biology currently has no generally accepted definition. Here are a few examples:
It is the science of emerging genetic and physical engineering to produce new (and, therefore, synthetic) life forms. In order to develop organisms with novel or enhanced characteristics, this emerging field of study combines biology, engineering, and related disciplines' knowledge and techniques to design chemically synthesised DNA.Biomolecular engineering includes approaches that aim to create a toolkit of functional units that can be introduced to present new technological functions in living cells. Genetic engineering includes approaches to construct synthetic chromosomes or minimal organisms like Mycoplasma laboratorium.
Biomolecular design refers to the general idea of de novo design and additive combination of biomolecular components. Each of these approaches share a similar task: to develop a more synthetic entity at a higher level of complexity by inventively manipulating a simpler part at the preceding level. Optimizing these exogenous pathways in unnatural systems takes iterative fine tuning of the individual biomolecular components to select for the highest concentrations of the desired product.On the other hand, ""re-writers"" are synthetic biologists interested in testing the irreducibility of biological systems. Due to the complexity of natural biological systems, it would be simpler to rebuild the natural systems of interest from the ground up; In order to provide engineered surrogates that are easier to comprehend, control and manipulate. Re-writers draw inspiration from refactoring, a process sometimes used to improve computer software.


== Enabling technologies ==
Several novel enabling technologies were critical to the success of synthetic biology. Concepts include standardization of biological parts and hierarchical abstraction to permit using those parts in synthetic systems. DNA serves as the guide for how biological processes should function, like the score to a complex symphony of life. Our ability to comprehend and design biological systems has undergone significant modifications as a result of developments in the previous few decades in both reading (sequencing) and writing (synthesis) DNA sequences. These developments have produced ground-breaking techniques for designing, assembling, and modifying DNA-encoded genes, materials, circuits, and metabolic pathways, enabling an ever-increasing amount of control over biological systems and even entire organisms.Basic technologies include reading and writing DNA (sequencing and fabrication). Measurements under multiple conditions are needed for accurate modeling and computer-aided design (CAD).


=== DNA and gene synthesis ===
Driven by dramatic decreases in costs of oligonucleotide (""oligos"") synthesis and the advent of PCR, the sizes of DNA constructions from oligos have increased to the genomic level. In 2000, researchers reported synthesis of the 9.6 kbp (kilo bp) Hepatitis C virus genome from chemically synthesized 60 to 80-mers. In 2002 researchers at Stony Brook University succeeded in synthesizing the 7741 bp poliovirus genome from its published sequence, producing the second synthetic genome, spanning two years. In 2003 the 5386 bp genome of the bacteriophage Phi X 174 was assembled in about two weeks. In 2006, the same team, at the J. Craig Venter Institute, constructed and patented a synthetic genome of a novel minimal bacterium, Mycoplasma laboratorium and were working on getting it functioning in a living cell.In 2007 it was reported that several companies were offering synthesis of genetic sequences up to 2000 base pairs (bp) long, for a price of about $1 per bp and a turnaround time of less than two weeks. Oligonucleotides harvested from a photolithographic- or inkjet-manufactured DNA chip combined with PCR and DNA mismatch error-correction allows inexpensive large-scale changes of codons in genetic systems to improve gene expression or incorporate novel amino-acids (see George M. Church's and Anthony Forster's synthetic cell projects.) This favors a synthesis-from-scratch approach.
Additionally, the CRISPR/Cas system has emerged as a promising technique for gene editing. It was described as ""the most important innovation in the synthetic biology space in nearly 30 years"". While other methods take months or years to edit gene sequences, CRISPR speeds that time up to weeks. Due to its ease of use and accessibility, however, it has raised ethical concerns, especially surrounding its use in biohacking.


=== Sequencing ===
DNA sequencing determines the order of nucleotide bases in a DNA molecule. Synthetic biologists use DNA sequencing in their work in several ways. First, large-scale genome sequencing efforts continue to provide information on naturally occurring organisms. This information provides a rich substrate from which synthetic biologists can construct parts and devices. Second, sequencing can verify that the fabricated system is as intended. Third, fast, cheap, and reliable sequencing can facilitate rapid detection and identification of synthetic systems and organisms.


=== Modularity ===
This is the ability of a system or component to operate without reference to its context.The most used: 22–23  standardized DNA parts are BioBrick plasmids, invented by Tom Knight in 2003. Biobricks are stored at the Registry of Standard Biological Parts in Cambridge, Massachusetts. The BioBrick standard has been used by tens of thousands of students worldwide in the international Genetically Engineered Machine (iGEM) competition. BioBrick Assembly Standard 10 promotes modularity by allowing BioBrick coding sequences to be spliced out and exchanged using restriction enzymes EcoRI or XbaI (BioBrick prefix) and SpeI and PstI (BioBrick suffix).: 22–23 Sequence overlap between two genetic elements (genes or coding sequences), called overlapping genes, can prevent their individual manipulation. To increase genome modularity, the practice of genome refactoring or improving ""the internal structure of an existing system for future use, while simultaneously maintaining external system function"" has been adopted across synthetic biology disciplines. Some notable examples of refactoring including the nitrogen fixation cluster and type III secretion system along with bacteriophages T7 and ΦX174.While DNA is most important for information storage, a large fraction of the cell's activities are carried out by proteins. Tools can send proteins to specific regions of the cell and to link different proteins together. The interaction strength between protein partners should be tunable between a lifetime of seconds (desirable for dynamic signaling events) up to an irreversible interaction (desirable for device stability or resilient to harsh conditions). Interactions such as coiled coils, SH3 domain-peptide binding or SpyTag/SpyCatcher offer such control. In addition it is necessary to regulate protein-protein interactions in cells, such as with light (using light-oxygen-voltage-sensing domains) or cell-permeable small molecules by chemically induced dimerization.In a living cell, molecular motifs are embedded in a bigger network with upstream and downstream components. These components may alter the signaling capability of the modeling module. In the case of ultrasensitive modules, the sensitivity contribution of a module can differ from the sensitivity that the module sustains in isolation.


=== Modeling ===
Models inform the design of engineered biological systems by better predicting system behavior prior to fabrication. Synthetic biology benefits from better models of how biological molecules bind substrates and catalyze reactions, how DNA encodes the information needed to specify the cell and how multi-component integrated systems behave. Multiscale models of gene regulatory networks focus on synthetic biology applications. Simulations can model all biomolecular interactions in transcription, translation, regulation and induction of gene regulatory networks.


=== Microfluidics ===
Microfluidics, in particular droplet microfluidics, is an emerging tool used to construct new components, and to analyze and characterize them. It is widely employed in screening assays.


=== Synthetic transcription factors ===
Studies have considered the components of the DNA transcription mechanism. One desire of scientists creating synthetic biological circuits is to be able to control the transcription of synthetic DNA in unicellular organisms (prokaryotes) and in multicellular organisms (eukaryotes). One study tested the adjustability of synthetic transcription factors (sTFs) in areas of transcription output and cooperative ability among multiple transcription factor complexes. Researchers were able to mutate functional regions called zinc fingers, the DNA specific component of sTFs, to decrease their affinity for specific operator DNA sequence sites, and thus decrease the associated site-specific activity of the sTF (usually transcriptional regulation). They further used the zinc fingers as components of complex-forming sTFs, which are the eukaryotic translation mechanisms.


== Applications ==


=== Biosensors ===
A biosensor refers to an engineered organism, usually a bacterium, that is capable of reporting some ambient phenomenon such as the presence of heavy metals or toxins. One such system is the Lux operon of Aliivibrio fischeri, which codes for the enzyme that is the source of bacterial bioluminescence, and can be placed after a respondent promoter to express the luminescence genes in response to a specific environmental stimulus. One such sensor created, consisted of a bioluminescent bacterial coating on a photosensitive computer chip to detect certain petroleum pollutants. When the bacteria sense the pollutant, they luminesce. Another example of a similar mechanism is the detection of landmines by an engineered E.coli reporter strain capable of detecting TNT and its main degradation product DNT, and consequently producing a green fluorescent protein (GFP).Modified organisms can sense environmental signals and send output signals that can be detected and serve diagnostic purposes. Microbe cohorts have been used.Biosensors could also be used to detect pathogenic signatures – such as of SARS-CoV-2 – and can be wearable.


=== Food and drink ===

Cellular agriculture focuses on the production of agricultural products from cell cultures using a combination of biotechnology, tissue engineering, molecular biology, and synthetic biology to create and design new methods of producing proteins, fats, and tissues that would otherwise come from traditional agriculture. Most of the industry is focused on animal products such as meat, milk, and eggs, produced in cell culture rather than raising and slaughtering farmed livestock which is associated with substantial global problems of detrimental environmental impacts (e.g. of meat production), animal welfare, food security and human health. Cellular agriculture is field of the biobased economy. The most well known cellular agriculture concept is cultured meat. (Full article...)
However, not all synthetic nutrition products are animal food products – for instance, as of 2021 there are also products of synthetic coffee that are reported to be close to commercialization. Similar fields of research and production based on synthetic biology that can be used for the production of food and drink are:

Genetically engineered microbial food cultures (e.g. for solar-energy-based protein powder)
Cell-free artificial synthesis (e.g. synthetic starch; see Biobased economy#Agriculture)


=== Materials ===
Photosynthetic microbial cells have been used as a step to synthetic production of spider silk.


=== Biological computers ===
A biological computer refers to an engineered biological system that can perform computer-like operations, which is a dominant paradigm in synthetic biology. Researchers built and characterized a variety of logic gates in a number of organisms, and demonstrated both analog and digital computation in living cells. They demonstrated that bacteria can be engineered to perform both analog and/or digital computation. In human cells research demonstrated a universal logic evaluator that operates in mammalian cells in 2007. Subsequently, researchers utilized this paradigm to demonstrate a proof-of-concept therapy that uses biological digital computation to detect and kill human cancer cells in 2011. Another group of researchers demonstrated in 2016 that principles of computer engineering, can be used to automate digital circuit design in bacterial cells. In 2017, researchers demonstrated the 'Boolean logic and arithmetic through DNA excision' (BLADE) system to engineer digital computation in human cells. In 2019, researchers implemented a perceptron in biological systems opening the way for machine learning in these systems.


=== Cell transformation ===
Cells use interacting genes and proteins, which are called gene circuits, to implement diverse function, such as responding to environmental signals, decision making and communication. Three key components are involved: DNA, RNA and Synthetic biologist designed gene circuits that can control gene expression from several levels including transcriptional, post-transcriptional and translational levels.
Traditional metabolic engineering has been bolstered by the introduction of combinations of foreign genes and optimization by directed evolution. This includes engineering E. coli and yeast for commercial production of a precursor of the antimalarial drug, Artemisinin.Entire organisms have yet to be created from scratch, although living cells can be transformed with new DNA. Several ways allow constructing synthetic DNA components and even entire synthetic genomes, but once the desired genetic code is obtained, it is integrated into a living cell that is expected to manifest the desired new capabilities or phenotypes while growing and thriving. Cell transformation is used to create biological circuits, which can be manipulated to yield desired outputs.By integrating synthetic biology with materials science, it would be possible to use cells as microscopic molecular foundries to produce materials with properties whose properties were genetically encoded. Re-engineering has produced Curli fibers, the amyloid component of extracellular material of biofilms, as a platform for programmable nanomaterial. These nanofibers were genetically constructed for specific functions, including adhesion to substrates, nanoparticle templating and protein immobilization.


=== Designed proteins ===

Natural proteins can be engineered, for example, by directed evolution, novel protein structures that match or improve on the functionality of existing proteins can be produced. One group generated a helix bundle that was capable of binding oxygen with similar properties as hemoglobin, yet did not bind carbon monoxide. A similar protein structure was generated to support a variety of oxidoreductase activities  while another formed a structurally and sequentially novel ATPase. Another group generated a family of G-protein coupled receptors that could be activated by the inert small molecule clozapine N-oxide but insensitive to the native ligand, acetylcholine; these receptors are known as DREADDs. Novel functionalities or protein specificity can also be engineered using computational approaches.  One study was able to use two different computational methods – a bioinformatics and molecular modeling method to mine sequence databases, and a computational enzyme design method to reprogram enzyme specificity. Both methods resulted in designed enzymes with greater than 100 fold specificity for production of longer chain alcohols from sugar.Another common investigation is expansion of the natural set of 20 amino acids. Excluding stop codons, 61 codons have been identified, but only 20 amino acids are coded generally in all organisms. Certain codons are engineered to code for alternative amino acids including: nonstandard amino acids such as O-methyl tyrosine; or exogenous amino acids such as 4-fluorophenylalanine. Typically, these projects make use of re-coded nonsense suppressor tRNA-Aminoacyl tRNA synthetase pairs from other organisms, though in most cases substantial engineering is required.Other researchers investigated protein structure and function by reducing the normal set of 20 amino acids. Limited protein sequence libraries are made by generating proteins where groups of amino acids may be replaced by a single amino acid. For instance, several non-polar amino acids within a protein can all be replaced with a single non-polar amino acid. One project demonstrated that an engineered version of Chorismate mutase still had catalytic activity when only 9 amino acids were used.Researchers and companies practice synthetic biology to synthesize industrial enzymes with high activity, optimal yields and effectiveness. These synthesized enzymes aim to improve products such as detergents and lactose-free dairy products, as well as make them more cost effective. The improvements of metabolic engineering by synthetic biology is an example of a biotechnological technique utilized in industry to discover pharmaceuticals and fermentive chemicals. Synthetic biology may investigate modular pathway systems in biochemical production and increase yields of metabolic production. Artificial enzymatic activity and subsequent effects on metabolic reaction rates and yields may develop ""efficient new strategies for improving cellular properties ... for industrially important biochemical production"".


=== Designed nucleic acid systems ===
Scientists can encode digital information onto a single strand of synthetic DNA. In 2012, George M. Church encoded one of his books about synthetic biology in DNA. The 5.3 Mb of data was more than 1000 times greater than the previous largest amount of information to be stored in synthesized DNA. A similar project encoded the complete sonnets of William Shakespeare in DNA. More generally, algorithms such as NUPACK, ViennaRNA, Ribosome Binding Site Calculator, Cello, and Non-Repetitive Parts Calculator enables the design of new genetic systems.
Many technologies have been developed for incorporating unnatural nucleotides and amino acids into nucleic acids and proteins, both in vitro and in vivo. For example, in May 2014, researchers announced that they had successfully introduced two new artificial nucleotides into bacterial DNA. By including individual artificial nucleotides in the culture media, they were able to exchange the bacteria 24 times; they did not generate mRNA or proteins able to use the artificial nucleotides.


=== Space exploration ===
Synthetic biology raised NASA's interest as it could help to produce resources for astronauts from a restricted portfolio of compounds sent from Earth. On Mars, in particular, synthetic biology could lead to production processes based on local resources, making it a powerful tool in the development of occupied outposts with less dependence on Earth. Work has gone into developing plant strains that are able to cope with the harsh Martian environment, using similar techniques to those employed to increase resilience to certain environmental factors in agricultural crops.


=== Synthetic life ===

One important topic in synthetic biology is synthetic life, that is concerned with hypothetical organisms created in vitro from biomolecules and/or chemical analogues thereof. Synthetic life experiments attempt to either probe the origins of life, study some of the properties of life, or more ambitiously to recreate life from non-living (abiotic) components. Synthetic life biology attempts to create living organisms capable of carrying out important functions, from manufacturing pharmaceuticals to detoxifying polluted land and water. In medicine, it offers prospects of using designer biological parts as a starting point for new classes of therapies and diagnostic tools.A living ""artificial cell"" has been defined as a completely synthetic cell that can capture energy, maintain ion gradients, contain macromolecules as well as store information and have the ability to mutate. Nobody has been able to create such a cell.A completely synthetic bacterial chromosome was produced in 2010 by Craig Venter, and his team introduced it to genomically emptied bacterial host cells. The host cells were able to grow and replicate. The Mycoplasma laboratorium is the only living organism with completely engineered genome.
The first living organism with 'artificial' expanded DNA code was presented in 2014; the team used E. coli that had its genome extracted and replaced with a chromosome with an expanded genetic code. The nucleosides added are d5SICS and dNaM.In May 2019, researchers, in a milestone effort, reported the creation of a new synthetic (possibly artificial) form of viable life, a variant of the bacteria Escherichia coli, by reducing the natural number of 64 codons in the bacterial genome to 59 codons instead, in order to encode 20 amino acids.In 2017 the international Build-a-Cell large-scale open-source research collaboration for the construction of synthetic living cells was started, followed by national synthetic cell organizations in several countries, including FabriCell, MaxSynBio and BaSyC.  The European synthetic cell efforts were unified in 2019 as SynCellEU initiative.


=== Drug delivery platforms ===
Synthetic biology has achieved significant advancements in altering and simplifying the therapeutics scope in a relatively short period of time. In fact, new therapeutic platforms, from the discovery of disease mechanisms and drug targets to the manufacture and transport of small molecules, are made possible by the logical and model-guided design construction of biological components.


==== Engineered bacteria-based platform ====
Bacteria have long been used in cancer treatment. Bifidobacterium and Clostridium selectively colonize tumors and reduce their size. Recently synthetic biologists reprogrammed bacteria to sense and respond to a particular cancer state. Most often bacteria are used to deliver a therapeutic molecule directly to the tumor to minimize off-target effects. To target the tumor cells, peptides that can specifically recognize a tumor were expressed on the surfaces of bacteria. Peptides used include an affibody molecule that specifically targets human epidermal growth factor receptor 2 and a synthetic adhesin. The other way is to allow bacteria to sense the tumor microenvironment, for example hypoxia, by building an AND logic gate into bacteria. The bacteria then only release target therapeutic molecules to the tumor through either lysis or the bacterial secretion system. Lysis has the advantage that it can stimulate the immune system and control growth. Multiple types of secretion systems can be used and other strategies as well. The system is inducible by external signals. Inducers include chemicals, electromagnetic or light waves.
Multiple species and strains are applied in these therapeutics. Most commonly used bacteria are Salmonella typhimurium, Escherichia Coli, Bifidobacteria, Streptococcus, Lactobacillus, Listeria and Bacillus subtilis. Each of these species have their own property and are unique to cancer therapy in terms of tissue colonization, interaction with immune system and ease of application.


==== Cell-based platform ====
The immune system plays an important role in cancer and can be harnessed to attack cancer cells. Cell-based therapies focus on immunotherapies, mostly by engineering T cells.
T cell receptors were engineered and ‘trained’ to detect cancer epitopes. Chimeric antigen receptors (CARs) are composed of a fragment of an antibody fused to intracellular T cell signaling domains that can activate and trigger proliferation of the cell. A second generation CAR-based therapy was approved by FDA.Gene switches were designed to enhance safety of the treatment. Kill switches were developed to terminate the therapy should the patient show severe side effects. Mechanisms can more finely control the system and stop and reactivate it. Since the number of T-cells are important for therapy persistence and severity, growth of T-cells is also controlled to dial the effectiveness and safety of therapeutics.Although several mechanisms can improve safety and control, limitations include the difficulty of inducing large DNA circuits into the cells and risks associated with introducing foreign components, especially proteins, into cells.


=== Organoids ===
Synthetic biology has been used for organoids, which are lab-grown organs with application to medical research and transplantation.


=== Bioprinted organs ===

There are many applications for 3D bioprinting in the medical field. An infant patient with a rare respiratory disease known as tracheobronchomalacia (TBM) was given a tracheal splint that was created with 3D printing. 3D bioprinting can be used to reconstruct tissue from various regions of the body. Patients with end-stage bladder disease can be treated by using engineered bladder tissues to rebuild the damaged organ. This technology can also potentially be applied to bone, skin, cartilage and muscle tissue. Though one long-term goal of 3D bioprinting technology is to reconstruct an entire organ, there has been little success in printing fully functional organs. Unlike implantable stents, organs have complex shapes and are significantly harder to bioprint. A bioprinted heart, for example, must not only meet structural requirements, but also vascularization, mechanical load, and electrical signal propagation requirements. Israeli researchers constructed a rabbit-sized heart out of human cells in 2019.In 2022, first success of a clinical trial for a 3D bioprinted transplant that is made from the patient's own cells, an external ear to treat microtia, was reported.For bioprinted food like meat see #Food and drink.


=== Other transplants and induced regeneration ===
There is ongoing research and development into synthetic biology based methods for inducing regeneration in humans as well the creation of transplantable artificial organs.


=== Nanoparticles, artificial cells and micro-droplets ===

Synthetic biology can be used for creating nanoparticles which can be used for drug-delivery as well as for other purposes. Complementing research and development seeks to and has created synthetic cells that mimics functions of biological cells. Applications include medicine such as designer-nanoparticles that make blood cells eat away – from the inside out – portions of atherosclerotic plaque that cause heart attacks. Synthetic micro-droplets for algal cells or synergistic algal-bacterial multicellular spheroid microbial reactors, for example, could be used to produce hydrogen as hydrogen economy biotechnology.


== Ethics ==

The creation of new life and the tampering of existing life has raised ethical concerns in the field of synthetic biology and are actively being discussed.Common ethical questions include:

Is it morally right to tamper with nature?
Is one playing God when creating new life?
What happens if a synthetic organism accidentally escapes?
What if an individual misuses synthetic biology and creates a harmful entity (e.g., a biological weapon)?
Who will have control of and access to the products of synthetic biology?
Who will gain from these innovations? Investors? Medical patients? Industrial farmers?
Does the patent system allow patents on living organisms? What about parts of organisms, like HIV resistance genes in humans?
What if a new creation is deserving of moral or legal status?The ethical aspects of synthetic biology has 3 main features: biosafety, biosecurity, and the creation of new life forms. Other ethical issues mentioned include the regulation of new creations, patent management of new creations, benefit distribution, and research integrity.Ethical issues have surfaced for recombinant DNA and genetically modified organism (GMO) technologies and extensive regulations of genetic engineering and pathogen research were in place in many jurisdictions. Amy Gutmann, former head of the Presidential Bioethics Commission, argued that we should avoid the temptation to over-regulate synthetic biology in general, and genetic engineering in particular. According to Gutmann, ""Regulatory parsimony is especially important in emerging technologies...where the temptation to stifle innovation on the basis of uncertainty and fear of the unknown is particularly great.  The blunt instruments of statutory and regulatory restraint may not only inhibit the distribution of new benefits,  but can be counterproductive to security and safety by preventing researchers from developing effective safeguards."".


=== The ""creation"" of life ===
One ethical question is whether or not it is acceptable to create new life forms, sometimes known as ""playing God"". Currently, the creation of new life forms not present in nature is at small-scale, the potential benefits and dangers remain unknown, and careful consideration and oversight are ensured for most studies. Many advocates express the great potential value—to agriculture, medicine, and academic knowledge, among other fields—of creating artificial life forms. Creation of new entities could expand scientific knowledge well beyond what is currently known from studying natural phenomena. Yet there is concern that artificial life forms may reduce nature's ""purity"" (i.e., nature could be somehow corrupted by human intervention and manipulation) and potentially influence the adoption of more engineering-like principles instead of biodiversity- and nature-focused ideals. Some are also concerned that if an artificial life form were to be released into nature, it could hamper biodiversity by beating out natural species for resources (similar to how algal blooms kill marine species). Another concern involves the ethical treatment of newly created entities if they happen to sense pain, sentience, and self-perception. There is an ongoing debate as to whether such life forms should be granted moral or legal rights, though no consensus exists as to how these rights would be administered or enforced.


=== Ethical support for synthetic biology ===
Ethics and moral rationales that support certain applications of synthetic biology include their potential mititgation of substantial global problems of detrimental environmental impacts of conventional agriculture (including meat production), animal welfare, food security and human health, as well as potential reduction of human labor needs and, via therapies of diseases, reduction of human suffering and prolonged life.


=== Biosafety and biocontainment ===
What is most ethically appropriate when considering biosafety measures? How can accidental introduction of synthetic life in the natural environment be avoided? Much ethical consideration and critical thought has been given to these questions. Biosafety not only refers to biological containment; it also refers to strides taken to protect the public from potentially hazardous biological agents. Even though such concerns are important and remain unanswered, not all products of synthetic biology present concern for biological safety or negative consequences for the environment. It is argued that most synthetic technologies are benign and are incapable of flourishing in the outside world due to their ""unnatural"" characteristics as there is yet to be an example of a transgenic microbe conferred with a fitness advantage in the wild.
In general, existing hazard controls, risk assessment methodologies, and regulations developed for traditional genetically modified organisms (GMOs) are considered to be sufficient for synthetic organisms.  ""Extrinsic"" biocontainment methods in a laboratory context include physical containment through biosafety cabinets and gloveboxes, as well as personal protective equipment.  In an agricultural context they include isolation distances and pollen barriers, similar to methods for biocontainment of GMOs.  Synthetic organisms may offer increased hazard control because they can be engineered with ""intrinsic"" biocontainment methods that limit their growth in an uncontained environment, or prevent horizontal gene transfer to natural organisms.  Examples of intrinsic biocontainment include auxotrophy, biological kill switches, inability of the organism to replicate or to pass modified or synthetic genes to offspring, and the use of xenobiological organisms using alternative biochemistry, for example using artificial xeno nucleic acids (XNA) instead of DNA. Regarding auxotrophy, bacteria and yeast can be engineered to be unable to produce histidine, an important amino acid for all life. Such organisms can thus only be grown on histidine-rich media in laboratory conditions, nullifying fears that they could spread into undesirable areas.


=== Biosecurity and Bioterrorism ===
Some ethical issues relate to biosecurity, where biosynthetic technologies could be deliberately used to cause harm to society and/or the environment. Since synthetic biology raises ethical issues and biosecurity issues, humanity must consider and plan on how to deal with potentially harmful creations, and what kinds of ethical measures could possibly be employed to deter nefarious biosynthetic technologies.  With the exception of regulating synthetic biology and biotechnology companies, however, the issues are not seen as new because they were raised during the earlier recombinant DNA and genetically modified organism (GMO) debates and extensive regulations of genetic engineering and pathogen research are already in place in many jurisdictions.Additionally, the development of synthetic biology tools has made it easier for individuals with less education, training, and access to equipment to modify and use pathogenic organisms as bioweapons. This increases the threat of bioterrorism, especially as terrorist groups become aware of the significant social, economic, and political disruption caused by pandemics like COVID-19. As new techniques are developed in the field of synthetic biology, the risk of bioterrorism is likely to continue to grow. As Juan Zarate, who served as Deputy National Security Advisor for Combating Terrorism from 2005 to 2009, noted that ""“the severity and extreme disruption of a novel coronavirus will likely spur the imagination of the most creative and dangerous groups and individuals to reconsider bioterrorist attacks.”


=== European Union ===
The European Union-funded project SYNBIOSAFE has issued reports on how to manage synthetic biology. A 2007 paper identified key issues in safety, security, ethics and the science-society interface, which the project defined as public education and ongoing dialogue among scientists, businesses, government and ethicists. The key security issues that SYNBIOSAFE identified involved engaging companies that sell synthetic DNA and the biohacking community of amateur biologists. Key ethical issues concerned the creation of new life forms.
A subsequent report focused on biosecurity, especially the so-called dual-use challenge. For example, while synthetic biology may lead to more efficient production of medical treatments, it may also lead to synthesis or modification of harmful pathogens (e.g., smallpox). The biohacking community remains a source of special concern, as the distributed and diffuse nature of open-source biotechnology makes it difficult to track, regulate or mitigate potential concerns over biosafety and biosecurity.COSY, another European initiative, focuses on public perception and communication. To better communicate synthetic biology and its societal ramifications to a broader public, COSY and SYNBIOSAFE published SYNBIOSAFE, a 38-minute documentary film, in October 2009.The International Association Synthetic Biology has proposed self-regulation. This proposes specific measures that the synthetic biology industry, especially DNA synthesis companies, should implement. In 2007, a group led by scientists from leading DNA-synthesis companies published a ""practical plan for developing an effective oversight framework for the DNA-synthesis industry"".


=== United States ===
In January 2009, the Alfred P. Sloan Foundation funded the Woodrow Wilson Center, the Hastings Center, and the J. Craig Venter Institute to examine the public perception, ethics and policy implications of synthetic biology.On July 9–10, 2009, the National Academies' Committee of Science, Technology & Law convened a symposium on ""Opportunities and Challenges in the Emerging Field of Synthetic Biology"".After the publication of the first synthetic genome and the accompanying media coverage about ""life"" being created, President Barack Obama established the Presidential Commission for the Study of Bioethical Issues to study synthetic biology. The commission convened a series of meetings, and issued a report in December 2010 titled ""New Directions: The Ethics of Synthetic Biology and Emerging Technologies."" The commission stated that ""while Venter's achievement marked a significant technical advance in demonstrating that a relatively large genome could be accurately synthesized and substituted for another, it did not amount to the “creation of life”. It noted that synthetic biology is an emerging field, which creates potential risks and rewards. The commission did not recommend policy or oversight changes and called for continued funding of the research and new funding for monitoring, study of emerging ethical issues and public education.Synthetic biology, as a major tool for biological advances, results in the ""potential for developing biological weapons, possible unforeseen negative impacts on human health ... and any potential environmental impact"". The proliferation of such technology could also make the production of biological and chemical weapons available to a wider array of state and non-state actors. These security issues may be avoided by regulating industry uses of biotechnology through policy legislation. Federal guidelines on genetic manipulation are being proposed by ""the President's Bioethics Commission ... in response to the announced creation of a self-replicating cell from a chemically synthesized genome, put forward 18 recommendations not only for regulating the science ... for educating the public"".


=== Opposition ===
On March 13, 2012, over 100 environmental and civil society groups, including Friends of the Earth, the International Center for Technology Assessment and the ETC Group issued the manifesto The Principles for the Oversight of Synthetic Biology. This manifesto calls for a worldwide moratorium on the release and commercial use of synthetic organisms until more robust regulations and rigorous biosafety measures are established. The groups specifically call for an outright ban on the use of synthetic biology on the human genome or human microbiome. Richard Lewontin wrote that some of the safety tenets for oversight discussed in The Principles for the Oversight of Synthetic Biology are reasonable, but that the main problem with the recommendations in the manifesto is that ""the public at large lacks the ability to enforce any meaningful realization of those recommendations"".


== Health and safety ==

The hazards of synthetic biology include biosafety hazards to workers and the public, biosecurity hazards stemming from deliberate engineering of organisms to cause harm, and environmental hazards.  The biosafety hazards are similar to those for existing fields of biotechnology, mainly exposure to pathogens and toxic chemicals, although novel synthetic organisms may have novel risks.   For biosecurity, there is concern that synthetic or redesigned organisms could theoretically be used for bioterrorism.  Potential risks include recreating known pathogens from scratch, engineering existing pathogens to be more dangerous, and engineering microbes to produce harmful biochemicals.  Lastly, environmental hazards include adverse effects on biodiversity and ecosystem services, including potential changes to land use resulting from agricultural use of synthetic organisms. Synthetic biology is an example of a dual-use technology with the potential to be used in ways that could intentionally or unintentionally harm humans and/or damage the environment. Often ""scientists, their host institutions and funding bodies"" consider whether the planned research could be misused and sometimes implement measures to reduce the likelihood of misuse.Existing risk analysis systems for GMOs are generally considered sufficient for synthetic organisms, although there may be difficulties for an organism built ""bottom-up"" from individual genetic sequences.  Synthetic biology generally falls under existing regulations for GMOs and biotechnology in general, and any regulations that exist for downstream commercial products, although there are generally no regulations in any jurisdiction that are specific to synthetic biology.


== See also ==


== References ==
204. NHGRI. (2019, March 13). Synthetic Biology. Genome.gov. https://www.genome.gov/about-genomics/policy-issues/Synthetic-Biology


== Bibliography ==
Church G, Regis E (2012). How Synthetic Biology will Reinvent Nature and Ourselves. New York, NY: Basic Books. ISBN 978-0465021758.
Synthetic biology and biodiversity ; Science for Environment Policy (PDF). Future Brief 15. Produced for the European Commission DG Environment by the Science Communication Unit, UWE, Bristol (Report). European Commission. 2016.
Venter C (2013). Life at the Speed of Light: The Double Helix and the Dawn of Digital Life. New York, NY: Penguin Books. ISBN 978-0670025404.


== External links ==

Engineered Pathogens and Unnatural Biological Weapons: The Future Threat of Synthetic Biology . Threats and considerations
Synthetic biology books popular science book and textbooks
Introductory Summary of Synthetic Biology. Concise overview of synthetic biology concepts, developments and applications
Collaborative overview article on Synthetic Biology
Controversial DNA startup wants to let customers create creatures (2015-01-03), San Francisco Chronicle
It's Alive, But Is It Life: Synthetic Biology and the Future of Creation (28 September 2016), World Science Festival",1177666,1321,"All Wikipedia articles in need of updating, All articles that may have off-topic sections, All articles with unsourced statements, Appropriate technology, Articles with GND identifiers, Articles with NKC identifiers, Articles with short description, Articles with unsourced statements from April 2018, Articles with unsourced statements from December 2020, Biocybernetics, Bioinformatics, Biotechnology, Bioterrorism, CS1: Julian–Gregorian uncertainty, CS1 maint: url-status, Commons category link from Wikidata, Emerging technologies, Gene expression programming, Molecular genetics, Short description matches Wikidata, Synthetic biology, Systems biology, Webarchive template wayback links, Wikipedia articles in need of updating from January 2019, Wikipedia articles that may have off-topic sections from October 2021",1134204907,biology
https://en.wikipedia.org/wiki/Class_(biology),Class (biology),"In biological classification, class (Latin: classis) is  a taxonomic rank, as well as a taxonomic unit, a taxon, in that rank. It is a group of related taxonomic orders. Other well-known ranks in descending order of size are life, domain, kingdom, phylum, order, family, genus, and species, with class fitting between phylum and order.","In biological classification, class (Latin: classis) is  a taxonomic rank, as well as a taxonomic unit, a taxon, in that rank. It is a group of related taxonomic orders. Other well-known ranks in descending order of size are life, domain, kingdom, phylum, order, family, genus, and species, with class fitting between phylum and order.


== History ==
The class as a distinct rank of biological classification having its own distinctive name (and not just called a top-level genus (genus summum)) was first introduced by the French botanist Joseph Pitton de Tournefort in his classification of plants that appeared in his Eléments de botanique, 1694.
Insofar as a general definition of a class is available, it has historically been conceived as embracing taxa that combine a distinct grade of organization—i.e. a 'level of complexity', measured in terms of how differentiated their organ systems are into distinct regions or sub-organs—with a distinct type of construction, which is to say a particular layout of organ systems. This said, the composition of each class is ultimately determined by the subjective judgment of taxonomists. Often there is no exact agreement, with different taxonomists taking different positions. There are no objective rules for describing a class, but for well-known animals there is likely to be consensus. 
In the first edition of his Systema Naturae (1735), Carl Linnaeus divided all three of his kingdoms of Nature (minerals, plants, and animals) into classes. Only in the animal kingdom are Linnaeus's classes similar to the classes used today; his classes and orders of plants were never intended to represent natural groups, but rather to provide a convenient ""artificial key"" according to his Systema Sexuale, largely based on the arrangement of flowers. In botany, classes are now rarely discussed. Since the first publication of the APG system in 1998, which proposed a taxonomy of the flowering plants up to the level of orders, many sources have preferred to treat ranks higher than orders as informal clades. Where formal ranks have been assigned, the ranks have been reduced to a very much lower level, e.g. class Equisitopsida for the land plants, with the major divisions within the class assigned to subclasses and superorders.The class was considered the highest level of the taxonomic hierarchy until George Cuvier's embranchements, first called Phyla by Ernst Haeckel, were introduced in the early nineteenth century.


== Hierarchy of ranks below and above the level of class ==
As with the other principal ranks, classes can be grouped and subdivided. Here are some examples.


== See also ==
Cladistics
List of animal classes
Phylogenetics
Systematics
Taxonomy


== Explanatory notes ==


== References ==",1236461,612,"Articles containing Latin-language text, Articles with GND identifiers, Articles with short description, Bacterial nomenclature, Biology terminology, Botanical nomenclature, Classes (biology), Plant taxonomy, Short description is different from Wikidata",1132376165,biology
https://en.wikipedia.org/wiki/Tissue_(biology),Tissue (biology),"In biology, tissue is a historically derived biological organizational level between cells and a complete organ. A tissue is therefore often thought of as an ensemble of similar cells and their extracellular matrix from the same origin that together carry out a specific function. Organs are then formed by the functional grouping together of multiple tissues.
The English word ""tissue"" derives from the French word ""tissu"", the past participle of the verb tisser, ""to weave"".
The study of tissues is known as histology or, in connection with disease, as histopathology. Xavier Bichat is considered as the ""Father of Histology"". Plant histology is studied in both plant anatomy and physiology. The classical tools for studying tissues are the paraffin block in which tissue is embedded and then sectioned, the  histological stain, and the optical microscope. Developments in electron microscopy, immunofluorescence, and the use of frozen tissue-sections have enhanced the detail that can be observed in tissues. With these tools, the classical appearances of tissues can be examined in health and disease, enabling considerable refinement of medical diagnosis and prognosis.","In biology, tissue is a historically derived biological organizational level between cells and a complete organ. A tissue is therefore often thought of as an ensemble of similar cells and their extracellular matrix from the same origin that together carry out a specific function. Organs are then formed by the functional grouping together of multiple tissues.
The English word ""tissue"" derives from the French word ""tissu"", the past participle of the verb tisser, ""to weave"".
The study of tissues is known as histology or, in connection with disease, as histopathology. Xavier Bichat is considered as the ""Father of Histology"". Plant histology is studied in both plant anatomy and physiology. The classical tools for studying tissues are the paraffin block in which tissue is embedded and then sectioned, the  histological stain, and the optical microscope. Developments in electron microscopy, immunofluorescence, and the use of frozen tissue-sections have enhanced the detail that can be observed in tissues. With these tools, the classical appearances of tissues can be examined in health and disease, enabling considerable refinement of medical diagnosis and prognosis.


== Plant tissue ==

In plant anatomy, tissues are categorized broadly into three tissue systems: the epidermis, the ground tissue, and the vascular tissue.

Epidermis – Cells forming the outer surface of the leaves and of the young plant body.
Vascular tissue – The primary components of vascular tissue are the xylem and phloem. These transport fluids and nutrients internally.
Ground tissue – Ground tissue is less differentiated than other tissues. Ground tissue manufactures nutrients by photosynthesis and stores reserve nutrients.Plant tissues can also be divided differently into two types:

Meristematic tissues
Permanent tissues.


=== Meristematic tissue ===

Meristematic tissue consists of actively dividing cells and leads to increase in length and thickness of the plant. The primary growth of a plant occurs only in certain specific regions, such as in the tips of stems or roots. It is in these regions that meristematic tissue is present. Cells of this type of tissue are roughly spherical or polyhedral to rectangular in shape, with thin cell walls. New cells produced by meristem are initially those of meristem itself, but as the new cells grow and mature, their characteristics slowly change and they become differentiated as components of meristematic tissue, being classified as:

Apical meristem : Present at the growing tips of stems and roots, they increase the length of the stem and root. They form growing parts at the apices of roots and stems and are responsible for the increase in length, also called primary growth. This meristem is responsible for the linear growth of an organ.
 Lateral meristem: Cells which mainly divide in one plane and cause the organ to increase in diameter and girth. Lateral meristem usually occurs beneath the bark of the tree as cork cambium and in vascular bundles of dicotyledons as vascular cambium. The activity of this cambium forms secondary growth.
Intercalary meristem: Located between permanent tissues, it is usually present at the base of the node, internode, and on leaf base. They are responsible for growth in length of the plant and increasing the size of the internode. They result in branch formation and growth.The cells of meristematic tissue are similar in structure and have a thin and elastic primary cell wall made of cellulose. They are compactly arranged without inter-cellular spaces between them. Each cell contains a dense cytoplasm and a prominent cell nucleus. The dense protoplasm of meristematic cells contains very few vacuoles. Normally the meristematic cells are oval, polygonal, or rectangular in shape.
Meristematic tissue cells have a large nucleus with small or no vacuoles because they have no need to store anything, as opposed to their function of multiplying and increasing the girth and length of the plant, with no intercellular spaces.


=== Permanent tissues ===
Permanent tissues may be defined as a group of living or dead cells formed by meristematic tissue and have lost their ability to divide and have permanently placed at fixed positions in the plant body. Meristematic tissues that take up a specific role lose the ability to divide. This process of taking up a permanent shape, size and a function is called cellular differentiation. Cells of meristematic tissue differentiate to form different types of permanent tissues. There are 2 types of permanent tissues:

simple permanent tissues
complex permanent tissues


==== Simple permanent tissue ====
Simple permanent tissue is a group of cells which are similar in origin, structure, and function. They are of three types:

Parenchyma
Collenchyma
Sclerenchyma


===== Parenchyma =====
Parenchyma (Greek, para – 'beside'; enchyma– infusion – 'tissue') is the bulk of a substance. In plants, it consists of relatively unspecialized living cells with thin cell walls that are usually loosely packed so that intercellular spaces are found between cells of this tissue. These are generally isodiametric, in shape. They contain small number of vacuoles or sometimes they even may not contain any vacuole. Even if they do so the vacuole  is of much smaller size than of normal animal cells. This tissue provides support to plants and also stores food. Chlorenchyma is a special type of parenchyma that contains chlorophyll and performs photosynthesis. In aquatic plants, aerenchyma tissues, or large air cavities, give support to float on water by making them buoyant. Parenchyma cells called idioblasts have metabolic waste. Spindle shape fiber also contained into this cell to support them and known as prosenchyma, succulent parenchyma also noted. In xerophytes, parenchyma tissues store water.


===== Collenchyma =====

Collenchyma (Greek, ‘Colla’ means gum and ‘enchyma’ means infusion) is a living tissue of primary body like Parenchyma. Cells are thin-walled but possess thickening of cellulose, water and pectin substances (pectocellulose) at the corners where a number of cells join. This tissue gives tensile strength to the plant and the cells are compactly arranged and have very little inter-cellular spaces. It occurs chiefly in hypodermis of stems and leaves. It is absent in monocots and in roots.
Collenchymatous tissue acts as a supporting tissue in stems of young plants. It provides mechanical support, elasticity, and tensile strength to the plant body. It helps in manufacturing sugar and storing it as starch. It is present in the margin of leaves and resists tearing effect of the wind.


===== Sclerenchyma =====
Sclerenchyma (Greek, Sclerous means hard and enchyma means infusion) consists of thick-walled, dead cells and protoplasm is negligible. These cells have hard and extremely thick secondary walls due to uniform distribution and high secretion of lignin and have a function of providing mechanical support. They do not have inter-molecular space between them. Lignin deposition is so thick that the cell walls become strong, rigid and impermeable to water which is also known as a stone cell or sclereids. These tissues are mainly of two types: sclerenchyma fiber and sclereids.
Sclerenchyma fibre cells have a narrow lumen and are long, narrow and unicellular. Fibers are elongated cells that are strong and flexible, often used in ropes. Sclereids have extremely thick cell walls and are brittle, and are found in nutshells and legumes.


===== Epidermis =====
The entire surface of the plant consists of a single layer of cells called epidermis or surface tissue. The entire surface of the plant has this outer layer of the epidermis. Hence it is also called  surface tissue. Most of the epidermal cells are relatively flat. The outer and lateral walls of the cell are often thicker than the inner walls. The cells form a continuous sheet without intercellular spaces. It protects all parts of the plant. The outer epidermis is coated with a waxy thick layer called cutin which prevents loss of water. The epidermis also consists of stomata (singular:stoma) which helps in transpiration.


==== Complex permanent tissue ====
The complex permanent tissue consists of more than one type of cells having a common origin which work together as a unit. Complex tissues are mainly concerned with the transportation of mineral nutrients, organic solutes (food materials), and water. That's why it is also known as conducting and vascular tissue. The common types of complex permanent tissue are:

Xylem (or wood)
Phloem (or bast).Xylem and phloem together form vascular bundles.


===== Xylem =====

Xylem (Greek, xylos = wood) serves as a chief conducting tissue of vascular plants. It is responsible for the conduction of water and inorganic solutes. Xylem consists of four kinds of cells:

Tracheids
Vessels (or tracheae)
Xylem fibres or Xylem sclerenchyma
Xylem parenchyma
Xylem tissue is organised in a tube-like fashion along the main axes of stems and roots. It consists of a combination of parenchyma cells, fibers, vessels, tracheids, and ray cells. Longer tubes made up of individual cellssels tracheids, while vessel members are open at each end. Internally, there may be bars of wall material extending across the open space. These cells are joined end to end to form long tubes. Vessel members and tracheids are dead at maturity. Tracheids have thick secondary cell walls and are tapered at the ends. They do not have end openings such as the vessels. The  end overlap with each other, with pairs of pits present. The pit pairs allow water to pass from cell to cell.
Though most conduction in xylem tissue is vertical, lateral conduction along the diameter of a stem is facilitated via rays. Rays are horizontal rows of long-living parenchyma cells that arise out of the vascular cambium.


===== Phloem =====
Phloem consists of:

Sieve tube
Companion cell
Phloem fibre
Phloem parenchyma.Phloem is an equally important plant tissue as it also is part of the 'plumbing system' of a plant. Primarily, phloem carries dissolved food substances throughout the plant. This conduction system is composed of sieve-tube member and companion cells, that are without secondary walls. The parent cells of the vascular cambium produce both xylem and phloem. This usually also includes fibers, parenchyma and ray cells. Sieve tubes are formed from sieve-tube members laid end to end. The end walls, unlike vessel members in xylem, do not have openings. The end walls, however, are full of small pores where cytoplasm extends from cell to cell. These porous connections are called sieve plates. In spite of the fact that their cytoplasm is actively involved in the conduction of food materials, sieve-tube members do not have nuclei at maturity. It is the companion cells that are nestled between sieve-tube members that function in some manner bringing about the conduction of food. Sieve-tube members that are alive contain a polymer called callose, a carbohydrate polymer, forming the callus pad/callus, the colourless substance that covers the sieve plate. Callose stays in solution as long as the cell contents are under pressure. Phloem transports food and materials in plants upwards and downwards as required.


== Animal tissue ==
Animal tissues are grouped into four basic types: connective, muscle, nervous, and epithelial. Collections of tissues joined in units to serve a common function compose organs. While most animals can generally be considered to contain the four tissue types, the manifestation of these tissues can differ depending on the type of organism. For example, the origin of the cells comprising a particular tissue type may differ developmentally for different classifications of animals. Tissue appeared for the first time in the diploblasts, but modern forms only appeared in triploblasts.
The epithelium in all animals is derived from the ectoderm and endoderm (or their precursor in sponges), with a small contribution from the mesoderm, forming the endothelium, a specialized type of epithelium that composes the vasculature. By contrast, a true epithelial tissue is present only in a single layer of cells held together via occluding junctions called tight junctions, to create a selectively permeable barrier. This tissue covers all organismal surfaces that come in contact with the external environment such as the skin, the airways, and the digestive tract. It serves functions of protection, secretion, and absorption, and is separated from other tissues below by a basal lamina.
The connective tissue and the muscular are derived from the mesoderm. The nervous tissue is derived from the ectoderm.


=== Epithelial tissues ===

The epithelial tissues are formed by cells that cover the organ surfaces, such as the surface of skin, the airways, surfaces of soft organs, the reproductive tract, and the inner lining of the digestive tract. The cells comprising an epithelial layer are linked via semi-permeable, tight junctions; hence, this tissue provides a barrier between the external environment and the organ it covers. In addition to this protective function, epithelial tissue may also be specialized to function in secretion, excretion and absorption. Epithelial tissue helps to protect organs from microorganisms, injury, and fluid loss.
Functions of epithelial tissue:

The principle function of epithelial tissues are covering and lining of free surface
The cells of the body's surface form the outer layer of skin.
Inside the body, epithelial cells form the lining of the mouth and alimentary canal and protect these organs.
Epithelial tissues help in the elimination of waste.
Epithelial tissues secrete enzymes and/or hormones in the form of glands.
Some epithelial tissue perform secretory functions. They secrete a variety of substances including sweat, saliva, mucus, enzymes.There are many kinds of epithelium, and nomenclature is somewhat variable. Most classification schemes combine a description of the cell-shape in the upper layer of the epithelium with a word denoting the number of layers: either simple (one layer of cells) or stratified (multiple layers of cells). However, other cellular features such as cilia may also be described in the classification system. Some common kinds of epithelium are listed below:

Simple squamous (pavement) epithelium
Simple cuboidal epithelium
Simple Columnar epithelium
Simple ciliated (pseudostratified) columnar epithelium
Simple glandular columnar epithelium
Stratified non-keratinized squamous epithelium
Stratified keratinized epithelium
Stratified transitional epithelium


=== Connective tissue ===

Connective tissues are fibrous tissues made up of cells separated by non-living material, which is called an extracellular matrix. This matrix can be liquid or rigid. For example, blood contains plasma as its matrix and bone's matrix is rigid. Connective tissue gives shape to organs and holds them in place. Blood, bone, tendon, ligament, adipose, and areolar tissues are examples of connective tissues. One method of classifying connective tissues is to divide them into three types: fibrous connective tissue, skeletal connective tissue, and fluid connective tissue.


=== Muscular tissue ===

Muscle cells form the active contractile tissue of the body known as muscle tissue or muscular tissue. Muscle tissue functions to produce force and cause motion, either locomotion or movement within internal organs. Muscle tissue is separated into three distinct categories: visceral or smooth muscle, found in the inner linings of organs; skeletal muscle, typically attached to bones, which generate gross movement; and cardiac muscle, found in the heart, where it contracts to pump blood throughout an organism.


=== Nervous tissue ===

Cells comprising the central nervous system and peripheral nervous system are classified as nervous (or neural) tissue. In the central nervous system, neural tissues form the brain and spinal cord. In the peripheral nervous system, neural tissues form the cranial nerves and spinal nerves, inclusive of the motor neurons.


=== Mineralized tissues ===

Mineralized tissues are biological tissues that incorporate minerals into soft matrices. Such tissues may be found in both plants and animals,


== History ==

Xavier Bichat introduced word tissue into the study of anatomy by 1801. He was ""the first to propose that tissue is a central element in human anatomy, and he considered organs as collections of often disparate tissues, rather than as entities in themselves"". Although he worked without a microscope, Bichat distinguished 21 types of elementary tissues from which the organs of the human body are composed, a number later reduced by other authors.


== See also ==
Generative tissue
Laser capture microdissection
Tissue microarray
Tissue stress


== References ==


== Sources ==
Raven, Peter H., Evert, Ray F., & Eichhorn, Susan E. (1986). Biology of Plants (4th ed.). New York: Worth Publishers. ISBN 087901315X.
Roeckelein, Jon E. (1998). Dictionary of Theories, Laws, and Concepts in Psychology. Greenwood Publishing Group. ISBN 978-0313304606. Retrieved 1 January 2013.


== External links ==
 Media related to Biological tissues at Wikimedia Commons
List of tissues in ExPASy",4126554,2998,"All articles needing additional references, All articles with unsourced statements, Anatomy, Articles needing additional references from February 2019, Articles with GND identifiers, Articles with NKC identifiers, Articles with short description, Articles with unsourced statements from August 2022, CS1: long volume value, CS1 maint: DOI inactive as of December 2022, Commons category link is on Wikidata, Short description is different from Wikidata, Tissues (biology)",1132552366,biology
https://en.wikipedia.org/wiki/Mutualism_(biology),Mutualism (biology),"Mutualism describes the ecological interaction between two or more species where each species has a net benefit. Mutualism is a common type of ecological interaction. Prominent examples include most vascular plants engaged in mutualistic interactions with mycorrhizae, flowering plants being pollinated by animals, vascular plants being dispersed by animals, and corals with zooxanthellae, among many others. Mutualism can be contrasted with interspecific competition, in which each species experiences reduced fitness, and exploitation, or parasitism, in which one species benefits at the expense of the other.
The term mutualism was introduced by Pierre-Joseph van Beneden in his 1876 book Animal Parasites and Messmates to mean ""mutual aid among species"".Mutualism is often conflated with two other types of ecological phenomena: cooperation and symbiosis. Cooperation most commonly refers to increases in fitness through within-species (intraspecific) interactions, although it has been used (especially in the past) to refer to mutualistic interactions, and it is sometimes used to refer to mutualistic interactions that are not obligate. Symbiosis involves two species living in close physical contact over a long period of their existence and may be mutualistic, parasitic, or commensal, so symbiotic relationships are not always mutualistic, and mutualistic interactions are not always symbiotic. Despite a different definition between mutualistic interactions and symbiosis, mutualistic and symbiosis have been largely used interchangeably in the past, and confusion on their use has persisted.Mutualism plays a key part in ecology and evolution. For example, mutualistic interactions are vital for terrestrial ecosystem function as about 80% of land plants species rely on mycorrhizal relationships with fungi to provide them with inorganic compounds and trace elements. As another example, the estimate of tropical rainforest plants with seed dispersal mutualisms with animals ranges at least from 70–93.5%. In addition, mutualism is thought to have driven the evolution of much of the biological diversity we see, such as flower forms (important for pollination mutualisms) and co-evolution between groups of species. 
Mutualism has also been linked to major evolutionary events, such as the evolution of the eukaryotic cell (symbiogenesis) or the colonization of land by plants in association with mycorrhizal fungi.","Mutualism describes the ecological interaction between two or more species where each species has a net benefit. Mutualism is a common type of ecological interaction. Prominent examples include most vascular plants engaged in mutualistic interactions with mycorrhizae, flowering plants being pollinated by animals, vascular plants being dispersed by animals, and corals with zooxanthellae, among many others. Mutualism can be contrasted with interspecific competition, in which each species experiences reduced fitness, and exploitation, or parasitism, in which one species benefits at the expense of the other.
The term mutualism was introduced by Pierre-Joseph van Beneden in his 1876 book Animal Parasites and Messmates to mean ""mutual aid among species"".Mutualism is often conflated with two other types of ecological phenomena: cooperation and symbiosis. Cooperation most commonly refers to increases in fitness through within-species (intraspecific) interactions, although it has been used (especially in the past) to refer to mutualistic interactions, and it is sometimes used to refer to mutualistic interactions that are not obligate. Symbiosis involves two species living in close physical contact over a long period of their existence and may be mutualistic, parasitic, or commensal, so symbiotic relationships are not always mutualistic, and mutualistic interactions are not always symbiotic. Despite a different definition between mutualistic interactions and symbiosis, mutualistic and symbiosis have been largely used interchangeably in the past, and confusion on their use has persisted.Mutualism plays a key part in ecology and evolution. For example, mutualistic interactions are vital for terrestrial ecosystem function as about 80% of land plants species rely on mycorrhizal relationships with fungi to provide them with inorganic compounds and trace elements. As another example, the estimate of tropical rainforest plants with seed dispersal mutualisms with animals ranges at least from 70–93.5%. In addition, mutualism is thought to have driven the evolution of much of the biological diversity we see, such as flower forms (important for pollination mutualisms) and co-evolution between groups of species. 
Mutualism has also been linked to major evolutionary events, such as the evolution of the eukaryotic cell (symbiogenesis) or the colonization of land by plants in association with mycorrhizal fungi.


== Types ==


=== Resource-resource relationships ===
Mutualistic relationships can be thought of as a form of ""biological barter"" in mycorrhizal associations between plant roots and fungi, with the plant providing carbohydrates to the fungus in return for primarily phosphate but also nitrogenous compounds. Other examples include rhizobia bacteria that fix nitrogen for leguminous plants (family Fabaceae) in return for energy-containing carbohydrates.


=== Service-resource relationships ===

Service-resource relationships are common. Three important types are pollination, cleaning symbiosis, and zoochory.
In pollination, a plant trades food resources in the form of nectar or pollen for the service of pollen dispersal. However, daciniphilous Bulbophyllum orchid species trade sex pheromone precursor or booster components via floral synomones/attractants in a true mutualistic interactions with males of Dacini fruit flies (Diptera: Tephritidae: Dacinae). Phagophiles feed (resource) on ectoparasites, thereby providing anti-pest service, as in cleaning symbiosis.
Elacatinus and Gobiosoma, genera of gobies, feed on ectoparasites of their clients while cleaning them.Zoochory is the dispersal of the seeds of plants by animals. This is similar to pollination in that the plant produces food resources (for example, fleshy fruit, overabundance of seeds) for animals that disperse the seeds (service). Plants may advertise these resources using colour  and a variety of other fruit characteristics.
Another type is ant protection of aphids, where the aphids trade sugar-rich honeydew (a by-product of their mode of feeding on plant sap) in return for defense against predators such as ladybugs.


=== Service-service relationships ===

Strict service-service interactions are very rare, for reasons that are far from clear. One example is the relationship between sea anemones and anemone fish in the family Pomacentridae: the anemones provide the fish with protection from predators (which cannot tolerate the stings of the anemone's tentacles) and the fish defend the anemones against butterflyfish (family Chaetodontidae), which eat anemones. However, in common with many mutualisms, there is more than one aspect to it: in the anemonefish-anemone mutualism, waste ammonia from the fish feeds the symbiotic algae that are found in the anemone's tentacles. Therefore, what appears to be a service-service mutualism in fact has a service-resource component. A second example is that of the relationship between some ants in the genus Pseudomyrmex and trees in the genus Acacia, such as the whistling thorn and bullhorn acacia. The ants nest inside the plant's thorns. In exchange for shelter, the ants protect acacias from attack by herbivores (which they frequently eat when those are small enough, introducing a resource component to this service-service relationship) and competition from other plants by trimming back vegetation that would shade the acacia. In addition, another service-resource component is present, as the ants regularly feed on lipid-rich food-bodies called Beltian bodies that are on the Acacia plant.In the neotropics, the ant Myrmelachista schumanni makes its nest in special cavities in Duroia hirsute. Plants in the vicinity that belong to other species are killed with formic acid. This selective gardening can be so aggressive that small areas of the rainforest are dominated by Duroia hirsute. These peculiar patches are known by local people as ""devil's gardens"".In some of these relationships, the cost of the ant's protection can be quite expensive. Cordia sp. trees in the Amazonian rainforest have a kind of partnership with Allomerus sp. ants, which make their nests in modified leaves. To increase the amount of living space available, the ants will destroy the tree's flower buds. The flowers die and leaves develop instead, providing the ants with more dwellings. Another type of Allomerus sp. ant lives with the Hirtella sp. tree in the same forests, but in this relationship, the tree has turned the tables on the ants. When the tree is ready to produce flowers, the ant abodes on certain branches begin to wither and shrink, forcing the occupants to flee, leaving the tree's flowers to develop free from ant attack.The term ""species group"" can be used to describe the manner in which individual organisms group together. In this non-taxonomic context one can refer to ""same-species groups"" and ""mixed-species groups.""  While same-species groups are the norm, examples of mixed-species groups abound. For example, zebra (Equus burchelli) and wildebeest (Connochaetes taurinus) can remain in association during periods of long distance migration across the Serengeti as a strategy for thwarting predators. Cercopithecus mitis and Cercopithecus ascanius, species of monkey in the Kakamega Forest of Kenya, can stay in close proximity and travel along exactly the same routes through the forest for periods of up to 12 hours. These mixed-species groups cannot be explained by the coincidence of sharing the same habitat. Rather, they are created by the active behavioural choice of at least one of the species in question.


== Mathematical modeling ==
Mathematical treatments of mutualisms, like the study of mutualisms in general, has lagged behind those of predation, or predator-prey, consumer-resource, interactions. In models of mutualisms, the terms ""type I"" and ""type II"" functional responses refer to the linear and saturating relationships, respectively, between benefit provided to an individual of species 1 (y-axis) on the density of species 2 (x-axis).


=== Type I functional response ===
One of the simplest frameworks for modeling species interactions is the Lotka–Volterra equations. In this model, the change in population density of the two mutualists is quantified as:

  
    
      
        
          
            
              
                
                  
                    
                      d
                      
                        N
                        
                          1
                        
                      
                    
                    
                      d
                      t
                    
                  
                
              
              
                
                =
                
                  r
                  
                    1
                  
                
                
                  N
                  
                    1
                  
                
                −
                
                  α
                  
                    11
                  
                
                
                  N
                  
                    1
                  
                  
                    2
                  
                
                +
                
                  β
                  
                    12
                  
                
                
                  N
                  
                    1
                  
                
                
                  N
                  
                    2
                  
                
              
            
            
              
                
                  
                    
                      d
                      
                        N
                        
                          2
                        
                      
                    
                    
                      d
                      t
                    
                  
                
              
              
                
                =
                
                  r
                  
                    2
                  
                
                
                  N
                  
                    2
                  
                
                −
                
                  α
                  
                    22
                  
                
                
                  N
                  
                    2
                  
                  
                    2
                  
                
                +
                
                  β
                  
                    21
                  
                
                
                  N
                  
                    1
                  
                
                
                  N
                  
                    2
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\frac {dN_{1}}{dt}}&=r_{1}N_{1}-\alpha _{11}N_{1}^{2}+\beta _{12}N_{1}N_{2}\\[8pt]{\frac {dN_{2}}{dt}}&=r_{2}N_{2}-\alpha _{22}N_{2}^{2}+\beta _{21}N_{1}N_{2}\end{aligned}}}
  where

  
    
      
        
          N
          
            i
          
        
      
    
    {\displaystyle N_{i}}
   = the population densities.

  
    
      
        
          r
          
            i
          
        
      
    
    {\displaystyle r_{i}}
   = the intrinsic growth rate of the population.

  
    
      
        
          α
          
            i
            i
          
        
      
    
    {\displaystyle \alpha _{ii}}
   = the negative effect of within-species crowding.

  
    
      
        
          β
          
            i
            j
          
        
      
    
    {\displaystyle \beta _{ij}}
   = the beneficial effect of a mutualistic partner's density.Mutualism is in essence the logistic growth equation + mutualistic interaction. The mutualistic interaction term represents the increase in population growth of species one as a result of the presence of greater numbers of species two, and vice versa. As the mutualistic term is always positive, it may lead to unrealistic unbounded growth as it happens with the simple model. So, it is important to include a saturation mechanism to avoid the problem.


=== Type II functional response ===
In 1989, David Hamilton Wright modified the Lotka–Volterra equations by adding a new term, βM/K, to represent a mutualistic relationship. Wright also considered the concept of saturation, which means that with higher densities, there are decreasing benefits of further increases of the mutualist population. Without saturation, species' densities would increase indefinitely. Because that is not possible due to environmental constraints and carrying capacity, a model that includes saturation would be more accurate. Wright's mathematical theory is based on the premise of a simple two-species mutualism model in which the benefits of mutualism become saturated due to limits posed by handling time. Wright defines handling time as the time needed to process a food item, from the initial interaction to the start of a search for new food items and assumes that processing of food and searching for food are mutually exclusive. Mutualists that display foraging behavior are exposed to the restrictions on handling time. Mutualism can be associated with symbiosis.
Handling time interactions
In 1959, C. S. Holling performed his classic disc experiment that assumed the following: that (1), the number of food items captured is proportional to the allotted searching time; and (2), that there is a variable of handling time that exists separately from the notion of search time. He then developed an equation for the Type II functional response, which showed that the feeding rate is equivalent to

  
    
      
        
          
            
              
                
              
              
                
                  a
                  x
                
              
            
            
              
                
              
              
                
                  1
                  +
                  a
                  x
                  
                    T
                    
                      H
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\cfrac {ax}{1+axT_{H}}}}
  where,

a=the instantaneous discovery rate
x=food item density
TH=handling timeThe equation that incorporates Type II functional response and mutualism is:

  
    
      
        
          
            
              d
              N
            
            
              d
              t
            
          
        
        =
        N
        
          [
          
            r
            (
            1
            −
            c
            N
            )
            +
            
              
                
                  
                    
                  
                  
                    
                      b
                      a
                      M
                    
                  
                
                
                  
                    
                  
                  
                    
                      1
                      +
                      a
                      
                        T
                        
                          H
                        
                      
                      M
                    
                  
                
              
            
          
          ]
        
      
    
    {\displaystyle {\frac {dN}{dt}}=N\left[r(1-cN)+{\cfrac {baM}{1+aT_{H}M}}\right]}
  where

N and M=densities of the two mutualists
r=intrinsic rate of increase of N
c=coefficient measuring negative intraspecific interaction. This is equivalent to inverse of the carrying capacity, 1/K, of N, in the logistic equation.
a=instantaneous discovery rate
b=coefficient converting encounters with M to new units of Nor, equivalently,

  
    
      
        
          
            
              d
              N
            
            
              d
              t
            
          
        
        =
        N
        [
        r
        (
        1
        −
        c
        N
        )
        +
        β
        M
        
          /
        
        (
        X
        +
        M
        )
        ]
      
    
    {\displaystyle {\frac {dN}{dt}}=N[r(1-cN)+\beta M/(X+M)]}
  where

X=1/a TH
β=b/THThis model is most effectively applied to free-living species that encounter a number of individuals of the mutualist part in the course of their existences. Wright notes that models of biological mutualism tend to be similar qualitatively, in that the featured isoclines generally have a positive decreasing slope, and by and large similar isocline diagrams. Mutualistic interactions are best visualized as positively sloped isoclines, which can be explained by the fact that the saturation of benefits accorded to mutualism or restrictions posed by outside factors contribute to a decreasing slope.
The type II functional response is visualized as the graph of 
  
    
      
        
          
            
              
                
              
              
                
                  b
                  a
                  M
                
              
            
            
              
                
              
              
                
                  1
                  +
                  a
                  
                    T
                    
                      H
                    
                  
                  M
                
              
            
          
        
      
    
    {\displaystyle {\cfrac {baM}{1+aT_{H}M}}}
   vs. M.


== Structure of networks ==
Mutualistic networks made up out of the interaction between plants and pollinators were found to have a similar structure in very different ecosystems on different continents, consisting of entirely different species. The structure of these mutualistic networks may have large consequences for the way in which pollinator communities respond to increasingly harsh conditions and on the community carrying capacity.Mathematical models that examine the consequences of this network structure for the stability of pollinator communities suggest that the specific way in which plant-pollinator networks are organized minimizes competition between pollinators, reduce the spread of indirect effects and thus enhance ecosystem stability and may even lead to strong indirect facilitation between pollinators when conditions are harsh. This means that pollinator species together can survive under harsh conditions. But it also means that pollinator species collapse simultaneously when conditions pass a critical point. This simultaneous collapse occurs, because pollinator species depend on each other when surviving under difficult conditions.Such a community-wide collapse, involving many pollinator species, can occur suddenly when increasingly harsh conditions pass a critical point and recovery from such a collapse might not be easy. The improvement in conditions needed for pollinators to recover could be substantially larger than the improvement needed to return to conditions at which the pollinator community collapsed.


== Humans ==

Humans are involved in mutualisms with other species: their gut flora is essential for efficient digestion. Infestations of head lice might have been beneficial for humans by fostering an immune response that helps to reduce the threat of body louse borne lethal diseases.Some relationships between humans and domesticated animals and plants are to different degrees mutualistic. For example, agricultural varieties of maize provide food for humans and are unable to reproduce without human intervention because the leafy sheath does not fall open, and the seedhead (the ""corn on the cob"") does not shatter to scatter the seeds naturally.In traditional agriculture, some plants have mutualist as companion plants, providing each other with shelter, soil fertility and/or natural pest control. For example, beans may grow up cornstalks as a trellis, while fixing nitrogen in the soil for the corn, a phenomenon that is used in Three Sisters farming.One researcher has proposed that the key advantage Homo sapiens had over Neanderthals in competing over similar habitats was the former's mutualism with dogs.


== Evolution of mutualism ==


=== Evolution by type ===
Every generation of every organism needs nutrients – and similar nutrients – more than they need particular defensive characteristics, as the fitness benefit of these vary heavily especially by environment. This may be the reason that hosts are more likely to evolve to become dependent on vertically transmitted bacterial mutualists which provide nutrients than those providing defensive benefits. This pattern is generalized beyond bacteria by Yamada et al 2015's demonstration that undernourished Drosophila are heavily dependent on their fungal symbiont Issatchenkia orientalis for amino acids.


=== Mutualism breakdown ===
Mutualisms are not static, and can be lost by evolution. Sachs and Simms (2006) suggest that this can occur via four main pathways: 

One mutualist shifts to parasitism, and no longer benefits its partner, such as headlice
One partner abandons the mutualism and lives autonomously
One partner may go extinct
A partner may be switched to another speciesThere are many examples of mutualism breakdown. For example, plant lineages inhabiting nutrient-rich environments have evolutionarily abandoned mycorrhizal mutualisms many times independently.


== Measuring and defining mutualism ==
Measuring the exact fitness benefit to the individuals in a mutualistic relationship is not always straightforward, particularly when the individuals can receive benefits from a variety of species, for example most plant-pollinator mutualisms. It is therefore common to categorise mutualisms according to the closeness of the association, using terms such as obligate and facultative. Defining ""closeness"", however, is also problematic. It can refer to mutual dependency (the species cannot live without one another) or the biological intimacy of the relationship in relation to physical closeness (e.g., one species living within the tissues of the other species).


== See also ==
Arbuscular mycorrhiza
Co-adaptation
Coevolution
Ecological facilitation
Frugivore
Greater honeyguide – has a mutualism with humans
Interspecies communication
Müllerian mimicry
Mutualisms and conservation
Mutual Aid: A Factor of Evolution
Symbiogenesis


== References ==


== Further references ==


== Further reading ==

Boucher, D. G.; James, S.; Keeler, K. (1984). ""The ecology of mutualism"". Annual Review of Ecology and Systematics. 13: 315–347. doi:10.1146/annurev.es.13.110182.001531.
Boucher, D. H. (editor) (1985) The Biology of Mutualism : Ecology and Evolution London : Croom Helm 388 p. ISBN 0-7099-3238-3",1869228,1460,"All articles with unsourced statements, Articles with GND identifiers, Articles with LNB identifiers, Articles with NKC identifiers, Articles with short description, Articles with unsourced statements from August 2022, Articles with unsourced statements from January 2021, Biological interactions, Commons category link is on Wikidata, Ethology, Mutualism (biology), Short description is different from Wikidata, Symbiosis, Use dmy dates from September 2019, Wikipedia articles needing page number citations from December 2017",1134276088,biology
https://en.wikipedia.org/wiki/Endogeny_(biology),Endogeny (biology),"Endogenous substances and processes are those that originate from within a living system such as an organism, tissue, or cell.In contrast, exogenous substances and processes are those that originate from outside of an organism.
For example, estradiol is an endogenous estrogen hormone produced within the body, whereas ethinylestradiol is an exogenous synthetic estrogen, commonly used in birth control pills.","Endogenous substances and processes are those that originate from within a living system such as an organism, tissue, or cell.In contrast, exogenous substances and processes are those that originate from outside of an organism.
For example, estradiol is an endogenous estrogen hormone produced within the body, whereas ethinylestradiol is an exogenous synthetic estrogen, commonly used in birth control pills.


== References ==


== External links ==
 The dictionary definition of endogeny at Wiktionary",429621,247,"Articles with short description, Biology, Short description matches Wikidata",1094666604,biology
https://en.wikipedia.org/wiki/Polymorphism_(biology),Polymorphism (biology),"In biology, polymorphism is the occurrence of two or more clearly different morphs or forms, also referred to as alternative phenotypes, in the population of a species. To be classified as such, morphs must occupy the same habitat at the same time and belong to a panmictic population (one with random mating).Put simply, polymorphism is when there are two or more possibilities of a trait on a gene. For example, there is more than one possible trait in terms of a jaguar's skin colouring; they can be light morph or dark morph. Due to having more than one possible variation for this gene, it is termed 'polymorphism'. However, if the jaguar has only one possible trait for that gene, it would be termed ""monomorphic"". For example, if there was only one possible skin colour that a jaguar could have, it would be termed monomorphic.
The term polyphenism can be used to clarify that the different forms arise from the same genotype. Genetic polymorphism is a term used somewhat differently by geneticists and molecular biologists to describe certain mutations in the genotype, such as single nucleotide polymorphisms that may not always correspond to a phenotype, but always corresponds to a branch in the genetic tree. See below.
Polymorphism is common in nature; it is related to biodiversity, genetic variation, and adaptation. Polymorphism usually functions to retain a variety of forms in a population living in a varied environment.: 126  The most common example is sexual dimorphism, which occurs in many organisms. Other examples are mimetic forms of butterflies (see mimicry), and human hemoglobin and blood types.
According to the theory of evolution, polymorphism results from evolutionary processes, as does any aspect of a species. It is heritable and is modified by natural selection. In polyphenism, an individual's genetic makeup allows for different morphs, and the switch mechanism that determines which morph is shown is environmental. In genetic polymorphism, the genetic makeup determines the morph.
The term polymorphism also refers to the occurrence of structurally and functionally more than two different types of individuals, called zooids, within the same organism. It is a characteristic feature of cnidarians.
For example,  Obelia has feeding individuals, the gastrozooids; the individuals capable of asexual reproduction only, the gonozooids, blastostyles; and free-living or sexually reproducing individuals, the medusae.
Balanced polymorphism refers to the maintenance of different phenotypes in population.","In biology, polymorphism is the occurrence of two or more clearly different morphs or forms, also referred to as alternative phenotypes, in the population of a species. To be classified as such, morphs must occupy the same habitat at the same time and belong to a panmictic population (one with random mating).Put simply, polymorphism is when there are two or more possibilities of a trait on a gene. For example, there is more than one possible trait in terms of a jaguar's skin colouring; they can be light morph or dark morph. Due to having more than one possible variation for this gene, it is termed 'polymorphism'. However, if the jaguar has only one possible trait for that gene, it would be termed ""monomorphic"". For example, if there was only one possible skin colour that a jaguar could have, it would be termed monomorphic.
The term polyphenism can be used to clarify that the different forms arise from the same genotype. Genetic polymorphism is a term used somewhat differently by geneticists and molecular biologists to describe certain mutations in the genotype, such as single nucleotide polymorphisms that may not always correspond to a phenotype, but always corresponds to a branch in the genetic tree. See below.
Polymorphism is common in nature; it is related to biodiversity, genetic variation, and adaptation. Polymorphism usually functions to retain a variety of forms in a population living in a varied environment.: 126  The most common example is sexual dimorphism, which occurs in many organisms. Other examples are mimetic forms of butterflies (see mimicry), and human hemoglobin and blood types.
According to the theory of evolution, polymorphism results from evolutionary processes, as does any aspect of a species. It is heritable and is modified by natural selection. In polyphenism, an individual's genetic makeup allows for different morphs, and the switch mechanism that determines which morph is shown is environmental. In genetic polymorphism, the genetic makeup determines the morph.
The term polymorphism also refers to the occurrence of structurally and functionally more than two different types of individuals, called zooids, within the same organism. It is a characteristic feature of cnidarians.
For example,  Obelia has feeding individuals, the gastrozooids; the individuals capable of asexual reproduction only, the gonozooids, blastostyles; and free-living or sexually reproducing individuals, the medusae.
Balanced polymorphism refers to the maintenance of different phenotypes in population.


== Terminology ==
Monomorphism means having only one form. Dimorphism means having two forms.

Polymorphism does not cover characteristics showing continuous variation (such as weight), though this has a heritable component. Polymorphism deals with forms in which the variation is discrete (discontinuous) or strongly bimodal or polymodal.
Morphs must occupy the same habitat at the same time; this excludes geographical races and seasonal forms. The use of the words ""morph"" or ""polymorphism"" for what is a visibly different geographical race or variant is common, but incorrect. The significance of geographical variation is that it may lead to allopatric speciation, whereas true polymorphism takes place in panmictic populations.
The term was first used to describe visible forms, but it has been extended to include cryptic morphs, for instance blood types, which can be revealed by a test.
Rare variations are not classified as polymorphisms, and mutations by themselves do not constitute polymorphisms. To qualify as a polymorphism, some kind of balance must exist between morphs underpinned by inheritance. The criterion is that the frequency of the least common morph is too high simply to be the result of new mutations or, as a rough guide, that it is greater than 1% (though that is far higher than any normal mutation rate for a single allele).: ch. 5 


=== Nomenclature ===
Polymorphism crosses several discipline boundaries, including ecology, genetics, evolution theory, taxonomy, cytology, and biochemistry. Different disciplines may give the same concept different names, and different concepts may be given the same name. For example, there are the terms established in ecological genetics by E.B. Ford (1975), and for classical genetics by John Maynard Smith (1998). The shorter term morphism was preferred by the evolutionary biologist Julian Huxley (1955).Various synonymous terms exist for the various polymorphic forms of an organism. The most common are morph and morpha, while a more formal term is morphotype.  Form and phase are sometimes used, but are easily confused in zoology with, respectively, ""form"" in a population of animals, and ""phase"" as a color or other change in an organism due to environmental conditions (temperature, humidity, etc.). Phenotypic traits and characteristics are also possible descriptions, though that would imply just a limited aspect of the body.
In the taxonomic nomenclature of zoology, the word ""morpha"" plus a Latin name for the morph can be added to a binomial or trinomial name. However, this invites confusion with geographically variant ring species or subspecies, especially if polytypic. Morphs have no formal standing in the ICZN. In botanical taxonomy, the concept of morphs is represented with the terms ""variety"", ""subvariety"" and ""form"", which are formally regulated by the ICN. Horticulturists sometimes confuse this usage of ""variety"" both with cultivar (""variety"" in viticultural usage, rice agriculture jargon, and informal gardening lingo) and with the legal concept ""plant variety"" (protection of a cultivar as a form of intellectual property).


== Mechanisms ==
Three mechanisms may cause polymorphism:
Genetic polymorphism – where the phenotype of each individual is genetically determined
A conditional development strategy, where the phenotype of each individual is set by environmental cues
A mixed development strategy, where the phenotype is randomly assigned during development


== Relative frequency ==
Endler's survey of natural selection gave an indication of the relative importance of polymorphisms among studies showing natural selection. The results, in summary: Number of species demonstrating natural selection: 141. Number showing quantitative traits: 56. Number showing polymorphic traits: 62. Number showing both Q and P traits: 23. This shows that polymorphisms are found to be at least as common as continuous variation in studies of natural selection, and hence just as likely to be part of the evolutionary process.


== Genetics ==


=== Genetic polymorphism ===

Since all polymorphism has a genetic basis, genetic polymorphism has a particular meaning:

Genetic polymorphism is the simultaneous occurrence in the same locality of two or more discontinuous forms in such proportions that the rarest of them cannot be maintained just by recurrent mutation or immigration, originally defined by Ford (1940).: 11  The later definition by Cavalli-Sforza & Bodmer (1971) is currently used: ""Genetic polymorphism is the occurrence in the same population of two or more alleles at one locus, each with appreciable frequency"", where the minimum frequency is typically taken as 1%.The definition has three parts: a) sympatry: one interbreeding population; b) discrete forms; and c) not maintained just by mutation.
In simple words, the term polymorphism was originally used to describe variations in shape and form that distinguish normal individuals within a species from each other. Presently, geneticists use the term genetic polymorphism to describe the inter-individual, functionally silent differences in DNA sequence that make each human genome unique.Genetic polymorphism is actively and steadily maintained in populations by natural selection, in contrast to transient polymorphisms where a form is progressively replaced by another.: 6–7  By definition, genetic polymorphism relates to a balance or equilibrium between morphs. The mechanisms that conserve it are types of balancing selection.


=== Mechanisms of balancing selection ===
Heterosis (or heterozygote advantage): ""Heterosis: the heterozygote at a locus is fitter than either homozygote"".: 65 
Frequency dependent selection: The fitness of a particular phenotype is dependent on its frequency relative to other phenotypes in a given population. Example: prey switching, where rare morphs of prey are actually fitter due to predators concentrating on the more frequent morphs.
Fitness varies in time and space. Fitness of a genotype may vary greatly between larval and adult stages, or between parts of a habitat range.: 26 
Selection acts differently at different levels. The fitness of a genotype may depend on the fitness of other genotypes in the population: this covers many natural situations where the best thing to do (from the point of view of survival and reproduction) depends on what other members of the population are doing at the time.: 17 & ch. 7 


=== Pleiotropism ===
Most genes have more than one effect on the phenotype of an organism (pleiotropism). Some of these effects may be visible, and others cryptic, so it is often important to look beyond the most obvious effects of a gene to identify other effects. Cases occur where a gene affects an unimportant visible character, yet a change in fitness is recorded. In such cases, the gene's other (cryptic or 'physiological') effects may be responsible for the change in fitness. Pleiotropism is posing continual challenges for many clinical dysmorphologists in their attempt to explain birth defects which affect one or more organ system, with only a single underlying causative agent. For many pleiotropic disorders, the connection between the gene defect and the various manifestations is neither obvious, nor well understood.
""If a neutral trait is pleiotropically linked to an advantageous one, it may emerge because of a process of natural selection. It was selected but this doesn't mean it is an adaptation. The reason is that, although it was selected, there was no selection for that trait.""


=== Epistasis ===
Epistasis occurs when the expression of one gene is modified by another gene. For example, gene A only shows its effect when allele B1 (at another locus) is present, but not if it is absent. This is one of the ways in which two or more genes may combine to produce a coordinated change in more than one characteristic (for instance, in mimicry). Unlike the supergene, epistatic genes do not need to be closely linked or even on the same chromosome.
Both pleiotropism and epistasis show that a gene need not relate to a character in the simple manner that was once supposed.


=== The origin of supergenes ===
Although a polymorphism can be controlled by alleles at a single locus (e.g. human ABO blood groups), the more complex forms are controlled by supergenes consisting of several tightly linked genes on a single chromosome. Batesian mimicry in butterflies and heterostyly in angiosperms are good examples. There is a long-standing debate as to how this situation could have arisen, and the question is not yet resolved.
Whereas a gene family (several tightly linked genes performing similar or identical functions) arises by duplication of a single original gene, this is usually not the case with supergenes. In a supergene some of the constituent genes have quite distinct functions, so they must have come together under selection. This process might involve suppression of crossing-over, translocation of chromosome fragments and possibly occasional cistron duplication. That crossing-over can be suppressed by selection has been known for many years.Debate has centered round the question of whether the component genes in a super-gene could have started off on separate chromosomes, with subsequent reorganization, or if it is necessary for them to start on the same chromosome. Originally, it was held that chromosome rearrangement would play an important role. This explanation was accepted by E. B. Ford and incorporated into his accounts of ecological genetics.: ch. 6 : 17–25 However, many believe it more likely that the genes start on the same chromosome. They argue that supergenes arose in situ. This is known as Turner's sieve hypothesis. John Maynard Smith agreed with this view in his authoritative textbook, but the question is still not definitively settled.


== Ecology ==
Selection, whether natural or artificial, changes the frequency of morphs within a population; this occurs when morphs reproduce with different degrees of success. A genetic (or balanced) polymorphism usually persists over many generations, maintained by two or more opposed and powerful selection pressures. Diver (1929) found banding morphs in Cepaea nemoralis could be seen in prefossil shells going back to the Mesolithic Holocene. Non-human apes have similar blood groups to humans; this strongly suggests that this kind of polymorphism is ancient, at least as far back as the last common ancestor of the apes and man, and possibly even further.

The relative proportions of the morphs may vary; the actual values are determined by the effective fitness of the morphs at a particular time and place. The mechanism of heterozygote advantage assures the population of some alternative alleles at the locus or loci involved. Only if competing selection disappears will an allele disappear. However, heterozygote advantage is not the only way a polymorphism can be maintained. Apostatic selection, whereby a predator consumes a common morph whilst overlooking rarer morphs is possible and does occur. This would tend to preserve rarer morphs from extinction.
Polymorphism is strongly tied to the adaptation of a species to its environment, which may vary in colour, food supply, and predation and in many other ways including sexual harassment avoidance. Polymorphism is one good way the opportunities get to be used; it has survival value, and the selection of modifier genes may reinforce the polymorphism. In addition, polymorphism seems to be associated with a higher rate of speciation. 


=== Polymorphism and niche diversity ===
G. Evelyn Hutchinson, a founder of niche research, commented ""It is very likely from an ecological point of view that all species, or at least all common species, consist of populations adapted to more than one niche"". He gave as examples sexual size dimorphism and mimicry. In many cases where the male is short-lived and smaller than the female, he does not compete with her during her late pre-adult and adult life. Size difference may permit both sexes to exploit different niches. In elaborate cases of mimicry, such as the African butterfly Papilio dardanus, female morphs mimic a range of distasteful models called Batesian mimicry, often in the same region. The fitness of each type of mimic decreases as it becomes more common, so the polymorphism is maintained by frequency-dependent selection. Thus the efficiency of the mimicry is maintained in a much increased total population. However it can exist within one gender.: ch. 13 Female-limited polymorphism and sexual assault avoidance
Female-limited polymorphism in Papilio dardanus can be described as an outcome of sexual conflict. Cook et al. (1994) argued that the male-like phenotype in some females in P. dardanus population on Pemba Island, Tanzania functions to avoid detection from a mate-searching male. The researchers found that male mate preference is controlled by frequency-dependent selection, which means that the rare morph suffers less from mating attempt than the common morph. The reasons why females try to avoid male sexual harassment are that male mating attempt can reduce female fitness in many ways such as fecundity and longevity.


=== The switch ===
The mechanism which decides which of several morphs an individual displays is called the switch. This switch may be genetic, or it may be environmental. Taking sex determination as the example, in humans the determination is genetic, by the XY sex-determination system. In Hymenoptera (ants, bees and wasps), sex determination is by haplo-diploidy: the females are all diploid, the males are haploid. However, in some animals an environmental trigger determines the sex: alligators are a famous case in point. In ants the distinction between workers and guards is environmental, by the feeding of the grubs. Polymorphism with an environmental trigger is called polyphenism.
The polyphenic system does have a degree of environmental flexibility not present in the genetic polymorphism. However, such environmental triggers are the less common of the two methods.


=== Investigative methods ===
Investigation of polymorphism requires use of both field and laboratory techniques. In the field:

detailed survey of occurrence, habits and predation
selection of an ecological area or areas, with well-defined boundaries
capture, mark, release, recapture data
relative numbers and distribution of morphs
estimation of population sizesAnd in the laboratory:

genetic data from crosses
population cages
chromosome cytology if possible
use of chromatography, biochemistry or similar techniques if morphs are crypticWithout proper field-work, the significance of the polymorphism to the species is uncertain and without laboratory breeding the genetic basis is obscure. Even with insects, the work may take many years; examples of Batesian mimicry noted in the nineteenth century are still being researched.


== Relevance for evolutionary theory ==
Polymorphism was crucial to research in ecological genetics by E. B. Ford and his co-workers from the mid-1920s to the 1970s (similar work continues today, especially on mimicry). The results had a considerable effect on the mid-century evolutionary synthesis, and on present evolutionary theory. The work started at a time when natural selection was largely discounted as the leading mechanism for evolution, continued through the middle period when Sewall Wright's ideas on drift were prominent, to the last quarter of the 20th century when ideas such as Kimura's neutral theory of molecular evolution was given much attention. The significance of the work on ecological genetics is that it has shown how important selection is in the evolution of natural populations, and that selection is a much stronger force than was envisaged even by those population geneticists who believed in its importance, such as Haldane and Fisher.In just a couple of decades the work of Fisher, Ford, Arthur Cain, Philip Sheppard and Cyril Clarke promoted natural selection as the primary explanation of variation in natural populations, instead of genetic drift. Evidence can be seen in Mayr's famous book Animal Species and Evolution, and Ford's Ecological Genetics. Similar shifts in emphasis can be seen in most of the other participants in the evolutionary synthesis, such as Stebbins and Dobzhansky, though the latter was slow to change.Kimura drew a distinction between molecular evolution, which he saw as dominated by selectively neutral mutations, and phenotypic characters, probably dominated by natural selection rather than drift.


== Examples ==


== See also ==
Fondation Jean Dausset-CEPH
Single-nucleotide polymorphism (SNP)


== References ==


== External links ==

Guide to reptile morphs
Heterostyly in the Cowslip (Primula veris L.)
McNamara, Don (1998). ""Notes on Rearing Scarlet tiger moth Callimorpha dominula (L.)"". Amateur Entomologists' Society. Retrieved 15 August 2006.",1628280,994,"All Wikipedia articles needing clarification, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with short description, Commons category link is locally defined, Morphas, Polymorphism (biology), Short description is different from Wikidata, Use dmy dates from October 2020, Wikipedia articles needing clarification from August 2018, Wikipedia articles needing clarification from May 2010",1106116855,biology
https://en.wikipedia.org/wiki/Homology_(biology),Homology (biology),"In biology, homology is similarity due to shared ancestry between a pair of structures or genes in different taxa. A common example of homologous structures is the forelimbs of vertebrates, where the wings of bats and birds, the arms of primates, the front flippers of whales and the forelegs of four-legged vertebrates like dogs and crocodiles are all derived from the same ancestral tetrapod structure. Evolutionary biology explains homologous structures adapted to different purposes as the result of descent with modification from a common ancestor. The term was first applied to biology in a non-evolutionary context by the anatomist Richard Owen in 1843. Homology was later explained by Charles Darwin's theory of evolution in 1859, but had been observed before this, from Aristotle onwards, and it was explicitly analysed by Pierre Belon in 1555.
In developmental biology, organs that developed in the embryo in the same manner and from similar origins, such as from matching primordia in successive segments of the same animal, are serially homologous. Examples include the legs of a centipede, the maxillary palp and labial palp of an insect, and the spinous processes of successive vertebrae in a vertebral column. Male and female reproductive organs are homologous if they develop from the same embryonic tissue, as do the ovaries and testicles of mammals including humans.Sequence homology between protein or DNA sequences is similarly defined in terms of shared ancestry. Two segments of DNA can have shared ancestry because of either a speciation event (orthologs) or a duplication event (paralogs). Homology among proteins or DNA is inferred from their sequence similarity. Significant similarity is strong evidence that two sequences are related by divergent evolution from a common ancestor. Alignments of multiple sequences are used to discover the homologous regions.
Homology remains controversial in animal behaviour, but there is suggestive evidence that, for example, dominance hierarchies are homologous across the primates.","In biology, homology is similarity due to shared ancestry between a pair of structures or genes in different taxa. A common example of homologous structures is the forelimbs of vertebrates, where the wings of bats and birds, the arms of primates, the front flippers of whales and the forelegs of four-legged vertebrates like dogs and crocodiles are all derived from the same ancestral tetrapod structure. Evolutionary biology explains homologous structures adapted to different purposes as the result of descent with modification from a common ancestor. The term was first applied to biology in a non-evolutionary context by the anatomist Richard Owen in 1843. Homology was later explained by Charles Darwin's theory of evolution in 1859, but had been observed before this, from Aristotle onwards, and it was explicitly analysed by Pierre Belon in 1555.
In developmental biology, organs that developed in the embryo in the same manner and from similar origins, such as from matching primordia in successive segments of the same animal, are serially homologous. Examples include the legs of a centipede, the maxillary palp and labial palp of an insect, and the spinous processes of successive vertebrae in a vertebral column. Male and female reproductive organs are homologous if they develop from the same embryonic tissue, as do the ovaries and testicles of mammals including humans.Sequence homology between protein or DNA sequences is similarly defined in terms of shared ancestry. Two segments of DNA can have shared ancestry because of either a speciation event (orthologs) or a duplication event (paralogs). Homology among proteins or DNA is inferred from their sequence similarity. Significant similarity is strong evidence that two sequences are related by divergent evolution from a common ancestor. Alignments of multiple sequences are used to discover the homologous regions.
Homology remains controversial in animal behaviour, but there is suggestive evidence that, for example, dominance hierarchies are homologous across the primates.


== History ==

Homology was noticed by Aristotle (c. 350 BC), and was explicitly analysed by Pierre Belon in his 1555 Book of Birds, where he systematically compared the skeletons of birds and humans. The pattern of similarity was interpreted as part of the static great chain of being through the mediaeval and early modern periods: it was not then seen as implying evolutionary change.
In the German Naturphilosophie tradition, homology was of special interest as demonstrating unity in nature.
In 1790, Goethe stated his foliar theory in his essay ""Metamorphosis of Plants"", showing that flower part are derived from leaves.
The serial homology of limbs was described late in the 18th century.
The French zoologist Etienne Geoffroy Saint-Hilaire showed in 1818 in his theorie d'analogue (""theory of homologues"") that structures were shared between fishes, reptiles, birds, and mammals. When Geoffroy went further and sought homologies between Georges Cuvier's embranchements, such as vertebrates and molluscs, his claims triggered the 1830 Cuvier-Geoffroy debate. Geoffroy stated the principle of connections, namely that what is important is the relative position of different structures and their connections to each other.
The Estonian embryologist Karl Ernst von Baer stated what are now called von Baer's laws in 1828, noting that related animals begin their development as similar embryos and then diverge: thus, animals in the same family are more closely related and diverge later than animals which are only in the same order and have fewer homologies. von Baer's theory recognises that each taxon (such as a family) has distinctive shared features, and that embryonic development parallels the taxonomic hierarchy: not the same as recapitulation theory.
The term ""homology"" was first used in biology by the anatomist Richard Owen in 1843 when studying the similarities of vertebrate fins and limbs, defining it as the ""same organ in different animals under every variety of form and function"", and contrasting it with the matching term ""analogy"" which he used to describe different structures with the same function.
Owen codified 3 main criteria for determining if features were homologous: position, development, and composition. In 1859, Charles Darwin explained homologous structures as meaning that the organisms concerned shared a body plan from a common ancestor, and that taxa were branches of a single tree of life.


== Definition ==

The word homology, coined in about 1656, is derived from the Greek ὁμόλογος homologos from ὁμός homos ""same"" and λόγος logos ""relation"".Similar biological structures or sequences in different taxa are homologous if they are derived from a common ancestor. Homology thus implies divergent evolution. For example, many insects (such as dragonflies) possess two pairs of flying wings. In beetles, the first pair of wings has evolved into a pair of hard wing covers, while in Dipteran flies the second pair of wings has evolved into small halteres used for balance.Similarly, the forelimbs of ancestral vertebrates have evolved into the front flippers of whales, the wings of birds, the running forelegs of dogs, deer, and horses, the short forelegs of frogs and lizards, and the grasping hands of primates including humans. The same major forearm bones (humerus, radius, and ulna) are found in fossils of lobe-finned fish such as Eusthenopteron.


=== Homology vs. analogy ===

The opposite of homologous organs are analogous organs which do similar jobs in two taxa that were not present in their most recent common ancestor but rather evolved separately. For example, the wings of insects and birds evolved independently in widely separated groups, and converged functionally to support powered flight, so they are analogous. Similarly, the wings of a sycamore maple seed and the wings of a bird are analogous but not homologous, as they develop from quite different structures. A structure can be homologous at one level, but only analogous at another. Pterosaur, bird and bat wings are analogous as wings, but homologous as forelimbs because the organ served as a forearm (not a wing) in the last common ancestor of tetrapods, and evolved in different ways in the three groups. Thus, in the pterosaurs, the ""wing"" involves both the forelimb and the hindlimb. Analogy is called homoplasy in cladistics, and convergent or parallel evolution in evolutionary biology.


=== In cladistics ===

Specialised terms are used in taxonomic research. Primary homology is a researcher's initial hypothesis based on similar structure or anatomical connections, suggesting that a character state in two or more taxa share is shared due to common ancestry. Primary homology may be conceptually broken down further: we may consider all of the states of the same character as ""homologous"" parts of a single, unspecified, transformation series. This has been referred to as topographical correspondence. For example, in an aligned DNA sequence matrix, all of the A, G, C, T or implied gaps at a given nucleotide site are homologous in this way. Character state identity is the hypothesis that the particular condition in two or more taxa is ""the same"" as far as our character coding scheme is concerned. Thus, two Adenines at the same aligned nucleotide site are hypothesized to be homologous unless that hypothesis is subsequently contradicted by other evidence. Secondary homology is implied by parsimony analysis, where a character state that arises only once on a tree is taken to be homologous. As implied in this definition, many cladists consider secondary homology to be synonymous with synapomorphy, a shared derived character or trait state that distinguishes a clade from other organisms.Shared ancestral character states, symplesiomorphies, represent either synapomorphies of a more inclusive group, or complementary states (often absences) that unite no natural group of organisms. For example, the presence of wings is a synapomorphy for pterygote insects, but a symplesiomorphy for holometabolous insects. Absence of wings in non-pterygote insects and other organisms is a complementary symplesiomorphy that unites no group (for example, absence of wings provides no evidence of common ancestry of silverfish, spiders and annelid worms). On the other hand, absence (or secondary loss) of wings is a synapomorphy for fleas. Patterns such as these lead many cladists to consider the concept of homology and the concept of synapomorphy to be equivalent. Some cladists follow the pre-cladistic definition of homology of Haas and Simpson, and view both synapomorphies and symplesiomorphies as homologous character states.


== In different taxa ==

Homologies provide the fundamental basis for all biological classification, although some may be highly counter-intuitive. For example, deep homologies like the pax6 genes that control the development of the eyes of vertebrates and arthropods were unexpected, as the organs are anatomically dissimilar and appeared to have evolved entirely independently.


=== In arthropods ===

The embryonic body segments (somites) of different arthropod taxa have diverged from a simple body plan with many similar appendages which are serially homologous, into a variety of body plans with fewer segments equipped with specialised appendages. The homologies between these have been discovered by comparing genes in evolutionary developmental biology.

Among insects, the stinger of the female honey bee is a modified ovipositor, homologous with ovipositors in other insects such as the Orthoptera, Hemiptera, and those Hymenoptera without stingers.


=== In mammals ===

The three small bones in the middle ear of mammals including humans, the malleus, incus, and stapes, are today used to transmit sound from the eardrum to the inner ear. The malleus and incus develop in the embryo from structures that form jaw bones (the quadrate and the articular) in lizards, and in fossils of lizard-like ancestors of mammals. Both lines of evidence show that these bones are homologous, sharing a common ancestor.Among the many homologies in mammal reproductive systems, ovaries and testicles are homologous.Rudimentary organs such as the human tailbone, now much reduced from their functional state, are readily understood as signs of evolution, the explanation being that they were cut down by natural selection from functioning organs when their functions were no longer needed, but make no sense at all if species are considered to be fixed. The tailbone is homologous to the tails of other primates.


=== In plants ===


==== Leaves, stems, and roots ====
In many plants, defensive or storage structures are made by modifications of the development of primary leaves, stems, and roots. Leaves are variously modified from photosynthetic structures to form the insect-trapping pitchers of pitcher plants, the insect-trapping jaws of Venus flytrap, and the spines of cactuses, all homologous.
Certain compound leaves of flowering plants are partially homologous both to leaves and shoots, because their development has evolved from a genetic mosaic of leaf and shoot development.

		
		
		
		
		
		
		


==== Flower parts ====

The four types of flower parts, namely carpels, stamens, petals, and sepals, are homologous with and derived from leaves, as Goethe correctly noted in 1790. The development of these parts through a pattern of gene expression in the growing zones (meristems) is described by the ABC model of flower development. Each of the four types of flower parts is serially repeated in concentric whorls, controlled by a small number of genes acting in various combinations. Thus, A genes working alone result in sepal formation; A and B together produce petals; B and C together create stamens; C alone produces carpels. When none of the genes are active, leaves are formed. Two more groups of genes, D to form ovules and E for the floral whorls, complete the model. The genes are evidently ancient, as old as the flowering plants themselves.


== Developmental biology ==

Developmental biology can identify homologous structures that arose from the same tissue in embryogenesis. For example, adult snakes have no legs, but their early embryos have limb-buds for hind legs, which are soon lost as the embryos develop. The implication that the ancestors of snakes had hind legs is confirmed by fossil evidence: the Cretaceous snake Pachyrhachis problematicus had hind legs complete with hip bones (ilium, pubis, ischium), thigh bone (femur), leg bones (tibia, fibula) and foot bones (calcaneum, astragalus) as in tetrapods with legs today.


== Sequence homology ==

As with anatomical structures, sequence homology between protein or DNA sequences is defined in terms of shared ancestry. Two segments of DNA can have shared ancestry because of either a speciation event (orthologs) or a duplication event (paralogs). Homology among proteins or DNA is typically inferred from their sequence similarity. Significant similarity is strong evidence that two sequences are related by divergent evolution of a common ancestor. Alignments of multiple sequences are used to indicate which regions of each sequence are homologous.Homologous sequences are orthologous if they are descended from the same ancestral sequence separated by a speciation event: when a species diverges into two separate species, the copies of a single gene in the two resulting species are said to be orthologous. The term ""ortholog"" was coined in 1970 by the molecular evolutionist Walter Fitch.Homologous sequences are paralogous if they were created by a duplication event within the genome. For gene duplication events, if a gene in an organism is duplicated, the two copies are paralogous. They can shape the structure of whole genomes and thus explain genome evolution to a large extent. Examples include the Homeobox (Hox) genes in animals. These genes not only underwent gene duplications within chromosomes but also whole genome duplications. As a result, Hox genes in most vertebrates are spread across multiple chromosomes: the HoxA–D clusters are the best studied.Structural homology. Some sequences are homologous, but they have diverged so much that their sequence similarity is not sufficient to establish homology. However, many proteins have retained very similar structures which can be used to demonstrate their homology.


== In behaviour ==

It has been suggested that some behaviours might be homologous, based either on sharing across related taxa or on common origins of the behaviour in an individual's development; however, the notion of homologous behavior remains controversial, largely because behavior is more prone to multiple realizability than other biological traits. For example, D. W. Rajecki and Randall C. Flanery, using data on humans and on nonhuman primates, argue that patterns of behaviour in dominance hierarchies are homologous across the primates.As with morphological features or DNA, shared similarity in behavior provides evidence for common ancestry.  The hypothesis that a behavioral character is not homologous should be based on an incongruent distribution of that character with respect to other features that are presumed to reflect the true pattern of relationships.  This is an application of Willi Hennig's  auxiliary principle.


== Notes ==


== References ==


== Further reading ==
Brigandt, Ingo (2011) ""Essay: Homology."" In: The Embryo Project Encyclopedia. ISSN 1940-5030. http://embryo.asu.edu/handle/10776/1754
Carroll, Sean B. (2006). Endless Forms Most Beautiful. New York: W.W. Norton & Co. ISBN 978-0-297-85094-6.
Carroll, Sean B. (2006). The making of the fittest: DNA and the ultimate forensic record of evolution. New York: W.W. Norton & Co. ISBN 978-0-393-06163-5.
DePinna, M.C. (1991). ""Concepts and tests of homology in the cladistic paradigm"". Cladistics. 7 (4): 367–94. CiteSeerX 10.1.1.487.2259. doi:10.1111/j.1096-0031.1991.tb00045.x. S2CID 3551391.
Dewey, C.N.; Pachter, L. (April 2006). ""Evolution at the nucleotide level: the problem of multiple whole-genome alignment"". Human Molecular Genetics. 15 (Spec No 1): R51–R56. doi:10.1093/hmg/ddl056. PMID 16651369.
Fitch, W.M. (May 2000). ""Homology a personal view on some of the problems"". Trends in Genetics. 16 (5): 227–31. doi:10.1016/S0168-9525(00)02005-9. PMID 10782117.
Gegenbaur, G. (1898). Vergleichende Anatomie der Wirbelthiere ... Leipzig.
Haeckel, Еrnst (1866). Generelle Morphologie der Organismen. Bd 1-2. Вerlin.{{cite book}}:  CS1 maint: location (link)
Kuzniar, A.; van Ham, R.C.; Pongor, S.; Leunissen, J.A. (November 2008). ""The quest for orthologs: finding the corresponding gene across genomes"". Trends Genet. 24 (11): 539–551. doi:10.1016/j.tig.2008.08.009. PMID 18819722.
Mindell, D.P.; Meyer, A. (2001). ""Homology evolving"" (PDF). Trends in Ecology and Evolution. 16 (8): 434–40. doi:10.1016/S0169-5347(01)02206-6. Archived from the original (PDF) on 27 June 2010.
Owen, Richard (1847). On the archetype and homologies of the vertebrate skeleton. London: John van Voorst , Paternoster Row.


== External links ==
 Media related to Homology at Wikimedia Commons",1844028,1229,"All articles with unsourced statements, Articles with short description, Articles with unsourced statements from December 2022, CS1 maint: location, Commons link is on Wikidata, Comparative anatomy, Evolutionary biology concepts, Good articles, Pages using multiple image with auto scaled images, Phylogenetics, Short description is different from Wikidata, Use dmy dates from April 2017",1133813026,biology
https://en.wikipedia.org/wiki/Transcription_(biology),Transcription (biology),"Transcription is the process of copying a segment of DNA into RNA.  The segments of DNA transcribed into RNA molecules that can encode proteins are said to produce messenger RNA (mRNA).  Other segments of DNA are copied into RNA molecules called non-coding RNAs (ncRNAs).  mRNA comprises only 1–3% of total RNA samples. Less than 2% of the human genome can be transcribed into mRNA (Human genome#Coding vs. noncoding DNA), while at least 80% of mammalian genomic DNA can be actively transcribed (in one or more types of cells), with the majority of this 80% considered to be ncRNA.Both DNA and RNA are nucleic acids, which use base pairs of nucleotides as a complementary language.  During transcription, a DNA sequence is read by an RNA polymerase, which produces a complementary, antiparallel RNA strand called a primary transcript.
Transcription proceeds in the following general steps:

RNA polymerase, together with one or more general transcription factors, binds to promoter DNA.
RNA polymerase generates a transcription bubble, which separates the two strands of the DNA helix. This is done by breaking the hydrogen bonds between complementary DNA nucleotides.
RNA polymerase adds RNA nucleotides (which are complementary to the nucleotides of one DNA strand).
RNA sugar-phosphate backbone forms with assistance from RNA polymerase to form an RNA strand.
Hydrogen bonds of the RNA–DNA helix break, freeing the newly synthesized RNA strand.
If the cell has a nucleus, the RNA may be further processed. This may include polyadenylation, capping, and splicing.
The RNA may remain in the nucleus or exit to the cytoplasm through the nuclear pore complex.If the stretch of DNA is transcribed into an RNA molecule that encodes a protein, the RNA is termed messenger RNA (mRNA); the mRNA, in turn, serves as a template for the protein's synthesis through translation. Other stretches of DNA may be transcribed into small non-coding RNAs such as microRNA, transfer RNA (tRNA), small nucleolar RNA (snoRNA), small nuclear RNA (snRNA), or enzymatic RNA molecules called ribozymes as well as larger non-coding RNAs such as ribosomal RNA (rRNA), and long non-coding RNA (lncRNA).  Overall, RNA helps synthesize, regulate, and process proteins; it therefore plays a fundamental role in performing functions within a cell.
In virology, the term transcription may also be used when referring to mRNA synthesis from an RNA molecule (i.e., equivalent to RNA replication). For instance, the genome of a negative-sense single-stranded RNA (ssRNA -) virus may be a template for a positive-sense single-stranded RNA (ssRNA +). This is because the positive-sense strand contains the sequence information needed to translate the viral proteins needed for viral replication. This process is catalyzed by a viral RNA replicase.","Transcription is the process of copying a segment of DNA into RNA.  The segments of DNA transcribed into RNA molecules that can encode proteins are said to produce messenger RNA (mRNA).  Other segments of DNA are copied into RNA molecules called non-coding RNAs (ncRNAs).  mRNA comprises only 1–3% of total RNA samples. Less than 2% of the human genome can be transcribed into mRNA (Human genome#Coding vs. noncoding DNA), while at least 80% of mammalian genomic DNA can be actively transcribed (in one or more types of cells), with the majority of this 80% considered to be ncRNA.Both DNA and RNA are nucleic acids, which use base pairs of nucleotides as a complementary language.  During transcription, a DNA sequence is read by an RNA polymerase, which produces a complementary, antiparallel RNA strand called a primary transcript.
Transcription proceeds in the following general steps:

RNA polymerase, together with one or more general transcription factors, binds to promoter DNA.
RNA polymerase generates a transcription bubble, which separates the two strands of the DNA helix. This is done by breaking the hydrogen bonds between complementary DNA nucleotides.
RNA polymerase adds RNA nucleotides (which are complementary to the nucleotides of one DNA strand).
RNA sugar-phosphate backbone forms with assistance from RNA polymerase to form an RNA strand.
Hydrogen bonds of the RNA–DNA helix break, freeing the newly synthesized RNA strand.
If the cell has a nucleus, the RNA may be further processed. This may include polyadenylation, capping, and splicing.
The RNA may remain in the nucleus or exit to the cytoplasm through the nuclear pore complex.If the stretch of DNA is transcribed into an RNA molecule that encodes a protein, the RNA is termed messenger RNA (mRNA); the mRNA, in turn, serves as a template for the protein's synthesis through translation. Other stretches of DNA may be transcribed into small non-coding RNAs such as microRNA, transfer RNA (tRNA), small nucleolar RNA (snoRNA), small nuclear RNA (snRNA), or enzymatic RNA molecules called ribozymes as well as larger non-coding RNAs such as ribosomal RNA (rRNA), and long non-coding RNA (lncRNA).  Overall, RNA helps synthesize, regulate, and process proteins; it therefore plays a fundamental role in performing functions within a cell.
In virology, the term transcription may also be used when referring to mRNA synthesis from an RNA molecule (i.e., equivalent to RNA replication). For instance, the genome of a negative-sense single-stranded RNA (ssRNA -) virus may be a template for a positive-sense single-stranded RNA (ssRNA +). This is because the positive-sense strand contains the sequence information needed to translate the viral proteins needed for viral replication. This process is catalyzed by a viral RNA replicase.


== Background ==
A DNA transcription unit encoding for a protein may contain both a coding sequence, which will be translated into the protein, and regulatory sequences, which direct and regulate the synthesis of that protein. The regulatory sequence before (""upstream"" from) the coding sequence is called the five prime untranslated region (5'UTR); the sequence after (""downstream"" from) the coding sequence is called the three prime untranslated region (3'UTR).As opposed to DNA replication, transcription results in an RNA complement that includes the nucleotide uracil (U) in all instances where thymine (T) would have occurred in a DNA complement.
Only one of the two DNA strands serve as a template for transcription. The antisense strand of DNA is read by RNA polymerase from the 3' end to the 5' end during transcription (3' → 5'). The complementary RNA is created in the opposite direction, in the 5' → 3' direction, matching the sequence of the sense strand with the exception of switching uracil for thymine. This directionality is because RNA polymerase can only add nucleotides to the 3' end of the growing mRNA chain. This use of only the 3' → 5' DNA strand eliminates the need for the Okazaki fragments that are seen in DNA replication. This also removes the need for an RNA primer to initiate RNA synthesis, as is the case in DNA replication.
The non-template (sense) strand of DNA is called the coding strand, because its sequence is the same as the newly created RNA transcript (except for the substitution of uracil for thymine). This is the strand that is used by convention when presenting a DNA sequence.Transcription has some proofreading mechanisms, but they are fewer and less effective than the controls for copying DNA. As a result, transcription has a lower copying fidelity than DNA replication.


== Major steps ==

Transcription is divided into initiation, promoter escape, elongation, and termination.


=== Setting up for transcription ===


==== Enhancers, transcription factors, Mediator complex and DNA loops in mammalian transcription ====

Setting up for transcription in mammals is regulated by many cis-regulatory elements, including core promoter and promoter-proximal elements that are located near the transcription start sites of genes.  Core promoters combined with general transcription factors are sufficient to direct transcription initiation, but generally have low basal activity.  Other important cis-regulatory modules are localized in DNA regions that are distant from the transcription start sites. These include enhancers, silencers, insulators and tethering elements. Among this constellation of elements, enhancers and their associated transcription factors have a leading role in the initiation of gene transcription.  An enhancer localized in a DNA region distant from the promoter of a gene can have a very large effect on gene transcription, with some genes undergoing up to 100-fold increased transcription due to an activated enhancer.Enhancers are regions of the genome that are major gene-regulatory elements.  Enhancers control cell-type-specific gene transcription programs, most often by looping through long distances to come in physical proximity with the promoters of their target genes.  While there are hundreds of thousands of enhancer DNA regions, for a particular type of tissue only specific enhancers are brought into proximity with the promoters that they regulate.  In a study of brain cortical neurons, 24,937 loops were found, bringing enhancers to their target promoters.  Multiple enhancers, each often at tens or hundred of thousands of nucleotides distant from their target genes, loop to their target gene promoters and can coordinate with each other to control transcription of their common target gene.The schematic illustration in this section shows an enhancer looping around to come into close physical proximity with the promoter of a target gene.  The loop is stabilized by a dimer of a connector protein (e.g. dimer of CTCF or YY1), with one member of the dimer anchored to its binding motif on the enhancer and the other member anchored to its binding motif on the promoter (represented by the red zigzags in the illustration).  Several cell function specific transcription factors (there are about 1,600 transcription factors in a human cell) generally bind to specific motifs on an enhancer and a small combination of these enhancer-bound transcription factors, when brought close to a promoter by a DNA loop, govern level of transcription of the target gene.  Mediator (a complex usually consisting of about 26 proteins in an interacting structure) communicates regulatory signals from enhancer DNA-bound transcription factors directly to the RNA polymerase II (pol II) enzyme bound to the promoter.Enhancers, when active, are generally transcribed from both strands of DNA with RNA polymerases acting in two different directions, producing two enhancer RNAs (eRNAs) as illustrated in the Figure.  An inactive enhancer may be bound by an inactive transcription factor.  Phosphorylation of the transcription factor may activate it and that activated transcription factor may then activate the enhancer to which it is bound (see small red star representing phosphorylation of transcription factor bound to enhancer in the illustration).  An activated enhancer begins transcription of its RNA before activating transcription of messenger RNA from its target gene.


==== CpG island methylation and demethylation ====

Transcription regulation at about 60% of promoters is also controlled by methylation of cytosines within CpG dinucleotides (where 5’ cytosine is followed by 3’ guanine or CpG sites).  5-methylcytosine (5-mC) is a methylated form of the DNA base cytosine (see Figure).  5-mC is an epigenetic marker found predominantly within CpG sites.  About 28 million CpG dinucleotides occur in the human genome.  In most tissues of mammals, on average, 70% to 80% of CpG cytosines are methylated (forming 5-methylCpG or 5-mCpG).  Methylated cytosines within 5’cytosine-guanine 3’ sequences often occur in groups, called CpG islands.  About 60% of promoter sequences have a CpG island while only about 6% of enhancer sequences have a CpG island.  CpG islands constitute regulatory sequences, since if CpG islands are methylated in the promoter of a gene this can reduce or silence gene transcription.DNA methylation regulates gene transcription through interaction with methyl binding domain (MBD) proteins, such as MeCP2, MBD1 and MBD2.  These MBD proteins bind most strongly to highly methylated CpG islands.  These MBD proteins have both a methyl-CpG-binding domain as well as a transcription repression domain.  They bind to methylated DNA and guide or direct protein complexes with chromatin remodeling and/or histone modifying activity to methylated CpG islands. MBD proteins generally repress local chromatin such as by catalyzing the introduction of repressive histone marks, or creating an overall repressive chromatin environment through nucleosome remodeling and chromatin reorganization.

As noted in the previous section, transcription factors are proteins that bind to specific DNA sequences in order to regulate the expression of a gene.  The binding sequence for a transcription factor in DNA is usually about 10 or 11 nucleotides long.  As summarized in 2009, Vaquerizas et al. indicated there are approximately 1,400 different transcription factors encoded in the human genome by genes that constitute about 6% of all human protein encoding genes.  About 94% of transcription factor binding sites (TFBSs) that are associated with signal-responsive genes occur in enhancers while only about 6% of such TFBSs occur in promoters.EGR1 protein is a particular transcription factor that is important for regulation of methylation of CpG islands.  An EGR1 transcription factor binding site is frequently located in enhancer or promoter sequences.  There are about 12,000 binding sites for EGR1 in the mammalian genome and about half of EGR1 binding sites are located in promoters and half in enhancers.  The binding of EGR1 to its target DNA binding site is insensitive to cytosine methylation in the DNA.While only small amounts of EGR1 transcription factor protein are detectable in cells that are un-stimulated, translation of the EGR1 gene into protein at one hour after stimulation is drastically elevated.  Production of EGR1 transcription factor proteins, in various types of cells, can be stimulated by growth factors, neurotransmitters, hormones, stress and injury.  In the brain, when neurons are activated, EGR1 proteins are up-regulated and they bind to (recruit) the pre-existing TET1 enzymes that are produced in high amounts in neurons.  TET enzymes can catalyse demethylation of 5-methylcytosine.  When EGR1 transcription factors bring TET1 enzymes to EGR1 binding sites in promoters, the TET enzymes can demethylate the methylated CpG islands at those promoters.  Upon demethylation, these promoters can then initiate transcription of their target genes.  Hundreds of genes in neurons are differentially expressed after neuron activation through EGR1 recruitment of TET1 to methylated regulatory sequences in their promoters.The methylation of promoters is also altered in response to signals.  The three mammalian DNA methyltransferasess (DNMT1, DNMT3A, and DNMT3B) catalyze the addition of methyl groups to cytosines in DNA.  While DNMT1 is a “maintenance” methyltransferase, DNMT3A and DNMT3B can carry out new methylations.  There are also two splice protein isoforms produced from the DNMT3A gene: DNA methyltransferase proteins DNMT3A1 and DNMT3A2.The splice isoform DNMT3A2  behaves like the product of a classical immediate-early gene and, for instance, it is robustly and transiently produced after neuronal activation.  Where the DNA methyltransferase isoform DNMT3A2 binds and adds methyl groups to cytosines appears to be determined by histone post translational modifications.On the other hand, neural activation causes degradation of DNMT3A1 accompanied by reduced methylation of at least one evaluated targeted promoter.


=== Initiation ===
Transcription begins with the binding of RNA polymerase, together with one or more general transcription factors, to a specific DNA sequence referred to as a ""promoter"" to form an RNA polymerase-promoter ""closed complex"". In the ""closed complex"" the promoter DNA is still fully double-stranded.RNA polymerase, assisted by one or more general transcription factors, then unwinds approximately 14 base pairs of DNA to form an RNA polymerase-promoter ""open complex"". In the ""open complex"" the promoter DNA is partly unwound and single-stranded. The exposed, single-stranded DNA is referred to as the ""transcription bubble.""RNA polymerase, assisted by one or more general transcription factors, then selects a transcription start site in the transcription bubble, binds to an initiating NTP and an extending NTP (or a short RNA primer and an extending NTP) complementary to the transcription start site sequence, and catalyzes bond formation to yield an initial RNA product.In bacteria, RNA polymerase holoenzyme consists of five subunits: 2 α subunits, 1 β subunit, 1 β' subunit, and 1 ω subunit. In bacteria, there is one general RNA transcription factor known as a sigma factor. RNA polymerase core enzyme binds to the bacterial general transcription (sigma) factor to form RNA polymerase holoenzyme and then binds to a promoter.
(RNA polymerase is called a holoenzyme when sigma subunit is attached to the core enzyme which is consist of 2 α subunits, 1 β subunit, 1 β' subunit only). Unlike eukaryotes, the initiating nucleotide of nascent bacterial mRNA is not capped with a modified guanine nucleotide. The initiating nucleotide of bacterial transcripts bears a 5′ triphosphate (5′-PPP), which can be used for genome-wide mapping of transcription initiation sites.In archaea and eukaryotes, RNA polymerase contains subunits homologous to each of the five RNA polymerase subunits in bacteria and also contains additional subunits. In archaea and eukaryotes, the functions of the bacterial general transcription factor sigma are performed by multiple general transcription factors that work together. In archaea, there are three general transcription factors: TBP, TFB, and TFE. In eukaryotes, in RNA polymerase II-dependent transcription, there are six general transcription factors: TFIIA, TFIIB (an ortholog of archaeal TFB), TFIID (a multisubunit factor in which the key subunit, TBP, is an ortholog of archaeal TBP), TFIIE (an ortholog of archaeal TFE), TFIIF, and TFIIH. The TFIID is the first component to bind to DNA due to binding of TBP, while TFIIH is the last component to be recruited. In archaea and eukaryotes, the RNA polymerase-promoter closed complex is usually referred to as the ""preinitiation complex.""Transcription initiation is regulated by additional proteins, known as activators and repressors, and, in some cases, associated coactivators or corepressors, which modulate formation and function of the transcription initiation complex.


=== Promoter escape ===
After the first bond is synthesized, the RNA polymerase must escape the promoter.  During this time there is a tendency to release the RNA transcript and produce truncated transcripts.  This is called abortive initiation, and is common for both eukaryotes and prokaryotes. Abortive initiation continues to occur until an RNA product of a threshold length of approximately 10 nucleotides is synthesized, at which point promoter escape occurs and a transcription elongation complex is formed.
Mechanistically, promoter escape occurs through DNA scrunching, providing the energy needed to break interactions between RNA polymerase holoenzyme and the promoter.In bacteria, it was historically thought that the sigma factor is definitely released after promoter clearance occurs. This theory had been known as the obligate release model. However, later data showed that upon and following promoter clearance, the sigma factor is released according to a stochastic model known as the stochastic release model.In eukaryotes, at an RNA polymerase II-dependent promoter, upon promoter clearance, TFIIH phosphorylates serine 5 on the carboxy terminal domain of RNA polymerase II, leading to the recruitment of capping enzyme (CE). The exact mechanism of how CE induces promoter clearance in eukaryotes is not yet known.


=== Elongation ===

One strand of the DNA, the template strand (or noncoding strand), is used as a template for RNA synthesis.  As transcription proceeds, RNA polymerase traverses the template strand and uses base pairing complementarity with the DNA template to create an RNA copy (which elongates during the traversal).  Although RNA polymerase traverses the template strand from 3' → 5', the coding (non-template) strand and newly formed RNA can also be used as reference points, so transcription can be described as occurring 5' → 3'.  This produces an RNA molecule from 5' → 3', an exact copy of the coding strand (except that thymines are replaced with uracils, and the nucleotides are composed of a ribose (5-carbon) sugar where DNA has deoxyribose (one fewer oxygen atom) in its sugar-phosphate backbone).mRNA transcription can involve multiple RNA polymerases on a single DNA template and multiple rounds of transcription (amplification of particular mRNA), so many mRNA molecules can be rapidly produced from a single copy of a gene. The characteristic elongation rates in prokaryotes and eukaryotes are about 10–100 nts/sec. In eukaryotes, however, nucleosomes act as major barriers to transcribing polymerases during transcription elongation. In these organisms, the pausing induced by nucleosomes can be regulated by transcription elongation factors such as TFIIS.Elongation also involves a proofreading mechanism that can replace incorrectly incorporated bases. In eukaryotes, this may correspond with short pauses during transcription that allow appropriate RNA editing factors to bind. These pauses may be intrinsic to the RNA polymerase or due to chromatin structure.


=== Termination ===

Bacteria use two different strategies for transcription termination – Rho-independent termination and Rho-dependent termination. In Rho-independent transcription termination, RNA transcription stops when the newly synthesized RNA molecule forms a G-C-rich hairpin loop followed by a run of Us. When the hairpin forms, the mechanical stress breaks the weak rU-dA bonds, now filling the DNA–RNA hybrid.  This pulls the poly-U transcript out of the active site of the RNA polymerase, terminating transcription. In the ""Rho-dependent"" type of termination, a protein factor called ""Rho"" destabilizes the interaction between the template and the mRNA, thus releasing the newly synthesized mRNA from the elongation complex.Transcription termination in eukaryotes is less well understood than in bacteria, but involves cleavage of the new transcript followed by template-independent addition of adenines at its new 3' end, in a process called polyadenylation.


== Role of RNA polymerase in post-transcriptional changes in RNA ==

RNA polymerase plays a very crucial role in all steps including post-transcriptional changes in RNA.

As shown in the image in the right it is evident that the CTD (C Terminal Domain) is a tail that changes its shape; this tail will be used as a carrier of splicing, capping and polyadenylation, as shown in the image on the left.


== Inhibitors ==
Transcription inhibitors can be used as antibiotics against, for example, pathogenic bacteria (antibacterials) and fungi (antifungals). An example of such an antibacterial is rifampicin, which inhibits bacterial transcription of DNA into mRNA by inhibiting DNA-dependent RNA polymerase by binding its beta-subunit, while 8-hydroxyquinoline is an antifungal transcription inhibitor. The effects of histone methylation may also work to inhibit the action of transcription. Potent, bioactive natural products like triptolide that inhibit mammalian transcription via inhibition of the XPB subunit of the general transcription factor TFIIH has been recently reported as a glucose conjugate for targeting hypoxic cancer cells with increased glucose transporter production.


== Endogenous inhibitors ==

In vertebrates, the majority of gene promoters contain a CpG island with numerous CpG sites.  When many of a gene's promoter CpG sites are methylated the gene becomes inhibited (silenced).  Colorectal cancers typically have 3 to 6 driver mutations and 33 to 66 hitchhiker or passenger mutations.   However, transcriptional inhibition (silencing) may be of more importance than mutation in causing progression to cancer.  For example, in colorectal cancers about 600 to 800 genes are transcriptionally inhibited by CpG island methylation (see regulation of transcription in cancer).  Transcriptional repression in cancer can also occur by other epigenetic mechanisms, such as altered production of microRNAs.  In breast cancer, transcriptional repression of BRCA1 may occur more frequently by over-produced microRNA-182 than by hypermethylation of the BRCA1 promoter (see Low expression of BRCA1 in breast and ovarian cancers).


== Transcription factories ==

Active transcription units are clustered in the nucleus, in discrete sites called transcription factories or euchromatin. Such sites can be visualized by allowing engaged polymerases to extend their transcripts in tagged precursors (Br-UTP or Br-U) and immuno-labeling the tagged nascent RNA. Transcription factories can also be localized using fluorescence in situ hybridization or marked by antibodies directed against polymerases. There are ~10,000 factories in the nucleoplasm of a HeLa cell, among which are ~8,000 polymerase II factories and ~2,000 polymerase III factories. Each polymerase II factory contains ~8 polymerases. As most active transcription units are associated with only one polymerase, each factory usually contains ~8 different transcription units. These units might be associated through promoters and/or enhancers, with loops forming a ""cloud"" around the factor.


== History ==
A molecule that allows the genetic material to be realized as a protein was first hypothesized by François Jacob and Jacques Monod. Severo Ochoa won a Nobel Prize in Physiology or Medicine in 1959 for developing a process for synthesizing RNA in vitro with polynucleotide phosphorylase, which was useful for cracking the genetic code. RNA synthesis by RNA polymerase was established in vitro by several laboratories by 1965; however, the RNA synthesized by these enzymes had properties that suggested the existence of an additional factor needed to terminate transcription correctly.In 1972, Walter Fiers became the first person to actually prove the existence of the terminating enzyme.
Roger D. Kornberg won the 2006 Nobel Prize in Chemistry ""for his studies of the molecular basis of eukaryotic transcription"".


== Measuring and detecting ==

Transcription can be measured and detected in a variety of ways:
G-Less Cassette transcription assay: measures promoter strength
Run-off transcription assay: identifies transcription start sites (TSS)
Nuclear run-on assay: measures the relative abundance of newly formed transcripts
KAS-seq: measures single-stranded DNA generated by RNA polymerases; can work with 1,000 cells.
RNase protection assay and ChIP-Chip of RNAP: detect active transcription sites
RT-PCR: measures the absolute abundance of total or nuclear RNA levels, which may however differ from transcription rates
DNA microarrays: measures the relative abundance of the global total or nuclear RNA levels; however, these may differ from transcription rates
In situ hybridization: detects the presence of a transcript
MS2 tagging: by incorporating RNA stem loops, such as MS2, into a gene, these become incorporated into newly synthesized RNA. The stem loops can then be detected using a fusion of GFP and the MS2 coat protein, which has a high affinity, sequence-specific interaction with the MS2 stem loops. The recruitment of GFP to the site of transcription is visualized as a single fluorescent spot. This new approach has revealed that transcription occurs in discontinuous bursts, or pulses (see Transcriptional bursting). With the notable exception of in situ techniques, most other methods provide cell population averages, and are not capable of detecting this fundamental property of genes.
Northern blot: the traditional method, and until the advent of RNA-Seq, the most quantitative
RNA-Seq: applies next-generation sequencing techniques to sequence whole transcriptomes, which allows the measurement of relative abundance of RNA, as well as the detection of additional variations such as fusion genes, post-transcriptional edits and novel splice sites
Single cell RNA-Seq: amplifies and reads partial transcriptomes from isolated cells, allowing for detailed analyses of RNA in tissues, embryos, and cancers


== Reverse transcription ==

Some viruses (such as HIV, the cause of AIDS), have the ability to transcribe RNA into DNA. HIV has an RNA genome that is reverse transcribed into DNA. The resulting DNA can be merged with the DNA genome of the host cell. The main enzyme responsible for synthesis of DNA from an RNA template is called reverse transcriptase.
In the case of HIV, reverse transcriptase is responsible for synthesizing a complementary DNA strand (cDNA) to the viral RNA genome. The enzyme ribonuclease H then digests the RNA strand, and reverse transcriptase synthesises a complementary strand of DNA to form a double helix DNA structure (""cDNA""). The cDNA is integrated into the host cell's genome by the enzyme integrase, which causes the host cell to generate viral proteins that reassemble into new viral particles.  In HIV, subsequent to this, the host cell undergoes programmed cell death, or apoptosis of T cells.  However, in other retroviruses, the host cell remains intact as the virus buds out of the cell.
Some eukaryotic cells contain an enzyme with reverse transcription activity called telomerase.  Telomerase is a reverse transcriptase that lengthens the ends of linear chromosomes. Telomerase carries an RNA template from which it synthesizes a repeating sequence of DNA, or ""junk"" DNA. This repeated sequence of DNA is called a telomere and can be thought of as a ""cap"" for a chromosome. It is important because every time a linear chromosome is duplicated, it is shortened. With this ""junk"" DNA or ""cap"" at the ends of chromosomes, the shortening eliminates some of the non-essential, repeated sequence rather than the protein-encoding DNA sequence, that is farther away from the chromosome end.
Telomerase is often activated in cancer cells to enable cancer cells to duplicate their genomes indefinitely without losing important protein-coding DNA sequence. Activation of telomerase could be part of the process that allows cancer cells to become immortal. The immortalizing factor of cancer via telomere lengthening due to telomerase has been proven to occur in 90% of all carcinogenic tumors in vivo with the remaining 10% using an alternative telomere maintenance route called ALT or Alternative Lengthening of Telomeres.


== See also ==
Life
Cell (biology)
Cell division
DBTSS
gene
gene regulation
gene expression
Epigenetics
Genome
Crick's central dogma, in which the product of transcription, mRNA, is translated to form polypeptides, and where it is asserted that the reverse processes never occur
Gene regulation
Long non-coding RNA
Missense mRNA
Splicing – process of removing introns from precursor messenger RNA (pre-mRNA) to make messenger RNA (mRNA)
Transcriptomics
Translation (biology)


== References ==


== External links ==

Interactive Java simulation of transcription initiation. Archived 2011-07-22 at the Wayback Machine From Center for Models of Life at the Niels Bohr Institute.
Interactive Java simulation of transcription interference—a game of promoter dominance in bacterial virus. Archived 2011-08-26 at the Wayback Machine From Center for Models of Life at the Niels Bohr Institute.
Virtual Cell Animation Collection, Introducing Transcription",2300089,1849,"All articles with unsourced statements, All pages needing cleanup, Articles needing cleanup from September 2021, Articles with GND identifiers, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with NDL identifiers, Articles with NKC identifiers, Articles with short description, Articles with unsourced statements from January 2011, Articles with unsourced statements from October 2019, CS1 Russian-language sources (ru), Cellular processes, Cleanup tagged articles with a reason field from September 2021, Commons category link is on Wikidata, Gene expression, Molecular biology, Short description is different from Wikidata, Webarchive template wayback links, Wikipedia articles needing clarification from July 2019, Wikipedia articles needing clarification from May 2019, Wikipedia pages needing cleanup from September 2021",1133001156,biology
https://en.wikipedia.org/wiki/Morphology_(biology),Morphology (biology),"Morphology is a branch of biology dealing with the study of the form and structure of organisms and their specific structural features.This includes aspects of the outward appearance (shape, structure, colour, pattern, size), i.e. external morphology (or eidonomy), as well as the form and structure of the internal parts like bones and organs, i.e. internal morphology (or anatomy). This is in contrast to physiology, which deals primarily with function. Morphology is a branch of life science dealing with the study of gross structure of an organism or taxon and its component parts.","Morphology is a branch of biology dealing with the study of the form and structure of organisms and their specific structural features.This includes aspects of the outward appearance (shape, structure, colour, pattern, size), i.e. external morphology (or eidonomy), as well as the form and structure of the internal parts like bones and organs, i.e. internal morphology (or anatomy). This is in contrast to physiology, which deals primarily with function. Morphology is a branch of life science dealing with the study of gross structure of an organism or taxon and its component parts.


== History ==
The etymology of the word ""morphology"" is from the Ancient Greek μορφή (morphḗ), meaning ""form"", and λόγος (lógos), meaning ""word, study, research"".While the concept of form in biology, opposed to function, dates back to Aristotle (see Aristotle's biology), the field of morphology was developed by Johann Wolfgang von Goethe (1790) and independently by the German anatomist and physiologist Karl Friedrich Burdach (1800).Among other important theorists of morphology are Lorenz Oken, Georges Cuvier, Étienne Geoffroy Saint-Hilaire, Richard Owen, Karl Gegenbaur and Ernst Haeckel.In 1830, Cuvier and E.G.Saint-Hilaire engaged in a famous debate, which is said to exemplify the two major deviations in biological thinking at the time – whether animal structure was due to function or evolution.


== Divisions of morphology ==
Comparative morphology is analysis of the patterns of the locus of structures within the body plan of an organism, and forms the basis of taxonomical categorization.
Functional morphology is the study of the relationship between the structure and function of morphological features.
Experimental morphology is the study of the effects of external factors upon the morphology of organisms under experimental conditions, such as the effect of genetic mutation.
Anatomy is a ""branch of morphology that deals with the structure of organisms"".
Molecular morphology is a rarely used term, usually referring to the superstructure of polymers such as fiber formation or to larger composite assemblies. The term is commonly not applied to the spatial structure of individual molecules.
Gross morphology refers to the collective structures of an organism as a whole as a general description of the form and structure of an organism, taking into account all of its structures without specifying an individual structure.


== Morphology and classification ==
Most taxa differ morphologically from other taxa. Typically, closely related taxa differ much less than more distantly related ones, but there are exceptions to this. Cryptic species are species which look very similar, or perhaps even outwardly identical, but are reproductively isolated. Conversely, sometimes unrelated taxa acquire a similar appearance as a result of convergent evolution or even mimicry. In addition, there can be morphological differences within a species, such as in Apoica flavissima where queens are significantly smaller than workers. A further problem with relying on morphological data is that what may appear, morphologically speaking, to be two distinct species, may in fact be shown by DNA analysis to be a single species. The significance of these differences can be examined through the use of allometric engineering in which one or both species are manipulated to phenocopy the other species.
A step relevant to the evaluation of morphology between traits/features within species, includes an assessment of the terms: homology and homoplasy. Homology between features indicate that those features have been derived from a common ancestor. Alternatively, homoplasy between features describes those that can resemble each other, but derive independently via parallel or convergent evolution.


== 3D cell morphology: classification ==
Invention and development of microscopy enable the observation of 3-D cell morphology with both high spatial and temporal resolution. The dynamic processes of these cell morphology which are controlled by a complex system play an important role in varied important biological process, such as immune and invasive responses.


== See also ==


== References ==


== External links ==
 Media related to Morphology (biology) at Wikimedia Commons",1785246,482,"Articles with BNF identifiers, Articles with GND identifiers, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with LNB identifiers, Articles with NDL identifiers, Articles with NKC identifiers, Articles with short description, Branches of biology, Commons category link from Wikidata, Comparative anatomy, Morphology (biology), Short description matches Wikidata, Webarchive template wayback links",1134337487,biology
https://en.wikipedia.org/wiki/Botany,Botany,"Botany, also called plant science(s), plant biology or phytology, is the science of plant life and a branch of biology. A botanist, plant scientist or phytologist is a scientist who specialises in this field. The term ""botany"" comes from the Ancient Greek word βοτάνη (botanē) meaning ""pasture"", ""herbs"" ""grass"", or ""fodder""; βοτάνη is in turn derived from βόσκειν (boskein), ""to feed"" or ""to graze"".  Traditionally, botany has also included the study of fungi and algae by mycologists and phycologists respectively, with the study of these three groups of organisms remaining within the sphere of interest of the International Botanical Congress. Nowadays, botanists (in the strict sense) study approximately 410,000 species of land plants of which some 391,000 species are vascular plants (including approximately 369,000 species of flowering plants), and approximately 20,000 are bryophytes.Botany originated in prehistory as herbalism with the efforts of early humans to identify – and later cultivate – plants that were edible, poisonous, and possibly medicinal, making it one of the first endeavors of human investigation. Medieval physic gardens, often attached to monasteries, contained plants possibly having medicinal benefit. They were forerunners of the first botanical gardens attached to universities, founded from the 1540s onwards. One of the earliest was the Padua botanical garden. These gardens facilitated the academic study of plants. Efforts to catalogue and describe their collections were the beginnings of plant taxonomy, and led in 1753 to the binomial system of nomenclature of Carl Linnaeus that remains in use to this day for the naming of all biological species.
In the 19th and 20th centuries, new techniques were developed for the study of plants, including methods of optical microscopy and live cell imaging, electron microscopy, analysis of chromosome number, plant chemistry and the structure and function of enzymes and other proteins. In the last two decades of the 20th century, botanists exploited the techniques of molecular genetic analysis, including genomics and proteomics and DNA sequences to classify plants more accurately.
Modern botany is a broad, multidisciplinary subject with contributions and insights from most other areas of science and technology. Research topics include the study of plant structure, growth and differentiation, reproduction, biochemistry and primary metabolism, chemical products, development, diseases, evolutionary relationships, systematics, and plant taxonomy. Dominant themes in 21st century plant science are molecular genetics and epigenetics, which study the mechanisms and control of gene expression during differentiation of plant cells and tissues. Botanical research has diverse applications in providing staple foods, materials such as timber, oil, rubber, fibre and drugs, in modern horticulture, agriculture and forestry, plant propagation, breeding and genetic modification, in the synthesis of chemicals and raw materials for construction and energy production, in environmental management, and the maintenance of biodiversity.","Botany, also called plant science(s), plant biology or phytology, is the science of plant life and a branch of biology. A botanist, plant scientist or phytologist is a scientist who specialises in this field. The term ""botany"" comes from the Ancient Greek word βοτάνη (botanē) meaning ""pasture"", ""herbs"" ""grass"", or ""fodder""; βοτάνη is in turn derived from βόσκειν (boskein), ""to feed"" or ""to graze"".  Traditionally, botany has also included the study of fungi and algae by mycologists and phycologists respectively, with the study of these three groups of organisms remaining within the sphere of interest of the International Botanical Congress. Nowadays, botanists (in the strict sense) study approximately 410,000 species of land plants of which some 391,000 species are vascular plants (including approximately 369,000 species of flowering plants), and approximately 20,000 are bryophytes.Botany originated in prehistory as herbalism with the efforts of early humans to identify – and later cultivate – plants that were edible, poisonous, and possibly medicinal, making it one of the first endeavors of human investigation. Medieval physic gardens, often attached to monasteries, contained plants possibly having medicinal benefit. They were forerunners of the first botanical gardens attached to universities, founded from the 1540s onwards. One of the earliest was the Padua botanical garden. These gardens facilitated the academic study of plants. Efforts to catalogue and describe their collections were the beginnings of plant taxonomy, and led in 1753 to the binomial system of nomenclature of Carl Linnaeus that remains in use to this day for the naming of all biological species.
In the 19th and 20th centuries, new techniques were developed for the study of plants, including methods of optical microscopy and live cell imaging, electron microscopy, analysis of chromosome number, plant chemistry and the structure and function of enzymes and other proteins. In the last two decades of the 20th century, botanists exploited the techniques of molecular genetic analysis, including genomics and proteomics and DNA sequences to classify plants more accurately.
Modern botany is a broad, multidisciplinary subject with contributions and insights from most other areas of science and technology. Research topics include the study of plant structure, growth and differentiation, reproduction, biochemistry and primary metabolism, chemical products, development, diseases, evolutionary relationships, systematics, and plant taxonomy. Dominant themes in 21st century plant science are molecular genetics and epigenetics, which study the mechanisms and control of gene expression during differentiation of plant cells and tissues. Botanical research has diverse applications in providing staple foods, materials such as timber, oil, rubber, fibre and drugs, in modern horticulture, agriculture and forestry, plant propagation, breeding and genetic modification, in the synthesis of chemicals and raw materials for construction and energy production, in environmental management, and the maintenance of biodiversity.


== History ==


=== Early botany ===

Botany originated as herbalism, the study and use of plants for their possible medicinal properties. The early recorded history of botany includes many ancient writings and plant classifications. Examples of early botanical works have been found in ancient texts from India dating back to before 1100 BCE, Ancient Egypt, in archaic Avestan writings, and in works from China purportedly from before 221 BCE.Modern botany traces its roots back to Ancient Greece specifically to Theophrastus (c. 371–287 BCE), a student of Aristotle who invented and described many of its principles and is widely regarded in the scientific community as the ""Father of Botany"". His major works, Enquiry into Plants and On the Causes of Plants, constitute the most important contributions to botanical science until the Middle Ages, almost seventeen centuries later.Another work from Ancient Greece that made an early impact on botany is De materia medica, a five-volume encyclopedia about preliminary herbal medicine written in the middle of the first century by Greek physician and pharmacologist Pedanius Dioscorides. De materia medica was widely read for more than 1,500 years. Important contributions from the medieval Muslim world include Ibn Wahshiyya's Nabatean Agriculture, Abū Ḥanīfa Dīnawarī's (828–896) the Book of Plants, and Ibn Bassal's The Classification of Soils. In the early 13th century, Abu al-Abbas al-Nabati, and Ibn al-Baitar (d. 1248) wrote on botany in a systematic and scientific manner.In the mid-16th century, botanical gardens were founded in a number of Italian universities. The Padua botanical garden in 1545 is usually considered to be the first which is still in its original location. These gardens continued the practical value of earlier ""physic gardens"", often associated with monasteries, in which plants were cultivated for suspected medicinal uses. They supported the growth of botany as an academic subject. Lectures were given about the plants grown in the gardens. Botanical gardens came much later to northern Europe; the first in England was the University of Oxford Botanic Garden in 1621.German physician Leonhart Fuchs (1501–1566) was one of ""the three German fathers of botany"", along with theologian Otto Brunfels (1489–1534) and physician Hieronymus Bock (1498–1554) (also called Hieronymus Tragus). Fuchs and Brunfels broke away from the tradition of copying earlier works to make original observations of their own. Bock created his own system of plant classification.
Physician Valerius Cordus (1515–1544) authored a botanically and pharmacologically important herbal Historia Plantarum in 1544 and a pharmacopoeia of lasting importance, the Dispensatorium in 1546. Naturalist Conrad von Gesner (1516–1565) and herbalist John Gerard (1545–c. 1611) published herbals covering the supposed medicinal uses of plants. Naturalist Ulisse Aldrovandi (1522–1605) was considered the father of natural history, which included the study of plants. In 1665, using an early microscope, Polymath Robert Hooke discovered cells, a term he coined, in cork, and a short time later in living plant tissue.


=== Early modern botany ===

During the 18th century, systems of plant identification were developed comparable to dichotomous keys, where unidentified plants are placed into taxonomic groups (e.g. family, genus and species) by making a series of choices between pairs of characters. The choice and sequence of the characters may be artificial in keys designed purely for identification (diagnostic keys) or more closely related to the natural or phyletic order of the taxa in synoptic keys. By the 18th century, new plants for study were arriving in Europe in increasing numbers from newly discovered countries and the European colonies worldwide. In 1753, Carl von Linné (Carl Linnaeus) published his Species Plantarum, a hierarchical classification of plant species that remains the reference point for modern botanical nomenclature. This established a standardised binomial or two-part naming scheme where the first name represented the genus and the second identified the species within the genus. For the purposes of identification, Linnaeus's Systema Sexuale classified plants into 24 groups according to the number of their male sexual organs. The 24th group, Cryptogamia, included all plants with concealed reproductive parts, mosses, liverworts, ferns, algae and fungi.Increasing knowledge of plant anatomy, morphology and life cycles led to the realisation that there were more natural affinities between plants than the artificial sexual system of Linnaeus. Adanson (1763), de Jussieu (1789), and Candolle (1819) all proposed various alternative natural systems of classification that grouped plants using a wider range of shared characters and were widely followed. The Candollean system reflected his ideas of the progression of morphological complexity and the later Bentham & Hooker system, which was influential until the mid-19th century, was influenced by Candolle's approach. Darwin's publication of the Origin of Species in 1859 and his concept of common descent required modifications to the Candollean system to reflect evolutionary relationships as distinct from mere morphological similarity.Botany was greatly stimulated by the appearance of the first ""modern"" textbook, Matthias Schleiden's Grundzüge der Wissenschaftlichen Botanik, published in English in 1849 as Principles of Scientific Botany.  Schleiden was a microscopist and an early plant anatomist who co-founded the cell theory with Theodor Schwann and Rudolf Virchow and was among the first to grasp the significance of the cell nucleus that had been described by Robert Brown in 1831.
In 1855, Adolf Fick formulated Fick's laws that enabled the calculation of the rates of molecular diffusion in biological systems.


=== Late modern botany ===
Building upon the gene-chromosome theory of heredity that originated with Gregor Mendel (1822–1884), August Weismann (1834–1914) proved that inheritance only takes place through gametes. No other cells can pass on inherited characters. The work of Katherine Esau (1898–1997) on plant anatomy is still a major foundation of modern botany. Her books Plant Anatomy and Anatomy of Seed Plants have been key plant structural biology texts for more than half a century.

The discipline of plant ecology was pioneered in the late 19th century by botanists such as Eugenius Warming, who produced the hypothesis that plants form communities, and his mentor and successor Christen C. Raunkiær whose system for describing plant life forms is still in use today. The concept that the composition of plant communities such as temperate broadleaf forest changes by a process of ecological succession was developed by Henry Chandler Cowles, Arthur Tansley and Frederic Clements. Clements is credited with the idea of climax vegetation as the most complex vegetation that an environment can support and Tansley introduced the concept of ecosystems to biology. Building on the extensive earlier work of Alphonse de Candolle, Nikolai Vavilov (1887–1943) produced accounts of the biogeography, centres of origin, and evolutionary history of economic plants.Particularly since the mid-1960s there have been advances in understanding of the physics of plant physiological processes such as transpiration (the transport of water within plant tissues),  the temperature dependence of rates of water evaporation from the leaf surface and the molecular diffusion of water vapour and carbon dioxide through stomatal apertures. These developments, coupled with new methods for measuring the size of stomatal apertures, and the rate of photosynthesis have enabled precise description of the rates of gas exchange between plants and the atmosphere. Innovations in statistical analysis by Ronald Fisher, Frank Yates and others at Rothamsted Experimental Station facilitated rational experimental design and data analysis in botanical research. The discovery and identification of the auxin plant hormones by Kenneth V. Thimann in 1948 enabled regulation of plant growth by externally applied chemicals. Frederick Campion Steward pioneered techniques of micropropagation and plant tissue culture controlled by plant hormones. The synthetic auxin 2,4-dichlorophenoxyacetic acid or 2,4-D was one of the first commercial synthetic herbicides.

20th century developments in plant biochemistry have been driven by modern techniques of organic chemical analysis, such as spectroscopy, chromatography and electrophoresis. With the rise of the related molecular-scale biological approaches of molecular biology, genomics, proteomics and metabolomics, the relationship between the plant genome and most aspects of the biochemistry, physiology, morphology and behaviour of plants can be subjected to detailed experimental analysis. The concept originally stated by Gottlieb Haberlandt in 1902 that all plant cells are totipotent and can be grown in vitro ultimately enabled the use of genetic engineering experimentally to knock out a gene or genes responsible for a specific trait, or to add genes such as GFP that report when a gene of interest is being expressed. These technologies enable the biotechnological use of whole plants or plant cell cultures grown in bioreactors to synthesise pesticides, antibiotics or other pharmaceuticals, as well as the practical application of genetically modified crops designed for traits such as improved yield.Modern morphology recognises a continuum between the major morphological categories of root, stem (caulome), leaf (phyllome) and trichome. Furthermore, it emphasises structural dynamics. Modern systematics aims to reflect and discover phylogenetic relationships between plants. Modern Molecular phylogenetics largely ignores morphological characters, relying on DNA sequences as data. Molecular analysis of DNA sequences from most families of flowering plants enabled the Angiosperm Phylogeny Group to publish in 1998 a phylogeny of flowering plants, answering many of the questions about relationships among angiosperm families and species. The theoretical possibility of a practical method for identification of plant species and commercial varieties by DNA barcoding is the subject of active current research.


== Scope and importance ==

The study of plants is vital because they underpin almost all animal life on Earth by generating a large proportion of the oxygen and food that provide humans and other organisms with aerobic respiration with the chemical energy they need to exist. Plants, algae and cyanobacteria are the major groups of organisms that carry out photosynthesis, a process that uses the energy of sunlight to convert water and carbon dioxide into sugars that can be used both as a source of chemical energy and of organic molecules that are used in the structural components of cells. As a by-product of photosynthesis, plants release oxygen into the atmosphere, a gas that is required by nearly all living things to carry out cellular respiration. In addition, they are influential in the global carbon and water cycles and plant roots bind and stabilise soils, preventing soil erosion. Plants are crucial to the future of human society as they provide food, oxygen, biochemicals, and products for people, as well as creating and preserving soil.Historically, all living things were classified as either animals or plants and botany covered the study of all organisms not considered animals. Botanists examine both the internal functions and processes within plant organelles, cells, tissues, whole plants, plant populations and plant communities. At each of these levels, a botanist may be concerned with the classification (taxonomy), phylogeny and evolution, structure (anatomy and morphology), or function (physiology) of plant life.The strictest definition of ""plant"" includes only the ""land plants"" or embryophytes, which include seed plants (gymnosperms, including the pines, and flowering plants) and the free-sporing cryptogams including ferns, clubmosses, liverworts, hornworts and mosses. Embryophytes are multicellular eukaryotes descended from an ancestor that obtained its energy from sunlight by photosynthesis. They have life cycles with alternating haploid and diploid phases. The sexual haploid phase of embryophytes, known as the gametophyte, nurtures the developing diploid embryo sporophyte within its tissues for at least part of its life, even in the seed plants, where the gametophyte itself is nurtured by its parent sporophyte. Other groups of organisms that were previously studied by botanists include bacteria (now studied in bacteriology), fungi (mycology) – including lichen-forming fungi (lichenology), non-chlorophyte algae (phycology), and viruses (virology). However, attention is still given to these groups by botanists, and fungi (including lichens) and photosynthetic protists are usually covered in introductory botany courses.Palaeobotanists study ancient plants in the fossil record to provide information about the evolutionary history of plants. Cyanobacteria, the first oxygen-releasing photosynthetic organisms on Earth, are thought to have given rise to the ancestor of plants by entering into an endosymbiotic relationship with an early eukaryote, ultimately becoming the chloroplasts in plant cells. The new photosynthetic plants (along with their algal relatives) accelerated the rise in atmospheric oxygen started by the cyanobacteria, changing the ancient oxygen-free, reducing, atmosphere to one in which free oxygen has been abundant for more than 2 billion years.Among the important botanical questions of the 21st century are the role of plants as primary producers in the global cycling of life's basic ingredients: energy, carbon, oxygen, nitrogen and water, and ways that our plant stewardship can help address the global environmental issues of resource management, conservation, human food security, biologically invasive organisms, carbon sequestration, climate change, and sustainability.


=== Human nutrition ===

Virtually all staple foods come either directly from primary production by plants, or indirectly from animals that eat them.  Plants and other photosynthetic organisms are at the base of most food chains because they use the energy from the sun and nutrients from the soil and atmosphere, converting them into a form that can be used by animals. This is what ecologists call the first trophic level.  The modern forms of the major staple foods, such as hemp, teff, maize, rice, wheat and other cereal grasses, pulses, bananas and plantains, as well as hemp, flax and cotton grown for their fibres, are the outcome of prehistoric selection over thousands of years from among wild ancestral plants with the most desirable characteristics.Botanists study how plants produce food and how to increase yields, for example through plant breeding, making their work important to humanity's ability to feed the world and provide food security for future generations. Botanists also study weeds, which are a considerable problem in agriculture, and the biology and control of plant pathogens in agriculture and natural ecosystems. Ethnobotany is the study of the relationships between plants and people. When applied to the investigation of historical plant–people relationships ethnobotany may be referred to as archaeobotany or palaeoethnobotany. Some of the earliest plant-people relationships arose between the indigenous people of Canada in identifying edible plants from inedible plants. This relationship the indigenous people had with plants was recorded by ethnobotanists.


== Plant biochemistry ==
Plant biochemistry is the study of the chemical processes used by plants. Some of these processes are used in their primary metabolism like the photosynthetic Calvin cycle and crassulacean acid metabolism. Others make specialised materials like the cellulose and lignin used to build their bodies, and secondary products like resins and aroma compounds.

Plants and various other groups of photosynthetic eukaryotes collectively known as ""algae"" have unique organelles known as chloroplasts. Chloroplasts are thought to be descended from cyanobacteria that formed endosymbiotic relationships with ancient plant and algal ancestors. Chloroplasts and cyanobacteria contain the blue-green pigment chlorophyll a. Chlorophyll a (as well as its plant and green algal-specific cousin chlorophyll b) absorbs light in the blue-violet and orange/red parts of the spectrum while reflecting and transmitting the green light that we see as the characteristic colour of these organisms. The energy in the red and blue light that these pigments absorb is used by chloroplasts to make energy-rich carbon compounds from carbon dioxide and water by oxygenic photosynthesis, a process that generates molecular oxygen (O2) as a by-product.

The light energy captured by chlorophyll a is initially in the form of electrons (and later a proton gradient) that's used to make molecules of ATP and NADPH which temporarily store and transport energy. Their energy is used in the light-independent reactions of the Calvin cycle by the enzyme rubisco to produce molecules of the 3-carbon sugar glyceraldehyde 3-phosphate (G3P). Glyceraldehyde 3-phosphate is the first product of photosynthesis and the raw material from which glucose and almost all other organic molecules of biological origin are synthesised. Some of the glucose is converted to starch which is stored in the chloroplast. Starch is the characteristic energy store of most land plants and algae, while inulin, a polymer of fructose is used for the same purpose in the sunflower family Asteraceae. Some of the glucose is converted to sucrose (common table sugar) for export to the rest of the plant.
Unlike in animals (which lack chloroplasts), plants and their eukaryote relatives have delegated many biochemical roles to their chloroplasts, including synthesising all their fatty acids, and most amino acids. The fatty acids that chloroplasts make are used for many things, such as providing material to build cell membranes out of and making the polymer cutin which is found in the plant cuticle that protects land plants from drying out. Plants synthesise a number of unique polymers like the polysaccharide molecules cellulose, pectin and xyloglucan from which the land plant cell wall is constructed.
Vascular land plants make lignin, a polymer used to strengthen the secondary cell walls of xylem tracheids and vessels to keep them from collapsing when a plant sucks water through them under water stress. Lignin is also used in other cell types like sclerenchyma fibres that provide structural support for a plant and is a major constituent of wood. Sporopollenin is a chemically resistant polymer found in the outer cell walls of spores and pollen of land plants responsible for the survival of early land plant spores and the pollen of seed plants in the fossil record. It is widely regarded as a marker for the start of land plant evolution during the Ordovician period.
The concentration of carbon dioxide in the atmosphere today is much lower than it was when plants emerged onto land during the Ordovician and Silurian periods. Many monocots like maize and the pineapple and some dicots like the Asteraceae have since independently evolved pathways like Crassulacean acid metabolism and the C4 carbon fixation pathway for photosynthesis which avoid the losses resulting from photorespiration in the more common C3 carbon fixation pathway. These biochemical strategies are unique to land plants.


=== Medicine and materials ===
Phytochemistry is a branch of plant biochemistry primarily concerned with the chemical substances produced by plants during secondary metabolism. Some of these compounds are toxins such as the alkaloid coniine from hemlock. Others, such as the essential oils peppermint oil and lemon oil are useful for their aroma, as flavourings and spices (e.g., capsaicin), and in medicine as pharmaceuticals as in opium from opium poppies. Many medicinal and recreational drugs, such as tetrahydrocannabinol (active ingredient in cannabis), caffeine, morphine and nicotine come directly from plants. Others are simple derivatives of botanical natural products. For example, the pain killer aspirin is the acetyl ester of salicylic acid, originally isolated from the bark of willow trees, and a wide range of opiate painkillers like heroin are obtained by chemical modification of morphine obtained from the opium poppy. Popular stimulants come from plants, such as caffeine from coffee, tea and chocolate, and nicotine from tobacco. Most alcoholic beverages come from fermentation of carbohydrate-rich plant products such as barley (beer), rice (sake) and grapes (wine). Native Americans have used various plants as ways of treating illness or disease for thousands of years. This knowledge Native Americans have on plants has been recorded by enthnobotanists and then in turn has been used by pharmaceutical companies as a way of drug discovery.Plants can synthesise coloured dyes and pigments such as the anthocyanins responsible for the red colour of red wine, yellow weld and blue woad used together to produce Lincoln green, indoxyl, source of the blue dye indigo traditionally used to dye denim and the artist's pigments gamboge and rose madder.
Sugar, starch, cotton, linen, hemp, some types of rope, wood and particle boards, papyrus and paper, vegetable oils, wax, and natural rubber are examples of commercially important materials made from plant tissues or their secondary products. Charcoal, a pure form of carbon made by pyrolysis of wood, has a long history as a metal-smelting fuel, as a filter material and adsorbent and as an artist's material and is one of the three ingredients of gunpowder. Cellulose, the world's most abundant organic polymer, can be converted into energy, fuels, materials and chemical feedstock. Products made from cellulose include rayon and cellophane, wallpaper paste, biobutanol and gun cotton. Sugarcane, rapeseed and soy are some of the plants with a highly fermentable sugar or oil content that are used as sources of biofuels, important alternatives to fossil fuels, such as biodiesel.  Sweetgrass was used by Native Americans to ward off bugs like mosquitoes. These bug repelling properties of sweetgrass were later found by the American Chemical Society in the molecules phytol and coumarin.


== Plant ecology ==

Plant ecology is the science of the functional relationships between plants and their habitats – the environments where they complete their life cycles. Plant ecologists study the composition of local and regional floras, their biodiversity, genetic diversity and fitness, the adaptation of plants to their environment, and their competitive or mutualistic interactions with other species. Some ecologists even rely on empirical data from indigenous people that is gathered by ethnobotanists. This information can relay a great deal of information on how the land once was thousands of years ago and how it has changed over that time. The goals of plant ecology are to understand the causes of their distribution patterns, productivity, environmental impact, evolution, and responses to environmental change.Plants depend on certain edaphic (soil) and climatic factors in their environment but can modify these factors too. For example, they can change their environment's albedo, increase runoff interception, stabilise mineral soils and develop their organic content, and affect local temperature. Plants compete with other organisms in their ecosystem for resources. They interact with their neighbours at a variety of spatial scales in groups, populations and communities that collectively constitute vegetation. Regions with characteristic vegetation types and dominant plants as well as similar abiotic and biotic factors, climate, and geography make up biomes like tundra or tropical rainforest.Herbivores eat plants, but plants can defend themselves and some species are parasitic or even carnivorous. Other organisms form mutually beneficial relationships with plants. For example, mycorrhizal fungi and rhizobia provide plants with nutrients in exchange for food, ants are recruited by ant plants to provide protection, honey bees, bats and other animals pollinate flowers and humans and other animals act as dispersal vectors to spread spores and seeds.


=== Plants, climate and environmental change ===
Plant responses to climate and other environmental changes can inform our understanding of how these changes affect ecosystem function and productivity. For example, plant phenology can be a useful proxy for temperature in historical climatology, and the biological impact of climate change and global warming. Palynology, the analysis of fossil pollen deposits in sediments from thousands or millions of years ago allows the reconstruction of past climates. Estimates of atmospheric CO2 concentrations since the Palaeozoic have been obtained from stomatal densities and the leaf shapes and sizes of ancient land plants. Ozone depletion can expose plants to higher levels of ultraviolet radiation-B (UV-B), resulting in lower growth rates. Moreover, information from studies of community ecology, plant systematics, and taxonomy is essential to understanding vegetation change, habitat destruction and species extinction.


== Genetics ==

Inheritance in plants follows the same fundamental principles of genetics as in other multicellular organisms. Gregor Mendel discovered the genetic laws of inheritance by studying inherited traits such as shape in Pisum sativum (peas). What Mendel learned from studying plants has had far-reaching benefits outside of botany. Similarly, ""jumping genes"" were discovered by Barbara McClintock while she was studying maize. Nevertheless, there are some distinctive genetic differences between plants and other organisms.
Species boundaries in plants may be weaker than in animals, and cross species hybrids are often possible. A familiar example is peppermint, Mentha × piperita, a sterile hybrid between Mentha aquatica and spearmint, Mentha spicata. The many cultivated varieties of wheat are the result of multiple inter- and intra-specific crosses between wild species and their hybrids. Angiosperms with monoecious flowers often have self-incompatibility mechanisms that operate between the pollen and stigma so that the pollen either fails to reach the stigma or fails to germinate and produce male gametes. This is one of several methods used by plants to promote outcrossing. In many land plants the male and female gametes are produced by separate individuals. These species are said to be dioecious when referring to vascular plant sporophytes and dioicous when referring to bryophyte gametophytes.Unlike in higher animals, where parthenogenesis is rare, asexual reproduction may occur in plants by several different mechanisms. The formation of stem tubers in potato is one example. Particularly in arctic or alpine habitats, where opportunities for fertilisation of flowers by animals are rare, plantlets or bulbs, may develop instead of flowers, replacing sexual reproduction with asexual reproduction and giving rise to clonal populations genetically identical to the parent. This is one of several types of apomixis that occur in plants. Apomixis can also happen in a seed, producing a seed that contains an embryo genetically identical to the parent.Most sexually reproducing organisms are diploid, with paired chromosomes, but doubling of their chromosome number may occur due to errors in cytokinesis. This can occur early in development to produce an autopolyploid or partly autopolyploid organism, or during normal processes of cellular differentiation to produce some cell types that are polyploid (endopolyploidy), or during gamete formation. An allopolyploid plant may result from a hybridisation event between two different species. Both autopolyploid and allopolyploid plants can often reproduce normally, but may be unable to cross-breed successfully with the parent population because there is a mismatch in chromosome numbers. These plants that are reproductively isolated from the parent species but live within the same geographical area, may be sufficiently successful to form a new species. Some otherwise sterile plant polyploids can still reproduce vegetatively or by seed apomixis, forming clonal populations of identical individuals. Durum wheat is a fertile tetraploid allopolyploid, while bread wheat is a fertile hexaploid. The commercial banana is an example of a sterile, seedless triploid hybrid. Common dandelion is a triploid that produces viable seeds by apomictic seed.
As in other eukaryotes, the inheritance of endosymbiotic organelles like mitochondria and chloroplasts in plants is non-Mendelian. Chloroplasts are inherited through the male parent in gymnosperms but often through the female parent in flowering plants.


=== Molecular genetics ===

A considerable amount of new knowledge about plant function comes from studies of the molecular genetics of model plants such as the Thale cress, Arabidopsis thaliana, a weedy species in the mustard family (Brassicaceae). The genome or hereditary information contained in the genes of this species is encoded by about 135 million base pairs of DNA, forming one of the smallest genomes among flowering plants. Arabidopsis was the first plant to have its genome sequenced, in 2000. The sequencing of some other relatively small genomes, of rice (Oryza sativa) and Brachypodium distachyon, has made them important model species for understanding the genetics, cellular and molecular biology of cereals, grasses and monocots generally.
Model plants such as Arabidopsis thaliana are used for studying the molecular biology of plant cells and the chloroplast. Ideally, these organisms have small genomes that are well known or completely sequenced, small stature and short generation times. Corn has been used to study mechanisms of photosynthesis and phloem loading of sugar in C4 plants. The single celled green alga Chlamydomonas reinhardtii, while not an embryophyte itself, contains a green-pigmented chloroplast related to that of land plants, making it useful for study. A red alga Cyanidioschyzon merolae has also been used to study some basic chloroplast functions. Spinach, peas, soybeans and a moss Physcomitrella patens are commonly used to study plant cell biology.Agrobacterium tumefaciens, a soil rhizosphere bacterium, can attach to plant cells and infect them with a callus-inducing Ti plasmid by horizontal gene transfer, causing a callus infection called crown gall disease. Schell and Van Montagu (1977) hypothesised that the Ti plasmid could be a natural vector for introducing the Nif gene responsible for nitrogen fixation in the root nodules of legumes and other plant species. Today, genetic modification of the Ti plasmid is one of the main techniques for introduction of transgenes to plants and the creation of genetically modified crops.


=== Epigenetics ===

Epigenetics is the study of heritable changes in gene function that cannot be explained by changes in the underlying DNA sequence but cause the organism's genes to behave (or ""express themselves"") differently. One example of epigenetic change is the marking of the genes by DNA methylation which determines whether they will be expressed or not. Gene expression can also be controlled by repressor proteins that attach to silencer regions of the DNA and prevent that region of the DNA code from being expressed. Epigenetic marks may be added or removed from the DNA during programmed stages of development of the plant, and are responsible, for example, for the differences between anthers, petals and normal leaves, despite the fact that they all have the same underlying genetic code. Epigenetic changes may be temporary or may remain through successive cell divisions for the remainder of the cell's life. Some epigenetic changes have been shown to be heritable, while others are reset in the germ cells.
Epigenetic changes in eukaryotic biology serve to regulate the process of cellular differentiation. During morphogenesis, totipotent stem cells become the various pluripotent cell lines of the embryo, which in turn become fully differentiated cells. A single fertilised egg cell, the zygote, gives rise to the many different plant cell types including parenchyma, xylem vessel elements, phloem sieve tubes, guard cells of the epidermis, etc. as it continues to divide. The process results from the epigenetic activation of some genes and inhibition of others.Unlike animals, many plant cells, particularly those of the parenchyma, do not terminally differentiate, remaining totipotent with the ability to give rise to a new individual plant. Exceptions include highly lignified cells, the sclerenchyma and xylem which are dead at maturity, and the phloem sieve tubes which lack nuclei. While plants use many of the same epigenetic mechanisms as animals, such as chromatin remodelling, an alternative hypothesis is that plants set their gene expression patterns using positional information from the environment and surrounding cells to determine their developmental fate.Epigenetic changes can lead to paramutations, which do not follow the Mendelian heritage rules. These epigenetic marks are carried from one generation to the next, with one allele inducing a change on the other.


== Plant evolution ==

The chloroplasts of plants have a number of biochemical, structural and genetic similarities to cyanobacteria, (commonly but incorrectly known as ""blue-green algae"") and are thought to be derived from an ancient endosymbiotic relationship between an ancestral eukaryotic cell and a cyanobacterial resident.The algae are a polyphyletic group and are placed in various divisions, some more closely related to plants than others. There are many differences between them in features such as cell wall composition, biochemistry, pigmentation, chloroplast structure and nutrient reserves. The algal division Charophyta, sister to the green algal division Chlorophyta, is considered to contain the ancestor of true plants. The Charophyte class Charophyceae and the land plant sub-kingdom Embryophyta together form the monophyletic group or clade Streptophytina.Nonvascular land plants are embryophytes that lack the vascular tissues xylem and phloem. They include mosses, liverworts and hornworts. Pteridophytic vascular plants with true xylem and phloem that reproduced by spores germinating into free-living gametophytes evolved during the Silurian period and diversified into several lineages during the late Silurian and early Devonian. Representatives of the lycopods have survived to the present day. By the end of the Devonian period, several groups, including the lycopods, sphenophylls and progymnosperms, had independently evolved ""megaspory"" – their spores were of two distinct sizes, larger megaspores and smaller microspores. Their reduced gametophytes developed from megaspores retained within the spore-producing organs (megasporangia) of the sporophyte, a condition known as endospory. Seeds consist of an endosporic megasporangium surrounded by one or two sheathing layers (integuments). The young sporophyte develops within the seed, which on germination splits to release it. The earliest known seed plants date from the latest Devonian Famennian stage. Following the evolution of the seed habit, seed plants diversified, giving rise to a number of now-extinct groups, including seed ferns, as well as the modern gymnosperms and angiosperms. Gymnosperms produce ""naked seeds"" not fully enclosed in an ovary; modern representatives include conifers, cycads, Ginkgo, and Gnetales. Angiosperms produce seeds enclosed in a structure such as a carpel or an ovary. Ongoing research on the molecular phylogenetics of living plants appears to show that the angiosperms are a sister clade to the gymnosperms.


== Plant physiology ==

Plant physiology encompasses all the internal chemical and physical activities of plants associated with life. Chemicals obtained from the air, soil and water form the basis of all plant metabolism. The energy of sunlight, captured by oxygenic photosynthesis and released by cellular respiration, is the basis of almost all life. Photoautotrophs, including all green plants, algae and cyanobacteria gather energy directly from sunlight by photosynthesis. Heterotrophs including all animals, all fungi, all completely parasitic plants, and non-photosynthetic bacteria take in organic molecules produced by photoautotrophs and respire them or use them in the construction of cells and tissues. Respiration is the oxidation of carbon compounds by breaking them down into simpler structures to release the energy they contain, essentially the opposite of photosynthesis.Molecules are moved within plants by transport processes that operate at a variety of spatial scales. Subcellular transport of ions, electrons and molecules such as water and enzymes occurs across cell membranes. Minerals and water are transported from roots to other parts of the plant in the transpiration stream. Diffusion, osmosis, and active transport and mass flow are all different ways transport can occur. Examples of elements that plants need to transport are nitrogen, phosphorus, potassium, calcium, magnesium, and sulfur. In vascular plants, these elements are extracted from the soil as soluble ions by the roots and transported throughout the plant in the xylem. Most of the elements required for plant nutrition come from the chemical breakdown of soil minerals.  Sucrose produced by photosynthesis is transported from the leaves to other parts of the plant in the phloem and plant hormones are transported by a variety of processes.


=== Plant hormones ===

Plants are not passive, but respond to external signals such as light, touch, and injury by moving or growing towards or away from the stimulus, as appropriate. Tangible evidence of touch sensitivity is the almost instantaneous collapse of leaflets of Mimosa pudica, the insect traps of Venus flytrap and bladderworts, and the pollinia of orchids.The hypothesis that plant growth and development is coordinated by plant hormones or plant growth regulators first emerged in the late 19th century. Darwin experimented on the movements of plant shoots and roots towards light and gravity, and concluded ""It is hardly an exaggeration to say that the tip of the radicle . . acts like the brain of one of the lower animals . . directing the several movements"". About the same time, the role of auxins (from the Greek auxein, to grow) in control of plant growth was first outlined by the Dutch scientist Frits Went. The first known auxin, indole-3-acetic acid (IAA), which promotes cell growth, was only isolated from plants about 50 years later. This compound mediates the tropic responses of shoots and roots towards light and gravity. The finding in 1939 that plant callus could be maintained in culture containing IAA, followed by the observation in 1947 that it could be induced to form roots and shoots by controlling the concentration of growth hormones were key steps in the development of plant biotechnology and genetic modification.

Cytokinins are a class of plant hormones named for their control of cell division (especially cytokinesis). The natural cytokinin zeatin was discovered in corn, Zea mays, and is a derivative of the purine adenine. Zeatin is produced in roots and transported to shoots in the xylem where it promotes cell division, bud development, and the greening of chloroplasts. The gibberelins, such as gibberelic acid are diterpenes synthesised from acetyl CoA via the mevalonate pathway. They are involved in the promotion of germination and dormancy-breaking in seeds, in regulation of plant height by controlling stem elongation and the control of flowering. Abscisic acid (ABA) occurs in all land plants except liverworts, and is synthesised from carotenoids in the chloroplasts and other plastids. It inhibits cell division, promotes seed maturation, and dormancy, and promotes stomatal closure. It was so named because it was originally thought to control abscission. Ethylene is a gaseous hormone that is produced in all higher plant tissues from methionine. It is now known to be the hormone that stimulates or regulates fruit ripening and abscission, and it, or the synthetic growth regulator ethephon which is rapidly metabolised to produce ethylene, are used on industrial scale to promote ripening of cotton, pineapples and other climacteric crops.
Another class of phytohormones is the jasmonates, first isolated from the oil of Jasminum grandiflorum which regulates wound responses in plants by unblocking the expression of genes required in the systemic acquired resistance response to pathogen attack.In addition to being the primary energy source for plants, light functions as a signalling device, providing information to the plant, such as how much sunlight the plant receives each day. This can result in adaptive changes in a process known as photomorphogenesis. Phytochromes are the photoreceptors in a plant that are sensitive to light.


== Plant anatomy and morphology ==

Plant anatomy is the study of the structure of plant cells and tissues, whereas plant morphology is the study of their external form.
All plants are multicellular eukaryotes, their DNA stored in nuclei. The characteristic features of plant cells that distinguish them from those of animals and fungi include a primary cell wall composed of the polysaccharides cellulose, hemicellulose and pectin,  larger vacuoles than in animal cells and the presence of plastids with unique photosynthetic and biosynthetic functions as in the chloroplasts. Other plastids contain storage products such as starch (amyloplasts) or lipids (elaioplasts). Uniquely, streptophyte cells and those of the green algal order Trentepohliales divide by construction of a phragmoplast as a template for building a cell plate late in cell division.

The bodies of vascular plants including clubmosses, ferns and  seed plants (gymnosperms and angiosperms) generally have aerial and subterranean subsystems. The shoots consist of stems bearing green photosynthesising leaves and reproductive structures. The underground vascularised roots bear root hairs at their tips and generally lack chlorophyll. Non-vascular plants, the liverworts, hornworts and mosses do not produce ground-penetrating vascular roots and most of the plant participates in photosynthesis. The sporophyte generation is nonphotosynthetic in liverworts but may be able to contribute part of its energy needs by photosynthesis in mosses and hornworts.The root system and the shoot system are interdependent – the usually nonphotosynthetic root system depends on the shoot system for food, and the usually photosynthetic shoot system depends on water and minerals from the root system. Cells in each system are capable of creating cells of the other and producing adventitious shoots or roots. Stolons and tubers are examples of shoots that can grow roots. Roots that spread out close to the surface, such as those of willows, can produce shoots and ultimately new plants. In the event that one of the systems is lost, the other can often regrow it. In fact it is possible to grow an entire plant from a single leaf, as is the case with plants in Streptocarpus sect. Saintpaulia, or even a single cell – which can dedifferentiate into a callus (a mass of unspecialised cells) that can grow into a new plant.
In vascular plants, the xylem and phloem are the conductive tissues that transport resources between shoots and roots. Roots are often adapted to store food such as sugars or starch, as in sugar beets and carrots.Stems mainly provide support to the leaves and reproductive structures, but can store water in succulent plants such as cacti, food as in potato tubers, or reproduce vegetatively as in the stolons of strawberry plants or in the process of layering. Leaves gather sunlight and carry out photosynthesis. Large, flat, flexible, green leaves are called foliage leaves. Gymnosperms, such as conifers, cycads, Ginkgo, and gnetophytes are seed-producing plants with open seeds. Angiosperms are seed-producing plants that produce flowers and have enclosed seeds. Woody plants, such as azaleas and oaks, undergo a secondary growth phase resulting in two additional types of tissues: wood (secondary xylem) and bark (secondary phloem and cork). All gymnosperms and many angiosperms are woody plants. Some plants reproduce sexually, some asexually, and some via both means.Although reference to major morphological categories such as root, stem, leaf, and trichome are useful, one has to keep in mind that these categories are linked through intermediate forms so that a continuum between the categories results. Furthermore, structures can be seen as processes, that is, process combinations.


== Systematic botany ==

Systematic botany is part of systematic biology, which is concerned with the range and diversity of organisms and their relationships, particularly as determined by their evolutionary history. It involves, or is related to, biological classification, scientific taxonomy and phylogenetics. Biological classification is the method by which botanists group organisms into categories such as genera or species. Biological classification is a form of scientific taxonomy. Modern taxonomy is rooted in the work of Carl Linnaeus, who grouped species according to shared physical characteristics. These groupings have since been revised to align better with the Darwinian principle of common descent – grouping organisms by ancestry rather than superficial characteristics. While scientists do not always agree on how to classify organisms, molecular phylogenetics, which uses DNA sequences as data, has driven many recent revisions along evolutionary lines and is likely to continue to do so. The dominant classification system is called Linnaean taxonomy. It includes ranks and binomial nomenclature. The nomenclature of botanical organisms is codified in the International Code of Nomenclature for algae, fungi, and plants (ICN) and administered by the International Botanical Congress.Kingdom Plantae belongs to Domain Eukaryota and is broken down recursively until each species is separately classified. The order is: Kingdom; Phylum (or Division); Class; Order; Family; Genus (plural genera); Species. The scientific name of a plant represents its genus and its species within the genus, resulting in a single worldwide name for each organism. For example, the tiger lily is Lilium columbianum. Lilium is the genus, and columbianum the specific epithet. The combination is the name of the species. When writing the scientific name of an organism, it is proper to capitalise the first letter in the genus and put all of the specific epithet in lowercase. Additionally, the entire term is ordinarily italicised (or underlined when italics are not available).The evolutionary relationships and heredity of a group of organisms is called its phylogeny. Phylogenetic studies attempt to discover phylogenies. The basic approach is to use similarities based on shared inheritance to determine relationships. As an example, species of Pereskia are trees or bushes with prominent leaves. They do not obviously resemble a typical leafless cactus such as an Echinocactus. However, both Pereskia and Echinocactus have spines produced from areoles (highly specialised pad-like structures) suggesting that the two genera are indeed related.

Judging relationships based on shared characters requires care, since plants may resemble one another through convergent evolution in which characters have arisen independently. Some euphorbias have leafless, rounded bodies adapted to water conservation similar to those of globular cacti, but characters such as the structure of their flowers make it clear that the two groups are not closely related. The cladistic method takes a systematic approach to characters, distinguishing between those that carry no information about shared evolutionary history – such as those evolved separately in different groups (homoplasies) or those left over from ancestors (plesiomorphies) – and derived characters, which have been passed down from innovations in a shared ancestor (apomorphies). Only derived characters, such as the spine-producing areoles of cacti, provide evidence for descent from a common ancestor. The results of cladistic analyses are expressed as cladograms: tree-like diagrams showing the pattern of evolutionary branching and descent.From the 1990s onwards, the predominant approach to constructing phylogenies for living plants has been molecular phylogenetics, which uses molecular characters, particularly DNA sequences, rather than morphological characters like the presence or absence of spines and areoles. The difference is that the genetic code itself is used to decide evolutionary relationships, instead of being used indirectly via the characters it gives rise to. Clive Stace describes this as having ""direct access to the genetic basis of evolution."" As a simple example, prior to the use of genetic evidence, fungi were thought either to be plants or to be more closely related to plants than animals. Genetic evidence suggests that the true evolutionary relationship of multicelled organisms is as shown in the cladogram below – fungi are more closely related to animals than to plants.

In 1998, the Angiosperm Phylogeny Group published a phylogeny for flowering plants based on an analysis of DNA sequences from most families of flowering plants. As a result of this work, many questions, such as which families represent the earliest branches of angiosperms, have now been answered. Investigating how plant species are related to each other allows botanists to better understand the process of evolution in plants. Despite the study of model plants and increasing use of DNA evidence, there is ongoing work and discussion among taxonomists about how best to classify plants into various taxa. Technological developments such as computers and electron microscopes have greatly increased the level of detail studied and speed at which data can be analysed.


== Symbols ==
A few symbols are in current use in botany. A number of others are obsolete; for example, Linnaeus used planetary symbols for woody, herbaceous and perennial plants, and Willd used ♄ (Saturn) for neuter in addition to ☿ (Mercury) for hermaphroditic. The following symbols are still used:
♀ female
♂ male
⚥ hermaphrodite/bisexual
⚲ vegetative (asexual) reproduction
◊ sex unknown
☉ annual
 (⚇) biennial
 (♾) perennial
☠ poisonous
🛈 further information
× crossbred hybrid
+ grafted hybrid


== See also ==


== Notes ==


== References ==


=== Citations ===


=== Sources ===


== External links ==

 Media related to Botany at Wikimedia Commons",3382434,3999,"Articles containing Ancient Greek (to 1453)-language text, Articles containing German-language text, Articles containing video clips, Articles with BNE identifiers, Articles with BNF identifiers, Articles with GND identifiers, Articles with HDS identifiers, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with NARA identifiers, Articles with NDL identifiers, Articles with NKC identifiers, Articles with short description, Botany, CS1: long volume value, CS1 German-language sources (de), Commons category link is on Wikidata, Good articles, Pages using multiple image with manual scaled images, Short description matches Wikidata, Use British English from September 2016, Webarchive template wayback links",1132012884,biology
https://en.wikipedia.org/wiki/Translation_(biology),Translation (biology),"In molecular biology and genetics, translation is the process in which ribosomes in the cytoplasm or endoplasmic reticulum synthesize proteins after the process of transcription of DNA to RNA in the cell's nucleus.   The entire process is called gene expression.
In translation, messenger RNA (mRNA) is decoded in a ribosome, outside the nucleus, to produce a specific amino acid chain, or polypeptide. The polypeptide later folds into an active protein and performs its functions in the cell. The ribosome facilitates decoding by inducing the binding of complementary tRNA anticodon sequences to mRNA codons. The tRNAs carry specific amino acids that are chained together into a polypeptide as the mRNA passes through and is ""read"" by the ribosome.
Translation proceeds in three phases: 

Initiation: The ribosome assembles around the target mRNA. The first tRNA is attached at the start codon.
Elongation: The last tRNA validated by the small ribosomal subunit (accommodation) transfers the amino acid it carries to the large ribosomal subunit which binds it to the one of the precedingly admitted tRNA (transpeptidation). The ribosome then moves to the next mRNA codon to continue the process (translocation), creating an amino acid chain.
Termination: When a stop codon is reached, the ribosome releases the polypeptide. The ribosomal complex remains intact and moves on to the next mRNA to be translated.In prokaryotes (bacteria and archaea), translation occurs in the cytosol, where the large and small subunits of the ribosome bind to the mRNA. In eukaryotes, translation occurs in the cytoplasm or across the membrane of the endoplasmic reticulum in a process called co-translational translocation.  In co-translational translocation, the entire ribosome/mRNA complex binds to the outer membrane of the rough endoplasmic reticulum (ER) and the new protein is synthesized and released into the ER; the newly created polypeptide can be stored inside the ER for future vesicle transport and secretion outside the cell, or immediately secreted.
Many types of transcribed RNA, such as transfer RNA, ribosomal RNA, and small nuclear RNA, do not undergo translation into proteins.
A number of antibiotics act by inhibiting translation. These include anisomycin, cycloheximide, chloramphenicol, tetracycline, streptomycin, erythromycin, and puromycin. Prokaryotic ribosomes have a different structure from that of eukaryotic ribosomes, and thus antibiotics can specifically target bacterial infections without any harm to a eukaryotic host's cells.","In molecular biology and genetics, translation is the process in which ribosomes in the cytoplasm or endoplasmic reticulum synthesize proteins after the process of transcription of DNA to RNA in the cell's nucleus.   The entire process is called gene expression.
In translation, messenger RNA (mRNA) is decoded in a ribosome, outside the nucleus, to produce a specific amino acid chain, or polypeptide. The polypeptide later folds into an active protein and performs its functions in the cell. The ribosome facilitates decoding by inducing the binding of complementary tRNA anticodon sequences to mRNA codons. The tRNAs carry specific amino acids that are chained together into a polypeptide as the mRNA passes through and is ""read"" by the ribosome.
Translation proceeds in three phases: 

Initiation: The ribosome assembles around the target mRNA. The first tRNA is attached at the start codon.
Elongation: The last tRNA validated by the small ribosomal subunit (accommodation) transfers the amino acid it carries to the large ribosomal subunit which binds it to the one of the precedingly admitted tRNA (transpeptidation). The ribosome then moves to the next mRNA codon to continue the process (translocation), creating an amino acid chain.
Termination: When a stop codon is reached, the ribosome releases the polypeptide. The ribosomal complex remains intact and moves on to the next mRNA to be translated.In prokaryotes (bacteria and archaea), translation occurs in the cytosol, where the large and small subunits of the ribosome bind to the mRNA. In eukaryotes, translation occurs in the cytoplasm or across the membrane of the endoplasmic reticulum in a process called co-translational translocation.  In co-translational translocation, the entire ribosome/mRNA complex binds to the outer membrane of the rough endoplasmic reticulum (ER) and the new protein is synthesized and released into the ER; the newly created polypeptide can be stored inside the ER for future vesicle transport and secretion outside the cell, or immediately secreted.
Many types of transcribed RNA, such as transfer RNA, ribosomal RNA, and small nuclear RNA, do not undergo translation into proteins.
A number of antibiotics act by inhibiting translation. These include anisomycin, cycloheximide, chloramphenicol, tetracycline, streptomycin, erythromycin, and puromycin. Prokaryotic ribosomes have a different structure from that of eukaryotic ribosomes, and thus antibiotics can specifically target bacterial infections without any harm to a eukaryotic host's cells.


== Basic mechanisms ==

The basic process of protein production is addition of one amino acid at a time to the end of a protein. This operation is performed by a ribosome. A ribosome is made up of two subunits, a small subunit and a large subunit. These subunits come together before translation of mRNA into a protein to provide a location for translation to be carried out and a polypeptide to be produced. The choice of amino acid type to add is determined by an mRNA molecule. Each amino acid added is matched to a three nucleotide subsequence of the mRNA. For each such triplet possible, the corresponding amino acid is accepted. The successive amino acids added to the chain are matched to successive nucleotide triplets in the mRNA. In this way the sequence of nucleotides in the template mRNA chain determines the sequence of amino acids in the generated amino acid chain.
Addition of an amino acid occurs at the C-terminus of the peptide and thus translation is said to be amine-to-carboxyl directed.The mRNA carries genetic information encoded as a ribonucleotide sequence from the chromosomes to the ribosomes. The ribonucleotides are ""read"" by translational machinery in a sequence of nucleotide triplets called codons. Each of those triplets codes for a specific amino acid.
The ribosome molecules translate this code to a specific sequence of amino acids. The ribosome is a multisubunit structure containing rRNA and proteins. It is the ""factory"" where amino acids are assembled into proteins.
tRNAs are small noncoding RNA chains (74–93 nucleotides) that transport amino acids to the ribosome. The repertoire of tRNA genes varies widely between species, with some Bacteria having between 20 and 30 genes while complex eukaryotes could have thousands. tRNAs have a site for amino acid attachment, and a site called an anticodon. The anticodon is an RNA triplet complementary to the mRNA triplet that codes for their cargo amino acid.
Aminoacyl tRNA synthetases (enzymes) catalyze the bonding between specific tRNAs and the amino acids that their anticodon sequences call for. The product of this reaction is an aminoacyl-tRNA. In bacteria, this aminoacyl-tRNA is carried to the ribosome by EF-Tu, where mRNA codons are matched through complementary base pairing to specific tRNA anticodons. Aminoacyl-tRNA synthetases that mispair tRNAs with the wrong amino acids can produce mischarged aminoacyl-tRNAs, which can result in inappropriate amino acids at the respective position in protein. This ""mistranslation"" of the genetic code naturally occurs at low levels in most organisms, but certain cellular environments cause an increase in permissive mRNA decoding, sometimes to the benefit of the cell.
The ribosome has two binding sites for tRNA. They are the aminoacyl site (abbreviated A), the peptidyl site/ exit site (abbreviated P/E). With respect to the mRNA, the three sites are oriented 5’ to 3’ E-P-A, because ribosomes move toward the 3' end of mRNA. The A-site binds the incoming tRNA with the complementary codon on the mRNA. The P/E-site holds the tRNA with the growing polypeptide chain. When an aminoacyl-tRNA initially binds to its corresponding codon on the mRNA, it is in the A site. Then, a peptide bond forms between the amino acid of the tRNA in the A site and the amino acid of the charged tRNA in the P/E site. The growing polypeptide chain is transferred to the tRNA in the A site. Translocation occurs, moving the tRNA in the P/E site, now without an amino acid; the tRNA that was in the A site, now charged with the polypeptide chain, is moved to the P/E site and the tRNA leaves and another aminoacyl-tRNA enters the A site to repeat the process.After the new amino acid is added to the chain, and after the tRNA is released out of the ribosome and into the cytosol, the energy provided by the hydrolysis of a GTP bound to the translocase EF-G (in bacteria) and a/eEF-2 (in eukaryotes and archaea) moves the ribosome down one codon towards the 3' end. The energy required for translation of proteins is significant. For a protein containing n amino acids, the number of high-energy phosphate bonds required to translate it is 4n-1. The rate of translation varies; it is significantly higher in prokaryotic cells (up to 17–21 amino acid residues per second) than in eukaryotic cells (up to 6–9 amino acid residues per second).Even though the ribosomes are usually considered accurate and processive machines, the translation process is subject to errors that can lead either to the synthesis of erroneous proteins or to the premature abandonment of translation, either because a tRNA couples to a wrong codon or because a tRNA is coupled to the wrong amino acid.   The rate of error in synthesizing proteins has been estimated to be between 1 in 105 and 1 in 103 misincorporated amino acids, depending on the experimental conditions. The rate of premature translation abandonment, instead, has been estimated to be of the order of magnitude of 10−4 events per translated codon.
The correct amino acid is covalently bonded to the correct transfer RNA (tRNA) by amino acyl transferases. The amino acid is joined by its carboxyl group to the 3' OH of the tRNA by an ester bond. When the tRNA has an amino acid linked to it, the tRNA is termed ""charged"". Initiation involves the small subunit of the ribosome binding to the 5' end of mRNA with the help of initiation factors (IF). In bacteria and a minority of archaea, initiation of protein synthesis involves the recognition of a purine-rich initiation sequence on the mRNA called the Shine-Dalgarno sequence.  The Shine-Dalgarno sequence binds to a complementary pyrimidine-rich sequence on the 3' end of the 16S rRNA part of the 30S ribosomal subunit.  The binding of these complementary sequences ensures that the 30S ribosomal subunit is bound to the mRNA and is aligned such that the initiation codon is placed in the 30S portion of the P-site.  Once the mRNA and 30S subunit are properly bound, an initiation factor brings the initiator tRNA-amino acid complex, f-Met-tRNA, to the 30S P site.  The initiation phase is completed once a 50S subunit joins the 30 subunit, forming an active 70S ribosome. Termination of the polypeptide occurs when the A site of the ribosome is occupied by a stop codon (UAA, UAG, or UGA) on the mRNA, creating the primary structure of a protein. tRNA usually cannot recognize or bind to stop codons. Instead, the stop codon induces the binding of a release factor protein (RF1 & RF2) that prompts the disassembly of the entire ribosome/mRNA complex by the hydrolysis of the polypeptide chain from the peptidyl transferase center  of the ribosome.  Drugs or special sequence motifs on the mRNA can change the ribosomal structure so that near-cognate tRNAs are bound to the stop codon instead of the release factors. In such cases of 'translational readthrough', translation continues until the ribosome encounters the next stop codon.The process of translation is highly regulated in both eukaryotic and prokaryotic organisms. Regulation of translation can impact the global rate of protein synthesis which is closely coupled to the metabolic and proliferative state of a cell. In addition, recent work has revealed that genetic differences and their subsequent expression as mRNAs can also impact translation rate in an RNA-specific manner.


== Clinical significance ==
Translational control is critical for the development and survival of cancer. Cancer cells must frequently regulate the translation phase of gene expression, though it is not fully understood why translation is targeted over steps like transcription. While cancer cells often have genetically altered translation factors, it is much more common for cancer cells to modify the levels of existing translation factors. Several major oncogenic signaling pathways, including the RAS–MAPK, PI3K/AKT/mTOR, MYC, and WNT–β-catenin pathways, ultimately reprogram the genome via translation. Cancer cells also control translation to adapt to cellular stress. During stress, the cell translates mRNAs that can mitigate the stress and promote survival. An example of this is the expression of AMPK in various cancers; its activation triggers a cascade that can ultimately allow the cancer to escape apoptosis (programmed cell death) triggered by nutrition deprivation. Future cancer therapies may involve disrupting the translation machinery of the cell to counter the downstream effects of cancer.


== Mathematical modeling of translation ==

The transcription-translation process description, mentioning only the most basic ”elementary” processes, consists of:

production of mRNA molecules (including splicing),
initiation of these molecules with help of initiation factors (e.g., the initiation can include the circularization step though it is not universally required),
initiation of translation, recruiting the small ribosomal subunit,
assembly of full ribosomes,
elongation, (i.e. movement of ribosomes along mRNA with production of protein),
termination of translation,
degradation of mRNA molecules,
degradation of proteins.The process of amino acid building to create protein in translation is a subject of various physic models for a long time starting from the first detailed kinetic models such as or others taking into account stochastic aspects of translation and using computer simulations. Many chemical kinetics-based models of protein synthesis have been developed and analyzed in the last four decades. Beyond chemical kinetics, various modeling formalisms such as Totally Asymmetric Simple Exclusion Process (TASEP),Probabilistic Boolean Networks (PBN), Petri Nets and max-plus algebra have been applied to model the detailed kinetics of protein synthesis or some of its stages. A basic model of protein synthesis that takes into account all eight 'elementary' processes has been developed, following  the paradigm that ""useful models are simple and extendable"". The simplest model M0 is represented by the reaction kinetic mechanism (Figure M0). It was generalised to include 40S, 60S and initiation factors (IF) binding (Figure M1'). It was extended further to include effect of microRNA on protein synthesis. Most of models in this hierarchy can be solved analytically. These solutions were used to extract 'kinetic signatures' of different specific mechanisms of synthesis regulation.


== Genetic code ==

It is also possible to translate either by hand (for short sequences) or by computer (after first programming one appropriately, see section below); this allows biologists and chemists to draw out the chemical structure of the encoded protein on paper.
First, convert each template DNA base to its RNA complement (note that the complement of A is now U), as shown below. Note that the template strand of the DNA is the one the RNA is polymerized against; the other DNA strand would be the same as the RNA, but with thymine instead of uracil.

DNA -> RNA
 A  ->  U
 T  ->  A
 C  ->  G
 G  ->  C
 A=T-> A=U

Then split the RNA into triplets (groups of three bases). Note that there are 3 translation ""windows"", or reading frames, depending on where you start reading the code.
Finally, use the table at Genetic code to translate the above into a structural formula as used in chemistry.
This will give you the primary structure of the protein.  However, proteins tend to fold, depending in part on hydrophilic and hydrophobic segments along the chain. Secondary structure can often still be guessed at, but the proper tertiary structure is often very hard to determine.
Whereas other aspects such as the 3D structure, called tertiary structure, of protein can only be predicted using sophisticated algorithms, the amino acid sequence, called primary structure, can be determined solely from the nucleic acid sequence with the aid of a translation table.
This approach may not give the correct amino acid composition of the protein, in particular if unconventional amino acids such as selenocysteine are incorporated into the protein, which is coded for by a conventional stop codon in combination with a downstream hairpin (SElenoCysteine Insertion Sequence, or SECIS).
There are many computer programs capable of translating a DNA/RNA sequence into a protein sequence. Normally this is performed using the Standard Genetic Code, however, few programs can handle all the ""special"" cases, such as the use of the alternative initiation codons which are biologically significant. For instance, the rare alternative start codon CTG codes for Methionine when used as a start codon, and for Leucine in all other positions.
Example: Condensed translation table for the Standard Genetic Code (from the NCBI Taxonomy webpage).
 AAs    = FFLLSSSSYY**CC*WLLLLPPPPHHQQRRRRIIIMTTTTNNKKSSRRVVVVAAAADDEEGGGG
 Starts = ---M---------------M---------------M----------------------------
 Base1  = TTTTTTTTTTTTTTTTCCCCCCCCCCCCCCCCAAAAAAAAAAAAAAAAGGGGGGGGGGGGGGGG
 Base2  = TTTTCCCCAAAAGGGGTTTTCCCCAAAAGGGGTTTTCCCCAAAAGGGGTTTTCCCCAAAAGGGG
 Base3  = TCAGTCAGTCAGTCAGTCAGTCAGTCAGTCAGTCAGTCAGTCAGTCAGTCAGTCAGTCAGTCAG

The ""Starts"" row indicate three start codons, UUG, CUG, and the very common AUG. It also indicates the first amino acid residue when interpreted as a start: in this case it is all methionine.


=== Translation tables ===

Even when working with ordinary eukaryotic sequences such as the Yeast genome, it is often desired to be able to use alternative translation tables—namely for translation of the mitochondrial genes. Currently the following translation tables are defined by the NCBI Taxonomy Group for the translation of the sequences in GenBank:


== See also ==


== References ==


== Further reading ==


== External links ==

Virtual Cell Animation Collection: Introducing Translation
Translate tool (from DNA or RNA sequence)",2678460,1199,"Articles with short description, CS1 maint: url-status, Cellular processes, Commons category link is on Wikidata, Gene expression, Molecular biology, Protein biosynthesis, Short description is different from Wikidata",1132553169,biology
https://en.wikipedia.org/wiki/Geology,Geology,"Geology (from Ancient Greek  γῆ (gê) 'earth', and  λoγία (-logía) 'study of, discourse') is a branch of natural science concerned with Earth and other astronomical objects, the features or rocks of which it is composed, and the processes by which they change over time. Modern geology significantly overlaps all other Earth sciences, including hydrology, and so is treated as one major aspect of integrated Earth system science and planetary science.
Geology describes the structure of the Earth on and beneath its surface, and the processes that have shaped that structure. It also provides tools to determine the relative and absolute ages of rocks found in a given location, and also to describe the histories of those rocks. By combining these tools, geologists are able to chronicle the geological history of the Earth as a whole, and also to demonstrate the age of the Earth. Geology provides the primary evidence for plate tectonics, the evolutionary history of life, and the Earth's past climates.
Geologists broadly study the properties and processes of Earth and other terrestrial planets and  predominantly solid planetary bodies. Geologists use a wide variety of methods to understand the Earth's structure and evolution, including field work, rock description, geophysical techniques, chemical analysis, physical experiments, and numerical modelling. In practical terms, geology is important for mineral and hydrocarbon exploration and exploitation, evaluating water resources, understanding natural hazards, the remediation of environmental problems, and providing insights into past climate change. Geology is a major academic discipline, and it is central to geological engineering and plays an important role in geotechnical engineering.","Geology (from Ancient Greek  γῆ (gê) 'earth', and  λoγία (-logía) 'study of, discourse') is a branch of natural science concerned with Earth and other astronomical objects, the features or rocks of which it is composed, and the processes by which they change over time. Modern geology significantly overlaps all other Earth sciences, including hydrology, and so is treated as one major aspect of integrated Earth system science and planetary science.
Geology describes the structure of the Earth on and beneath its surface, and the processes that have shaped that structure. It also provides tools to determine the relative and absolute ages of rocks found in a given location, and also to describe the histories of those rocks. By combining these tools, geologists are able to chronicle the geological history of the Earth as a whole, and also to demonstrate the age of the Earth. Geology provides the primary evidence for plate tectonics, the evolutionary history of life, and the Earth's past climates.
Geologists broadly study the properties and processes of Earth and other terrestrial planets and  predominantly solid planetary bodies. Geologists use a wide variety of methods to understand the Earth's structure and evolution, including field work, rock description, geophysical techniques, chemical analysis, physical experiments, and numerical modelling. In practical terms, geology is important for mineral and hydrocarbon exploration and exploitation, evaluating water resources, understanding natural hazards, the remediation of environmental problems, and providing insights into past climate change. Geology is a major academic discipline, and it is central to geological engineering and plays an important role in geotechnical engineering.


== Geological material ==
The majority of geological data comes from research on solid Earth materials. Meteorites and other extraterrestrial natural materials are also studied by geological methods.


=== Mineral ===

Minerals are natural occurring elements and compounds with a definite homogeneous chemical composition and ordered atomic composition.
Each mineral has distinct physical properties, and there are many tests to determine each of them. The specimens can be tested for:
Luster: Quality of light reflected from the surface of a mineral. Examples are metallic, pearly, waxy, dull.
Color: Minerals are grouped by their color. Mostly diagnostic but impurities can change a mineral's color.
Streak: Performed by scratching the sample on a porcelain plate. The color of the streak can help name the mineral.
Hardness: The resistance of a mineral to scratching.
Breakage pattern: A mineral can either show fracture or cleavage, the former being breakage of uneven surfaces, and the latter a breakage along closely spaced parallel planes.
Specific gravity: the weight of a specific volume of a mineral.
Effervescence: Involves dripping hydrochloric acid on the mineral to test for fizzing.
Magnetism: Involves using a magnet to test for magnetism.
Taste: Minerals can have a distinctive taste, such as halite (which tastes like table salt).


=== Rock ===

A rock is any naturally occurring solid mass or aggregate of minerals or mineraloids. Most research in geology is associated with the study of rocks, as they provide the primary record of the majority of the geological history of the Earth. There are three major types of rock: igneous, sedimentary, and metamorphic. The rock cycle 
illustrates the relationships among them (see diagram).
When a rock solidifies or crystallizes from melt (magma or lava), it is an igneous rock. This rock can be weathered and eroded, then redeposited and lithified into a sedimentary rock. It can then be turned into a metamorphic rock by heat and pressure that change its mineral content, resulting in a characteristic fabric. All three types may melt again, and when this happens, new magma is formed, from which an igneous rock may once more solidify.
Organic matter, such as coal, bitumen, oil and natural gas, is linked mainly to organic-rich sedimentary rocks.

To study all three types of rock, geologists evaluate the minerals of which they are composed and their other physical properties, such as texture and fabric. 


=== Unlithified material ===
Geologists also study unlithified materials (referred to as superficial deposits) that lie above the bedrock. This study is often known as Quaternary geology, after the Quaternary period of geologic history, which is the most recent period of geologic time.


==== Magma ====

Magma is the original unlithified source of all igneous rocks. The active flow of molten rock is closely studied in volcanology, and igneous petrology aims to determine the history of igneous rocks from their original molten source to their final crystallization.


== Whole-Earth structure ==


=== Plate tectonics ===

In the 1960s, it was discovered that the Earth's lithosphere, which includes the crust and rigid uppermost portion of the upper mantle, is separated into tectonic plates that move across the plastically deforming, solid, upper mantle, which is called the asthenosphere. This theory is supported by several types of observations, including seafloor spreading and the global distribution of mountain terrain and seismicity.
There is an intimate coupling between the movement of the plates on the surface and the convection of the mantle (that is, the heat transfer caused by the slow movement of ductile mantle rock). Thus, oceanic plates and the adjoining mantle convection currents always move in the same direction – because the oceanic lithosphere is actually the rigid upper thermal boundary layer of the convecting mantle. This coupling between rigid plates moving on the surface of the Earth and the convecting mantle is called plate tectonics.

The development of plate tectonics has provided a physical basis for many observations of the solid Earth. Long linear regions of geological features are explained as plate boundaries.

For example:

Mid-ocean ridges, high regions on the seafloor where hydrothermal vents and volcanoes exist, are seen as divergent boundaries, where two plates move apart.
Arcs of volcanoes and earthquakes are theorized as convergent boundaries, where one plate subducts, or moves, under another.Transform boundaries, such as the San Andreas Fault system, resulted in widespread powerful earthquakes. Plate tectonics also has provided a mechanism for Alfred Wegener's theory of continental drift, in which the continents move across the surface of the Earth over geological time. They also provided a driving force for crustal deformation, and a new setting for the observations of structural geology. The power of the theory of plate tectonics lies in its ability to combine all of these observations into a single theory of how the lithosphere moves over the convecting mantle.


=== Earth structure ===

Advances in seismology, computer modeling, and mineralogy and crystallography at high temperatures and pressures give insights into the internal composition and structure of the Earth.
Seismologists can use the arrival times of seismic waves to image the interior of the Earth. Early advances in this field showed the existence of a liquid outer core (where shear waves were not able to propagate) and a dense solid inner core. These advances led to the development of a layered model of the Earth, with a crust and lithosphere on top, the mantle below (separated within itself by seismic discontinuities at 410 and 660 kilometers), and the outer core and inner core below that. More recently, seismologists have been able to create detailed images of wave speeds inside the earth in the same way a doctor images a body in a CT scan. These images have led to a much more detailed view of the interior of the Earth, and have replaced the simplified layered model with a much more dynamic model.
Mineralogists have been able to use the pressure and temperature data from the seismic and modeling studies alongside knowledge of the elemental composition of the Earth to reproduce these conditions in experimental settings and measure changes in crystal structure. These studies explain the chemical changes associated with the major seismic discontinuities in the mantle and show the crystallographic structures expected in the inner core of the Earth.


== Geological time ==

The geological time scale encompasses the history of the Earth. It is bracketed at the earliest by the dates of the first Solar System material at 4.567 Ga (or 4.567 billion years ago) and the formation of the Earth at
4.54 Ga
(4.54 billion years), which is the beginning of the informally recognized Hadean eon – a division of geological time. At the later end of the scale, it is marked by the present day (in the Holocene epoch).


=== Timescale of the Earth ===
The following five timelines show the geologic time scale to scale. The first shows the entire time from the formation of the Earth to the present, but this gives little space for the most recent eon. The second timeline shows an expanded view of the most recent eon. In a similar way, the most recent era is expanded in the third timeline, the most recent period is expanded in the fourth timeline, and the most recent epoch is expanded in the fifth timeline.


=== Important milestones on Earth ===

4.567 Ga (gigaannum: billion years ago): Solar system formation
4.54 Ga: Accretion, or formation, of Earth
c. 4 Ga: End of Late Heavy Bombardment, the first life
c. 3.5 Ga: Start of photosynthesis
c. 2.3 Ga: Oxygenated atmosphere, first snowball Earth
730–635 Ma (megaannum: million years ago): second snowball Earth
541 ± 0.3 Ma: Cambrian explosion – vast multiplication of hard-bodied life; first abundant fossils; start of the Paleozoic
c. 380 Ma: First vertebrate land animals
250 Ma: Permian-Triassic extinction – 90% of all land animals die; end of Paleozoic and beginning of Mesozoic
66 Ma: Cretaceous–Paleogene extinction – Dinosaurs die; end of Mesozoic and beginning of Cenozoic
c. 7 Ma: First hominins appear
3.9 Ma: First Australopithecus, direct ancestor to modern Homo sapiens, appear
200 ka (kiloannum: thousand years ago): First modern Homo sapiens appear in East Africa


=== Timescale of the Moon ===


=== Timescale of Mars ===


== Dating methods ==


=== Relative dating ===

Methods for relative dating were developed when geology first emerged as a natural science. Geologists still use the following principles today as a means to provide information about geological history and the timing of geological events.
The principle of uniformitarianism states that the geological processes observed in operation that modify the Earth's crust at present have worked in much the same way over geological time. A fundamental principle of geology advanced by the 18th-century Scottish physician and geologist James Hutton is that ""the present is the key to the past."" In Hutton's words: ""the past history of our globe must be explained by what can be seen to be happening now.""The principle of intrusive relationships concerns crosscutting intrusions. In geology, when an igneous intrusion cuts across a formation of sedimentary rock, it can be determined that the igneous intrusion is younger than the sedimentary rock. Different types of intrusions include stocks, laccoliths, batholiths, sills and dikes.
The principle of cross-cutting relationships pertains to the formation of faults and the age of the sequences through which they cut. Faults are younger than the rocks they cut; accordingly, if a fault is found that penetrates some formations but not those on top of it, then the formations that were cut are older than the fault, and the ones that are not cut must be younger than the fault. Finding the key bed in these situations may help determine whether the fault is a normal fault or a thrust fault.The principle of inclusions and components states that, with sedimentary rocks, if inclusions (or clasts) are found in a formation, then the inclusions must be older than the formation that contains them. For example, in sedimentary rocks, it is common for gravel from an older formation to be ripped up and included in a newer layer. A similar situation with igneous rocks occurs when xenoliths are found. These foreign bodies are picked up as magma or lava flows, and are incorporated, later to cool in the matrix. As a result, xenoliths are older than the rock that contains them.

The principle of original horizontality states that the deposition of sediments occurs as essentially horizontal beds. Observation of modern marine and non-marine sediments in a wide variety of environments supports this generalization (although cross-bedding is inclined, the overall orientation of cross-bedded units is horizontal).The principle of superposition states that a sedimentary rock layer in a tectonically undisturbed sequence is younger than the one beneath it and older than the one above it. Logically a younger layer cannot slip beneath a layer previously deposited. This principle allows sedimentary layers to be viewed as a form of the vertical timeline, a partial or complete record of the time elapsed from deposition of the lowest layer to deposition of the highest bed.The principle of faunal succession is based on the appearance of fossils in sedimentary rocks. As organisms exist during the same period throughout the world, their presence or (sometimes) absence provides a relative age of the formations where they appear. Based on principles that William Smith laid out almost a hundred years before the publication of Charles Darwin's theory of evolution, the principles of succession developed independently of evolutionary thought. The principle becomes quite complex, however, given the uncertainties of fossilization, localization of fossil types due to lateral changes in habitat (facies change in sedimentary strata), and that not all fossils formed globally at the same time.


=== Absolute dating ===

Geologists also use methods to determine the absolute age of rock samples and geological events. These dates are useful on their own and may also be used in conjunction with relative dating methods or to calibrate relative methods.At the beginning of the 20th century, advancement in geological science was facilitated by the ability to obtain accurate absolute dates to geological events using radioactive isotopes and other methods. This changed the understanding of geological time. Previously, geologists could only use fossils and stratigraphic correlation to date sections of rock relative to one another. With isotopic dates, it became possible to assign absolute ages to rock units, and these absolute dates could be applied to fossil sequences in which there was datable material, converting the old relative ages into new absolute ages.
For many geological applications, isotope ratios of radioactive elements are measured in minerals that give the amount of time that has passed since a rock passed through its particular closure temperature, the point at which different radiometric isotopes stop diffusing into and out of the crystal lattice. These are used in geochronologic and thermochronologic studies. Common methods include uranium–lead dating, potassium–argon dating, argon–argon dating and uranium–thorium dating. These methods are used for a variety of applications. Dating of lava and volcanic ash layers found within a stratigraphic sequence can provide absolute age data for sedimentary rock units that do not contain radioactive isotopes and calibrate relative dating techniques. These methods can also be used to determine ages of pluton emplacement.
Thermochemical techniques can be used to determine temperature profiles within the crust, the uplift of mountain ranges, and paleo-topography.
Fractionation of the lanthanide series elements is used to compute ages since rocks were removed from the mantle.
Other methods are used for more recent events. Optically stimulated luminescence and cosmogenic radionuclide dating are used to date surfaces and/or erosion rates. Dendrochronology can also be used for the dating of landscapes. Radiocarbon dating is used for geologically young materials containing organic carbon.


== Geological development of an area ==

The geology of an area changes through time as rock units are deposited and inserted, and deformational processes change their shapes and locations.
Rock units are first emplaced either by deposition onto the surface or intrusion into the overlying rock. Deposition can occur when sediments settle onto the surface of the Earth and later lithify into sedimentary rock, or when as volcanic material such as volcanic ash or lava flows blanket the surface. Igneous intrusions such as batholiths, laccoliths, dikes, and sills, push upwards into the overlying rock, and crystallize as they intrude.
After the initial sequence of rocks has been deposited, the rock units can be deformed and/or metamorphosed. Deformation typically occurs as a result of horizontal shortening, horizontal extension, or side-to-side (strike-slip) motion. These structural regimes broadly relate to convergent boundaries, divergent boundaries, and transform boundaries, respectively, between tectonic plates.
When rock units are placed under horizontal compression, they shorten and become thicker. Because rock units, other than muds, do not significantly change in volume, this is accomplished in two primary ways: through faulting and folding. In the shallow crust, where brittle deformation can occur, thrust faults form, which causes the deeper rock to move on top of the shallower rock. Because deeper rock is often older, as noted by the principle of superposition, this can result in older rocks moving on top of younger ones. Movement along faults can result in folding, either because the faults are not planar or because rock layers are dragged along, forming drag folds as slip occurs along the fault. Deeper in the Earth, rocks behave plastically and fold instead of faulting. These folds can either be those where the material in the center of the fold buckles upwards, creating ""antiforms"", or where it buckles downwards, creating ""synforms"". If the tops of the rock units within the folds remain pointing upwards, they are called anticlines and synclines, respectively. If some of the units in the fold are facing downward, the structure is called an overturned anticline or syncline, and if all of the rock units are overturned or the correct up-direction is unknown, they are simply called by the most general terms, antiforms, and synforms.

Even higher pressures and temperatures during horizontal shortening can cause both folding and metamorphism of the rocks. This metamorphism causes changes in the mineral composition of the rocks; creates a foliation, or planar surface, that is related to mineral growth under stress. This can remove signs of the original textures of the rocks, such as bedding in sedimentary rocks, flow features of lavas, and crystal patterns in crystalline rocks.
Extension causes the rock units as a whole to become longer and thinner. This is primarily accomplished through normal faulting and through the ductile stretching and thinning. Normal faults drop rock units that are higher below those that are lower. This typically results in younger units ending up below older units. Stretching of units can result in their thinning. In fact, at one location within the Maria Fold and Thrust Belt, the entire sedimentary sequence of the Grand Canyon appears over a length of less than a meter. Rocks at the depth to be ductilely stretched are often also metamorphosed. These stretched rocks can also pinch into lenses, known as boudins, after the French word for ""sausage"" because of their visual similarity.
Where rock units slide past one another, strike-slip faults develop in shallow regions, and become shear zones at deeper depths where the rocks deform ductilely.

The addition of new rock units, both depositionally and intrusively, often occurs during deformation. Faulting and other deformational processes result in the creation of topographic gradients, causing material on the rock unit that is increasing in elevation to be eroded by hillslopes and channels. These sediments are deposited on the rock unit that is going down. Continual motion along the fault maintains the topographic gradient in spite of the movement of sediment and continues to create accommodation space for the material to deposit. Deformational events are often also associated with volcanism and igneous activity. Volcanic ashes and lavas accumulate on the surface, and igneous intrusions enter from below. Dikes, long, planar igneous intrusions, enter along cracks, and therefore often form in large numbers in areas that are being actively deformed. This can result in the emplacement of dike swarms, such as those that are observable across the Canadian shield, or rings of dikes around the lava tube of a volcano.
All of these processes do not necessarily occur in a single environment and do not necessarily occur in a single order. The Hawaiian Islands, for example, consist almost entirely of layered basaltic lava flows. The sedimentary sequences of the mid-continental United States and the Grand Canyon in the southwestern United States contain almost-undeformed stacks of sedimentary rocks that have remained in place since Cambrian time. Other areas are much more geologically complex. In the southwestern United States, sedimentary, volcanic, and intrusive rocks have been metamorphosed, faulted, foliated, and folded. Even older rocks, such as the Acasta gneiss of the Slave craton in northwestern Canada, the oldest known rock in the world have been metamorphosed to the point where their origin is indiscernible without laboratory analysis. In addition, these processes can occur in stages. In many places, the Grand Canyon in the southwestern United States being a very visible example, the lower rock units were metamorphosed and deformed, and then deformation ended and the upper, undeformed units were deposited. Although any amount of rock emplacement and rock deformation can occur, and they can occur any number of times, these concepts provide a guide to understanding the geological history of an area.


== Methods of geology ==

Geologists use a number of fields, laboratory, and numerical modeling methods to decipher Earth history and to understand the processes that occur on and inside the Earth. In typical geological investigations, geologists use primary information related to petrology (the study of rocks), stratigraphy (the study of sedimentary layers), and structural geology (the study of positions of rock units and their deformation). In many cases, geologists also study modern soils, rivers, landscapes, and glaciers; investigate past and current life and biogeochemical pathways, and use geophysical methods to investigate the subsurface. Sub-specialities of geology may distinguish endogenous and exogenous geology.


=== Field methods ===

Geological field work varies depending on the task at hand. Typical fieldwork could consist of:

Geological mappingStructural mapping: identifying the locations of major rock units and the faults and folds that led to their placement there.
Stratigraphic mapping: pinpointing the locations of sedimentary facies (lithofacies and biofacies) or the mapping of isopachs of equal thickness of sedimentary rock
Surficial mapping: recording the locations of soils and surficial deposits
Surveying of topographic features
compilation of topographic maps
Work to understand change across landscapes, including:
Patterns of erosion and deposition
River-channel change through migration and avulsion
Hillslope processes
Subsurface mapping through geophysical methodsThese methods include:
Shallow seismic surveys
Ground-penetrating radar
Aeromagnetic surveys
Electrical resistivity tomography
They aid in:
Hydrocarbon exploration
Finding groundwater
Locating buried archaeological artifacts
High-resolution stratigraphy
Measuring and describing stratigraphic sections on the surface
Well drilling and logging
Biogeochemistry and geomicrobiologyCollecting samples to:
determine biochemical pathways
identify new species of organisms
identify new chemical compounds
and to use these discoveries to:
understand early life on Earth and how it functioned and metabolized
find important compounds for use in pharmaceuticals
Paleontology: excavation of fossil material
For research into past life and evolution
For museums and education
Collection of samples for geochronology and thermochronology
Glaciology: measurement of characteristics of glaciers and their motion


=== Petrology ===

In addition to identifying rocks in the field (lithology), petrologists identify rock samples in the laboratory. Two of the primary methods for identifying rocks in the laboratory are through optical microscopy and by using an electron microprobe. In an optical mineralogy analysis, petrologists analyze thin sections of rock samples using a petrographic microscope, where the minerals can be identified through their different properties in plane-polarized and cross-polarized light, including their birefringence, pleochroism, twinning, and interference properties with a conoscopic lens. In the electron microprobe, individual locations are analyzed for their exact chemical compositions and variation in composition within individual crystals. Stable and radioactive isotope studies provide insight into the geochemical evolution of rock units.
Petrologists can also use fluid inclusion data and perform high temperature and pressure physical experiments to understand the temperatures and pressures at which different mineral phases appear, and how they change through igneous and metamorphic processes. This research can be extrapolated to the field to understand metamorphic processes and the conditions of crystallization of igneous rocks. This work can also help to explain processes that occur within the Earth, such as subduction and magma chamber evolution.


=== Structural geology ===

Structural geologists use microscopic analysis of oriented thin sections of geological samples to observe the fabric within the rocks, which gives information about strain within the crystalline structure of the rocks. They also plot and combine measurements of geological structures to better understand the orientations of faults and folds to reconstruct the history of rock deformation in the area. In addition, they perform analog and numerical experiments of rock deformation in large and small settings.
The analysis of structures is often accomplished by plotting the orientations of various features onto stereonets. A stereonet is a stereographic projection of a sphere onto a plane, in which planes are projected as lines and lines are projected as points. These can be used to find the locations of fold axes, relationships between faults, and relationships between other geological structures.
Among the most well-known experiments in structural geology are those involving orogenic wedges, which are zones in which mountains are built along convergent tectonic plate boundaries. In the analog versions of these experiments, horizontal layers of sand are pulled along a lower surface into a back stop, which results in realistic-looking patterns of faulting and the growth of a critically tapered (all angles remain the same) orogenic wedge. Numerical models work in the same way as these analog models, though they are often more sophisticated and can include patterns of erosion and uplift in the mountain belt. This helps to show the relationship between erosion and the shape of a mountain range. These studies can also give useful information about pathways for metamorphism through pressure, temperature, space, and time.


=== Stratigraphy ===

In the laboratory, stratigraphers analyze samples of stratigraphic sections that can be returned from the field, such as those from drill cores. Stratigraphers also analyze data from geophysical surveys that show the locations of stratigraphic units in the subsurface. Geophysical data and well logs can be combined to produce a better view of the subsurface, and stratigraphers often use computer programs to do this in three dimensions. Stratigraphers can then use these data to reconstruct ancient processes occurring on the surface of the Earth, interpret past environments, and locate areas for water, coal, and hydrocarbon extraction.
In the laboratory, biostratigraphers analyze rock samples from outcrop and drill cores for the fossils found in them. These fossils help scientists to date the core and to understand the depositional environment in which the rock units formed. Geochronologists precisely date rocks within the stratigraphic section to provide better absolute bounds on the timing and rates of deposition.
Magnetic stratigraphers look for signs of magnetic reversals in igneous rock units within the drill cores. Other scientists perform stable-isotope studies on the rocks to gain information about past climate.


== Planetary geology ==

With the advent of space exploration in the twentieth century, geologists have begun to look at other planetary bodies in the same ways that have been developed to study the Earth. This new field of study is called planetary geology (sometimes known as astrogeology) and relies on known geological principles to study other bodies of the solar system. This is a major aspect of planetary science, and largely focuses on the terrestrial planets, icy moons, asteroids, comets, and meteorites. However, some planetary geophysicists study the giant planets and exoplanets.Although the Greek-language-origin prefix geo refers to Earth, ""geology"" is often used in conjunction with the names of other planetary bodies when describing their composition and internal processes: examples are ""the geology of Mars"" and ""Lunar geology"". Specialized terms such as selenology (studies of the Moon), areology (of Mars), etc., are also in use.
Although planetary geologists are interested in studying all aspects of other planets, a significant focus is to search for evidence of past or present life on other worlds. This has led to many missions whose primary or ancillary purpose is to examine planetary bodies for evidence of life. One of these is the Phoenix lander, which analyzed Martian polar soil for water, chemical, and mineralogical constituents related to biological processes.


== Applied geology ==


=== Economic geology ===

Economic geology is a branch of geology that deals with aspects of economic minerals that humankind uses to fulfill various needs. Economic minerals are those extracted profitably for various practical uses. Economic geologists help locate and manage the Earth's natural resources, such as petroleum and coal, as well as mineral resources, which include metals such as iron, copper, and uranium.


==== Mining geology ====

Mining geology consists of the extractions of mineral resources from the Earth. Some resources of economic interests include gemstones, metals such as gold and copper, and many minerals such as asbestos, perlite, mica, phosphates, zeolites, clay, pumice, quartz, and silica, as well as elements such as sulfur, chlorine, and helium.


==== Petroleum geology ====

Petroleum geologists study the locations of the subsurface of the Earth that can contain extractable hydrocarbons, especially petroleum and natural gas. Because many of these reservoirs are found in sedimentary basins, they study the formation of these basins, as well as their sedimentary and tectonic evolution and the present-day positions of the rock units.


=== Engineering geology ===

Engineering geology is the application of geological principles to engineering practice for the purpose of assuring that the geological factors affecting the location, design, construction, operation, and maintenance of engineering works are properly addressed. Engineering geology is distinct from geological engineering, particularly in North America.

In the field of civil engineering, geological principles and analyses are used in order to ascertain the mechanical principles of the material on which structures are built. This allows tunnels to be built without collapsing, bridges and skyscrapers to be built with sturdy foundations, and buildings to be built that will not settle in clay and mud.


=== Hydrology ===

Geology and geological principles can be applied to various environmental problems such as stream restoration, the restoration of brownfields, and the understanding of the interaction between natural habitat and the geological environment. Groundwater hydrology, or hydrogeology, is used to locate groundwater, which can often provide a ready supply of uncontaminated water and is especially important in arid regions, and to monitor the spread of contaminants in groundwater wells.


=== Paleoclimatology ===

Geologists also obtain data through stratigraphy, boreholes, core samples, and ice cores. Ice cores and sediment cores are used for paleoclimate reconstructions, which tell geologists about past and present temperature, precipitation, and sea level across the globe. These datasets are our primary source of information on global climate change outside of instrumental data.


=== Natural hazards ===

Geologists and geophysicists study natural hazards in order to enact safe building codes and warning systems that are used to prevent loss of property and life. Examples of important natural hazards that are pertinent to geology (as opposed those that are mainly or only pertinent to meteorology) are:


== History ==

The study of the physical material of the Earth dates back at least to ancient Greece when Theophrastus (372–287 BCE) wrote the work Peri Lithon (On Stones). During the Roman period, Pliny the Elder wrote in detail of the many minerals and metals, then in practical use – even correctly noting the origin of amber. Additionally, in the 4th century BCE Aristotle made critical observations of the slow rate of geological change. He observed the composition of the land and formulated a theory where the Earth changes at a slow rate and that these changes cannot be observed during one person's lifetime. Aristotle developed one of the first evidence-based concepts connected to the geological realm regarding the rate at which the Earth physically changes.Abu al-Rayhan al-Biruni (973–1048 CE) was one of the earliest Persian geologists, whose works included the earliest writings on the geology of India, hypothesizing that the Indian subcontinent was once a sea. Drawing from Greek and Indian scientific literature that were not destroyed by the Muslim conquests, the Persian scholar Ibn Sina (Avicenna, 981–1037) proposed detailed explanations for the formation of mountains, the origin of earthquakes, and other topics central to modern geology, which provided an essential foundation for the later development of the science. In China, the polymath Shen Kuo (1031–1095) formulated a hypothesis for the process of land formation: based on his observation of fossil animal shells in a geological stratum in a mountain hundreds of miles from the ocean, he inferred that the land was formed by the erosion of the mountains and by deposition of silt.Nicolas Steno (1638–1686) is credited with the law of superposition, the principle of original horizontality, and the principle of lateral continuity: three defining principles of stratigraphy.
The word geology was first used by Ulisse Aldrovandi in 1603, then by Jean-André Deluc in 1778 and introduced as a fixed term by Horace-Bénédict de Saussure in 1779. The word is derived from the Greek γῆ, gê, meaning ""earth"" and λόγος, logos, meaning ""speech"". But according to another source, the word ""geology"" comes from a Norwegian, Mikkel Pedersøn Escholt (1600–1699), who was a priest and scholar. Escholt first used the definition in his book titled, Geologia Norvegica (1657).William Smith (1769–1839) drew some of the first geological maps and began the process of ordering rock strata (layers) by examining the fossils contained in them.In 1763, Mikhail Lomonosov published his treatise On the Strata of Earth. His work was the first narrative of modern geology, based on the unity of processes in time and explanation of the Earth's past from the present.James Hutton (1726-1797) is often viewed as the first modern geologist. In 1785 he presented a paper entitled Theory of the Earth to the Royal Society of Edinburgh. In his paper, he explained his theory that the Earth must be much older than had previously been supposed to allow enough time for mountains to be eroded and for sediments to form new rocks at the bottom of the sea, which in turn were raised up to become dry land. Hutton published a two-volume version of his ideas in 1795.Followers of Hutton were known as Plutonists because they believed that some rocks were formed by vulcanism, which is the deposition of lava from volcanoes, as opposed to the Neptunists, led by Abraham Werner, who believed that all rocks had settled out of a large ocean whose level gradually dropped over time.
The first geological map of the U.S. was produced in 1809 by William Maclure. In 1807, Maclure commenced the self-imposed task of making a geological survey of the United States. Almost every state in the Union was traversed and mapped by him, the Allegheny Mountains being crossed and recrossed some 50 times. The results of his unaided labours were submitted to the American Philosophical Society in a memoir entitled Observations on the Geology of the United States explanatory of a Geological Map, and published in the Society's Transactions, together with the nation's first geological map. This antedates William Smith's geological map of England by six years, although it was constructed using a different classification of rocks.
Sir Charles Lyell (1797-1875) first published his famous book, Principles of Geology, in 1830. This book, which influenced the thought of Charles Darwin, successfully promoted the doctrine of uniformitarianism. This theory states that slow geological processes have occurred throughout the Earth's history and are still occurring today. In contrast, catastrophism is the theory that Earth's features formed in single, catastrophic events and remained unchanged thereafter. Though Hutton believed in uniformitarianism, the idea was not widely accepted at the time.
Much of 19th-century geology revolved around the question of the Earth's exact age. Estimates varied from a few hundred thousand to billions of years. By the early 20th century, radiometric dating allowed the Earth's age to be estimated at two billion years. The awareness of this vast amount of time opened the door to new theories about the processes that shaped the planet.
Some of the most significant advances in 20th-century geology have been the development of the theory of plate tectonics in the 1960s and the refinement of estimates of the planet's age. Plate tectonics theory arose from two separate geological observations: seafloor spreading and continental drift. The theory revolutionized the Earth sciences. Today the Earth is known to be approximately 4.5 billion years old.

		
		
		


== Fields or related disciplines ==


== See also ==


== References ==


== External links ==

One Geology: This interactive geological map of the world is an international initiative of the geological surveys around the globe. This groundbreaking project was launched in 2007 and contributed to the 'International Year of Planet Earth', becoming one of their flagship projects. 
Earth Science News, Maps, Dictionary, Articles, Jobs
American Geophysical Union
American Geosciences Institute
European Geosciences Union
Geological Society of America
Geological Society of London
Video-interviews with famous geologists
Geology OpenTextbook
Chronostratigraphy benchmarks",4234884,3290,"All articles with failed verification, Articles containing Ancient Greek (to 1453)-language text, Articles with BNF identifiers, Articles with EMU identifiers, Articles with FAST identifiers, Articles with GND identifiers, Articles with HDS identifiers, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with NARA identifiers, Articles with NDL identifiers, Articles with NKC identifiers, Articles with failed verification from September 2019, Articles with short description, CS1: long volume value, Commons category link is on Wikidata, Geology, Pages using multiple image with auto scaled images, Pages using the EasyTimeline extension, Short description is different from Wikidata, Webarchive template wayback links",1134347625,biology
https://en.wikipedia.org/wiki/Current_Biology,Current Biology,"Current Biology is a biweekly peer-reviewed scientific journal that covers all areas of biology, especially molecular biology, cell biology, genetics, neurobiology, ecology, and evolutionary biology. The journal includes research articles, various types of review articles, as well as an editorial magazine section. The journal was established in 1991 by the Current Science group, acquired by Elsevier in 1998 and has since 2001 been part of Cell Press, a subdivision of Elsevier. According to Journal Citation Reports, the journal has a 2020 impact factor of 10.834. It was categorized as a ""high impact journal"" by the Superfund Research Program.","Current Biology is a biweekly peer-reviewed scientific journal that covers all areas of biology, especially molecular biology, cell biology, genetics, neurobiology, ecology, and evolutionary biology. The journal includes research articles, various types of review articles, as well as an editorial magazine section. The journal was established in 1991 by the Current Science group, acquired by Elsevier in 1998 and has since 2001 been part of Cell Press, a subdivision of Elsevier. According to Journal Citation Reports, the journal has a 2020 impact factor of 10.834. It was categorized as a ""high impact journal"" by the Superfund Research Program.


== References ==


== External links ==
Official website",136817,55,"All articles needing additional references, All stub articles, Articles needing additional references from January 2021, Articles with outdated impact factors from 2020, Articles with short description, Biology journal stubs, Biology journals, Biweekly journals, Biweekly journals (infobox), Cell Press academic journals, English-language journals, Official website different in Wikidata and Wikipedia, Publications established in 1991, Short description is different from Wikidata",1075873984,biology
https://en.wikipedia.org/wiki/Stress_(biology),Stress (biology),"Stress, either physiological, biological or psychological, is an organism's response to a stressor such as an environmental condition. Stress is the body's method of reacting to a condition such as a threat, challenge or physical and psychological barrier. There are two hormones that an individual produces during a stressful situation, these are well known as adrenaline and cortisol. There are two kinds of stress hormone levels. Resting (basal) cortisol levels are normal everyday quantities that are essential for standard functioning. Reactive cortisol levels are increases in cortisol in response to stressors. Stimuli that alter an organism's environment are responded to by multiple systems in the body. In humans and most mammals, the autonomic nervous system and hypothalamic-pituitary-adrenal (HPA) axis are the two major systems that respond to stress.The sympathoadrenal medullary (SAM) axis may activate the fight-or-flight response through the sympathetic nervous system, which dedicates energy to more relevant bodily systems to acute adaptation to stress, while the parasympathetic nervous system returns the body to homeostasis. 
The second major physiological stress-response center, the HPA axis, regulates the release of cortisol, which influences many bodily functions such as metabolic, psychological and immunological functions. The SAM and HPA axes are regulated by several brain regions, including the limbic system, prefrontal cortex, amygdala, hypothalamus, and stria terminalis. Through these mechanisms, stress can alter memory functions, reward, immune function, metabolism and susceptibility to diseases.Disease risk is particularly pertinent to mental illnesses, whereby chronic or severe stress remains a common risk factor for several mental illnesses.

","Stress, either physiological, biological or psychological, is an organism's response to a stressor such as an environmental condition. Stress is the body's method of reacting to a condition such as a threat, challenge or physical and psychological barrier. There are two hormones that an individual produces during a stressful situation, these are well known as adrenaline and cortisol. There are two kinds of stress hormone levels. Resting (basal) cortisol levels are normal everyday quantities that are essential for standard functioning. Reactive cortisol levels are increases in cortisol in response to stressors. Stimuli that alter an organism's environment are responded to by multiple systems in the body. In humans and most mammals, the autonomic nervous system and hypothalamic-pituitary-adrenal (HPA) axis are the two major systems that respond to stress.The sympathoadrenal medullary (SAM) axis may activate the fight-or-flight response through the sympathetic nervous system, which dedicates energy to more relevant bodily systems to acute adaptation to stress, while the parasympathetic nervous system returns the body to homeostasis. 
The second major physiological stress-response center, the HPA axis, regulates the release of cortisol, which influences many bodily functions such as metabolic, psychological and immunological functions. The SAM and HPA axes are regulated by several brain regions, including the limbic system, prefrontal cortex, amygdala, hypothalamus, and stria terminalis. Through these mechanisms, stress can alter memory functions, reward, immune function, metabolism and susceptibility to diseases.Disease risk is particularly pertinent to mental illnesses, whereby chronic or severe stress remains a common risk factor for several mental illnesses.


== Psychology ==

Acute stressful situations where the stress experienced is severe is a cause of change psychologically to the detriment of the well-being of the individual, such that symptomatic derealization and depersonalization, and anxiety and hyperarousal, are experienced. The International Classification of Diseases includes a group of mental and behavioral disorders which have their aetiology in reaction to severe stress and the consequent adaptive response. Chronic stress, and a lack of coping resources available, or used by an individual, can often lead to the development of psychological issues such as delusions, depression and anxiety (see below for further information). Chronic stress also causes brain atrophy, which is the loss of neurons and the connections between them. It affects the part of the brain that is important for learning, responding to the stressors and cognitive flexibility.Chronic stressors may not be as intense as acute stressors such as natural disaster or a major accident, but persist over longer periods of time, tend to have a more negative effect on health because they are sustained and thus require the body's physiological response to occur daily. This depletes the body's energy more quickly and usually occurs over long periods of time, especially when these microstressors cannot be avoided (i.e. stress of living in a dangerous neighborhood). See allostatic load for further discussion of the biological process by which chronic stress may affect the body. For example, studies have found that caregivers, particularly those of dementia patients, have higher levels of depression and slightly worse physical health than non-caregivers.When humans are under chronic stress, permanent changes in their physiological, emotional, and behavioral responses may occur. Chronic stress can include events such as caring for a spouse with dementia, or may result from brief focal events that have long term effects, such as experiencing a sexual assault. Studies have also shown that psychological stress may directly contribute to the disproportionately high rates of coronary heart disease morbidity and mortality and its etiologic risk factors. Specifically, acute and chronic stress have been shown to raise serum lipids and are associated with clinical coronary events.However, it is possible for individuals to exhibit hardiness—a term referring to the ability to be both chronically stressed and healthy. Even though psychological stress is often connected with illness or disease, most healthy individuals can still remain disease-free after being confronted with chronic stressful events. This suggests that there are individual differences in vulnerability to the potential pathogenic effects of stress; individual differences in vulnerability arise due to both genetic and psychological factors. In addition, the age at which the stress is experienced can dictate its effect on health. Research suggests chronic stress at a young age can have lifelong effects on the biological, psychological, and behavioral responses to stress later in life.


== Etymology and historical usage ==
The term ""stress"" had none of its contemporary connotations before the 1920s. It is a form of the Middle English destresse, derived via Old French from the Latin stringere, ""to draw tight"". The word had long been in use in physics to refer to the internal distribution of a force exerted on a material body, resulting in strain. In the 1920s and '30s, biological and psychological circles occasionally used the term to refer to a mental strain or to a harmful environmental agent that could cause illness.Walter Cannon used it in 1926 to refer to external factors that disrupted what he called homeostasis. But ""...stress as an explanation of lived experience is absent from both lay and expert life narratives before the 1930s"". Physiological stress represents a wide range of physical responses that occur as a direct effect of a stressor causing an upset in the homeostasis of the body. Upon immediate disruption of either psychological or physical equilibrium the body responds by stimulating the nervous, endocrine, and immune systems. The reaction of these systems causes a number of physical changes that have both short- and long-term effects on the body.The Holmes and Rahe stress scale was developed as a method of assessing the risk of disease from life changes. The scale lists both positive and negative changes that elicit stress. These include things such as a major holiday or marriage, or death of a spouse and firing from a job.


== Biological need for equilibrium ==
Homeostasis is a concept central to the idea of stress. In biology, most biochemical processes strive to maintain equilibrium (homeostasis), a steady state that exists more as an ideal and less as an achievable condition. Environmental factors, internal or external stimuli, continually disrupt homeostasis; an organism's present condition is a state of constant flux moving about a homeostatic point that is that organism's optimal condition for living. Factors causing an organism's condition to diverge too far from homeostasis can be experienced as stress. A life-threatening situation such as a major physical trauma or prolonged starvation can greatly disrupt homeostasis. On the other hand, an organism's attempt at restoring conditions back to or near homeostasis, often consuming energy and natural resources, can also be interpreted as stress. The brain cannot sustain a concentrated equilibrium under chronic stress, overtime if you constantly struggle in a simmering sea of stress, and you body budget accrued an ever-deepening deficit, that's called chronic stress and it does more than just make you miserable in that moment, It can and will gradually eat away at your brain and cause illness in your body. The ambiguity in defining this phenomenon was first recognized by Hans Selye (1907–1982) in 1926. In 1951 a commentator loosely summarized Selye's view of stress as something that ""...in addition to being itself, was also the cause of itself, and the result of itself"".First to use the term in a biological context, Selye continued to define stress as ""the non-specific response of the body to any demand placed upon it"". Neuroscientists such as Bruce McEwen and Jaap Koolhaas believe that stress, based on years of empirical research, ""should be restricted to conditions where an environmental demand exceeds the natural regulatory capacity of an organism"". The brain cannot live in an harsh family environment, it needs some sort of stability between another brain. People who have reported being raised in harsh environments such as verbal and physical aggression have showed a more immune dysfunction and more metabolic dysfunction.  Indeed, in 1995 Toates already defined stress as a ""chronic state that arises only when defense mechanisms are either being chronically stretched or are actually failing,"" while according to Ursin (1988) stress results from an inconsistency between expected events (""set value"") and perceived events (""actual value"") that cannot be resolved satisfactorily, which also puts stress into the broader context of cognitive-consistency theory.


== Biological background ==
Stress can have many profound effects on the human biological systems. Biology primarily attempts to explain major concepts of stress using a stimulus-response paradigm, broadly comparable to how a psychobiological sensory system operates. The central nervous system (brain and spinal cord) plays a crucial role in the body's stress-related mechanisms. Whether one should interpret these mechanisms as the body's response to a stressor or embody the act of stress itself is part of the ambiguity in defining what exactly stress is.
The central nervous system works closely with the body's endocrine system to regulate these mechanisms. The sympathetic nervous system becomes primarily active during a stress response, regulating many of the body's physiological functions in ways that ought to make an organism more adaptive to its environment. Below there follows a brief biological background of neuroanatomy and neurochemistry and how they relate to stress.Stress, either severe, acute stress or chronic low-grade stress may induce abnormalities in three principal regulatory systems in the body: serotonin systems, catecholamine systems, and the hypothalamic-pituitary-adrenocortical axis. Aggressive behavior has also been associated with abnormalities in these systems.


== Biology of stress ==

The brain endocrine interactions are relevant in the translation of stress into physiological and psychological changes. The autonomic nervous system (ANS), as mentioned above, plays an important role in translating stress into a response. The ANS responds reflexively to both physical stressors (for example baroreception), and to higher level inputs from the brain.The ANS is composed of the parasympathetic nervous system and sympathetic nervous system, two branches that are both tonically active with opposing activities. The ANS directly innervates tissue through the postganglionic nerves, which is controlled by preganglionic neurons originating in the intermediolateral cell column. The ANS receives inputs from the medulla, hypothalamus, limbic system, prefrontal cortex, midbrain and monoamine nuclei.The activity of the sympathetic nervous system drives what is called the ""fight or flight"" response. The fight or flight response to emergency or stress involves mydriasis, increased heart rate and force contraction, vasoconstriction, bronchodilation, glycogenolysis, gluconeogenesis, lipolysis, sweating, decreased motility of the digestive system, secretion of the epinephrine and cortisol from the adrenal medulla, and relaxation of the bladder wall. The parasympathetic nervous response, ""rest and digest"", involves return to maintaining homeostasis, and involves miosis, bronchoconstriction, increased activity of the digestive system, and contraction of the bladder walls. Complex relationships between protective and vulnerability factors on the effect of childhood home stress on psychological illness, cardiovascular illness and adaption have been observed. ANS related mechanisms are thought to contribute to increased risk of cardiovascular disease after major stressful events.The HPA axis is a neuroendocrine system that mediates a stress response. Neurons in the hypothalamus, particularly the paraventricular nucleus, release vasopressin and corticotropin releasing hormone, which travel through the hypophysial portal vessel where they travel to and bind to the corticotropin-releasing hormone receptor on the anterior pituitary gland. Multiple CRH peptides have been identified, and receptors have been identified on multiple areas of the brain, including the amygdala. CRH is the main regulatory molecule of the release of ACTH.The secretion of ACTH into systemic circulation allows it to bind to and activate Melanocortin receptor, where it stimulates the release of steroid hormones. Steroid hormones bind to glucocorticoid receptors in the brain, providing negative feedback by reducing ACTH release. Some evidence supports a second long term feedback that is non-sensitive to cortisol secretion. The PVN of the hypothalamus receives inputs from the nucleus of the solitary tract, and lamina terminalis. Through these inputs, it receives and can respond to changes in blood.The PVN innervation from the brain stem nuclei, particularly the noradrenergic nuclei stimulate CRH release. Other regions of the hypothalamus both directly and indirectly inhibit HPA axis activity. Hypothalamic neurons involved in regulating energy balance also influence HPA axis activity through the release of neurotransmitters such as neuropeptide Y, which stimulates HPA axis activity. Generally, the amygdala stimulates, and the prefrontal cortex and hippocampus attenuate, HPA axis activity; however, complex relationships do exist between the regions.The immune system may be heavily influenced by stress. The sympathetic nervous system innervates various immunological structures, such as bone marrow and the spleen, allowing for it to regulate immune function. The adrenergic substances released by the sympathetic nervous system can also bind to and influence various immunological cells, further providing a connection between the systems. The HPA axis ultimately results in the release of cortisol, which generally has immunosuppressive effects. However, the effect of stress on the immune system is disputed, and various models have been proposed in an attempt to account for both the supposedly ""immunodeficiency"" linked diseases and diseases involving hyper activation of the immune system. One model proposed to account for this suggests a push towards an imbalance of cellular immunity(Th1) and humoral immunity(Th2). The proposed imbalance involved hyperactivity of the Th2 system leading to some forms of immune hypersensitivity, while also increasing risk of some illnesses associated with decreased immune system function, such as infection and cancer.


== Effects of chronic stress ==

Chronic stress is a term sometimes used to differentiate it from acute stress. Definitions differ, and may be along the lines of continual activation of the stress response, stress that causes an allostatic shift in bodily functions, or just as ""prolonged stress"". For example, results of one study demonstrated that individuals who reported relationship conflict lasting one month or longer have a greater risk of developing illness and show slower wound healing. It can also reduce the benefits of receiving common vaccines. Similarly, the effects that acute stressors have on the immune system may be increased when there is perceived stress and/or anxiety due to other events. For example, students who are taking exams show weaker immune responses if they also report stress due to daily hassles. While responses to acute stressors typically do not impose a health burden on young, healthy individuals, chronic stress in older or unhealthy individuals may have long-term effects that are detrimental to health.


=== Immunological ===
Acute time-limited stressors, or stressors that lasted less than two hours, results in an up regulation of natural immunity and down regulation of specific immunity. This type of stress saw in increase in granulocytes, natural killer cells, IgA, Interleukin 6, and an increase in cell cytotoxicity. Brief naturalistic stressors elicit a shift from Th1(cellular) to Th2(humoral) immunity, while decreased T-cell proliferation, and natural killer cell cytotoxicity. Stressful event sequences did not elicit a consistent immune response; however, some observations such as decreased T-Cell proliferation and cytotoxicity, increase or decrease in natural killer cell cytotoxicity, and an increase in mitogen PHA. Chronic stress elicited a shift toward Th2 immunity, as well as decreased interleukin 2, T cell proliferation, and antibody response to the influenza vaccine. Distant stressors did not consistently elicit a change in immune function. Another response to high impacts of chronic stress that lasts for a long period of time, is more immune dysfunction and more metabolic dysfunction. It is proven in studies that when continuously being in stressful situations, it is more likely to get sick. Also when being exposed to stress, your body metabolizes the food in a certain way that adds extra calories to your meal, regardless of the nutritional values of the food.


=== Infectious ===
Some studies have observed increased risk of upper respiratory tract infection during chronic life stress. In patients with HIV, increased life stress and cortisol was associated with poorer progression of HIV. Also with an increased level of stress, studies have proven evidence that it can reactivate latent herpes viruses.


=== Chronic disease ===
A link has been suggested between chronic stress and cardiovascular disease. Stress appears to play a role in hypertension, and may further predispose people to other conditions associated with hypertension. Stress may precipitate abuse of drugs and/or alcohol. Stress may also contribute to aging and chronic diseases in aging, such as depression and metabolic disorders.The immune system also plays a role in stress and the early stages of wound healing. It is responsible for preparing the tissue for repair and promoting recruitment of certain cells to the wound area. Consistent with the fact that stress alters the production of cytokines, Graham et al. found that chronic stress associated with care giving for a person with Alzheimer's disease leads to delayed wound healing. Results indicated that biopsy wounds healed 25% more slowly in the chronically stressed group, or those caring for a person with Alzheimer's disease.


=== Development ===
Chronic stress has also been shown to impair developmental growth in children by lowering the pituitary gland's production of growth hormone, as in children associated with a home environment involving serious marital discord, alcoholism, or child abuse.  Chronic stress also has a lot of illnesses and health care problems other then mental that comes with it. Severe chronic stress for long periods of time can lead to an increased chance of catching illnesses such as diabetes, cancer, depression, heart disease and Alzheimer's disease.  More generally, prenatal life, infancy, childhood, and adolescence are critical periods in which the vulnerability to stressors is particularly high. This can lead to psychiatric and physical diseases which have long term impacts on an individual. 


=== Psychopathology ===
Chronic stress is seen to affect the parts of the brain where memories are processed through and stored. When people feel stressed, stress hormones get over-secreted, which affects the brain. This secretion is made up of glucocorticoids, including cortisol, which are steroid hormones that the adrenal gland releases, although this can increase storage of flashbulb memories it decreases long-term potentiation (LTP). The hippocampus is important in the brain for storing certain kinds of memories and damage to the hippocampus can cause trouble in storing new memories but old memories, memories stored before the damage, are not lost. Also high cortisol levels can be tied to the deterioration of the hippocampus and decline of memory that many older adults start to experience with age. These mechanisms and processes may therefore contribute to age-related disease, or originate risk for earlier-onset disorders. For instance, extreme stress (e.g. trauma) is a requisite factor to produce stress-related disorders such as post-traumatic stress disorder.Chronic stress also shifts learning, forming a preference for habit based learning, and decreased task flexibility and spatial working memory, probably through alterations of the dopaminergic systems. Stress may also increase reward associated with food, leading to weight gain and further changes in eating habits. Stress may contribute to various disorders, such as fibromyalgia, chronic fatigue syndrome, depression, as well as other mental illnesses and functional somatic syndromes.


== Psychological concepts ==


=== Eustress ===
Selye published in year 1975 a model dividing stress into eustress and distress. Where stress enhances function (physical or mental, such as through strength training or challenging work), it may be considered eustress. Persistent stress that is not resolved through coping or adaptation, deemed distress, may lead to anxiety or withdrawal (depression) behavior.
The difference between experiences that result in eustress and those that result in distress is determined by the disparity between an experience (real or imagined) and personal expectations, and resources to cope with the stress. Alarming experiences, either real or imagined, can trigger a stress response.


=== Coping ===

Responses to stress include adaptation, psychological coping such as stress management, anxiety, and depression. Over the long term, distress can lead to diminished health and/or increased propensity to illness; to avoid this, stress must be managed.
Stress management encompasses techniques intended to equip a person with effective coping mechanisms for dealing with psychological stress, with stress defined as a person's physiological response to an internal or external stimulus that triggers the fight-or-flight response. Stress management is effective when a person uses strategies to cope with or alter stressful situations.
There are several ways of coping with stress, such as controlling the source of stress or learning to set limits and to say ""no"" to some of the demands that bosses or family members may make.
A person's capacity to tolerate the source of stress may be increased by thinking about another topic such as a hobby, listening to music, or spending time in a wilderness.
A way to control stress is first dealing with what is causing the stress if it is something the individual has control over. Other methods to control stress and reduce it can be: to not procrastinate and leave tasks for the last minute, do things you like, exercise, do breathing routines, go out with friends, and take a break. Having support from a loved one also helps a lot in reducing stress.One study showed that the power of having support from a loved one, or just having social support, lowered stress in individual subjects. Painful shocks were applied to married women's ankles. In some trials women were able to hold their husband's hand, in other trials they held a stranger's hand, and then held no one's hand. When the women were holding their husband's hand, the response was reduced in many brain areas. When holding the stranger's hand the response was reduced a little, but not as much as when they were holding their husband's hand. Social support helps reduce stress and even more so if the support is from a loved one.


=== Cognitive appraisal ===
Lazarus argued that, in order for a psychosocial situation to be stressful, it must be appraised as such. He argued that cognitive processes of appraisal are central in determining whether a situation is potentially threatening, constitutes a harm/loss or a challenge, or is benign.
Both personal and environmental factors influence this primary appraisal, which then triggers the selection of coping processes. Problem-focused coping is directed at managing the problem, whereas emotion-focused coping processes are directed at managing the negative emotions. Secondary appraisal refers to the evaluation of the resources available to cope with the problem, and may alter the primary appraisal.
In other words, primary appraisal includes the perception of how stressful the problem is and the secondary appraisal of estimating whether one has more than or less than adequate resources to deal with the problem that affects the overall appraisal of stressfulness. Further, coping is flexible in that, in general, the individual examines the effectiveness of the coping on the situation; if it is not having the desired effect, s/he will, in general, try different strategies.


== Assessment ==


=== Health risk factors ===
Both negative and positive stressors can lead to stress. The intensity and duration of stress changes depending on the circumstances and emotional condition of the person with it (Arnold. E and Boggs. K. 2007). Some common categories and examples of stressors include:

Sensory input such as pain, bright light, noise, temperatures, or environmental issues such as a lack of control over environmental circumstances, such as food, air and/or water quality, housing, health, freedom, or mobility.
Social issues can also cause stress, such as struggles with conspecific or difficult individuals and social defeat, or relationship conflict, deception, or break ups, and major events such as birth and deaths, marriage, and divorce.
Life experiences such as poverty, unemployment, clinical depression, obsessive compulsive disorder, heavy drinking, or insufficient sleep can also cause stress. Students and workers may face performance pressure stress from exams and project deadlines.
Adverse experiences during development (e.g. prenatal exposure to maternal stress, poor attachment histories, sexual abuse) are thought to contribute to deficits in the maturity of an individual's stress response systems. One evaluation of the different stresses in people's lives is the Holmes and Rahe stress scale.


=== General adaptation syndrome ===

Physiologists define stress as how the body reacts to a stressor - a stimulus, real or imagined. Acute stressors affect an organism in the short term; chronic stressors over the longer term. The general adaptation syndrome (GAS), developed by Hans Selye, is a profile of how organisms respond to stress; GAS is characterized by three phases: a nonspecific mobilization phase, which promotes sympathetic nervous system activity; a resistance phase, during which the organism makes efforts to cope with the threat; and an exhaustion phase, which occurs if the organism fails to overcome the threat and depletes its physiological resources.


==== Stage 1 ====
Alarm is the first stage, which is divided into two phases: the shock phase and the antishock phase.
Shock phase: During this phase, the body can endure changes such as hypovolemia, hypoosmolarity, hyponatremia, hypochloremia, hypoglycemia—the stressor effect. This phase resembles Addison's disease. The organism's resistance to the stressor drops temporarily below the normal range and some level of shock (e.g. circulatory shock) may be experienced.
Antishock phase: When the threat or stressor is identified or realized, the body starts to respond and is in a state of alarm. During this stage, the locus coeruleus and sympathetic nervous system activate the production of catecholamines including adrenaline, engaging the popularly-known fight-or-flight response. Adrenaline temporarily provides increased muscular tonus, increased blood pressure due to peripheral vasoconstriction and tachycardia, and increased glucose in blood. There is also some activation of the HPA axis, producing glucocorticoids (cortisol, aka the S-hormone or stress-hormone).


==== Stage 2 ====
Resistance is the second stage. During this stage, increased secretion of glucocorticoids intensifies the body's systemic response. Glucocorticoids can increase the concentration of glucose, fat, and amino acid in blood. In high doses, one glucocorticoid, cortisol, begins to act similarly to a mineralocorticoid (aldosterone) and brings the body to a state similar to hyperaldosteronism. If the stressor persists, it becomes necessary to attempt some means of coping with the stress. The body attempts to respond to stressful stimuli, but after prolonged activation, the body's chemical resources will be gradually depleted, leading to the final stage.


==== Stage 3 ====
The third stage could be either exhaustion or recovery:

Recovery stage follows when the system's compensation mechanisms have successfully overcome the stressor effect (or have completely eliminated the factor which caused the stress). The high glucose, fat and amino acid levels in blood prove useful for anabolic reactions, restoration of homeostasis and regeneration of cells.
Exhaustion is the alternative third stage in the GAS model. At this point, all of the body's resources are eventually depleted and the body is unable to maintain normal function. The initial autonomic nervous system symptoms may reappear (panic attacks, muscle aches, sore eyes, difficulty breathing, fatigue, heartburn, high blood pressure, and difficult time sleeping, etc.). If stage three is extended, long-term damage may result (prolonged vasoconstriction results in ischemia which in turn leads to cell necrosis), as the body's immune system becomes exhausted, and bodily functions become impaired, resulting in decompensation.The result can manifest itself in obvious illnesses, such as general trouble with the digestive system (e.g. occult bleeding, melena, constipation/obstipation), diabetes, or even cardiovascular problems (angina pectoris), along with clinical depression and other mental illnesses.


== Stress relief ==
A person can stop stress from being overwhelming by exercising when symptoms of stress become apparent, reflecting on their day (thinking about what they accomplished, not what they have not), and speaking to a therapist about their concerns.


== History in research ==
The current usage of the word stress arose out of Hans Selye's 1930s experiments. He started to use the term to refer not just to the agent but to the state of the organism as it responded and adapted to the environment. His theories of a universal non-specific stress response attracted great interest and contention in academic physiology and he undertook extensive research programs and publication efforts.While the work attracted continued support from advocates of psychosomatic medicine, many in experimental physiology concluded that his concepts were too vague and unmeasurable. During the 1950s, Selye turned away from the laboratory to promote his concept through popular books and lecture tours. He wrote for both non-academic physicians and, in an international bestseller entitled Stress of Life, for the general public.
A broad biopsychosocial concept of stress and adaptation offered the promise of helping everyone achieve health and happiness by successfully responding to changing global challenges and the problems of modern civilization. Selye coined the term ""eustress"" for positive stress, by contrast to distress. He argued that all people have a natural urge and need to work for their own benefit, a message that found favor with industrialists and governments. He also coined the term stressor to refer to the causative event or stimulus, as opposed to the resulting state of stress.
Selye was in contact with the tobacco industry from 1958 and they were undeclared allies in litigation and the promotion of the concept of stress, clouding the link between smoking and cancer, and portraying smoking as a ""diversion"", or in Selye's concept a ""deviation"", from environmental stress.From the late 1960s, academic psychologists started to adopt Selye's concept; they sought to quantify ""life stress"" by scoring ""significant life events"", and a large amount of research was undertaken to examine links between stress and disease of all kinds. By the late 1970s, stress had become the medical area of greatest concern to the general population, and more basic research was called for to better address the issue. There was also renewed laboratory research into the neuroendocrine, molecular, and immunological bases of stress, conceived as a useful heuristic not necessarily tied to Selye's original hypotheses. The US military became a key center of stress research, attempting to understand and reduce combat neurosis and psychiatric casualties.The psychiatric diagnosis post-traumatic stress disorder (PTSD) was coined in the mid-1970s, in part through the efforts of anti-Vietnam War activists and the Vietnam Veterans Against the War, and Chaim F. Shatan. The condition was added to the Diagnostic and Statistical Manual of Mental Disorders as posttraumatic stress disorder in 1980. PTSD was considered a severe and ongoing emotional reaction to an extreme psychological trauma, and as such often associated with soldiers, police officers, and other emergency personnel. The stressor may involve threat to life (or viewing the actual death of someone else), serious physical injury, or threat to physical or psychological integrity. In some cases, it can also be from profound psychological and emotional trauma, apart from any actual physical harm or threat. Often, however, the two are combined.
By the 1990s, ""stress"" had become an integral part of modern scientific understanding in all areas of physiology and human functioning, and one of the great metaphors of Western life. Focus grew on stress in certain settings, such as workplace stress, and stress management techniques were developed. The term also became a euphemism, a way of referring to problems and eliciting sympathy without being explicitly confessional, just ""stressed out"". It came to cover a huge range of phenomena from mild irritation to the kind of severe problems that might result in a real breakdown of health. In popular usage, almost any event or situation between these extremes could be described as stressful. During this time society spent less attention to the actual danger and severeness to mental health, this society might not have cared about those consequences of what we say or do. We might not agree that those consequences of being harsh to another individual verbally is to be considered abuse but they nonetheless have costs that we all pay. The American Psychological Association's 2015 Stress In America Study found that nationwide stress is on the rise and that the three leading sources of stress were ""money"", ""family responsibility"", and ""work"".


== See also ==
Autonomic nervous system
Defense physiology
HPA axis
Trier social stress test
Xenohormesis
Stress in early childhood
Weathering hypothesis


== References ==


== External links ==

The American Institute of Stress
""Research on Work-Related Stress"", European Agency for Safety and Health at Work (EU-OSHA)
Coping With Stress
Stages of GAS & Evolving the Definition",1791362,3418,"All articles with unsourced statements, Articles with GND identifiers, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with NKC identifiers, Articles with short description, Articles with unsourced statements from April 2015, Articles with unsourced statements from August 2016, Articles with unsourced statements from October 2021, CS1 errors: missing periodical, CS1 maint: date and year, CS1 maint: url-status, Endocrine system, Short description matches Wikidata, Stress (biological and psychological), Stress (biology), Sympathetic nervous system, Use dmy dates from June 2015, Webarchive template wayback links",1134394421,biology
https://en.wikipedia.org/wiki/Outline_of_biology,Outline of biology,"Biology – The natural science that studies life. Areas of focus include structure, function, growth, origin, evolution, distribution, and taxonomy.

","Biology – The natural science that studies life. Areas of focus include structure, function, growth, origin, evolution, distribution, and taxonomy.


== History of biology ==

History of anatomy
History of biochemistry
History of biotechnology
History of ecology
History of genetics
History of evolutionary thought:
The eclipse of Darwinism – Catastrophism – Lamarckism – Orthogenesis – Mutationism – Structuralism – Vitalism
Modern (evolutionary) synthesis
History of molecular evolution
History of speciation
History of medicine
History of model organisms
History of molecular biology
Natural history
History of plant systematics


== Overview ==
Biology
Science
Life
Properties: Adaptation – Energy processing – Growth – Order – Regulation – Reproduction – Response to environment
Biological organization: atom – molecule – cell – tissue – organ – organ system – organism – population – community – ecosystem – biosphere
Approach: Reductionism – emergent property – mechanistic
Biology as a science:
Natural science
Scientific method: observation – research question – hypothesis – testability – prediction – experiment – data – statistics
Scientific theory – scientific law
Research method
List of research methods in biology
Scientific literature
List of biology journals: peer review


== Chemical basis ==
Outline of biochemistry

Atoms and molecules
matter – element – atom – proton – neutron – electron– Bohr model – isotope – chemical bond – ionic bond – ions – covalent bond – hydrogen bond – molecule
Water:
properties of water – solvent – cohesion – surface tension – Adhesion – pH
Organic compounds:
carbon – carbon-carbon bonds – hydrocarbon – monosaccharide – amino acids – nucleotide – functional group – monomer – adenosine triphosphate (ATP) –  lipids – oil – sugar – vitamins – neurotransmitter – wax
Macromolecules:
polysaccharide: cellulose – carbohydrate – chitin – glycogen – starch
proteins: primary structure – secondary structure – tertiary structure – conformation – native state – protein folding – enzyme – receptor – transmembrane receptor – ion channel – membrane transporter – collagen – pigments: chlorophyll – carotenoid – xanthophyll – melanin – prion
lipids: cell membrane – fats – phospholipids
nucleic acids: DNA – RNA


== Cells ==
Outline of cell biology

Cell structure:
Cell coined by Robert Hooke
Techniques: cell culture – microscope – light microscope – electron microscopy – SEM – TEM
Organelles: Cytoplasm – Vacuole – Peroxisome – Plastid
Cell nucleus
Nucleoplasm – Nucleolus – Chromatin – Chromosome
Endomembrane system
Nuclear envelope – Endoplasmic reticulum – Golgi apparatus – Vesicles – Lysosome
Energy creators: Mitochondrion and Chloroplast
Biological membranes:
Plasma membrane – Mitochondrial membrane – Chloroplast membrane
Other subcellular features: Cell wall – pseudopod – cytoskeleton – mitotic spindle – flagellum – cilium
Cell transport: Diffusion – Osmosis – isotonic – active transport – phagocytosis
Cellular reproduction: cytokinesis – centromere – meiosis
Nuclear reproduction: mitosis – interphase – prophase – metaphase – anaphase – telophase
programmed cell death – apoptosis – cell senescence
Metabolism:
enzyme - activation energy - proteolysis – cooperativity
Cellular respiration
Glycolysis – Pyruvate dehydrogenase complex – Citric acid cycle – electron transport chain – fermentation
Photosynthesis
light-dependent reactions – Calvin cycle
Cell cycle
mitosis – chromosome – haploid – diploid – polyploidy – prophase – metaphase – anaphase – cytokinesis – meiosis


== Genetics ==
Outline of Genetics

Inheritance
heredity – Mendelian inheritance – gene – locus – trait – allele – polymorphism – homozygote – heterozygote – hybrid – hybridization – dihybrid cross – Punnett square – inbreeding
genotype–phenotype distinction – genotype – phenotype – dominant gene – recessive gene
genetic interactions – Mendel's law of segregation – genetic mosaic – maternal effect – penetrance – complementation – suppression – epistasis – genetic linkage
Model organisms: Drosophila – Arabidopsis – Caenorhabditis elegans – mouse – Saccharomyces cerevisiae – Escherichia coli – Lambda phage – Xenopus – chicken – zebrafish – Ciona intestinalis – amphioxus
Techniques: genetic screen – linkage map – genetic map
DNA
Nucleic acid double helix
Nucleobase: adenine (A) – cytosine (C) – guanine (G) – thymine (T) – uracil (U)
DNA replication – mutation – mutation rate – proofreading – DNA mismatch repair – point mutation – crossover – recombination – plasmid – transposon
Gene expression
Central dogma of molecular biology: nucleosome – genetic code – codon – transcription factor – transcription – translation – RNA – histone – telomere
heterochromatin – promoter – RNA polymerase
Protein biosynthesis – ribosomes
Gene regulation
operon – activator – repressor – corepressor – enhancer – alternative splicing
Genomes
DNA sequencing – high throughput sequencing – bioinformatics
Proteome – proteomics – metabolome – metabolomics
DNA paternity testing
Biotechnology (see also Outline of biochemical techniques and Molecular biology):
DNA fingerprinting – genetic fingerprint – microsatellite – gene knockout – imprinting – RNA interference Genomics – computational biology – bioinformatics – gel electrophoresis – transformation – PCR – PCR mutagenesis – primer – chromosome walking – RFLP – restriction enzyme – sequencing – shotgun sequencing – cloning – culture – DNA microarray – electrophoresis – protein tag – affinity chromatography – x-ray diffraction – Proteomics – mass spectrometry
Genes, development, and evolution
Apoptosis
French flag model
Pattern formation
Evo-devo gene toolkit
Transcription factor


== Evolution ==
Outline of evolution (see also evolutionary biology) 

Evolutionary processes
evolution
microevolution: adaptation – selection – natural selection – directional selection – sexual selection – genetic drift – sexual reproduction – asexual reproduction – colony – allele frequency – neutral theory of molecular evolution – population genetics – Hardy–Weinberg principle
Speciation
Species
Phylogeny
Lineage (evolution) – evolutionary tree – cladistics – species – taxon – clade – monophyletic – polyphyly – paraphyly – heredity – phenotypic trait – nucleic acid sequence – synapomorphy – homology – molecular clock – outgroup (cladistics) – maximum parsimony (phylogenetics) – Computational phylogenetics
Linnaean taxonomy: Carl Linnaeus – domain (biology) – kingdom (biology) – phylum – class (biology) – order (biology) – family (biology) – genus – species
Three-domain system: archaea – bacteria – eukaryote – protist – fungi – plant – animal
Binomial nomenclature: scientific classification – Homo sapiens
History of life
Origin of life – hierarchy of life – Miller–Urey experiment
Macroevolution: adaptive radiation – convergent evolution – extinction – mass extinction – fossil – taphonomy – geologic time – plate tectonics – continental drift – vicariance – Gondwana – Pangaea – endosymbiosis


== Diversity ==
Bacteria and Archaea
Protists
Plant diversity
Green algae
Chlorophyta
Charophyta
Bryophytes
Marchantiophyta
Anthocerotophyta
Moss
Pteridophytes
Lycopodiophyta
Polypodiophyta
Seed plants
Cycadophyta
Ginkgophyta
Pinophyta
Gnetophyta
Magnoliophyta
Fungi
Yeast – mold (fungus) – mushroom
Animal diversity
Invertebrates:
sponge – cnidarian – coral – jellyfish – Hydra (genus) – sea anemone
flatworms – nematodes
arthropods: crustacean – chelicerata – myriapoda – arachnids – insects – annelids – molluscs
Vertebrates:
fishes: – agnatha – chondrichthyes – osteichthyes
Tiktaalik
tetrapods
amphibians
reptiles
birds
flightless birds – Neognathae – dinosaurs
mammals
placental: primates
marsupial
monotreme
Viruses
DNA viruses – RNA viruses – retroviruses


== Plant form and function ==

Plant body
Organ systems: root – shoot – stem – leaf – flower
Plant nutrition and transport
Vascular tissue – bark (botany) – Casparian strip – turgor pressure – xylem – phloem – transpiration – wood – trunk (botany)
Plant development
tropism – taxis
seed – cotyledon – meristem – apical meristem – vascular cambium – cork cambium
alternation of generations – gametophyte – antheridium – archegonium – sporophyte – spore – sporangium
Plant reproduction
angiosperms – flower – reproduction – sperm – pollination – self-pollination – cross-pollination – nectar – pollen
Plant responses
Plant hormone – ripening – fruit – Ethylene as a plant hormone – toxin – pollinator – phototropism – skototropism – phototropin – phytochrome – auxin – photoperiodism – gravity


== Animal form and function ==

General features: morphology (biology) – anatomy – physiology – biological tissues – organ (biology) – organ systems
Water and salt balance
Body fluids: osmotic pressure – ionic composition – volume
Diffusion – osmosis) – Tonicity – sodium – potassium – calcium – chloride
Excretion
Nutrition and digestion
Digestive system: stomach – intestine – liver – nutrition – primary nutritional groups metabolism – kidney – excretion
Breathing
Respiratory system: lungs
Circulation
Circulatory system: heart – artery – vein – capillary – Blood – blood cell
Lymphatic system: lymph node
Muscle and movement
Skeletal system: bone – cartilage – joint – tendon
Muscular system:  muscle – actin – myosin – reflex
Nervous system
Neuron – dendrite – axon – nerve – electrochemical gradient – electrophysiology – action potential – signal transduction – synapse – receptor –
Central nervous system: brain – spinal cord
limbic system – memory – vestibular system
Peripheral nervous system
Sensory nervous system: eye – vision – audition – proprioception – olfaction –
Integumentary system: skin cell
Hormonal control
Endocrine system: hormone
Animal reproduction
Reproductive system: testes – ovary – pregnancy
Fish#Reproductive system
Mammalian reproductive system
Human reproductive system
Mammalian penis
Os penis
Penile spines
Genitalia of bottlenose dolphins
Genitalia of marsupials
Equine reproductive system
Even-toed ungulate#Genitourinary system
Bull#Reproductive anatomy
Carnivora#Reproductive system
Fossa (animal)#External genitalia
Female genitalia of spotted hyenas
Cat anatomy#Genitalia
Genitalia of dogs
Canine penis
Bulbus glandis
Animal development
stem cell – blastula – gastrula – egg (biology) – fetus – placenta - gamete – spermatid – ovum – zygote – embryo – cellular differentiation – morphogenesis – homeobox
Immune system
antibody – host – vaccine – immune cell – AIDS – T cell – leucocyte
Animal behavior
Behavior: mating – animal communication – seek shelter – migration (ecology)
Fixed action pattern
Altruism (biology)


== Ecology ==
Outline of ecology

Ecosystems:
Ecology –  Biodiversity – habitat – plankton – thermocline – saprobe
Abiotic component: water – light – radiation – temperature – humidity – atmosphere – acidity
Microbe – biomass  – organic matter – decomposer – decomposition – carbon – nutrient cycling – solar energy – topography – tilt – Windward and leeward – precipitation  Temperature – biome
Populations
Population ecology: organism – geographical area – sexual reproduction – population density – population growth – birth rate – death Rate – immigration rate – exponential growth – carrying capacity – logistic function – natural environment – competition (biology) – mating – biological dispersal – endemic (ecology) – growth curve (biology) – habitat – drinking water – resource – human population – technology – Green revolution
Communities
Community (ecology) – ecological niche – keystone species – mimicry – symbiosis – pollination – mutualism – commensalism – parasitism – predation – invasive species – environmental heterogeneity – edge effect
Consumer–resource interactions: food chain – food web – autotroph – heterotrophs – herbivore – carnivore – trophic level
Biosphere
lithosphere – atmosphere – hydrosphere
biogeochemical cycle: nitrogen cycle – carbon cycle – water cycle
Climate change: Fossil fuel – coal – oil – natural gas – World energy consumption – Climate change feedback – Albedo – water vapor Carbon sink
Conservation
Biodiversity – habitats – Ecosystem services – biodiversity loss – extinction – Sustainability – Holocene extinction


== Branches ==
Anatomy – study of form in animals, plants and other organisms, or specifically in humans. Simply, the study of internal structure of living organisms.
Comparative anatomy – the study of evolution of species through similarities and differences in their anatomy.
Osteology – study of bones.
Osteomyoarthrology – the study of the movement apparatus, including bones, joints, ligaments and muscles.
Viscerology – the study of organs
Neuroanatomy – the study of the nervous system.
Histology – also known as microscopic anatomy or microanatomy, the branch of biology which studies the microscopic anatomy of biological tissues.
Astrobiology – study of origin, early-evolution, distribution, and future of life in the universe. Also known as exobiology, and bioastronomy.
Bioarchaeology – study of human remains from archaeological sites.
Biochemistry – study of the chemical reactions required for life to exist and function, usually a focus on the cellular level.
Biocultural anthropology – the study of the relations between human biology and culture.
Biogeography – study of the distribution of species spatially and temporally.
Biolinguistics – study of biology and the evolution of language.
Biological economics – an interdisciplinary field in which the interaction of human biology and economics is studied.
Biophysics – study of biological processes through the methods traditionally used in the physical sciences.
Biomechanics – the study of the mechanics of living beings.
Neurophysics – study of the development of the nervous system on a molecular level.
Quantum biology – application of quantum mechanics and theoretical chemistry to biological objects and problems.
Virophysics – study of mechanics and dynamics driving the interactions between virus and cells.
Biotechnology – new and sometimes controversial branch of biology that studies the manipulation of living matter, including genetic modification and synthetic biology.
Bioinformatics – use of information technology for the study, collection, and storage of genomic and other biological data.
Bioengineering – study of biology through the means of engineering with an emphasis on applied knowledge and especially related to biotechnology.
Synthetic biology – research integrating biology and engineering; construction of biological functions not found in nature.
Botany – study of plants.
Photobiology – scientific study of the interactions of light (technically, non-ionizing radiation) and living organisms. The field includes the study of photosynthesis, photomorphogenesis, visual processing, circadian rhythms, bioluminescence, and ultraviolet radiation effects.
Phycology – scientific study of algae.
Plant physiology – subdiscipline of botany concerned with the functioning, or physiology, of plants.
Cell biology – study of the cell as a complete unit, and the molecular and chemical interactions that occur within a living cell.
Histology – study of the anatomy of cells and tissues of plants and animals using microscopy.
Chronobiology – field of biology that examines periodic (cyclic) phenomena in living organisms and their adaptation to solar- and lunar-related rhythms.
Dendrochronology – study of tree rings, using them to date the exact year they were formed in order to analyze atmospheric conditions during different periods in natural history.
Developmental biology – study of the processes through which an organism forms, from zygote to full structure
Embryology – study of the development of embryo (from fecundation to birth).
Gerontology – study of aging processes.
Ecology – study of the interactions of living organisms with one another and with the non-living elements of their environment.
Epidemiology – major component of public health research, studying factors affecting the health of populations.
Evolutionary biology – study of the origin and descent of species over time.
Evolutionary developmental biology – field of biology that compares the developmental processes of different organisms to determine the ancestral relationship between them, and to discover how developmental processes evolved.
Paleobiology – discipline which combines the methods and findings of the life sciences with the methods and findings of the earth science, paleontology.
Paleoanthropology – the study of fossil evidence for human evolution, mainly using remains from extinct hominin and other primate species to determine the morphological and behavioral changes in the human lineage, as well as the environment in which human evolution occurred.
Paleobotany – study of fossil plants.
Paleontology – study of fossils and sometimes geographic evidence of prehistoric life.
Paleopathology – the study of pathogenic conditions observable in bones or mummified soft tissue, and on nutritional disorders, variation in stature or morphology of bones over time, evidence of physical trauma, or evidence of occupationally derived biomechanic stress.
Genetics – study of genes and heredity.
Quantitative genetics – study of phenotypes that vary continuously (in characters such as height or mass)—as opposed to discretely identifiable phenotypes and gene-products (such as eye-colour, or the presence of a particular biochemical).
Geobiology – study of the interactions between the physical Earth and the biosphere.
Marine biology – study of ocean ecosystems, plants, animals, and other living beings.
Microbiology – study of microscopic organisms (microorganisms) and their interactions with other living things.
Bacteriology – study of bacteria
Immunology – study of immune systems in all organisms.
Mycology – study of fungi
Parasitology – study of parasites and parasitism.
Virology – study of viruses
Molecular biology – study of biology and biological functions at the molecular level, with some cross over from biochemistry.
Structural biology – a branch of molecular biology, biochemistry, and biophysics concerned with the molecular structure of biological macromolecules.
Neuroscience – study of the nervous system, including anatomy, physiology and emergent proprieties.
Behavioral neuroscience – study of physiological, genetic, and developmental mechanisms of behavior in humans and other animals.
Cellular neuroscience – study of neurons at a cellular level.
Cognitive neuroscience – study of biological substrates underlying cognition, with a focus on the neural substrates of mental processes.
Computational neuroscience – study of the information processing functions of the nervous system, and the use of digital computers to study the nervous system.
Developmental neuroscience – study of the cellular basis of brain development and addresses the underlying mechanisms.
Molecular neuroscience – studies the biology of the nervous system with molecular biology, molecular genetics, protein chemistry and related methodologies.
Neuroanatomy – study of the anatomy of nervous tissue and neural structures of the nervous system.
Neuroendocrinology – studies the interaction between the nervous system and the endocrine system, that is how the brain regulates the hormonal activity in the body.
Neuroethology – study of animal behavior and its underlying mechanistic control by the nervous system.
Neuroimmunology – study of the nervous system, and immunology, the study of the immune system.
Neuropharmacology – study of how drugs affect cellular function in the nervous system.
Neurophysiology – study of the function (as opposed to structure) of the nervous system.
Systems neuroscience – studies the function of neural circuits and systems. It is an umbrella term, encompassing a number of areas of study concerned with how nerve cells behave when connected together to form neural networks.
Physiology – study of the internal workings of organisms.
Endocrinology – study of the endocrine system.
Oncology – study of cancer processes, including virus or mutation, oncogenesis, angiogenesis and tissues remoldings.
Systems biology – computational modeling of biological systems.
Theoretical Biology – the mathematical modeling of biological phenomena.
Zoology – study of animals, including classification, physiology, development, and behavior. Subbranches include:
Arthropodology – biological discipline concerned with the study of arthropods, a phylum of animals that include the insects, arachnids, crustaceans and others that are characterized by the possession of jointed limbs.
Acarology – study of the taxon of arachnids that contains mites and ticks.
Arachnology – scientific study of spiders and related animals such as scorpions, pseudoscorpions, harvestmen, collectively called arachnids.
Entomology – study of insects.
Coleopterology – study of beetles.
Lepidopterology – study of a large order of insects that includes moths and butterflies (called lepidopterans).
Myrmecology – scientific study of ants.
Carcinology – study of crustaceans.
Myriapodology – study of centipedes, millipedes, and other myriapods.
Ethology – scientific study of animal behavior, usually with a focus on behavior under natural conditions.
Helminthology – study of worms, especially parasitic worms.
Herpetology – study of amphibians (including frogs, toads, salamanders, newts, and gymnophiona) and reptiles (including snakes, lizards, amphisbaenids, turtles, terrapins, tortoises, crocodilians, and the tuataras).
Batrachology – subdiscipline of herpetology concerned with the study of amphibians alone.
Ichthyology – study of fishes. This includes bony fishes (Osteichthyes), cartilaginous fishes (Chondrichthyes), and jawless fishes (Agnatha).
Malacology – branch of invertebrate zoology which deals with the study of the Mollusca (mollusks or molluscs), the second-largest phylum of animals in terms of described species after the arthropods.
Teuthology – branch of Malacology which deals with the study of cephalopods.
Mammalogy – study of mammals, a class of vertebrates with characteristics such as homeothermic metabolism, fur, four-chambered hearts, and complex nervous systems. Mammalogy has also been known as ""mastology,"" ""theriology,"" and ""therology."" There are about 4,200 different species of animals which are considered mammals.
Cetology – branch of marine mammal science that studies the approximately eighty species of whales, dolphins, and porpoise in the scientific order Cetacea.
Primatology – scientific study of primates
Human biology – interdisciplinary field studying the range of humans and human populations via biology/life sciences, anthropology/social sciences, applied/medical sciences
Biological anthropology – subfield of anthropology that studies the physical morphology, genetics and behavior of the human genus, other hominins and hominids across their evolutionary development
Human behavioral ecology – the study of behavioral adaptations (foraging, reproduction, ontogeny) from the evolutionary and ecologic perspectives (see behavioral ecology). It focuses on human adaptive responses (physiological, developmental, genetic) to environmental stresses.
Nematology – scientific discipline concerned with the study of nematodes, or roundworms.
Ornithology – scientific study of birds.


== Biologists ==

Lists of notable biologistsList of notable biologists
List of Nobel Prize winners in physiology or medicine
Lists of biologists by author abbreviation
List of authors of names published under the ICZNLists of biologists by subject
List of biochemists
List of ecologists
List of neuroscientists
List of physiologists


== See also ==
Bibliography of biology
Earliest known life forms
Invasion biology terminology
List of omics topics in biologyRelated outlines

Outline of life forms
Outline of zoology
Outline of engineering
Outline of technology
List of social sciencesJournals

Biology journals


== References ==


== External links ==

OSU's Phylocode
The Tree of Life: A multi-authored, distributed Internet project containing information about phylogeny and biodiversity.
MIT video lecture series on biology
A wiki site for protocol sharing run from MIT.
Biology and Bioethics.
Biology online wiki dictionary.
Biology Video Sharing Community.
What is Biotechnology Archived 19 April 2012 at the Wayback Machine : a voluntary program as Biotech for Beginners.",345631,639,"Articles with short description, Biology-related lists, Outlines of sciences, Pages using Sister project links with default search, Short description is different from Wikidata, Use dmy dates from September 2017, Webarchive template wayback links, Wikipedia outlines",1134330019,biology
https://en.wikipedia.org/wiki/Division_(biology),Division (biology),"Division is a taxonomic rank in biological classification that is used differently in zoology and in botany.
In botany and mycology, division refers to a rank equivalent to phylum. The use of either term is allowed under the International Code of Botanical Nomenclature, and both are commonly used in scientific literature.
The main Divisions of land plants, in the order in which they probably evolved, are the Marchantiophyta (liverworts),  Anthocerotophyta (hornworts), Bryophyta (mosses), Filicophyta (ferns), Sphenophyta (horsetails), Cycadophyta (cycads), Ginkgophyta (ginkgo)s, Pinophyta (conifers), Gnetophyta (gnetophytes), and the Magnoliophyta (Angiosperms, flowering plants). The flowering plants now dominate terrestrial ecosystems, comprising 80% of vascular plant species.In zoology, the term division is applied to an optional rank subordinate to the infraclass and superordinate to the cohort. A widely used classification (e.g. Carroll 1988) recognises teleost fishes as a Division Teleostei within Class Actinopterygii (the ray-finned fishes). Less commonly (as in Milner 1988), living tetrapods are ranked as Divisions Amphibia and Amniota within the clade of vertebrates with fleshy limbs (Sarcopterygii).

","Division is a taxonomic rank in biological classification that is used differently in zoology and in botany.
In botany and mycology, division refers to a rank equivalent to phylum. The use of either term is allowed under the International Code of Botanical Nomenclature, and both are commonly used in scientific literature.
The main Divisions of land plants, in the order in which they probably evolved, are the Marchantiophyta (liverworts),  Anthocerotophyta (hornworts), Bryophyta (mosses), Filicophyta (ferns), Sphenophyta (horsetails), Cycadophyta (cycads), Ginkgophyta (ginkgo)s, Pinophyta (conifers), Gnetophyta (gnetophytes), and the Magnoliophyta (Angiosperms, flowering plants). The flowering plants now dominate terrestrial ecosystems, comprising 80% of vascular plant species.In zoology, the term division is applied to an optional rank subordinate to the infraclass and superordinate to the cohort. A widely used classification (e.g. Carroll 1988) recognises teleost fishes as a Division Teleostei within Class Actinopterygii (the ray-finned fishes). Less commonly (as in Milner 1988), living tetrapods are ranked as Divisions Amphibia and Amniota within the clade of vertebrates with fleshy limbs (Sarcopterygii).


== References ==


=== Works cited ===
Carroll, Robert L. (1988), Vertebrate Paleontology and Evolution, New York: W.H. Freeman & Co., ISBN 0-716-7-1822-7
Milner, Andrew (1988), ""The relationships and origin of living amphibians"",  in M.J. Benton (ed.), 'The Phylogeny and Classification of the Tetrapods, vol. 1: Amphibians, Reptiles, Birds, Oxford: Clarendon Press, pp. 59–102",97523,93,"All articles lacking in-text citations, All articles needing additional references, All stub articles, Articles lacking in-text citations from September 2016, Articles needing additional references from September 2016, Articles with multiple maintenance issues, Articles with short description, Biology stubs, Botanical nomenclature, CS1: long volume value, Scientific classification, Short description is different from Wikidata",1127142665,biology
https://en.wikipedia.org/wiki/Genomics,Genomics,"Genomics is an interdisciplinary field of biology focusing on the structure, function, evolution, mapping, and editing of genomes. A genome is an organism's complete set of DNA, including all of its genes as well as its hierarchical, three-dimensional structural configuration.  In contrast to genetics, which refers to the study of individual genes and their roles in inheritance, genomics aims at the collective characterization and quantification of all of an organism's genes, their interrelations and influence on the organism. Genes may direct the production of proteins with the assistance of enzymes and messenger molecules. In turn, proteins make up body structures such as organs and tissues as well as control chemical reactions and carry signals between cells. Genomics also involves the sequencing and analysis of genomes through uses of high throughput DNA sequencing and bioinformatics to assemble and analyze the function and structure of entire genomes. Advances in genomics have triggered a revolution in discovery-based research and systems biology to facilitate understanding of even the most complex biological systems such as the brain.The field also includes studies of intragenomic (within the genome) phenomena such as epistasis (effect of one gene on another), pleiotropy (one gene affecting more than one trait), heterosis (hybrid vigour), and other interactions between loci and alleles within the genome.","Genomics is an interdisciplinary field of biology focusing on the structure, function, evolution, mapping, and editing of genomes. A genome is an organism's complete set of DNA, including all of its genes as well as its hierarchical, three-dimensional structural configuration.  In contrast to genetics, which refers to the study of individual genes and their roles in inheritance, genomics aims at the collective characterization and quantification of all of an organism's genes, their interrelations and influence on the organism. Genes may direct the production of proteins with the assistance of enzymes and messenger molecules. In turn, proteins make up body structures such as organs and tissues as well as control chemical reactions and carry signals between cells. Genomics also involves the sequencing and analysis of genomes through uses of high throughput DNA sequencing and bioinformatics to assemble and analyze the function and structure of entire genomes. Advances in genomics have triggered a revolution in discovery-based research and systems biology to facilitate understanding of even the most complex biological systems such as the brain.The field also includes studies of intragenomic (within the genome) phenomena such as epistasis (effect of one gene on another), pleiotropy (one gene affecting more than one trait), heterosis (hybrid vigour), and other interactions between loci and alleles within the genome.


== History ==


=== Etymology ===
From the Greek ΓΕΝ gen, ""gene"" (gamma, epsilon, nu, epsilon) meaning ""become, create, creation, birth"", and subsequent variants: genealogy, genesis, genetics, genic, genomere, genotype, genus etc. While the word genome (from the German Genom, attributed to Hans Winkler) was in use in English as early as 1926, the term genomics was coined by Tom Roderick, a geneticist at the Jackson Laboratory (Bar Harbor, Maine), over beers with Jim Womack, Tom Shows and Stephen O’Brien at a meeting held in Maryland on the mapping of the human genome in 1986. First as the name for a new journal and then as a whole new science discipline.


=== Early sequencing efforts ===
Following Rosalind Franklin's confirmation of the helical structure of DNA, James D. Watson and Francis Crick's publication of the structure of DNA in 1953 and Fred Sanger's publication of the Amino acid sequence of insulin in 1955, nucleic acid sequencing became a major target of early molecular biologists. In 1964, Robert W. Holley and colleagues published the first nucleic acid sequence ever determined, the ribonucleotide sequence of alanine transfer RNA. Extending this work, Marshall Nirenberg and Philip Leder revealed the triplet nature of the genetic code and were able to determine the sequences of 54 out of 64 codons in their experiments. In 1972, Walter Fiers and his team at the Laboratory of Molecular Biology of the University of Ghent (Ghent, Belgium) were the first to determine the sequence of a gene: the gene for Bacteriophage MS2 coat protein. Fiers' group expanded on their MS2 coat protein work, determining the complete nucleotide-sequence of bacteriophage MS2-RNA (whose genome encodes just four genes in 3569 base pairs [bp]) and Simian virus 40 in 1976 and 1978, respectively.


=== DNA-sequencing technology developed ===

In addition to his seminal work on the amino acid sequence of insulin, Frederick Sanger and his colleagues played a key role in the development of DNA sequencing techniques that enabled the establishment of comprehensive genome sequencing projects. In 1975, he and Alan Coulson published a sequencing procedure using DNA polymerase with radiolabelled nucleotides that he called the Plus and Minus technique. This involved two closely related methods that generated short oligonucleotides with defined 3' termini. These could be fractionated by electrophoresis on a polyacrylamide gel (called polyacrylamide gel electrophoresis) and visualised using autoradiography. The procedure could sequence up to 80 nucleotides in one go and was a big improvement, but was still very laborious.  Nevertheless, in 1977 his group was able to sequence most of the 5,386 nucleotides of the single-stranded bacteriophage φX174, completing the first fully sequenced DNA-based genome. The refinement of the Plus and Minus method resulted in the chain-termination, or Sanger method (see below), which formed the basis of the techniques of DNA sequencing, genome mapping, data storage, and bioinformatic analysis most widely used in the following quarter-century of research. In the same year Walter Gilbert and Allan Maxam of Harvard University independently developed the Maxam-Gilbert method (also known as the chemical method) of DNA sequencing, involving the preferential cleavage of DNA at known bases, a less efficient method. For their groundbreaking work in the sequencing of nucleic acids, Gilbert and Sanger shared half the 1980 Nobel Prize in chemistry with Paul Berg (recombinant DNA).


=== Complete genomes ===
The advent of these technologies resulted in a rapid intensification in the scope and speed of completion of genome sequencing projects. The first complete genome sequence of a eukaryotic organelle, the human mitochondrion (16,568 bp, about 16.6 kb [kilobase]), was reported in 1981, and the first chloroplast genomes followed in 1986. In 1992, the first eukaryotic chromosome, chromosome III of brewer's yeast Saccharomyces cerevisiae (315 kb) was sequenced. The first free-living organism to be sequenced was that of Haemophilus influenzae (1.8 Mb [megabase]) in 1995. The following year a consortium of researchers from laboratories across North America, Europe, and Japan announced the completion of the first complete genome sequence of a eukaryote, S. cerevisiae (12.1 Mb), and since then genomes have continued being sequenced at an exponentially growing pace. As of October 2011, the complete sequences are available for: 2,719 viruses, 1,115 archaea and bacteria, and 36 eukaryotes, of which about half are fungi.

Most of the microorganisms whose genomes have been completely sequenced are problematic pathogens, such as Haemophilus influenzae, which has resulted in a pronounced bias in their phylogenetic distribution compared to the breadth of microbial diversity. Of the other sequenced species, most were chosen because they were well-studied model organisms or promised to become good models. Yeast (Saccharomyces cerevisiae) has long been an important model organism for the eukaryotic cell, while the fruit fly Drosophila melanogaster has been a very important tool (notably in early pre-molecular genetics). The worm Caenorhabditis elegans is an often used simple model for multicellular organisms. The zebrafish Brachydanio rerio is used for many developmental studies on the molecular level, and the plant Arabidopsis thaliana is a model organism for flowering plants.  The Japanese pufferfish (Takifugu rubripes) and the spotted green pufferfish (Tetraodon nigroviridis) are interesting because of their small and compact genomes, which contain very little noncoding DNA compared to most species. The mammals dog (Canis familiaris), brown rat (Rattus norvegicus), mouse (Mus musculus), and chimpanzee (Pan troglodytes) are all important model animals in medical research.A rough draft of the human genome was completed by the Human Genome Project in early 2001, creating much fanfare. This project, completed in 2003, sequenced the entire genome for one specific person, and by 2007 this sequence was declared ""finished"" (less than one error in 20,000 bases and all chromosomes assembled). In the years since then, the genomes of many other individuals have been sequenced, partly under the auspices of the 1000 Genomes Project, which announced the sequencing of 1,092 genomes in October 2012. Completion of this project was made possible by the development of dramatically more efficient sequencing technologies and required the commitment of significant bioinformatics resources from a large international collaboration. The continued analysis of human genomic data has profound political and social repercussions for human societies.


=== The ""omics"" revolution ===

The English-language neologism omics informally refers to a field of study in biology ending in -omics, such as genomics, proteomics or metabolomics. The related suffix -ome is used to address the objects of study of such fields, such as the genome, proteome or metabolome respectively. The suffix -ome as used in molecular biology refers to a totality of some sort; similarly omics has come to refer generally to the study of large, comprehensive biological data sets. While the growth in the use of the term has led some scientists (Jonathan Eisen, among others) to claim that it has been oversold, it reflects the change in orientation towards the quantitative analysis of complete or near-complete assortment of all the constituents of a system. In the study of symbioses, for example, researchers which were once limited to the study of a single gene product can now simultaneously compare the total complement of several types of biological molecules.


== Genome analysis ==

After an organism has been selected, genome projects involve three components: the sequencing of DNA, the assembly of that sequence to create a representation of the original chromosome, and the annotation and analysis of that representation.


=== Sequencing ===

Historically, sequencing was done in sequencing centers, centralized facilities (ranging from large independent institutions such as Joint Genome Institute which sequence dozens of terabases a year, to local molecular biology core facilities)  which contain research laboratories with the costly instrumentation and technical support necessary. As sequencing technology continues to improve, however, a new generation of effective fast turnaround benchtop sequencers has come within reach of the average academic laboratory. On the whole, genome sequencing approaches fall into two broad categories, shotgun and high-throughput (or next-generation) sequencing.


==== Shotgun sequencing ====

Shotgun sequencing is a sequencing method designed for analysis of DNA sequences longer than 1000 base pairs, up to and including entire chromosomes. It is named by analogy with the rapidly expanding, quasi-random firing pattern of a shotgun. Since gel electrophoresis sequencing can only be used for fairly short sequences (100 to 1000 base pairs), longer DNA sequences must be broken into random small segments which are then sequenced to obtain reads. Multiple overlapping reads for the target DNA are obtained by performing several rounds of this fragmentation and sequencing. Computer programs then use the overlapping ends of different reads to assemble them into a continuous sequence.  Shotgun sequencing is a random sampling process, requiring over-sampling to ensure a given nucleotide is represented in the reconstructed sequence; the average number of reads by which a genome is over-sampled is referred to as coverage.For much of its history, the technology underlying shotgun sequencing was the classical chain-termination method or 'Sanger method', which is based on the selective incorporation of chain-terminating dideoxynucleotides by DNA polymerase during in vitro DNA replication. Recently, shotgun sequencing has been supplanted by high-throughput sequencing methods, especially for large-scale, automated genome analyses. However, the Sanger method remains in wide use, primarily for smaller-scale projects and for obtaining especially long contiguous DNA sequence reads (>500 nucleotides). Chain-termination methods require a single-stranded DNA template, a DNA primer, a DNA polymerase, normal deoxynucleosidetriphosphates (dNTPs), and modified nucleotides (dideoxyNTPs) that terminate DNA strand elongation. These chain-terminating nucleotides lack a 3'-OH group required for the formation of a phosphodiester bond between two nucleotides, causing DNA polymerase to cease extension of DNA when a ddNTP is incorporated. The ddNTPs may be radioactively or fluorescently labelled for detection in DNA sequencers. Typically, these machines can sequence up to 96 DNA samples in a single batch (run) in up to 48 runs a day.


==== High-throughput sequencing ====

The high demand for low-cost sequencing has driven the development of high-throughput sequencing technologies that parallelize the sequencing process, producing thousands or millions of sequences at once. High-throughput sequencing is intended to lower the cost of DNA sequencing beyond what is possible with standard dye-terminator methods. In ultra-high-throughput sequencing, as many as 500,000 sequencing-by-synthesis operations may be run in parallel.

The Illumina dye sequencing method is based on reversible dye-terminators and was developed in 1996 at the Geneva Biomedical Research Institute, by Pascal Mayer and Laurent Farinelli. In this method, DNA molecules and primers are first attached on a slide and amplified with polymerase so that local clonal colonies, initially coined ""DNA colonies"", are formed. To determine the sequence, four types of reversible terminator bases (RT-bases) are added and non-incorporated nucleotides are washed away. Unlike pyrosequencing, the DNA chains are extended one nucleotide at a time and image acquisition can be performed at a delayed moment, allowing for very large arrays of DNA colonies to be captured by sequential images taken from a single camera. Decoupling the enzymatic reaction and the image capture allows for optimal throughput and theoretically unlimited sequencing capacity; with an optimal configuration, the ultimate throughput of the instrument depends only on the A/D conversion rate of the camera. The camera takes images of the fluorescently labeled nucleotides, then the dye along with the terminal 3' blocker is chemically removed from the DNA, allowing the next cycle.An alternative approach, ion semiconductor sequencing, is based on standard DNA replication chemistry. This technology measures the release of a hydrogen ion each time a base is incorporated. A microwell containing template DNA is flooded with a single nucleotide, if the nucleotide is complementary to the template strand it will be incorporated and a hydrogen ion will be released. This release triggers an ISFET ion sensor. If a homopolymer is present in the template sequence multiple nucleotides will be incorporated in a single flood cycle, and the detected electrical signal will be proportionally higher.


=== Assembly ===

Sequence assembly refers to aligning and merging fragments of a much longer DNA sequence in order to reconstruct the original sequence. This is needed as current DNA sequencing technology cannot read whole genomes as a continuous sequence, but rather reads small pieces of between 20 and 1000 bases, depending on the technology used. Third generation sequencing technologies such as PacBio or Oxford Nanopore routinely generate sequencing reads >10 kb in length; however, they have a high error rate at approximately 15 percent. Typically the short fragments, called reads, result from shotgun sequencing genomic DNA, or gene transcripts (ESTs).


==== Assembly approaches ====
Assembly can be broadly categorized into two approaches: de novo assembly, for genomes which are not similar to any sequenced in the past, and comparative assembly, which uses the existing sequence of a closely related organism as a reference during assembly. Relative to comparative assembly, de novo assembly is computationally difficult (NP-hard), making it less favourable for short-read NGS technologies. Within the de novo assembly paradigm there are two primary strategies for assembly, Eulerian path strategies, and overlap-layout-consensus (OLC) strategies. OLC strategies ultimately try to create a Hamiltonian path through an overlap graph which is an NP-hard problem. Eulerian path strategies are computationally more tractable because they try to find a Eulerian path through a deBruijn graph.


==== Finishing ====
Finished genomes are defined as having a single contiguous sequence with no ambiguities representing each replicon.


=== Annotation ===

The DNA sequence assembly alone is of little value without additional analysis. Genome annotation is the process of attaching biological information to sequences, and consists of three main steps:
identifying portions of the genome that do not code for proteins
identifying elements on the genome, a process called gene prediction, and
attaching biological information to these elements.Automatic annotation tools try to perform these steps in silico, as opposed to manual annotation (a.k.a. curation) which involves human expertise and potential experimental verification. Ideally, these approaches co-exist and complement each other in the same annotation pipeline (also see below).
Traditionally, the basic level of annotation is using BLAST for finding similarities, and then annotating genomes based on homologues. More recently, additional information is added to the annotation platform. The additional information allows manual annotators to deconvolute discrepancies between genes that are given the same annotation. Some databases use genome context information, similarity scores, experimental data, and integrations of other resources to provide genome annotations through their Subsystems approach. Other databases (e.g. Ensembl) rely on both curated data sources as well as a range of software tools in their automated genome annotation pipeline. Structural annotation consists of the identification of genomic elements, primarily ORFs and their localisation, or gene structure. Functional annotation consists of attaching biological information to genomic elements.


=== Sequencing pipelines and databases ===
The need for reproducibility and efficient management of the large amount of data associated with genome projects mean that computational pipelines have important applications in genomics.


== Research areas ==


=== Functional genomics ===

Functional genomics is a field of molecular biology that attempts to make use of the vast wealth of data produced by genomic projects (such as genome sequencing projects) to describe gene (and protein) functions and interactions. Functional genomics focuses on the dynamic aspects such as gene transcription, translation, and protein–protein interactions, as opposed to the static aspects of the genomic information such as DNA sequence or structures. Functional genomics attempts to answer questions about the function of DNA at the levels of genes, RNA transcripts, and protein products. A key characteristic of functional genomics studies is their genome-wide approach to these questions, generally involving high-throughput methods rather than a more traditional “gene-by-gene” approach.
A major branch of genomics is still concerned with sequencing the genomes of various organisms, but the knowledge of full genomes has created the possibility for the field of functional genomics, mainly concerned with patterns of gene expression during various conditions. The most important tools here are microarrays and bioinformatics.


=== Structural genomics ===

Structural genomics seeks to describe the 3-dimensional structure of every protein encoded by a given genome. This genome-based approach allows for a high-throughput method of structure determination by a combination of experimental and modeling approaches. The principal difference between structural genomics and traditional structural prediction is that structural genomics attempts to determine the structure of every protein encoded by the genome, rather than focusing on one particular protein. With full-genome sequences available, structure prediction can be done more quickly through a combination of experimental and modeling approaches, especially because the availability of large numbers of sequenced genomes and previously solved protein structures allow scientists to model protein structure on the structures of previously solved homologs. Structural genomics involves taking a large number of approaches to structure determination, including experimental methods using genomic sequences or modeling-based approaches based on sequence or structural homology to a protein of known structure or based on chemical and physical principles for a protein with no homology to any known structure. As opposed to traditional structural biology, the determination of a protein structure through a structural genomics effort often (but not always) comes before anything is known regarding the protein function. This raises new challenges in structural bioinformatics, i.e. determining protein function from its 3D structure.


=== Epigenomics ===

Epigenomics is the study of the complete set of epigenetic modifications on the genetic material of a cell, known as the epigenome.   Epigenetic modifications are reversible modifications on a cell's DNA or histones that affect gene expression without altering the DNA sequence (Russell 2010 p. 475).  Two of the most characterized epigenetic modifications are DNA methylation and histone modification.  Epigenetic modifications play an important role in gene expression and regulation, and are involved in numerous cellular processes such as in differentiation/development and tumorigenesis.  The study of epigenetics on a global level has been made possible only recently through the adaptation of genomic high-throughput assays.


=== Metagenomics ===

Metagenomics is the study of metagenomes, genetic material recovered directly from environmental samples.  The broad field may also be referred to as environmental genomics, ecogenomics or community genomics. While traditional microbiology and microbial genome sequencing rely upon cultivated clonal cultures, early environmental gene sequencing cloned specific genes (often the 16S rRNA gene) to produce a profile of diversity in a natural sample.  Such work revealed that the vast majority of microbial biodiversity had been missed by cultivation-based methods. Recent studies use ""shotgun"" Sanger sequencing or massively parallel pyrosequencing to get largely unbiased samples of all genes from all the members of the sampled communities. Because of its power to reveal the previously hidden diversity of microscopic life, metagenomics offers a powerful lens for viewing the microbial world that has the potential to revolutionize understanding of the entire living world.


=== Model  systems ===


==== Viruses and bacteriophages ====
Bacteriophages have played and continue to play a key role in bacterial genetics and molecular biology. Historically, they were used to define gene structure and gene regulation. Also the first genome to be sequenced was a bacteriophage. However, bacteriophage research did not lead the genomics revolution, which is clearly dominated by bacterial genomics. Only very recently has the study of bacteriophage genomes become prominent, thereby enabling researchers to understand the mechanisms underlying phage evolution.  Bacteriophage genome sequences can be obtained through direct sequencing of isolated bacteriophages, but can also be derived as part of microbial genomes. Analysis of bacterial genomes has shown that a substantial amount of microbial DNA consists of prophage sequences and prophage-like elements. A detailed database mining of these sequences offers insights into the role of prophages in shaping the bacterial genome: Overall, this method verified many known bacteriophage groups, making this a useful tool for predicting the relationships of prophages from bacterial genomes.


==== Cyanobacteria ====
At present there are 24 cyanobacteria for which a total genome sequence is available. 15 of these cyanobacteria come from the marine environment. These are six Prochlorococcus strains, seven marine Synechococcus strains, Trichodesmium erythraeum IMS101 and Crocosphaera watsonii WH8501. Several studies have demonstrated how these sequences could be used very successfully to infer important ecological and physiological characteristics of marine cyanobacteria. However, there are many more genome projects currently in progress, amongst those there are further Prochlorococcus and marine Synechococcus isolates, Acaryochloris and Prochloron, the N2-fixing filamentous cyanobacteria Nodularia spumigena, Lyngbya aestuarii and Lyngbya majuscula, as well as bacteriophages infecting marine cyanobaceria. Thus, the growing body of genome information can also be tapped in a more general way to address global problems by applying a comparative approach. Some new and exciting examples of progress in this field are the identification of genes for regulatory RNAs, insights into the evolutionary origin of photosynthesis, or estimation of the contribution of horizontal gene transfer to the genomes that have been analyzed.


== Applications ==

Genomics has provided applications in many fields, including medicine, biotechnology, anthropology and other social sciences.


=== Genomic medicine ===
Next-generation genomic technologies allow clinicians and biomedical researchers to drastically increase the amount of genomic data collected on large study populations. When combined with new informatics approaches that integrate many kinds of data with genomic data in disease research, this allows researchers to better understand the genetic bases of drug response and disease. Early efforts to apply the genome to medicine included those by a Stanford team led by Euan Ashley who developed the first tools for the medical interpretation of a human genome. The Genomes2People research program at Brigham and Women’s Hospital, Broad Institute and Harvard Medical School was established in 2012 to conduct empirical research in translating genomics into health. Brigham and Women's Hospital opened a Preventive Genomics Clinic in August 2019, with Massachusetts General Hospital following a month later. The All of Us research program aims to collect genome sequence data from 1 million participants to become a critical component of the precision medicine research platform.


=== Synthetic biology and bioengineering ===
The growth of genomic knowledge has enabled increasingly sophisticated applications of synthetic biology. In 2010 researchers at the J. Craig Venter Institute announced the creation of a partially synthetic species of bacterium, Mycoplasma laboratorium, derived from the genome of Mycoplasma genitalium.


=== Population and conservation genomics ===
Population genomics has developed as a popular field of research, where genomic sequencing methods are used to conduct large-scale comparisons of DNA sequences among populations - beyond the limits of genetic markers such as short-range PCR products or microsatellites traditionally used in population genetics. Population genomics studies genome-wide effects to improve our understanding of microevolution so that we may learn the phylogenetic history and demography of a population. Population genomic methods are used for many different fields including evolutionary biology, ecology, biogeography, conservation biology and fisheries management. Similarly, landscape genomics has developed from landscape genetics to use genomic methods to identify relationships between patterns of environmental and genetic variation.
Conservationists can use the information gathered by genomic sequencing in order to better evaluate genetic factors key to species conservation, such as the genetic diversity of a population or whether an individual is heterozygous for a recessive inherited genetic disorder. By using genomic data to evaluate the effects of evolutionary processes and to detect patterns in variation throughout a given population, conservationists can formulate plans to aid a given species without as many variables left unknown as those unaddressed by standard genetic approaches.


== See also ==


== References ==


== Further reading ==


== External links ==

Learn All About Genetics Online",1489948,809,"All articles containing potentially dated statements, Articles containing potentially dated statements from October 2011, Articles with BNF identifiers, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with LNB identifiers, Articles with NKC identifiers, Articles with short description, CS1 maint: DOI inactive as of December 2022, Genomics, Pages containing links to subscription-only content, Short description is different from Wikidata, Webarchive template wayback links",1134057483,biology
https://en.wikipedia.org/wiki/Feminization_(biology),Feminization (biology),"In biology and medicine, feminization is the development in an organism of physical characteristics that are usually unique to the female of the species. This may represent a normal developmental process, contributing to sexual differentiation. Feminization can also be induced by environmental factors, and this phenomenon has been observed in several animal species. In the case of transgender hormone therapy, it is intentionally induced artificially.","In biology and medicine, feminization is the development in an organism of physical characteristics that are usually unique to the female of the species. This may represent a normal developmental process, contributing to sexual differentiation. Feminization can also be induced by environmental factors, and this phenomenon has been observed in several animal species. In the case of transgender hormone therapy, it is intentionally induced artificially.


== Pathological feminization ==
In animals, when feminization occurs in a male, or at an inappropriate developmental age, it is often due to a genetic or acquired disorder of the endocrine system. In humans, one of the more common manifestations of abnormal feminization is gynecomastia, the inappropriate development of breasts which may result from elevated levels of feminizing hormones such as estrogens. Deficiency or blockage of virilizing hormones (androgens) can also contribute to feminization. In some cases, high levels of androgens may produce both virilizing effects (increased body hair, deepened voice, increased muscle mass, etc.) and feminizing effects (gynecomastia) since androgens can be converted to estrogens by aromatase in the peripheral tissues.In insects, feminization can occur through inheritance of reproduction-manipulating endosymbionts. This promotes the inheritance of the endosymbionts because the endosymbionts are passed on by mothers to their eggs. As such, the more endosymbiont-infected females there are in a population, the more the endosymbionts are passed on to the next generation.


== See also ==
Defeminization
Virilization


== References ==",240485,42,"Articles with short description, Endocrinology, Short description is different from Wikidata, Wikipedia articles needing page number citations from March 2020",1126189080,biology
https://en.wikipedia.org/wiki/Biology_(song),Biology (song),"""Biology"" is a song performed by English-Irish all-female pop group Girls Aloud, taken from their third studio album Chemistry (2005). The progressive pop song was written by Miranda Cooper, Brian Higgins and Higgins' production team Xenomania, and produced by Higgins and Xenomania. Composed of distinct sections, it avoids the verse-chorus form present in most contemporary pop music. ""Biology"" was released as a single in November 2005, ahead of the album's release. Following the disappointment of ""Long Hot Summer"", ""Biology"" returned Girls Aloud to the top five of the UK Singles Chart and became their tenth top ten hit.
The music video, consisting only of group shots, witnesses Girls Aloud seamlessly moving through various sequences while performing disjointed choreography. ""Biology"" was promoted through a number of live appearances and has since been performed on all of Girls Aloud's subsequent concert tours. The song, which includes a variety of styles, received widespread acclaim from contemporary music critics. Considered one of Girls Aloud's signature songs, The Guardian referred to ""Biology"" as ""the best pop single of the last decade"".

","""Biology"" is a song performed by English-Irish all-female pop group Girls Aloud, taken from their third studio album Chemistry (2005). The progressive pop song was written by Miranda Cooper, Brian Higgins and Higgins' production team Xenomania, and produced by Higgins and Xenomania. Composed of distinct sections, it avoids the verse-chorus form present in most contemporary pop music. ""Biology"" was released as a single in November 2005, ahead of the album's release. Following the disappointment of ""Long Hot Summer"", ""Biology"" returned Girls Aloud to the top five of the UK Singles Chart and became their tenth top ten hit.
The music video, consisting only of group shots, witnesses Girls Aloud seamlessly moving through various sequences while performing disjointed choreography. ""Biology"" was promoted through a number of live appearances and has since been performed on all of Girls Aloud's subsequent concert tours. The song, which includes a variety of styles, received widespread acclaim from contemporary music critics. Considered one of Girls Aloud's signature songs, The Guardian referred to ""Biology"" as ""the best pop single of the last decade"".


== Background and composition ==
""Biology"" is composed of a number of distinctly different sections. The song begins with a 12/8 stanza which samples the main guitar and piano riff of the Animals 1965 song ""Club a Go-Go"". The tempo then changes to 4/4 and the first verse occurs, followed by two noticeably individual transitional bridges. Around two minutes into the song, the song reaches its climactic chorus before returning to the stanza heard in the introduction. The song repeats the chorus and the introduction is also used as an outro. The song avoids the typical AABA form and verse-chorus form present in most contemporary pop music.Brian Higgins and Xenomania created ""Biology"" in reaction to Girls Aloud's previous single ""Long Hot Summer"", which Higgins called ""a disaster record"". Higgins continued, ""I think that [Biology] is a wonderful record – so uplifting. It meant so much to us and it really set Chemistry up well."" The lyric referring to ""wicked games"", which is mentioned in the Animals-inspired riff, was inspired by Girls Aloud almost releasing a cover of Chris Isaak's ""Wicked Game"" as a single.The song's title inspired the album's title, Chemistry. Both the single and album title refer to the scientific fields of biology and chemistry.


== Release ==
For the new single and album, Girls Aloud employed stylist Victoria Adcock. Promotion for the single received a setback when Sarah Harding was diagnosed with kidney infection. Girls Aloud also announced dates for 2006's Chemistry Tour.The single was released on 14 November 2005. It was available on two CD single formats and as a digital download. The first disc included the Tony Lamezma Club Mix of Girls Aloud's 2004 single ""The Show"". The second CD format included a previously unreleased track entitled ""Nobody but You"", as well as the Tony Lamezma Remix of ""Biology"". The artwork was inspired by UK punk band X-Ray Spex's album cover Germfree Adolescents. Both covers show each member in a different pose, trapped inside a large vial. A live recording of ""Biology"" from Wembley Arena was featured on the iTunes version of The Sound of Girls Aloud: The Greatest Hits, and later on 2008's Girls A Live and Girls Aloud's singles boxset. Both the album version and Tony Lamezma Remix of ""Biology"" appear on Popjustice: 100% Solid Pop Music. ""Biology"" was released as a CD single in Australia on 20 February 2006.


== Critical reception ==
""Biology"" received universal acclaim from music critics. The song was particularly notable for its informal structure. Popjustice referred to the song as ""pop music which redefines the supposed boundaries of pop music."" BBC Music said ""the girls rip through a variety of styles, paces and Neneh Cherry-esque raps [...] all within the same song."" Virgin Media praised the song for ""blending the kind of saucy cabaret you'd expect to find in a gin-soaked saloon bar with a glorious chorus of fizzing, gliding synths and deceptively breakneck beats."" The song was described as ""about as far from tired formula as you can possibly get. It sounds like three separate melodies condensed into one, from the Muddy Waters-apeing riff at the start, through to the glorious pop sheen of the verses, and having the sheer balls to wait two minutes before even introducing a chorus."" musicOMH noted that the song ""breaks all the rules of manufactured pop"" and stated that ""Biology is yet more proof that Xenomania write the best pop songs around and that Girls Aloud are pretty much the perfect group to sing them [...] it's the single of the year."" Stylus Magazine also praised the song.Peter Cashmore, writing for The Guardian, described ""Biology"" as ""the best pop single of the last decade"". Peter Robinson of music website Popjustice said the song was ""a great example of a song which pleased people with no passion for pop but also managed to hit the spot with those who totally loved the stuff [...] At once avant garde and relentlessly, demented mainstream, 'Biology' quickly became one of Girls Aloud's signature tunes."" In September 2006, ""Biology"" won the award for the Popjustice £20 Music Prize, an annual prize awarded by a panel of judges organised by Popjustice to the singer(s) of the best British pop single of the past year. Girls Aloud had previously won the award in 2003 and 2005 for ""No Good Advice"" and ""Wake Me Up"" respectively. The song was listed at number 245 on American review site Pitchfork's ""The Top 500 Tracks of the 2000s"" list, despite Girls Aloud never receiving any sort of Stateside push. Billboard named the song #32 on their list of 100 Greatest Girl Group Songs of All Time.


== Chart performance ==
Following the disappointing chart position of ""Long Hot Summer"", ""Biology"" saw Girls Aloud return to the top five on the UK Singles Chart. The single entered the chart at number four. The song fell just one position to round at the top five the following week. It spent a third week in the top ten, slipping to number nine. The song spent a total of ten weeks in the UK's top 75. The song also peaked at number two on the official UK Singles Downloads Chart, held off by Madonna's ""Hung Up"".Similarly to the song's performance in the UK, ""Biology"" returned Girls Aloud to the top ten in Ireland, entering the Irish Singles Chart at number seven. It slipped just two places to number nine in its second week. The single spent three more weeks in Ireland's top twenty before falling. It spent a total of ten weeks in Ireland's top fifty. ""Biology"" peaked at number twenty-six in Australia, spending six weeks on the ARIA Singles Chart.


== Music video ==
The music video for ""Biology"" was directed by production team Harvey & Carolyn for Alchemy Films, with art direction from Maria Chryssikos. The video, which consists only of group shots, witnesses Girls Aloud seamlessly moving through various sequences in scenes of disjointed choreography. Like the song itself, the video showcases a variety of styles. The music video begins with a curtain being drawn back to reveal the band members posed in black jazz dresses, stood against a black background decorated with expensive-looking candelabra and chandeliers. Nadine Coyle, perched upon a black grand piano, sings the jazzy intro. As the song's introduction ends, the scene then morphs into a room with white wallpaper embellished with black butterflies. Girls Aloud's outfits turn into frilly pink and purple dresses as digitised butterflies begin to float by. The scene transitions into a room with pink wallpaper and black floral patterns, while the group's outfits change into the red and black outfits seen on the single's artwork. The scene reverts to the opening sequence as Girls Aloud perform choreography involving chairs. As the song reaches its climax, the group are seen seamlessly moving between the various scenes and the different outfits. The video ends with a curtain closing.
Peter Robinson noted that the single's video captured Girls Aloud's ""distinct visual style and some endearingly shambolic synchronised dance moves."" The video can be found on the DVD release of 2006's Chemistry Tour (released as The Greatest Hits Live from Wembley), as well as 2007's Style.


== Live performances ==
Girls Aloud performed ""Biology"" for the first time on Top of the Pops on 16 October 2005, wearing the black dresses from their music video. They appeared on CD:UK on 12 November. Following the show, Sarah Harding collapsed and was diagnosed with a kidney infection. They returned to the show just ten days later, performing in the black-and-red outfits seen on the single's artwork against the pink set from the music video. Girls Aloud also appeared on Children in Need 2005, GMTV, Ministry of Mayhem, and Top of the Pops Reloaded. They performed the song during the last ever Smash Hits Poll Winners Party at Wembley Arena and at London's G-A-Y nightclub. Girls Aloud performed ""Biology"" on a number of Australian shows during their week-long promotional trip, including 9am with David & Kim, Sunrise, and whatUwant.
""Biology"" has been performed by the group at a number of summer festivals and open-air concerts, such as T4 on the Beach in 2007 and V Festival in 2006 and 2008. The song was also performed during promotion of Girls Aloud's 2006 greatest hits album The Sound of Girls Aloud: The Greatest Hits. They appeared on The Album Chart Show, Children in Need 2006, Davina, The Green Room, and the Vodafone Live Music Awards.
""Biology"" has been performed at all of Girls Aloud's concert tours since its release. It served as the opening number of 2006's Chemistry Tour, following an introduction in which a mad scientist creates five women. Girls Aloud then rise from underneath the stage and perform ""Biology"". It served as the encore for the following year's The Greatest Hits Tour. ""Biology"" was performed as part of a cabaret section on 2008's Tangled Up Tour, accompanied by a swing-inspired dance break. The song was also featured in the first section of 2009's Out of Control Tour.


== Track listings and formats ==


== Credits and personnel ==
Guitar: Nick Coler, Shawn Lee
Keyboards: Brian Higgins, Tim Powell
Mastering: Dick Beetham for 360 Mastering
Mixing: Tim Powell
Production: Brian Higgins, Xenomania
Programming: Brian Higgins, Tim Powell
Songwriting: Miranda Cooper, Brian Higgins, Lisa Cowling, Giselle Sommerville
Vocals: Girls Aloud
Published by Warner/Chappell Music and Xenomania Music


== Charts ==


== Certifications ==


== References ==


== External links ==
Girls Aloud's official website",95451,431,"2005 singles, 2005 songs, All articles with unsourced statements, Articles with MusicBrainz release group identifiers, Articles with MusicBrainz work identifiers, Articles with hAudio microformats, Articles with short description, Articles with unsourced statements from May 2020, CS1 maint: others in cite AV media (notes), Certification Table Entry usages for United Kingdom, Girls Aloud songs, Pages using certification Table Entry with streaming figures, Pages using certification Table Entry with streaming footnote, Progressive pop songs, Short description is different from Wikidata, Single chart called without artist, Single chart called without song, Single chart usages for Australia, Single chart usages for Ireland2, Single chart usages for Scotland, Single chart usages for UK, Song recordings produced by Xenomania, Songs written by Brian Higgins (producer), Songs written by Lisa Cowling, Songs written by Miranda Cooper, Use British English from October 2018, Use dmy dates from October 2015",1133876285,biology
https://en.wikipedia.org/wiki/Mathematical_and_theoretical_biology,Mathematical and theoretical biology,"Mathematical and theoretical biology, or biomathematics, is a branch of biology which employs theoretical analysis, mathematical models and abstractions of the living organisms to investigate the principles that govern the structure, development and behavior of the systems, as opposed to experimental biology which deals with the conduction of experiments to prove and validate the scientific theories. The field is sometimes called mathematical biology or biomathematics to stress the mathematical side, or theoretical biology to stress the biological side. Theoretical biology focuses more on the development of theoretical principles for biology while mathematical biology focuses on the use of mathematical tools to study biological systems, even though the two terms are sometimes interchanged.Mathematical biology aims at the mathematical representation and modeling of biological processes, using techniques and tools of applied mathematics. It can be useful in both theoretical and practical research. Describing systems in a quantitative manner means their behavior can be better simulated, and hence properties can be predicted that might not be evident to the experimenter. This requires precise mathematical models.
Because of the complexity of the living systems, theoretical biology employs several fields of mathematics, and has contributed to the development of new techniques.","Mathematical and theoretical biology, or biomathematics, is a branch of biology which employs theoretical analysis, mathematical models and abstractions of the living organisms to investigate the principles that govern the structure, development and behavior of the systems, as opposed to experimental biology which deals with the conduction of experiments to prove and validate the scientific theories. The field is sometimes called mathematical biology or biomathematics to stress the mathematical side, or theoretical biology to stress the biological side. Theoretical biology focuses more on the development of theoretical principles for biology while mathematical biology focuses on the use of mathematical tools to study biological systems, even though the two terms are sometimes interchanged.Mathematical biology aims at the mathematical representation and modeling of biological processes, using techniques and tools of applied mathematics. It can be useful in both theoretical and practical research. Describing systems in a quantitative manner means their behavior can be better simulated, and hence properties can be predicted that might not be evident to the experimenter. This requires precise mathematical models.
Because of the complexity of the living systems, theoretical biology employs several fields of mathematics, and has contributed to the development of new techniques.


== History ==


=== Early history ===
Mathematics has been used in biology as early as the 13th century, when Fibonacci used the famous Fibonacci series to describe a growing population of rabbits. In the 18th century Daniel Bernoulli applied mathematics to describe the effect of smallpox on the human population. Thomas Malthus' 1789 essay on the growth of the human population was based on the concept of exponential growth. Pierre François Verhulst formulated the logistic growth model in 1836.
Fritz Müller described the evolutionary benefits of what is now called Müllerian mimicry in 1879, in an account notable for being the first use of a mathematical argument in evolutionary ecology to show how powerful the effect of natural selection would be, unless one includes Malthus's discussion of the effects of population growth that influenced Charles Darwin: Malthus argued that growth would be exponential (he uses the word ""geometric"") while resources (the environment's carrying capacity) could only grow arithmetically.The term ""theoretical biology"" was first used as a monograph title by Johannes Reinke in 1901, and soon after by Jakob von Uexküll in 1920. One founding text is considered to be On Growth and Form (1917) by D'Arcy Thompson, and other early pioneers include Ronald Fisher, Hans Leo Przibram, Vito Volterra, Nicolas Rashevsky and Conrad Hal Waddington.


=== Recent growth ===
Interest in the field has grown rapidly from the 1960s onwards. Some reasons for this include:

The rapid growth of data-rich information sets,  due to the genomics revolution, which are difficult to understand without the use of analytical tools
Recent development of mathematical tools such as chaos theory to help understand complex, non-linear mechanisms in biology
An increase in computing power, which facilitates calculations and simulations not previously possible
An increasing interest in in silico experimentation due to ethical considerations, risk, unreliability and other complications involved in human and animal research


== Areas of research ==
Several areas of specialized research in mathematical and theoretical biology as well as external links to related projects in various universities are concisely presented in the following subsections, including also a large number of appropriate validating references from a list of several thousands of published authors contributing to this field. Many of the included examples are characterised by highly complex, nonlinear, and supercomplex mechanisms, as it is being increasingly recognised that the result of such interactions may only be understood through a combination of mathematical, logical, physical/chemical, molecular and computational models.


=== Abstract relational biology ===
Abstract relational biology (ARB) is concerned with the study of general, relational models of complex biological systems, usually abstracting out specific morphological, or anatomical, structures. Some of the simplest models in ARB are the Metabolic-Replication, or (M,R)--systems introduced by Robert Rosen in 1957-1958 as abstract, relational models of cellular and organismal organization.
Other approaches include the notion of autopoiesis developed by Maturana and Varela, Kauffman's Work-Constraints cycles, and more recently the notion of closure of constraints.


=== Algebraic biology ===
Algebraic biology (also known as symbolic systems biology) applies the algebraic methods of symbolic computation to the study of biological problems, especially in genomics, proteomics, analysis of molecular structures and study of genes.


=== Complex systems biology ===
An elaboration of systems biology to understand the more complex life processes was developed since 1970 in connection with molecular set theory, relational biology and algebraic biology.


=== Computer models and automata theory ===
A monograph on this topic summarizes an extensive amount of published research in this area up to 1986, including subsections in the following areas: computer modeling in biology and medicine, arterial system models, neuron models, biochemical and oscillation networks, quantum automata, quantum computers in molecular biology and genetics, cancer modelling, neural nets, genetic networks, abstract categories in relational biology, metabolic-replication systems, category theory applications in biology and medicine, automata theory, cellular automata, tessellation models and complete self-reproduction, chaotic systems in organisms, relational biology and organismic theories.Modeling cell and molecular biology
This area has received a boost due to the growing importance of molecular biology.
Mechanics of biological tissues
Theoretical enzymology and enzyme kinetics
Cancer modelling and simulation
Modelling the movement of interacting cell populations
Mathematical modelling of scar tissue formation
Mathematical modelling of intracellular dynamics
Mathematical modelling of the cell cycle
Mathematical modelling of apoptosisModelling physiological systems

Modelling of arterial disease
Multi-scale modelling of the heart
Modelling electrical properties of muscle interactions, as in bidomain and monodomain models


=== Computational neuroscience ===
Computational neuroscience (also known as theoretical neuroscience or mathematical neuroscience) is the theoretical study of the nervous system.


=== Evolutionary biology ===
Ecology and evolutionary biology have traditionally been the dominant fields of mathematical biology.
Evolutionary biology has been the subject of extensive mathematical theorizing. The traditional approach in this area, which includes complications from genetics, is population genetics. Most population geneticists consider the appearance of new alleles by mutation, the appearance of new genotypes by recombination, and changes in the frequencies of existing alleles and genotypes at a small number of gene loci. When infinitesimal effects at a large number of gene loci are considered, together with the assumption of linkage equilibrium or quasi-linkage equilibrium, one derives quantitative genetics. Ronald Fisher made fundamental advances in statistics, such as analysis of variance, via his work on quantitative genetics. Another important branch of population genetics that led to the extensive development of coalescent theory is phylogenetics. Phylogenetics is an area that deals with the reconstruction and analysis of phylogenetic (evolutionary) trees and networks based on inherited characteristics Traditional population genetic models deal with alleles and genotypes, and are frequently stochastic.
Many population genetics models assume that population sizes are constant. Variable population sizes, often in the absence of genetic variation, are treated by the field of population dynamics. Work in this area dates back to the 19th century, and even as far as 1798 when Thomas Malthus formulated the first principle of population dynamics, which later became known as the Malthusian growth model. The Lotka–Volterra predator-prey equations are another famous example. Population dynamics overlap with another active area of research in mathematical biology: mathematical epidemiology, the study of infectious disease affecting populations. Various models of the spread of infections have been proposed and analyzed, and provide important results that may be applied to health policy decisions.
In evolutionary game theory, developed first by John Maynard Smith and George R. Price, selection acts directly on inherited phenotypes, without genetic complications. This approach has been mathematically refined to produce the field of adaptive dynamics.


=== Mathematical biophysics ===
The earlier stages of mathematical biology were dominated by mathematical biophysics, described as the application of mathematics in biophysics, often involving specific physical/mathematical models of biosystems and their components or compartments.
The following is a list of mathematical descriptions and their assumptions.


==== Deterministic processes (dynamical systems) ====
A fixed mapping between an initial state and a final state. Starting from an initial condition and moving forward in time, a deterministic process always generates the same trajectory, and no two trajectories cross in state space.

Difference equations/Maps – discrete time, continuous state space.
Ordinary differential equations – continuous time, continuous state space, no spatial derivatives. See also: Numerical ordinary differential equations.
Partial differential equations – continuous time, continuous state space, spatial derivatives. See also: Numerical partial differential equations.
Logical deterministic cellular automata – discrete time, discrete state space. See also: Cellular automaton.


==== Stochastic processes (random dynamical systems) ====
A random mapping between an initial state and a final state, making the state of the system a random variable with a corresponding probability distribution.

Non-Markovian processes – generalized master equation – continuous time with memory of past events, discrete state space, waiting times of events (or transitions between states) discretely occur.
Jump Markov process – master equation – continuous time with no memory of past events, discrete state space, waiting times between events discretely occur and are exponentially distributed. See also: Monte Carlo method for numerical simulation methods, specifically dynamic Monte Carlo method and Gillespie algorithm.
Continuous Markov process – stochastic differential equations or a Fokker–Planck equation – continuous time, continuous state space, events occur continuously according to a random Wiener process.


==== Spatial modelling ====
One classic work in this area is Alan Turing's paper on morphogenesis entitled The Chemical Basis of Morphogenesis, published in 1952 in the Philosophical Transactions of the Royal Society.

Travelling waves in a wound-healing assay
Swarming behaviour
A mechanochemical theory of morphogenesis
Biological pattern formation
Spatial distribution modeling using plot samples
Turing patterns


=== Mathematical methods ===
A model of a biological system is converted into a system of equations, although the word 'model' is often used synonymously with the system of corresponding equations. The solution of the equations, by either analytical or numerical means, describes how the biological system behaves either over time or at equilibrium. There are many different types of equations and the type of behavior that can occur is dependent on both the model and the equations used. The model often makes assumptions about the system. The equations may also make assumptions about the nature of what may occur.


=== Molecular set theory ===
Molecular set theory (MST) is a mathematical formulation of the wide-sense chemical kinetics of biomolecular reactions in terms of sets of molecules and their chemical transformations represented by set-theoretical mappings between molecular sets. It was introduced by Anthony Bartholomay, and its applications were developed in mathematical biology and especially in mathematical medicine.
In a more general sense, MST is the theory of molecular categories defined as categories of molecular sets and their chemical transformations represented as set-theoretical mappings of molecular sets. The theory has also contributed to biostatistics and the formulation of clinical biochemistry problems in mathematical formulations of pathological, biochemical changes of interest to Physiology, Clinical Biochemistry and Medicine.


=== Organizational biology ===
Theoretical approaches to biological organization aim to understand the interdependence between the parts of organisms. They emphasize the circularities that these interdependences lead to. Theoretical biologists developed several concepts to formalize this idea.
For example, abstract relational biology (ARB) is concerned with the study of general, relational models of complex biological systems, usually abstracting out specific morphological, or anatomical, structures. Some of the simplest models in ARB are the Metabolic-Replication, or (M,R)--systems introduced by Robert Rosen in 1957-1958 as abstract, relational models of cellular and organismal organization.


== Model example: the cell cycle ==

The eukaryotic cell cycle is very complex and is one of the most studied topics, since its misregulation leads to cancers.
It is possibly a good example of a mathematical model as it deals with simple calculus but gives valid results.  Two research groups  have produced several models of the cell cycle simulating several organisms. They have recently produced a generic eukaryotic cell cycle model that can represent a particular eukaryote depending on the values of the parameters, demonstrating that the idiosyncrasies of the individual cell cycles are due to different protein concentrations and affinities, while the underlying mechanisms are conserved (Csikasz-Nagy et al., 2006).
By means of a system of ordinary differential equations these models show the change in time (dynamical system) of the protein inside a single typical cell; this type of model is called a deterministic process (whereas a model describing a statistical distribution of protein concentrations in a population of cells is called a stochastic process).
To obtain these equations an iterative series of steps must be done: first the several models and observations are combined to form a consensus diagram and the appropriate kinetic laws are chosen to write the differential equations, such as rate kinetics for stoichiometric reactions, Michaelis-Menten kinetics for enzyme substrate reactions and Goldbeter–Koshland kinetics for ultrasensitive transcription factors, afterwards the parameters of the equations (rate constants, enzyme efficiency coefficients and Michaelis constants) must be fitted to match observations; when they cannot be fitted the kinetic equation is revised and when that is not possible the wiring diagram is modified. The parameters are fitted and validated using observations of both wild type and mutants, such as protein half-life and cell size.
To fit the parameters, the differential equations must be studied. This can be done either by simulation or by analysis. In a simulation, given a starting vector (list of the values of the variables), the progression of the system is calculated by solving the equations at each time-frame in small increments.

 In analysis, the properties of the equations are used to investigate the behavior of the system depending on the values of the parameters and variables. A system of differential equations can be represented as a vector field, where each vector described the change (in concentration of two or more protein) determining where and how fast the trajectory (simulation) is heading. Vector fields can have several special points: a stable point, called a sink, that attracts in all directions (forcing the concentrations to be at a certain value), an unstable point, either a source or a saddle point, which repels (forcing the concentrations to change away from a certain value), and a limit cycle, a closed trajectory towards which several trajectories spiral towards (making the concentrations oscillate).
A better representation, which handles the large number of variables and parameters, is a bifurcation diagram using bifurcation theory. The presence of these special steady-state points at certain values of a parameter (e.g. mass) is represented by a point and once the parameter passes a certain value, a qualitative change occurs, called a bifurcation, in which the nature of the space changes, with profound consequences for the protein concentrations: the cell cycle has phases (partially corresponding to G1 and G2) in which mass, via a stable point, controls cyclin levels, and phases (S and M phases) in which the concentrations change independently, but once the phase has changed at a bifurcation event (Cell cycle checkpoint), the system cannot go back to the previous levels since at the current mass the vector field is profoundly different and the mass cannot be reversed back through the bifurcation event, making a checkpoint irreversible. In particular the S and M checkpoints are regulated by means of special bifurcations called a Hopf bifurcation and an infinite period bifurcation.


== Societies and institutes ==
National Institute for Mathematical and Biological Synthesis
Society for Mathematical Biology
ESMTB: European Society for Mathematical and Theoretical Biology
The Israeli Society for Theoretical and Mathematical Biology
Société Francophone de Biologie Théorique
International Society for Biosemiotic Studies
School of Computational and Integrative Sciences, Jawaharlal Nehru University


== See also ==


== Notes ==


== References ==

Biologist Salary | PayScale
""Biologist Salary | Payscale"". Payscale.Com, 2021, https://www.payscale.com/research/US/Job=Biologist/Salary. Accessed 3 May 2021. 

Theoretical biology


== Further reading ==


== External links ==

The Society for Mathematical Biology
The Collection of Biostatistics Research Archive",651088,897,"All articles lacking reliable references, All articles needing additional references, All articles with unsourced statements, Articles lacking reliable references from August 2010, Articles needing additional references from March 2020, Articles with EMU identifiers, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with NKC identifiers, Articles with short description, Articles with unsourced statements from May 2011, CS1: long volume value, CS1 errors: missing periodical, CS1 maint: location, Commons category link from Wikidata, Mathematical and theoretical biology, Short description is different from Wikidata, Webarchive template wayback links",1123208062,biology
https://en.wikipedia.org/wiki/Vespertine_(biology),Vespertine (biology),"Vespertine is a term used in the life sciences to indicate something of, relating to, or occurring in the evening. In botany, a vespertine flower is one that opens or blooms in the evening. In zoology, the term is used for a creature that becomes active at dusk, such as bats and owls. Strictly speaking, however, the term means that activity ceases during the hours of full darkness and does not resume until the next evening. Activity that continues throughout the night should be described as nocturnal. 
Vespertine behaviour is a special case of crepuscular behaviour; like crepuscular activity, vespertine activity is limited to dusk rather than full darkness. Unlike vespertine activity, crepuscular activity may resume in dim twilight before dawn. A related term is matutinal, referring to activity limited to the dawn twilight.
The word vespertine is derived from the Latin word vespertīnus, meaning ""evening"".","Vespertine is a term used in the life sciences to indicate something of, relating to, or occurring in the evening. In botany, a vespertine flower is one that opens or blooms in the evening. In zoology, the term is used for a creature that becomes active at dusk, such as bats and owls. Strictly speaking, however, the term means that activity ceases during the hours of full darkness and does not resume until the next evening. Activity that continues throughout the night should be described as nocturnal. 
Vespertine behaviour is a special case of crepuscular behaviour; like crepuscular activity, vespertine activity is limited to dusk rather than full darkness. Unlike vespertine activity, crepuscular activity may resume in dim twilight before dawn. A related term is matutinal, referring to activity limited to the dawn twilight.
The word vespertine is derived from the Latin word vespertīnus, meaning ""evening"".


== See also ==
Crypsis
Matutinal


== References ==",176498,76,"All articles needing additional references, All stub articles, Articles needing additional references from November 2017, Articles with short description, Biology stubs, Botany, Botany stubs, Ethology, Ethology stubs, Short description matches Wikidata",1100887734,biology
https://en.wikipedia.org/wiki/Matrix_(biology),Matrix (biology),"In biology, matrix (plural: matrices) is the material (or tissue) in between a eukaryotic organism's cells.
The structure of connective tissues is an extracellular matrix. Fingernails and toenails grow from matrices. It is found in various connective tissues. It is generally used as a jelly-like structure instead of cytoplasm in connective tissue.","In biology, matrix (plural: matrices) is the material (or tissue) in between a eukaryotic organism's cells.
The structure of connective tissues is an extracellular matrix. Fingernails and toenails grow from matrices. It is found in various connective tissues. It is generally used as a jelly-like structure instead of cytoplasm in connective tissue.


== Tissue matrices ==


=== Extracellular matrix (ECM) ===

The main ingredients of the extracellular matrix are glycoproteins secreted by the cells. The most abundant glycoprotein in the ECM of most animal cells is collagen, which forms strong fibers outside the cells. In fact, collagen accounts for about 40% of the total protein in the human body. The collagen fibers are embedded in a network woven from proteoglycans. A proteoglycan molecule consists of a small core protein with many carbohydrate chains covalently attached, so that it may be up to 95% carbohydrate. Large proteoglycan complexes can form when hundreds of proteoglycans become noncovalently attached to a single long polysaccharide molecule. Some cells are attached to the ECM by still other ECM glycoproteins such as fibronectin. Fibronectin and other ECM proteins bind to cell surface receptor proteins called integrins that are built into the plasma membrane. Integrins span the membrane and bind on the cytoplasmic side to associated proteins attached to microfilaments of the cytoskeleton. The name integrin is based on the word integrate, integrins are in a position to transmit signals between the ECM and the cytoskeleton and thus to integrate changes occurring outside and inside the cell. Current research on fibronectin, other ECM molecules, and integrins is revealing the influential role of the ECM in the lives of cells. By communicating with a cell through integrins, the ECM can regulate a cell's behavior. For example, some cells in a developing embryo migrate along specific pathways by matching the orientation of their microfilaments to the ""grain"" of fibers in the ECM. Researchers are also learning that the ECM around a cell can influence the activity of genes in the nucleus. Information about the ECM probably reaches the nucleus by a combination of mechanical and chemical signaling pathways. Mechanical signaling involves fibronectin, integrins, and microfilaments of the cytoskeleton. Changes in the cytoskeleton may in turn trigger chemical signaling pathways inside the cell, leading to changes in the set of proteins being made by the cell and therefore changes in the cells function. In this way, the ECM of a particular tissue may help coordinate the behavior of all the cells within that tissue. Direct connections between cells also function in this coordination.


=== Bone matrix ===

Bone is a form of connective tissue found in the body, composed largely of hardened hydroxyapatite-containing collagen. In larger mammals, it is arranged in osteon regions. Bone matrix allows mineral salts such as calcium to be stored and provides protection for internal organs and support for locomotion.


=== Cartilage matrix ===
Cartilage is another form of connective tissue found in the body, providing a smooth surface for joints and a mechanism for growth of bones during development.


== Subcellular matrices ==


=== Mitochondrial matrix ===

In the mitochondrion, the matrix contains soluble enzymes that catalyze the oxidation of pyruvate and other small organic molecules.


=== Nuclear matrix ===

In the cell nucleus the matrix is the insoluble fraction that remains after extracting the solubled DNA.


=== Golgi matrix ===

The Golgi matrix is a protein scaffold around the Golgi apparatus made up of Golgins, GRASP's and miscellaneous other proteins on the cytoplasmic side of the Golgi apparatus involved in keeping its shape and membrane stacking.


== Matrix (medium) ==
A matrix is also a medium in which bacteria are grown (cultured). For instance, a Petri dish of agar may be the matrix for culturing a sample swabbed from a patient's throat.


== See also ==
Matricity


=== Tissues and cells ===
Germinal matrix
Hair matrix cell


=== Molecular biology ===
Matrix attachment region
Matrix metalloproteinase
Matrix protein


=== Bioinformatics and sequence evolution ===
PAM matrix
Position-specific scoring matrix
Similarity matrix
Substitution matrix


=== Botany and agriculture ===
Matrix Planting


=== Population biology and ecology ===
Matrix population models


== References ==",311445,152,"All articles needing additional references, Articles needing additional references from January 2021, Articles with short description, Cell anatomy, Connective tissue, Matrices (biology), Organelles, Short description is different from Wikidata",1115382104,biology
https://en.wikipedia.org/wiki/Colony_(biology),Colony (biology),"In biology, a colony is composed of two or more conspecific individuals living in close association with, or connected to, one another. This association is usually for mutual benefit such as stronger defense or the ability to attack bigger prey.Colonies can form in various shapes and ways depending on the organism involved. For instance, the bacterial colony is a cluster of identical cells (clones). These colonies often form and grow on the surface of (or within) a solid medium, usually derived from a single parent cell.Colonies, in the context of development, may be composed of two or more unitary (or solitary) organisms or be modular organisms. Unitary organisms have determinate development (set life stages) from zygote to adult form and individuals or groups of individuals (colonies) are visually distinct. Modular organisms have indeterminate growth forms (life stages not set) through repeated iteration of genetically identical modules (or individuals), and it can be difficult to distinguish between the colony as a whole and the modules within. In the latter case, modules may have specific functions within the colony.
In contrast, solitary organisms do not associate with colonies; they are ones in which all individuals live independently and have all of the functions needed to survive and reproduce.
Some organisms are primarily independent and form facultative colonies in reply to environmental conditions while others must live in a colony to survive (obligate). For example, some carpenter bees will form colonies when a dominant hierarchy is formed between two or more nest foundresses (facultative colony), while corals are animals that are physically connected by living tissue (the coenosarc) that contains a shared gastrovascular cavity.","In biology, a colony is composed of two or more conspecific individuals living in close association with, or connected to, one another. This association is usually for mutual benefit such as stronger defense or the ability to attack bigger prey.Colonies can form in various shapes and ways depending on the organism involved. For instance, the bacterial colony is a cluster of identical cells (clones). These colonies often form and grow on the surface of (or within) a solid medium, usually derived from a single parent cell.Colonies, in the context of development, may be composed of two or more unitary (or solitary) organisms or be modular organisms. Unitary organisms have determinate development (set life stages) from zygote to adult form and individuals or groups of individuals (colonies) are visually distinct. Modular organisms have indeterminate growth forms (life stages not set) through repeated iteration of genetically identical modules (or individuals), and it can be difficult to distinguish between the colony as a whole and the modules within. In the latter case, modules may have specific functions within the colony.
In contrast, solitary organisms do not associate with colonies; they are ones in which all individuals live independently and have all of the functions needed to survive and reproduce.
Some organisms are primarily independent and form facultative colonies in reply to environmental conditions while others must live in a colony to survive (obligate). For example, some carpenter bees will form colonies when a dominant hierarchy is formed between two or more nest foundresses (facultative colony), while corals are animals that are physically connected by living tissue (the coenosarc) that contains a shared gastrovascular cavity.


== Colony types ==


=== Social colonies ===

Unicellular and multicellular unitary organisms may aggregate to form colonies. For example,

Protists such as slime molds are many unicellular organisms that aggregate to form colonies when food resources are hard to come by, as together they are more reactive to chemical cues released by preferred prey.
Eusocial insects like ants and honey bees are multicellular animals that live in colonies with a highly organized social structure. Colonies of some social insects may be deemed superorganisms.
Animals, such as humans and rodents, form breeding or nesting colonies, potentially for more successful mating and to better protect offspring.
The Bracken Cave is the summer home to a colony of around 20 million Mexican free-tailed bats, making it the largest known concentration of mammals.


=== Modular organisms ===

Modular organisms are those in which a genet (or genetic individual formed from a sexually-produced zygote) asexually reproduces to form genetically identical clones called ramets.A clonal colony is when the ramets of a genet live in close proximity or are physically connected. Ramets may have all of the functions needed to survive on their own or be interdependent on other ramets. For example, some sea anemones go through the process of pedal laceration in which a genetically identical individual is asexually produced from tissue broken off from the anemone's pedal disc. In plants, clonal colonies are created through the propagation of genetically identical trees by stolons or rhizomes.
Colonial organisms are clonal colonies composed of many physically connected, interdependent individuals. The subunits of colonial organisms can be unicellular, as in the alga Volvox (a coenobium), or multicellular, as in the phylum Bryozoa. The former type may have been the first step toward multicellular organisms. Individuals within a multicellular colonial organism may be called ramets, modules, or zooids. Structural and functional variation (polymorphism), when present, designates ramet responsibilities such as feeding, reproduction, and defense. To that end, being physically connected allows the colonial organism to distribute nutrients and energy obtained by feeding zooids throughout the colony. The hydrozoan Portuguese man o' war is a classic example of a colonial organism, one of many in the taxonomic class.


=== Microbial colonies ===
A microbial colony  is defined as a visible cluster of microorganisms growing on the surface of or within a solid medium, presumably cultured from a single cell. Because the colony is clonal, with all organisms in it descending from a single ancestor (assuming no contamination), they are genetically identical, except for any mutations (which occur at low frequencies). Obtaining such genetically identical organisms (or pure strains) can be useful; this is done by spreading organisms on a culture plate and starting a new stock from a single resulting colony.A biofilm is a colony of microorganisms often comprising several species, with properties and capabilities greater than the aggregate of capabilities of the individual organisms.


== Life history ==
Individuals in social colonies and modular organisms receive benefit to such a lifestyle. For example, it may be easier to seek out food, defend a nesting site, or increase competitive ability against other species. Modular organisms' ability to reproduce asexually in addition to sexually allows them unique benefits that social colonies do not have.The energy required for sexual reproduction varies based on the frequency and length of reproductive activity, number and size of offspring, and parental care. While solitary individuals bear all of those energy costs, individuals in some social colonies share a portion of those costs.
Modular organisms save energy by using asexual reproduction during their life. Energy reserved in this way allows them to put more energy towards colony growth, regenerating lost modules (due to predation or other cause of death), or response to environmental conditions.


== See also ==


== References ==",704175,260,"All articles with unsourced statements, Articles with short description, Articles with unsourced statements from May 2022, Community ecology, Environmental terminology, Habitat, Microbiology terms, Short description is different from Wikidata",1120040668,biology
https://en.wikipedia.org/wiki/Animal,Animal,"Animals are multicellular, eukaryotic organisms in the biological kingdom Animalia. With few exceptions, animals consume organic material, breathe oxygen, are able to move, can reproduce sexually, and go through an ontogenetic stage in which their body consists of a hollow sphere of cells, the blastula, during embryonic development. Over 1.5 million living animal species have been described—of which around 1 million are insects—but it has been estimated there are over 7 million animal species in total. Animals range in length from 8.5 micrometres (0.00033 in) to 33.6 metres (110 ft). They have complex interactions with each other and their environments, forming intricate food webs. The scientific study of animals is known as zoology.
Most living animal species are in Bilateria, a clade whose members have a bilaterally symmetric body plan. The Bilateria include the protostomes, containing animals such as nematodes, arthropods, flatworms, annelids and molluscs, and the deuterostomes, containing the echinoderms and the chordates, the latter including the vertebrates. Life forms interpreted as early animals were present in the Ediacaran biota of the late Precambrian. Many modern animal phyla became clearly established in the fossil record as marine species during the Cambrian explosion, which began around 539 million years ago. 6,331 groups of genes common to all living animals have been identified; these may have arisen from a single common ancestor that lived 650 million years ago.
Historically, Aristotle divided animals into those with blood and those without. Carl Linnaeus created the first hierarchical biological classification for animals in 1758 with his Systema Naturae, which Jean-Baptiste Lamarck expanded into 14 phyla by 1809. In 1874, Ernst Haeckel divided the animal kingdom into the multicellular Metazoa (now synonymous for Animalia) and the Protozoa, single-celled organisms no longer considered animals. In modern times, the biological classification of animals relies on advanced techniques, such as molecular phylogenetics, which are effective at demonstrating the evolutionary relationships between taxa.
Humans make use of many animal species, such as for food (including meat, milk, and eggs), for materials (such as leather and wool), as pets, and as working animals including for transport. Dogs have been used in hunting, as have birds of prey, while many terrestrial and aquatic animals were hunted for sports. Nonhuman animals have appeared in art from the earliest times and are featured in mythology and religion.

","Animals are multicellular, eukaryotic organisms in the biological kingdom Animalia. With few exceptions, animals consume organic material, breathe oxygen, are able to move, can reproduce sexually, and go through an ontogenetic stage in which their body consists of a hollow sphere of cells, the blastula, during embryonic development. Over 1.5 million living animal species have been described—of which around 1 million are insects—but it has been estimated there are over 7 million animal species in total. Animals range in length from 8.5 micrometres (0.00033 in) to 33.6 metres (110 ft). They have complex interactions with each other and their environments, forming intricate food webs. The scientific study of animals is known as zoology.
Most living animal species are in Bilateria, a clade whose members have a bilaterally symmetric body plan. The Bilateria include the protostomes, containing animals such as nematodes, arthropods, flatworms, annelids and molluscs, and the deuterostomes, containing the echinoderms and the chordates, the latter including the vertebrates. Life forms interpreted as early animals were present in the Ediacaran biota of the late Precambrian. Many modern animal phyla became clearly established in the fossil record as marine species during the Cambrian explosion, which began around 539 million years ago. 6,331 groups of genes common to all living animals have been identified; these may have arisen from a single common ancestor that lived 650 million years ago.
Historically, Aristotle divided animals into those with blood and those without. Carl Linnaeus created the first hierarchical biological classification for animals in 1758 with his Systema Naturae, which Jean-Baptiste Lamarck expanded into 14 phyla by 1809. In 1874, Ernst Haeckel divided the animal kingdom into the multicellular Metazoa (now synonymous for Animalia) and the Protozoa, single-celled organisms no longer considered animals. In modern times, the biological classification of animals relies on advanced techniques, such as molecular phylogenetics, which are effective at demonstrating the evolutionary relationships between taxa.
Humans make use of many animal species, such as for food (including meat, milk, and eggs), for materials (such as leather and wool), as pets, and as working animals including for transport. Dogs have been used in hunting, as have birds of prey, while many terrestrial and aquatic animals were hunted for sports. Nonhuman animals have appeared in art from the earliest times and are featured in mythology and religion.


== Etymology ==
The word ""animal"" comes from the Latin animalis, meaning 'having breath', 'having soul' or 'living being'. The biological definition includes all members of the kingdom Animalia. In colloquial usage, the term animal is often used to refer only to nonhuman animals. The term ""metazoa"" is from  Ancient Greek μετα (meta, used to mean ""later"") and ζῷᾰ (zōia, plural of ζῷον zōion ""animal"").


== Characteristics ==

Animals have several characteristics that set them apart from other living things. Animals are eukaryotic and multicellular. Unlike plants and algae, which produce their own nutrients, animals are heterotrophic, feeding on organic material and digesting it internally. With very few exceptions, animals respire aerobically. All animals are motile (able to spontaneously move their bodies) during at least part of their life cycle, but some animals, such as sponges, corals, mussels, and barnacles, later become sessile. The blastula is a stage in embryonic development that is unique to animals, allowing cells to be differentiated into specialised tissues and organs.


=== Structure ===
All animals are composed of cells, surrounded by a characteristic extracellular matrix composed of collagen and elastic glycoproteins. During development, the animal extracellular matrix forms a relatively flexible framework upon which cells can move about and be reorganised, making the formation of complex structures possible. This may be calcified, forming structures such as shells, bones, and spicules. In contrast, the cells of other multicellular organisms (primarily algae, plants, and fungi) are held in place by cell walls, and so develop by progressive growth. Animal cells uniquely possess the cell junctions called tight junctions, gap junctions, and desmosomes.With few exceptions—in particular, the sponges and placozoans—animal bodies are differentiated into tissues. These include muscles, which enable locomotion, and nerve tissues, which transmit signals and coordinate the body. Typically, there is also an internal digestive chamber with either one opening (in Ctenophora, Cnidaria, and flatworms) or two openings (in most bilaterians).


=== Reproduction and development ===

Nearly all animals make use of some form of sexual reproduction. They produce haploid gametes by meiosis; the smaller, motile gametes are spermatozoa and the larger, non-motile gametes are ova. These fuse to form zygotes, which develop via mitosis into a hollow sphere, called a blastula. In sponges, blastula larvae swim to a new location, attach to the seabed, and develop into a new sponge. In most other groups, the blastula  undergoes more complicated rearrangement. It first invaginates to form a gastrula with a digestive chamber and two separate germ layers, an external ectoderm and an internal endoderm. In most cases, a third germ layer, the mesoderm, also develops between them. These germ layers then differentiate to form tissues and organs.Repeated instances of mating with a close relative during sexual reproduction generally leads to inbreeding depression within a population due to the increased prevalence of harmful recessive traits. Animals have evolved numerous mechanisms for avoiding close inbreeding.
Some animals are capable of asexual reproduction, which often results in a genetic clone of the parent. This may take place through fragmentation; budding, such as in Hydra and other cnidarians; or parthenogenesis, where fertile eggs are produced without mating, such as in aphids.


== Ecology ==

Animals are categorised into ecological groups depending on how they obtain or consume organic material, including carnivores, herbivores, omnivores, detritivores, and parasites. Interactions between animals form complex food webs. In carnivorous or omnivorous species, predation is a consumer–resource interaction where a predator feeds on another organism (called its prey). Selective pressures imposed on one another lead to an evolutionary arms race between predator and prey, resulting in various anti-predator adaptations. Almost all multicellular predators are animals. Some consumers use multiple methods; for example, in parasitoid wasps, the larvae feed on the hosts' living tissues, killing them in the process, but the adults primarily consume nectar from flowers. Other animals may have very specific feeding behaviours, such as hawksbill sea turtles primarily eating sponges.

Most animals rely on the biomass and energy produced by plants through photosynthesis. Herbivores eat plant material directly, while carnivores, and other animals on higher trophic levels typically acquire it indirectly by eating other animals. Animals oxidize carbohydrates, lipids, proteins, and other biomolecules, which allows the animal to grow and to sustain biological processes such as locomotion. Animals living close to hydrothermal vents and cold seeps on the dark sea floor consume organic matter of archaea and bacteria produced in these locations through chemosynthesis (by oxidizing inorganic compounds, such as hydrogen sulfide).Animals originally evolved in the sea. Lineages of arthropods colonised land around the same time as land plants, probably between 510 and 471 million years ago during the Late Cambrian or Early Ordovician. Vertebrates such as the lobe-finned fish Tiktaalik started to move on to land in the late Devonian, about 375 million years ago. Animals occupy virtually all of earth's habitats and microhabitats, including salt water, hydrothermal vents, fresh water, hot springs, swamps, forests, pastures, deserts, air, and the interiors of animals, plants, fungi and rocks. Animals are however not particularly heat tolerant; very few of them can survive at constant temperatures above 50 °C (122 °F). Only very few species of animals (mostly nematodes) inhabit the most extreme cold deserts of continental Antarctica.


== Diversity ==


=== Size ===

The blue whale (Balaenoptera musculus) is the largest animal that has ever lived, weighing up to 190 tonnes and measuring up to 33.6 metres (110 ft) long. The largest extant terrestrial animal is the African bush elephant (Loxodonta africana), weighing up to 12.25 tonnes and measuring up to 10.67 metres (35.0 ft) long. The largest terrestrial animals that ever lived were titanosaur sauropod dinosaurs such as Argentinosaurus, which may have weighed as much as 73 tonnes, and Supersaurus which may have reached 39 meters. Several animals are microscopic; some Myxozoa (obligate parasites within the Cnidaria) never grow larger than 20 µm, and one of the smallest species (Myxobolus shekel) is no more than 8.5 µm when fully grown.


=== Numbers and habitats ===
The following table lists estimated numbers of described extant species for all the animal groups, along with their principal habitats (terrestrial, fresh water, and marine), and free-living or parasitic ways of life. Species estimates shown here are based on numbers described scientifically; much larger estimates have been calculated based on various means of prediction, and these can vary wildly. For instance, around 25,000–27,000 species of nematodes have been described, while published estimates of the total number of nematode species include 10,000–20,000; 500,000; 10 million; and 100 million. Using patterns within the taxonomic hierarchy, the total number of animal species—including those not yet described—was calculated to be about 7.77 million in 2011.


== Evolutionary origin ==

Animals are found as long ago as the Ediacaran biota, towards the end of the Precambrian, and possibly somewhat earlier. It had long been doubted whether these life-forms included animals, but the discovery of the animal lipid cholesterol in fossils of Dickinsonia establishes their nature. Animals are thought to have originated under low-oxygen conditions, suggesting that they were capable of living entirely by anaerobic respiration, but as they became specialized for aerobic metabolism they became fully dependent on oxygen in their environments.Many animal phyla first appear in the fossil record during the Cambrian explosion, starting about 539 million years ago, in beds such as the Burgess shale. Extant phyla in these rocks include molluscs, brachiopods, onychophorans, tardigrades, arthropods, echinoderms and hemichordates, along with numerous now-extinct forms such as the predatory Anomalocaris. The apparent suddenness of the event may however be an artefact of the fossil record, rather than showing that all these animals appeared simultaneously. 
That view is supported by the discovery of Auroralumina attenboroughii, the earliest known Ediacaran crown-group cnidarian (557–562 mya, some 20 million years before the Cambrian explosion) from Charnwood Forest, England. It is thought to be one of the earliest predators, catching small prey with its nematocysts as modern cnidarians do.Some palaeontologists have suggested that animals appeared much earlier than the Cambrian explosion, possibly as early as 1 billion years ago. Early fossils that might represent animals appear for example in the 665-million-year-old rocks of the Trezona Formation of South Australia. These fossils are interpreted as most probably being early sponges.Trace fossils such as tracks and burrows found in the Tonian period (from 1 gya) may indicate the presence of triploblastic worm-like animals, roughly as large (about 5 mm wide) and complex as earthworms. However, similar tracks are produced today by the giant single-celled protist Gromia sphaerica, so the Tonian trace fossils may not indicate early animal evolution. Around the same time, the layered mats of microorganisms called stromatolites decreased in diversity, perhaps due to grazing by newly evolved animals. Objects such as sediment-filled tubes that resemble trace fossils of the burrows of wormlike animals have been found in 1.2 gya rocks in North America, in 1.5 gya rocks in Australia and North America, and in 1.7 gya rocks in Australia. Their interpretation as having an animal origin is disputed, as they might be water-escape or other structures.

		
		
		


== Phylogeny ==

Animals are monophyletic, meaning they are derived from a common ancestor. Animals are sister to the Choanoflagellata, with which they form the Choanozoa. The most basal animals, the Porifera, Ctenophora, Cnidaria, and Placozoa, have body plans that lack bilateral symmetry. Their relationships are still disputed; the sister group to all other animals could be the Porifera or the Ctenophora, both of which lack hox genes, important in body plan development.These genes are found in the Placozoa and the higher animals, the Bilateria. 6,331 groups of genes common to all living animals have been identified; these may have arisen from a single common ancestor that lived 650 million years ago in the Precambrian. 25 of these are novel core gene groups, found only in animals; of those, 8 are for essential components of the Wnt and TGF-beta signalling pathways which may have enabled animals to become multicellular by providing a pattern for the body's system of axes (in three dimensions), and another 7 are for transcription factors including homeodomain proteins involved in the control of development.The phylogenetic tree indicates approximately how many millions of years ago (mya) the lineages split.


=== Non-bilateria ===

Several animal phyla lack bilateral symmetry. Among these, the sponges (Porifera) probably diverged first, representing the oldest animal phylum. Sponges lack the complex organization found in most other animal phyla; their cells are differentiated, but in most cases not organised into distinct tissues. They typically feed by drawing in water through pores.The Ctenophora (comb jellies) and Cnidaria (which includes jellyfish, sea anemones, and corals) are radially symmetric and have digestive chambers with a single opening, which serves as both mouth and anus. They are sometimes placed together in the group Coelenterata because of common traits, not because of close relationships. Animals in both phyla have distinct tissues, but these are not organised into organs. They are diploblastic, having only two main germ layers, ectoderm and endoderm. The tiny placozoans are similar, but they do not have a permanent digestive chamber.


=== Bilateria ===

The remaining animals, the great majority—comprising some 29 phyla and over a million species—form a clade, the Bilateria, which have a bilaterally symmetric body plan. The Bilateria are triploblastic, with three well-developed germ layers, and their tissues form distinct organs. The digestive chamber has two openings, a mouth and an anus, and there is an internal body cavity, a coelom or pseudocoelom. These animals have a head end (anterior) and a tail end (posterior),  a back (dorsal) surface and a belly (ventral) surface, and a left and a right side.Having a front end means that this part of the body encounters stimuli, such as food, favouring cephalisation, the development of a head with sense organs and a mouth. Many bilaterians have a combination of circular muscles that constrict the body, making it longer, and an opposing set of longitudinal muscles, that shorten the body; these enable soft-bodied animals with a hydrostatic skeleton to move by peristalsis. They also have a gut that extends through the basically cylindrical body from mouth to anus. Many bilaterian phyla have primary larvae which swim with cilia and have an apical organ containing sensory cells. However, over evolutionary time, descendant spaces have evolved which have lost  one or more of each of these characteristics. For example, adult echinoderms are radially symmetric (unlike their larvae), while some parasitic worms have extremely simplified body structures.Genetic studies have considerably changed zoologists' understanding of the relationships within the Bilateria. Most appear to belong to two major lineages, the protostomes and the deuterostomes. The basalmost bilaterians are the Xenacoelomorpha.


==== Protostomes and deuterostomes ====

Protostomes and deuterostomes differ in several ways. Early in development, deuterostome embryos undergo radial cleavage during cell division, while many protostomes (the Spiralia) undergo spiral cleavage.
Animals from both groups possess a complete digestive tract, but in protostomes the first opening of the embryonic gut develops into the mouth, and the anus forms secondarily. In deuterostomes, the anus forms first while the mouth develops secondarily. Most protostomes have schizocoelous development, where cells simply fill in the interior of the gastrula to form the mesoderm. In deuterostomes, the mesoderm forms by enterocoelic pouching, through invagination of the endoderm.The main deuterostome phyla are the Echinodermata and the Chordata. Echinoderms are exclusively marine and include starfish, sea urchins, and sea cucumbers. The chordates are dominated by the vertebrates (animals with backbones), which consist of fishes, amphibians, reptiles, birds, and mammals. The deuterostomes also include the Hemichordata (acorn worms).


===== Ecdysozoa =====

The Ecdysozoa are protostomes, named after their shared trait of ecdysis, growth by moulting. They include the largest animal phylum, the Arthropoda, which contains insects, spiders, crabs, and their kin. All of these have a body divided into repeating segments, typically with paired appendages. Two smaller phyla, the Onychophora and Tardigrada, are close relatives of the arthropods and share these traits. The ecdysozoans also include the Nematoda or roundworms, perhaps the second largest animal phylum. Roundworms are typically microscopic, and occur in nearly every environment where there is water; some are important parasites. Smaller phyla related to them are the Nematomorpha or horsehair worms, and the Kinorhyncha, Priapulida, and Loricifera. These groups have a reduced coelom, called a pseudocoelom.


===== Spiralia =====

The Spiralia are a large group of protostomes that develop by spiral cleavage in the early embryo. The Spiralia's phylogeny has been disputed, but it contains a large clade, the superphylum Lophotrochozoa, and smaller groups of phyla such as the Rouphozoa which includes the gastrotrichs and the flatworms. All of these are grouped as the Platytrochozoa, which has a sister group, the Gnathifera, which includes the rotifers.The Lophotrochozoa includes the molluscs, annelids, brachiopods, nemerteans, bryozoa and entoprocts. The molluscs, the second-largest animal phylum by number of described species, includes snails, clams, and squids, while the annelids are the segmented worms, such as earthworms, lugworms, and leeches. These two groups have long been considered close relatives because they share trochophore larvae.


== History of classification ==

In the classical era, Aristotle divided animals, based on his own observations, into those with blood (roughly, the vertebrates) and those without. The animals were then arranged on a scale from man (with blood, 2 legs, rational soul) down through the live-bearing tetrapods (with blood, 4 legs, sensitive soul) and other groups such as crustaceans (no blood, many legs, sensitive soul) down to spontaneously generating creatures like sponges (no blood, no legs, vegetable soul). Aristotle was uncertain whether sponges were animals, which in his system ought to have sensation, appetite, and locomotion, or plants, which did not: he knew that sponges could sense touch, and would contract if about to be pulled off their rocks, but that they were rooted like plants and never moved about.In 1758, Carl Linnaeus created the first hierarchical classification in his Systema Naturae. In his original scheme, the animals were one of three kingdoms, divided into the classes of Vermes, Insecta, Pisces, Amphibia, Aves, and Mammalia. Since then the last four have all been subsumed into a single phylum, the Chordata, while his Insecta (which included the crustaceans and arachnids) and Vermes have been renamed or broken up. The process was begun in 1793 by Jean-Baptiste de Lamarck, who called the Vermes une espèce de chaos (a chaotic mess) and split the group into three new phyla: worms, echinoderms, and polyps (which contained corals and jellyfish). By 1809, in his Philosophie Zoologique, Lamarck had created 9 phyla apart from vertebrates (where he still had 4 phyla: mammals, birds, reptiles, and fish) and molluscs, namely cirripedes, annelids, crustaceans, arachnids, insects, worms, radiates, polyps, and infusorians.In his 1817 Le Règne Animal, Georges Cuvier used comparative anatomy to group the animals into four embranchements (""branches"" with different body plans, roughly corresponding to phyla), namely vertebrates, molluscs, articulated animals (arthropods and annelids), and zoophytes (radiata) (echinoderms, cnidaria and other forms). This division into four was followed by the embryologist Karl Ernst von Baer in 1828, the zoologist Louis Agassiz in 1857, and the comparative anatomist Richard Owen in 1860.In 1874, Ernst Haeckel divided the animal kingdom into two subkingdoms: Metazoa (multicellular animals, with five phyla: coelenterates, echinoderms, articulates, molluscs, and vertebrates) and Protozoa (single-celled animals), including a sixth animal phylum, sponges. The protozoa were later moved to the former kingdom Protista, leaving only the Metazoa as a synonym of Animalia.


== In human culture ==


=== Practical uses ===

The human population exploits a large number of other animal species for food, both of domesticated livestock species in animal husbandry and, mainly at sea, by hunting wild species. Marine fish of many species are caught commercially for food. A smaller number of species are farmed commercially. Humans and their livestock make up more than 90% of the biomass of all terrestrial vertebrates, and almost as much as all insects combined.Invertebrates including cephalopods, crustaceans, and bivalve or gastropod molluscs are hunted or farmed for food. Chickens, cattle, sheep, pigs, and other animals are raised as livestock for meat across the world.
Animal fibres such as wool are used to make textiles, while animal sinews have been used as lashings and bindings, and leather is widely used to make shoes and other items. Animals have been hunted and farmed for their fur to make items such as coats and hats. Dyestuffs including carmine (cochineal), shellac, and kermes have been made from the bodies of insects. Working animals including cattle and horses have been used for work and transport from the first days of agriculture.Animals such as the fruit fly Drosophila melanogaster serve a major role in science as experimental models. Animals have been used to create vaccines since their discovery in the 18th century. Some medicines such as the cancer drug Yondelis are based on toxins or other molecules of animal origin.

People have used hunting dogs to help chase down and retrieve animals, and birds of prey to catch birds and mammals, while tethered cormorants have been used to catch fish. Poison dart frogs have been used to poison the tips of blowpipe darts.
A wide variety of animals are kept as pets, from invertebrates such as tarantulas and octopuses, insects including praying mantises, reptiles such as snakes and chameleons, and birds including canaries, parakeets, and parrots all finding a place. However, the most kept pet species are mammals, namely dogs, cats, and rabbits. There is a tension between the role of animals as companions to humans, and their existence as individuals with rights of their own.
A wide variety of terrestrial and aquatic animals are hunted for sport.


=== Symbolic uses ===

Animals have been the subjects of art from the earliest times, both historical, as in Ancient Egypt, and prehistoric, as in the cave paintings at Lascaux. Major animal paintings include Albrecht Dürer's 1515 The Rhinoceros, and George Stubbs's c. 1762 horse portrait Whistlejacket. Insects, birds and mammals play roles in literature and film, such as in giant bug movies.Animals including insects and mammals feature in mythology and religion. In both Japan and Europe, a butterfly was seen as the personification of a person's soul, while the scarab beetle was sacred in ancient Egypt. Among the mammals, cattle, deer, horses, lions, bats, bears, and wolves are the subjects of myths and worship. The signs of the Western and Chinese zodiacs are based on animals.


== See also ==
Animal attacks
Animal coloration
Ethology
Fauna
List of animal names
Lists of organisms by population


== Notes ==


== References ==


== External links ==

Tree of Life Project Archived 12 June 2011 at the Wayback Machine
Animal Diversity Web – University of Michigan's database of animals
ARKive – multimedia database of endangered/protected species",22140736,7164,"All articles containing potentially dated statements, Animals, Articles containing Latin-language text, Articles containing potentially dated statements from 2013, Articles with 'species' microformats, Articles with BNF identifiers, Articles with GND identifiers, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with NARA identifiers, Articles with NDL identifiers, Articles with NKC identifiers, Articles with TDVİA identifiers, Articles with short description, CS1: long volume value, CS1 German-language sources (de), CS1 Latin-language sources (la), Commons category link is on Wikidata, Cryogenian first appearances, Good articles, Kingdoms (biology), Short description is different from Wikidata, Taxa named by Carl Linnaeus, Use British English from April 2017, Use dmy dates from September 2019, Webarchive template wayback links, Wikipedia indefinitely move-protected pages, Wikipedia indefinitely semi-protected pages",1134364335,biology
https://en.wikipedia.org/wiki/Developmental_biology,Developmental biology,"Developmental biology is the study of the process by which animals and plants grow and develop. Developmental biology also encompasses the biology of regeneration, asexual reproduction, metamorphosis, and the growth and differentiation of stem cells in the adult organism.

","Developmental biology is the study of the process by which animals and plants grow and develop. Developmental biology also encompasses the biology of regeneration, asexual reproduction, metamorphosis, and the growth and differentiation of stem cells in the adult organism.


== Perspectives ==
The main processes involved in the embryonic development of animals are: tissue patterning (via regional specification and patterned cell differentiation); tissue growth; and tissue morphogenesis.

Regional specification refers to the processes that create the spatial patterns in a ball or sheet of initially similar cells. This generally involves the action of cytoplasmic determinants, located within parts of the fertilized egg, and of inductive signals emitted from signaling centers in the embryo. The early stages of regional specification do not generate functional differentiated cells, but cell populations committed to developing to a specific region or part of the organism. These are defined by the expression of specific combinations of transcription factors.
Cell differentiation relates specifically to the formation of functional cell types such as nerve, muscle, secretory epithelia, etc. Differentiated cells contain large amounts of specific proteins associated with cell function.
Morphogenesis relates to the formation of a three-dimensional shape. It mainly involves the orchestrated movements of cell sheets and of individual cells. Morphogenesis is important for creating the three germ layers of the early embryo (ectoderm, mesoderm, and endoderm) and for building up complex structures during organ development.
Tissue growth involves both an overall increase in tissue size, and also the differential growth of parts (allometry) which contributes to morphogenesis. Growth mostly occurs through cell proliferation but also through changes in cell size or the deposition of extracellular materials.The development of plants involves similar processes to that of animals. However, plant cells are mostly immotile so morphogenesis is achieved by differential growth, without cell movements. Also, the inductive signals and the genes involved are different from those that control animal development.


== Developmental processes ==


=== Cell differentiation ===

Cell differentiation is the process whereby different functional cell types arise in development. For example, neurons, muscle fibers and hepatocytes (liver cells) are well known types of differentiated cells. Differentiated cells usually produce large amounts of a few proteins that are required for their specific function and this gives them the characteristic appearance that enables them to be recognized under the light microscope. The genes encoding these proteins are highly active. Typically their chromatin structure is very open, allowing access for the transcription enzymes, and specific transcription factors bind to regulatory sequences in the DNA in order to activate gene expression. For example, NeuroD is a key transcription factor for neuronal differentiation, myogenin for muscle differentiation, and HNF4 for hepatocyte differentiation.
Cell differentiation is usually the final stage of development, preceded by several states of commitment which are not visibly differentiated. A single tissue, formed from a single type of progenitor cell or stem cell, often consists of several differentiated cell types. Control of their formation involves a process of lateral inhibition, based on the properties of the Notch signaling pathway. For example, in the neural plate of the embryo this system operates to generate a population of neuronal precursor cells in which NeuroD is highly expressed.


=== Regeneration ===
Regeneration indicates the ability to regrow a missing part. This is very prevalent amongst plants, which show continuous growth, and also among colonial animals such as hydroids and ascidians. But most interest by developmental biologists has been shown in the regeneration of parts in free living animals. In particular four models have been the subject of much investigation. Two of these have the ability to regenerate whole bodies: Hydra, which can regenerate any part of the polyp from a small fragment, and planarian worms, which can usually regenerate both heads and tails. Both of these examples have continuous cell turnover fed by stem cells and, at least in planaria, at least some of the stem cells have been shown to be pluripotent. The other two models show only distal regeneration of appendages. These are the insect appendages, usually the legs of hemimetabolous insects such as the cricket, and the limbs of urodele amphibians. Considerable information is now available about amphibian limb regeneration and it is known that each cell type regenerates itself, except for connective tissues where there is considerable interconversion between cartilage, dermis and tendons. In terms of the pattern of structures, this is controlled by a re-activation of signals active in the embryo.
There is still debate about the old question of whether regeneration is a ""pristine"" or an ""adaptive"" property. If the former is the case, with improved knowledge, we might expect to be able to improve regenerative ability in humans. If the latter, then each instance of regeneration is presumed to have arisen by natural selection in circumstances particular to the species, so no general rules would be expected.


== Embryonic development of animals ==

The sperm and egg fuse in the process of fertilization to form a fertilized egg, or zygote. This undergoes a period of divisions to form a ball or sheet of similar cells called a blastula or blastoderm. These cell divisions are usually rapid with no growth so the daughter cells are half the size of the mother cell and the whole embryo stays about the same size. They are called cleavage divisions.
Mouse epiblast primordial germ cells (see Figure: “The initial stages of human embryogenesis”) undergo extensive epigenetic reprogramming. This process involves genome-wide DNA demethylation, chromatin reorganization and epigenetic imprint erasure leading to totipotency. DNA demethylation is carried out by a process that utilizes the DNA base excision repair pathway.Morphogenetic movements convert the cell mass into a three layered structure consisting of multicellular sheets called ectoderm, mesoderm and endoderm. These sheets are known as germ layers. This is the process of gastrulation. During cleavage and gastrulation the first regional specification events occur. In addition to the formation of the three germ layers themselves, these often generate extraembryonic structures, such as the mammalian placenta, needed for support and nutrition of the embryo, and also establish differences of commitment along the anteroposterior axis (head, trunk and tail).Regional specification is initiated by the presence of cytoplasmic determinants in one part of the zygote. The cells that contain the determinant become a signaling center and emit an inducing factor. Because the inducing factor is produced in one place, diffuses away, and decays, it forms a concentration gradient, high near the source cells and low further away. The remaining cells of the embryo, which do not contain the determinant, are competent to respond to different concentrations by upregulating specific developmental control genes. This results in a series of zones becoming set up, arranged at progressively greater distance from the signaling center. In each zone a different combination of developmental control genes is upregulated. These genes encode transcription factors which upregulate new combinations of gene activity in each region. Among other functions, these transcription factors control expression of genes conferring specific adhesive and motility properties on the cells in which they are active. Because of these different morphogenetic properties, the cells of each germ layer move to form sheets such that the ectoderm ends up on the outside, mesoderm in the middle, and endoderm on the inside. Morphogenetic movements not only change the shape and structure of the embryo, but by bringing cell sheets into new spatial relationships they also make possible new phases of signaling and response between them.
Growth in embryos is mostly autonomous. For each territory of cells the growth rate is controlled by the combination of genes that are active. Free-living embryos do not grow in mass as they have no external food supply. But embryos fed by a placenta or extraembryonic yolk supply can grow very fast, and changes to relative growth rate between parts in these organisms help to produce the final overall anatomy.
The whole process needs to be coordinated in time and how this is controlled is not understood. There may be a master clock able to communicate with all parts of the embryo that controls the course of events, or timing may depend simply on local causal sequences of events.


=== Metamorphosis ===
Developmental processes are very evident during the process of metamorphosis. This occurs in various types of animal. Well-known examples are seen in frogs, which usually hatch as a tadpole and metamorphoses to an adult frog, and certain insects which hatch as a larva and then become remodeled to the adult form during a pupal stage.
All the developmental processes listed above occur during metamorphosis. Examples that have been especially well studied include tail loss and other changes in the tadpole of the frog Xenopus, and the biology of the imaginal discs, which generate the adult body parts of the fly Drosophila melanogaster.


== Plant development ==

Plant development is the process by which structures originate and mature as a plant grows. It is studied in plant anatomy and plant physiology as well as plant morphology.
Plants constantly produce new tissues and structures throughout their life from meristems located at the tips of organs, or between mature tissues. Thus, a living plant always has embryonic tissues. By contrast, an animal embryo will very early produce all of the body parts that it will ever have in its life. When the animal is born (or hatches from its egg), it has all its body parts and from that point will only grow larger and more mature.
The properties of organization seen in a plant are emergent properties which are more than the sum of the individual parts. ""The assembly of these tissues and functions into an integrated multicellular organism yields not only the characteristics of the separate parts and processes but also quite a new set of characteristics which would not have been predictable on the basis of examination of the separate parts.""


=== Growth ===
A vascular plant begins from a single celled zygote, formed by fertilisation of an egg cell by a sperm cell. From that point, it begins to divide to form a plant embryo through the process of embryogenesis. As this happens, the resulting cells will organize so that one end becomes the first root, while the other end forms the tip of the shoot. In seed plants, the embryo will develop one or more ""seed leaves"" (cotyledons). By the end of embryogenesis, the young plant will have all the parts necessary to begin its life.
Once the embryo germinates from its seed or parent plant, it begins to produce additional organs (leaves, stems, and roots) through the process of organogenesis. New roots grow from root meristems located at the tip of the root, and new stems and leaves grow from shoot meristems located at the tip of the shoot. Branching occurs when small clumps of cells left behind by the meristem, and which have not yet undergone cellular differentiation to form a specialized tissue, begin to grow as the tip of a new root or shoot. Growth from any such meristem at the tip of a root or shoot is termed primary growth and results in the lengthening of that root or shoot. Secondary growth results in widening of a root or shoot from divisions of cells in a cambium.In addition to growth by cell division, a plant may grow through cell elongation. This occurs when individual cells or groups of cells grow longer. Not all plant cells will grow to the same length. When cells on one side of a stem grow longer and faster than cells on the other side, the stem will bend to the side of the slower growing cells as a result. This directional growth can occur via a plant's response to a particular stimulus, such as light (phototropism), gravity (gravitropism), water, (hydrotropism), and physical contact (thigmotropism).
Plant growth and development are mediated by specific plant hormones and plant growth regulators (PGRs) (Ross et al. 1983). Endogenous hormone levels are influenced by plant age, cold hardiness, dormancy, and other metabolic conditions; photoperiod, drought, temperature, and other external environmental conditions; and exogenous sources of PGRs, e.g., externally applied and of rhizospheric origin.


=== Morphological variation ===
Plants exhibit natural variation in their form and structure. While all organisms vary from individual to individual, plants exhibit an additional type of variation. Within a single individual, parts are repeated which may differ in form and structure from other similar parts. This variation is most easily seen in the leaves of a plant, though other organs such as stems and flowers may show similar variation. There are three primary causes of this variation: positional effects, environmental effects, and juvenility.


=== Evolution of plant morphology ===
Transcription factors and transcriptional regulatory networks play key roles in plant morphogenesis and their evolution. During plant landing, many novel transcription factor families emerged and are preferentially wired into the networks of multicellular development, reproduction, and organ development, contributing to more complex morphogenesis of land plants.Most land plants share a common ancestor, multicellular algae. An example of the evolution of plant morphology is seen in charophytes. Studies have shown that charophytes have traits that are homologous to land plants. There are two main theories of the evolution of plant morphology, these theories are the homologous theory and the antithetic theory. The commonly accepted theory for the evolution of plant morphology is the antithetic theory. The antithetic theory states that the multiple mitotic divisions that take place before meiosis, cause the development of the sporophyte. Then the sporophyte will development as an independent organism.


== Developmental model organisms ==
Much of developmental biology research in recent decades has focused on the use of a small number of model organisms. It has turned out that there is much conservation of developmental mechanisms across the animal kingdom. In early development different vertebrate species all use essentially the same inductive signals and the same genes encoding regional identity. Even invertebrates use a similar repertoire of signals and genes although the body parts formed are significantly different. Model organisms each have some particular experimental advantages which have enabled them to become popular among researchers. In one sense they are ""models"" for the whole animal kingdom, and in another sense they are ""models"" for human development, which is difficult to study directly for both ethical and practical reasons. Model organisms have been most useful for elucidating the broad nature of developmental mechanisms. The more detail is sought, the more they differ from each other and from humans.


=== Plants ===
Thale cress (Arabidopsis thaliana)


=== Vertebrates ===
Frog: Xenopus (X. laevis and X. tropicalis). Good embryo supply. Especially suitable for microsurgery.
Zebrafish: Danio rerio. Good embryo supply. Well developed genetics.
Chicken: Gallus gallus. Early stages similar to mammal, but microsurgery easier. Low cost.
Mouse: Mus musculus. A mammal with well developed genetics.


=== Invertebrates ===
Fruit fly: Drosophila melanogaster. Good embryo supply. Well developed genetics.
Nematode: Caenorhabditis elegans. Good embryo supply. Well developed genetics. Low cost.


=== Unicellular ===
Algae: Chlamydomonas
Yeast: Saccharomyces


=== Others ===
Also popular for some purposes have been sea urchins and ascidians. For studies of regeneration urodele amphibians such as the axolotl Ambystoma mexicanum are used, and also planarian worms such as Schmidtea mediterranea. Organoids have also been demonstrated as an efficient model for development. Plant development has focused on the thale cress Arabidopsis thaliana as a model organism....


== See also ==


== References ==


== Further reading ==


== External links ==

Society for Developmental Biology
Collaborative resources
Developmental Biology - 10th edition
Essential Developmental Biology 3rd edition",889097,631,"Articles with BNF identifiers, Articles with EMU identifiers, Articles with GND identifiers, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with LNB identifiers, Articles with NKC identifiers, Articles with short description, Commons category link from Wikidata, Developmental biology, Philosophy of biology, Short description is different from Wikidata",1132620921,biology
https://en.wikipedia.org/wiki/Medical_biology,Medical biology,"Medical biology is a field of biology that has practical applications in medicine, health care and laboratory diagnostics. It includes many biomedical disciplines and areas of specialty that typically contains the ""bio-"" prefix such as:

molecular biology, biochemistry, biophysics, biotechnology, cell biology, embryology,
nanobiotechnology, biological engineering, laboratory medical biology,
cytogenetics, genetics, gene therapy,
bioinformatics, biostatistics, systems biology,
microbiology, virology, parasitology,
physiology, pathology,
toxicology, and many others that generally concern life sciences as applied to medicine.Medical biology is the cornerstone of modern health care and laboratory diagnostics. It concernes a wide range of scientific and technological approaches: from an in vitro diagnostics to the in vitro fertilisation, from the molecular mechanisms of a cystic fibrosis to the population dynamics of the HIV, from the understanding molecular interactions to the study of the carcinogenesis, from a single-nucleotide polymorphism (SNP) to the gene therapy.
Medical biology based on molecular biology combines all issues of developing molecular medicine into large-scale structural and functional relationships of the human genome, transcriptome, proteome and metabolome with the particular point of view of devising new technologies for prediction, diagnosis and therapy.","Medical biology is a field of biology that has practical applications in medicine, health care and laboratory diagnostics. It includes many biomedical disciplines and areas of specialty that typically contains the ""bio-"" prefix such as:

molecular biology, biochemistry, biophysics, biotechnology, cell biology, embryology,
nanobiotechnology, biological engineering, laboratory medical biology,
cytogenetics, genetics, gene therapy,
bioinformatics, biostatistics, systems biology,
microbiology, virology, parasitology,
physiology, pathology,
toxicology, and many others that generally concern life sciences as applied to medicine.Medical biology is the cornerstone of modern health care and laboratory diagnostics. It concernes a wide range of scientific and technological approaches: from an in vitro diagnostics to the in vitro fertilisation, from the molecular mechanisms of a cystic fibrosis to the population dynamics of the HIV, from the understanding molecular interactions to the study of the carcinogenesis, from a single-nucleotide polymorphism (SNP) to the gene therapy.
Medical biology based on molecular biology combines all issues of developing molecular medicine into large-scale structural and functional relationships of the human genome, transcriptome, proteome and metabolome with the particular point of view of devising new technologies for prediction, diagnosis and therapy.


== See also ==


== External links ==


== References ==",114075,52,"Biomedicine, Clinical medicine, Webarchive template wayback links",1133343338,biology
https://en.wikipedia.org/wiki/Soma_(biology),Soma (biology),"In cellular neuroscience, the soma (pl. somata or somas; from Greek  σῶμα (sôma) 'body'), perikaryon (pl. perikarya), neurocyton, or cell body is the bulbous, non-process portion of a neuron or other brain cell type, containing the cell nucleus. Although it is often used to refer to neurons, it can also refer to other cell types as well, including astrocytes, oligodendrocytes, and microglia. There are many different specialized types of neurons, and their sizes vary from as small as about 5 micrometres to over 10 millimetres for some of the smallest and largest neurons of invertebrates, respectively.

The soma of a neuron (i.e., the main part of the neuron in which the dendrites branch off of) contains many organelles, including granules called Nissl granules, which are composed largely of rough endoplasmic reticulum and free polyribosomes. The cell nucleus is a key feature of the soma. The nucleus is the source of most of the RNA that is produced in neurons. In general, most proteins are produced from mRNAs that do not travel far from the cell nucleus. This creates a challenge for supplying new proteins to axon endings that can be a meter or more away from the soma. Axons contain microtubule-associated motor proteins that transport protein-containing vesicles between the soma and the synapses at the axon terminals. Such transport of molecules towards and away from the soma maintains critical cell functions. In case of neurons, the soma receives a large number of inhibitory synapses, which can regulate the activity of these cells. It has also been shown, that microglial processes constantly monitor neuronal functions through somatic junctions, and exert neuroprotection when needed.The axon hillock is a specialized domain of the neuronal cell body from which the axon originates. A high amount of protein synthesis occurs in this region, as it contains many Nissl granules (which are ribosomes wrapped in RER) and polyribosomes. Within the axon hillock, materials are sorted as either items that will enter the axon (like the components of the cytoskeletal architecture of the axon, mitochondria, etc.) or will remain in the soma. In addition, the axon hillock also has a specialized plasma membrane that contains large numbers of voltage-gated ion channels, since this is most often the site of action potential initiation.The survival of some sensory neurons depends on axon terminals making contact with sources of survival factors that prevent apoptosis. The survival factors are neurotrophic factors, including molecules such as nerve growth factor (NGF). NGF interacts with receptors at axon terminals, and this produces a signal that must be transported up the length of the axon to the nucleus. A current theory of how such survival signals are sent from axon endings to the soma includes the idea that NGF receptors are endocytosed from the surface of axon tips and that such endocytotic vesicles are transported up the axon.","In cellular neuroscience, the soma (pl. somata or somas; from Greek  σῶμα (sôma) 'body'), perikaryon (pl. perikarya), neurocyton, or cell body is the bulbous, non-process portion of a neuron or other brain cell type, containing the cell nucleus. Although it is often used to refer to neurons, it can also refer to other cell types as well, including astrocytes, oligodendrocytes, and microglia. There are many different specialized types of neurons, and their sizes vary from as small as about 5 micrometres to over 10 millimetres for some of the smallest and largest neurons of invertebrates, respectively.

The soma of a neuron (i.e., the main part of the neuron in which the dendrites branch off of) contains many organelles, including granules called Nissl granules, which are composed largely of rough endoplasmic reticulum and free polyribosomes. The cell nucleus is a key feature of the soma. The nucleus is the source of most of the RNA that is produced in neurons. In general, most proteins are produced from mRNAs that do not travel far from the cell nucleus. This creates a challenge for supplying new proteins to axon endings that can be a meter or more away from the soma. Axons contain microtubule-associated motor proteins that transport protein-containing vesicles between the soma and the synapses at the axon terminals. Such transport of molecules towards and away from the soma maintains critical cell functions. In case of neurons, the soma receives a large number of inhibitory synapses, which can regulate the activity of these cells. It has also been shown, that microglial processes constantly monitor neuronal functions through somatic junctions, and exert neuroprotection when needed.The axon hillock is a specialized domain of the neuronal cell body from which the axon originates. A high amount of protein synthesis occurs in this region, as it contains many Nissl granules (which are ribosomes wrapped in RER) and polyribosomes. Within the axon hillock, materials are sorted as either items that will enter the axon (like the components of the cytoskeletal architecture of the axon, mitochondria, etc.) or will remain in the soma. In addition, the axon hillock also has a specialized plasma membrane that contains large numbers of voltage-gated ion channels, since this is most often the site of action potential initiation.The survival of some sensory neurons depends on axon terminals making contact with sources of survival factors that prevent apoptosis. The survival factors are neurotrophic factors, including molecules such as nerve growth factor (NGF). NGF interacts with receptors at axon terminals, and this produces a signal that must be transported up the length of the axon to the nucleus. A current theory of how such survival signals are sent from axon endings to the soma includes the idea that NGF receptors are endocytosed from the surface of axon tips and that such endocytotic vesicles are transported up the axon.


== References ==


== External links ==
Histology image: 3_09 at the University of Oklahoma Health Sciences Center - ""Slide 3 Spinal cord""",529223,189,"All articles needing additional references, Articles needing additional references from December 2008, Articles with TA98 identifiers, Articles with short description, Neurohistology, Short description is different from Wikidata",1130635429,biology
https://en.wikipedia.org/wiki/Agglutination_(biology),Agglutination (biology),"Agglutination is the clumping of particles.  The word agglutination comes from the Latin agglutinare (glueing to).
Agglutination is the process that occurs if an antigen is mixed with its corresponding antibody called isoagglutinin. This term is commonly used in blood grouping.
This occurs in biology in two main examples:

The clumping of cells such as bacteria or red blood cells in the presence of an antibody or complement. The antibody or other molecule binds multiple particles and joins them, creating a large complex. This increases the efficacy of microbial elimination by phagocytosis as large clumps of bacteria can be eliminated in one pass, versus the elimination of single microbial antigens.
When people are given blood transfusions of the wrong blood group, the antibodies react with the incorrectly transfused blood group and as a result, the erythrocytes clump up and stick together causing them to agglutinate. The coalescing of small particles that are suspended in a solution; these larger masses are then (usually) precipitated.

","Agglutination is the clumping of particles.  The word agglutination comes from the Latin agglutinare (glueing to).
Agglutination is the process that occurs if an antigen is mixed with its corresponding antibody called isoagglutinin. This term is commonly used in blood grouping.
This occurs in biology in two main examples:

The clumping of cells such as bacteria or red blood cells in the presence of an antibody or complement. The antibody or other molecule binds multiple particles and joins them, creating a large complex. This increases the efficacy of microbial elimination by phagocytosis as large clumps of bacteria can be eliminated in one pass, versus the elimination of single microbial antigens.
When people are given blood transfusions of the wrong blood group, the antibodies react with the incorrectly transfused blood group and as a result, the erythrocytes clump up and stick together causing them to agglutinate. The coalescing of small particles that are suspended in a solution; these larger masses are then (usually) precipitated.


== In immunohematology ==


=== Hemagglutination ===

Hemagglutination is the process by which red blood cells agglutinate, meaning clump or clog. The agglutin involved in hemagglutination is called hemagglutinin.  In cross-matching, donor red blood cells and the recipient's serum or plasma are incubated together.  If agglutination occurs, this indicates that the donor and recipient blood types are incompatible.
When a person produces antibodies against their own red blood cells, as in cold agglutinin disease and other autoimmune conditions, the cells may agglutinate spontaneously. This is called autoagglutination and it can interfere with laboratory tests such as blood typing and the complete blood count.


=== Leukoagglutination ===
Leukoagglutination occurs when the particles involved are white blood cells.
An example is the PH-L form of phytohaemagglutinin.


== In microbiology ==
Agglutination is commonly used as a method of identifying specific bacterial antigens and the identity of such bacteria, and therefore is an important technique in diagnosis.


== History of discoveries ==
Two bacteriologists, Herbert Edward Durham (-1945) and Max von Gruber (1853–1927), discovered specific agglutination in 1896. The clumping became known as Gruber-Durham reaction. Gruber introduced the term agglutinin (from the Latin) for any substance that caused agglutination of cells.
French physician Fernand Widal (1862–1929) put Gruber and Durham's discovery to practical use later in 1896, using the reaction as the basis for a test for typhoid fever. Widal found that blood serum from a typhoid carrier caused a culture of typhoid bacteria to clump, whereas serum from a typhoid-free person did not. This Widal test was the first example of serum diagnosis.
Austrian physician Karl Landsteiner found another important practical application of the agglutination reaction in 1900. Landsteiner's agglutination tests and his discovery of ABO blood groups was the start of the science of blood transfusion and serology which has made transfusion possible and safer.


== See also ==
Agglutination-PCR
Blocking antibody
Coagulation
Immune system
Macrophage
Mannan oligosaccharides (MOS)


== References ==",1131770,180,"All articles needing additional references, Articles needing additional references from June 2015, Articles with BNF identifiers, Articles with short description, Hematology, Immunologic tests, Short description is different from Wikidata",1109893047,biology
https://en.wikipedia.org/wiki/Human_biology,Human biology,"Human biology is an interdisciplinary area of academic study that examines humans through the influences and interplay of many diverse fields such as genetics, evolution, physiology, anatomy, epidemiology, anthropology, ecology, nutrition, population genetics, and sociocultural influences. It is closely related to the biomedical sciences, biological anthropology and other biological fields tying in various aspects of human functionality. It wasn't until the 20th century when biogerontologist, Raymond Pearl, founder of the journal Human Biology, phrased the term ""human biology"" in a way to describe a separate subsection apart from biology.It is also a portmanteau term that describes all biological aspects of the human body, typically using the human body as a type organism for Mammalia, and in that context it is the basis for many undergraduate University degrees and modules.Most aspects of human biology are identical or very similar to general mammalian biology. In particular, and as examples, humans : 

maintain their body temperature
have an internal skeleton
have a circulatory system
have a nervous system to provide sensory information and operate and coordinate muscular activity.
have a reproductive system in which they bear live young and produce milk.
have an endocrine system and produce and eliminate hormones and other bio-chemical signalling agents
have a respiratory system where air is inhaled into lungs and oxygen is used to produce energy.
have an immune system to protect against disease
Excrete waste as urine and faeces.","Human biology is an interdisciplinary area of academic study that examines humans through the influences and interplay of many diverse fields such as genetics, evolution, physiology, anatomy, epidemiology, anthropology, ecology, nutrition, population genetics, and sociocultural influences. It is closely related to the biomedical sciences, biological anthropology and other biological fields tying in various aspects of human functionality. It wasn't until the 20th century when biogerontologist, Raymond Pearl, founder of the journal Human Biology, phrased the term ""human biology"" in a way to describe a separate subsection apart from biology.It is also a portmanteau term that describes all biological aspects of the human body, typically using the human body as a type organism for Mammalia, and in that context it is the basis for many undergraduate University degrees and modules.Most aspects of human biology are identical or very similar to general mammalian biology. In particular, and as examples, humans : 

maintain their body temperature
have an internal skeleton
have a circulatory system
have a nervous system to provide sensory information and operate and coordinate muscular activity.
have a reproductive system in which they bear live young and produce milk.
have an endocrine system and produce and eliminate hormones and other bio-chemical signalling agents
have a respiratory system where air is inhaled into lungs and oxygen is used to produce energy.
have an immune system to protect against disease
Excrete waste as urine and faeces.


== History ==


== Typical human attributes ==
The key aspects of human biology are those ways in which humans are substantially different from other mammals.Humans have a very large brain in a head that is very large for the size of the animal. This large brain has enabled a range of unique attributes including the development of complex languages and the ability to make and use a complex range of tools.The upright stance and bipedal locomotion is not unique to humans but humans are the only species to rely almost exclusively on this mode of locomotion. This has resulted in significant changes in the structure of the skeleton including the articulation of the pelvis and the femur and in the articulation of the head.
In comparison with most other mammals, humans are very long lived with an average age at death in the developed world of nearly 80 years old. Humans also have the longest childhood of any mammal with sexual maturity taking 12 to 16 years on average to be completed.
Humans lack fur. Although there is a residual covering of fine hair, which may be more developed in some men, and localised hair covering on the head, axillary and pubic regions, in terms of protection from cold, humans are almost naked. The reason for this development is still much debated.
The human eye can see objects in colour but is not well adapted to low light conditions. The sense of smell and of taste are present but are relatively inferior to a wide range of other mammals. Human hearing is efficient but lacks the acuity of some other mammals. Similarly human sense of touch is well developed especially in the hands where dextrous tasks are performed but the sensitivity is still significantly less than in other animals, particularly those equipped with sensory bristles such as cats.


== Scientific investigation ==
Human biology tries to understand and promotes research on humans as living beings as a scientific discipline. It makes use of various scientific methods, such as experiments and observations, to detail the biochemical and biophysical foundations of human life describe and formulate the underlying processes using models. As a basic science, it provides the knowledge base for medicine. A number of sub-disciplines include anatomy, cytology, histology and morphology.


== Medicine ==
The capabilities of the human brain and the human dexterity in making and using tools, has enabled humans to understand their own biology through scientific experiment, including dissection , autopsy, prophylactic medicine which has, in turn, enable humans to extend their life-span by understanding and mitigating the effects of diseases.
Understanding human biology has enabled and fostered a wider understanding of mammalian biology and by extension, the biology of all living organisms.


== Nutrition ==

Human nutrition is typical of mammalian omnivorous nutrition requiring a balanced input of carbohydrates, fats, proteins, vitamins, and minerals. However, the human diet has a few very specific requirements. These include two specific amino acids, alpha-linolenic acid and linoleic acid without which life is not sustainable in the medium to long term. All other fatty acids can be synthesized from dietary fats. Similarly, human life requires a range of vitamins to be present in food and if these are missing or are supplied at unacceptably low levels, metabolic disorders result which can end in death. The human metabolism is similar to most other mammals except for the need to have an intake of Vitamin C to prevent scurvy and other deficiency diseases. Unusually amongst mammals, a human can synthesize Vitamin D3 using natural UV light from the sun on the skin. This capability may be widespread in the mammalian world but few other mammals share the almost naked skin of humans. The darker the human's skin, the less it can manufacture Vitamin D3.


== Other organisms ==
Human biology also encompasses all those organisms that live on or in the human body. Such organisms range from parasitic insects such as fleas and ticks, parasitic helminths such as liver flukes through to bacterial and viral pathogens. Many of the organisms associated with human biology are the specialised biome in the large intestine and the biotic flora of the skin and pharyngeal and nasal region. Many of these biotic assemblages help protect humans from harm and assist in digestion, and are now known to have complex effects on mood, and well-being.


== Social behaviour ==
Humans in all civilizations are social animals and use their language skills and tool making skills to communicate.
These communication skills enable civilizations to grow and allow for the production of art, literature and music, and for the development of technology. All of these are wholly dependent on the human biological specialisms.
The deployment of these skills has allowed the human race to dominate the terrestrial biome to the detriment of most of the other species.


== References ==


== External links ==
Human Biology Association
Biology Dictionary",471528,432,"All articles to be expanded, Articles to be expanded from December 2022, Articles using small message boxes, Articles with BNF identifiers, Articles with GND identifiers, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with NARA identifiers, Articles with NKC identifiers, Articles with short description, Human biology, Short description matches Wikidata",1132883957,biology
https://en.wikipedia.org/wiki/Colonisation_(biology),Colonisation (biology),"Colonisation or colonization is the process in biology by which a species spreads to new areas. Colonisation often refers to successful immigration where a population becomes integrated into an ecological community, having resisted initial local extinction. In ecology, it is represented by the symbol λ (lowercase lambda) to denote the long-term intrinsic growth rate of a population.
One classic scientific model in biogeography posits that species must continue to colonize new areas through its life cycle (called a taxon cycle) in order to achieve longevity. Accordingly, colonisation and extinction are key components of island biogeography, a theory that has many applications in ecology, such as metapopulations.

","Colonisation or colonization is the process in biology by which a species spreads to new areas. Colonisation often refers to successful immigration where a population becomes integrated into an ecological community, having resisted initial local extinction. In ecology, it is represented by the symbol λ (lowercase lambda) to denote the long-term intrinsic growth rate of a population.
One classic scientific model in biogeography posits that species must continue to colonize new areas through its life cycle (called a taxon cycle) in order to achieve longevity. Accordingly, colonisation and extinction are key components of island biogeography, a theory that has many applications in ecology, such as metapopulations.


== Scale ==
Colonisation occurs on several scales. In the most basic form, as biofilm in the formation of communities of microorganisms on surfaces. In small scales such as colonising new sites, perhaps as a result of environmental change. And on larger scales where a species expands its range to encompass new areas. This can be via a series of small encroachments, such as in woody plant encroachment, or by long-distance dispersal. The term range expansion is also used.


== Use ==
The term is generally only used to refer to the spread of a species into new areas by natural means, as opposed to unnatural introduction or translocation by humans, which may lead to invasive species.


== Colonisation events ==

Large-scale notable pre-historic colonisation events include:


=== Humans ===
the early human migration and colonisation of areas outside Africa according to the recent African origin paradigm, resulting in the extinction of Pleistocene megafauna, although the role of humans in this event is controversial.
Some large-scale notable colonisation events during the 20th century are:


=== Birds ===
the colonisation of the New World by the cattle egret and the little egret
the colonisation of Britain by the little egret
the colonisation of western North America by the barred owl
the colonisation of the East Coast of North America by the Brewer's blackbird
the colonisation-westwards spread across Europe of the collared dove
the spread across the eastern USA of the house finch
the expansion into the southern and western areas of South Africa by the Hadeda Ibis


=== Reptiles ===
the colonisation of Anguilla by Green iguanas following a rafting event in 1995


=== Dragonflies ===
the colonisation of Britain by the small red-eyed damselfly


=== Moths ===
the colonisation of Britain by Blair's shoulder-knot


== See also ==
Colony (biology)
Invasive species
Pioneer species


== References ==",121842,80,"All articles with unsourced statements, All stub articles, Articles with limited geographic scope from April 2014, Articles with short description, Articles with unsourced statements from September 2010, CS1 maint: archived copy as title, Community ecology, Ecological processes, Ecology stubs, Ecology terminology, Short description matches Wikidata",1126321019,biology
https://en.wikipedia.org/wiki/Cognitive_biology,Cognitive biology,"Cognitive biology is an emerging science that regards natural cognition as a biological function. It is based on the theoretical assumption that every organism—whether a single cell or multicellular—is continually engaged in systematic acts of cognition coupled with intentional behaviors, i.e., a sensory-motor coupling. That is to say, if an organism can sense stimuli in its environment and respond accordingly, it is cognitive. Any explanation of how natural cognition may manifest in an organism is constrained by the biological conditions in which its genes survive from one generation to the next. And since by Darwinian theory the species of every organism is evolving from a common root, three further elements of cognitive biology are required: (i) the study of cognition in one species of organism is useful, through contrast and comparison, to the study of another species’ cognitive abilities; (ii) it is useful to proceed from organisms with simpler to those with more complex cognitive systems, and (iii) the greater the number and variety of species studied in this regard, the more we understand the nature of cognition.","Cognitive biology is an emerging science that regards natural cognition as a biological function. It is based on the theoretical assumption that every organism—whether a single cell or multicellular—is continually engaged in systematic acts of cognition coupled with intentional behaviors, i.e., a sensory-motor coupling. That is to say, if an organism can sense stimuli in its environment and respond accordingly, it is cognitive. Any explanation of how natural cognition may manifest in an organism is constrained by the biological conditions in which its genes survive from one generation to the next. And since by Darwinian theory the species of every organism is evolving from a common root, three further elements of cognitive biology are required: (i) the study of cognition in one species of organism is useful, through contrast and comparison, to the study of another species’ cognitive abilities; (ii) it is useful to proceed from organisms with simpler to those with more complex cognitive systems, and (iii) the greater the number and variety of species studied in this regard, the more we understand the nature of cognition.


== Overview ==
While cognitive science endeavors to explain human thought and the conscious mind, the work of cognitive biology is focused on the most fundamental process of cognition for any organism. In the past several decades, biologists have investigated cognition in organisms large and small, both plant and animal. “Mounting evidence suggests that even bacteria grapple with problems long familiar to cognitive scientists, including: integrating information from multiple sensory channels to marshal an effective response to fluctuating conditions; making decisions under conditions of uncertainty; communicating with conspecifics and others (honestly and deceptively); and coordinating collective behaviour to increase the chances of survival.” Without thinking or perceiving as humans would have it, an act of basic cognition is arguably a simple step-by-step process through which an organism senses a stimulus, then finds an appropriate response in its repertoire and enacts the response. However, the biological details of such basic cognition have neither been delineated for a great many species nor sufficiently generalized to stimulate further investigation. This lack of detail is due to the lack of a science dedicated to the task of elucidating the cognitive ability common to all biological organisms. That is to say, a science of cognitive biology has yet to be established. A prolegomena for such science was presented in 2007 and several authors have published their thoughts on the subject since the late 1970s. Yet, as the examples in the next section suggest, there is neither consensus on the theory nor widespread application in practice.
Although the two terms are sometimes used synonymously, cognitive biology should not be confused with the biology of cognition in the sense that it is used by adherents to the Chilean School of Biology of Cognition. Also known as the Santiago School, the biology of cognition is based on the work of Francisco Varela and Humberto Maturana, who crafted the doctrine of autopoiesis. Their work began in 1970 while the first mention of cognitive biology by Brian Goodwin (discussed below) was in 1977 from a different perspective.


== History ==
'Cognitive biology' first appeared in the literature as a paper with that title by Brian C. Goodwin in 1977. There and in several related publications Goodwin explained the advantage of cognitive biology in the context of his work on morphogenesis. He subsequently moved on to other issues of structure, form, and complexity with little further mention of cognitive biology. Without an advocate, Goodwin's concept of cognitive biology has yet to gain widespread acceptance. 
Aside from an essay regarding Goodwin's conception by Margaret Boden in 1980, the next appearance of ‘cognitive biology’ as a phrase in the literature came in 1986 from a professor of biochemistry, Ladislav Kováč. His conception, based on natural principles grounded in bioenergetics and molecular biology, is briefly discussed below. Kováč's continued advocacy has had a greater influence in his homeland, Slovakia, than elsewhere partly because several of his most important papers were written and published only in Slovakian.
By the 1990s, breakthroughs in molecular, cell, evolutionary, and developmental biology generated a cornucopia of data-based theory relevant to cognition. Yet aside from the theorists already mentioned, no one was addressing cognitive biology except for Kováč.


=== Kováč’s cognitive biology ===
Ladislav Kováč's “Introduction to cognitive biology” (Kováč, 1986a) lists ten ‘Principles of Cognitive Biology.’ A closely related thirty page paper was published the following year: “Overview: Bioenergetics between chemistry, genetics and physics.” (Kováč, 1987). Over the following decades, Kováč elaborated, updated, and expanded these themes in frequent publications, including  ""Fundamental principles of cognitive biology"" (Kováč, 2000), “Life, chemistry, and cognition” (Kováč, 2006a), ""Information and Knowledge in Biology: Time for Reappraisal” (Kováč, 2007) and ""Bioenergetics: A key to brain and mind"" (Kováč, 2008).


== Academic usage ==


=== University seminar ===
The concept of cognitive biology is exemplified by this seminar description:
Cognitive science has focused primarily on human cognitive activities. These include perceiving, remembering and learning, evaluating and deciding, planning actions, etc. But humans are not the only organisms that engage in these activities. Indeed, virtually all organisms need to be able to procure information both about their own condition and their environment and regulate their activities in ways appropriate to this information. In some cases species have developed distinctive ways of performing cognitive tasks. But in many cases these mechanisms have been conserved and modified in other species. This course will focus on a variety of organisms not usually considered in cognitive science such as bacteria, planaria, leeches, fruit flies, bees, birds and various rodents, asking about the sorts of cognitive activities these organisms perform, the mechanisms they employ to perform them, and what lessons about cognition more generally we might acquire from studying them.


=== University workgroup ===
The University of Adelaide has established a ""Cognitive Biology"" workgroup using this operating concept:

Cognition is, first and foremost, a natural biological phenomenon — regardless of how the engineering of artificial intelligence proceeds. As such, it makes sense to approach cognition like other biological phenomena. This means first assuming a meaningful degree of continuity among different types of organisms—an assumption borne out more and more by comparative biology, especially genomics—studying simple model systems (e.g., microbes, worms, flies) to understand the basics, then scaling up to more complex examples, such as mammals and primates, including humans.
Members of the group study the biological literature on simple organisms (e.g., nematode) in regard to cognitive process and look for homologues in more complex organisms (e.g., crow) already well studied. This comparative approach is expected to yield simple cognitive concepts common to all organisms. “It is hoped a theoretically well-grounded toolkit of basic cognitive concepts will facilitate the use and discussion of research carried out in different fields to increase understanding of two foundational issues: what cognition is and what cognition does in the biological context.” (Bold letters from original text.)
The group's choice of name, as they explain on a separate webpage, might have been ‘embodied cognition’ or ‘biological cognitive science.’ But the group chose ‘cognitive biology’ for the sake of (i) emphasis and (ii) method. For the sake of emphasis, (i) “We want to keep the focus on biology because for too long cognition was considered a function that could be almost entirely divorced from its physical instantiation, to the extent that whatever could be said of cognition almost by definition had to be applicable to both organisms and machines.” (ii) The method is to “assume (if only for the sake of enquiry) that cognition is a biological function similar to other biological functions—such as respiration, nutrient circulation, waste elimination, and so on.”The method supposes that the genesis of cognition is biological, i.e., the method is biogenic. The host of the group's website has said elsewhere that cognitive biology requires a biogenic approach, having identified ten principles of biogenesis in an earlier work. The first four biogenic principles are quoted here to illustrate the depth at which the foundations have been set at the Adelaide school of cognitive biology:

“Complex cognitive capacities have evolved from simpler forms of cognition. There is a continuous line of meaningful descent.”
“Cognition directly or indirectly modulates the physico-chemical-electrical processes that constitute an organism .”
“Cognition enables the establishment of reciprocal causal relations with an environment, leading to exchanges of matter and energy that are essential to the organism’s continued persistence, well-being or replication.”
“Cognition relates to the (more or less) continuous assessment of system needs relative to prevailing circumstances, the potential for interaction, and whether the current interaction is working or not.”


=== Other universities ===
As another example, the Department für Kognitionsbiologie at the University of Vienna declares in its mission statement a strong commitment “to experimental evaluation of multiple, testable hypotheses” regarding cognition in terms of evolutionary and developmental history as well as adaptive function and mechanism, whether the mechanism is cognitive, neural, and/or hormonal. “The approach is strongly comparative: multiple species are studied, and compared within a rigorous phylogenetic framework, to understand the evolutionary history and adaptive function of cognitive mechanisms (‘cognitive phylogenetics’).” Their website offers a sample of their work: “Social Cognition and the Evolution of Language: Constructing Cognitive Phylogenies.”
A more restricted example can be found with the Cognitive Biology Group, Institute of Biology, Faculty of Science, Otto-von-Guericke University (OVGU) in Magdeburg, Germany. The group offers courses titled “Neurobiology of Consciousness” and “Cognitive Neurobiology.” Its website lists the papers generated from its lab work, focusing on the neural correlates of perceptual consequences and visual attention. The group's current work is aimed at detailing a dynamic known as ‘multistable perception.’ The phenomenon, described in a sentence: “Certain visual displays are not perceived in a stable way but, from time to time and seemingly spontaneously, their appearance wavers and settles in a distinctly different form.”
A final example of university commitment to cognitive biology can be found at Comenius University in Bratislava, Slovakia. There in the Faculty of Natural Sciences, the Bratislava Biocenter is presented as a consortium of research teams working in biomedical sciences. Their website lists the Center for Cognitive Biology in the Department of Biochemistry at the top of the page, followed by five lab groups, each at a separate  department of bioscience. The webpage for the Center for Cognitive Biology  offers a link to ""Foundations of Cognitive Biology,"" a page that simply contains a quotation from a paper authored by Ladislav Kováč, the site's founder. His perspective is briefly discussed below.


== Cognitive biology as a category ==
The words ‘cognitive’ and ‘biology’ are also used together as the name of a category. The category of cognitive biology has no fixed content but, rather, the content varies with the user. If the content can only be recruited from cognitive science, then cognitive biology would seem limited to a selection of items in the main set of sciences included by the interdisciplinary concept—cognitive psychology, artificial intelligence, linguistics, philosophy, neuroscience, and cognitive anthropology. These six separate sciences were allied “to bridge the gap between brain and mind” with an interdisciplinary approach in the mid-1970s. Participating scientists were concerned only with human cognition. As it gained momentum, the growth of cognitive science in subsequent decades seemed to offer a big tent to a variety of researchers. Some, for example, considered evolutionary epistemology a fellow-traveler. Others appropriated the keyword, as for example Donald Griffin in 1978, when he advocated the establishment of cognitive ethology.Meanwhile, breakthroughs in molecular, cell, evolutionary, and developmental biology generated a cornucopia of data-based theory relevant to cognition. Categorical assignments were problematic. For example, the decision to append cognitive to a body of biological research on neurons, e.g. the cognitive biology of neuroscience, is separate from the decision to put such body of research in a category named cognitive sciences. No less difficult a decision needs be made—between the computational and constructivist approach to cognition, and the concomitant issue of simulated v. embodied cognitive models—before appending biology to a body of cognitive research, e.g. the cognitive science of artificial life.
One solution is to consider cognitive biology only as a subset of cognitive science. For example, a major publisher's website displays links to material in a dozen domains of major scientific endeavor. One of which is described thus: “Cognitive science is the study of how the mind works, addressing cognitive functions such as perception and action, memory and learning, reasoning and problem solving, decision-making and consciousness.” Upon its selection from the display, the Cognitive Science page offers in nearly alphabetical order these topics: Cognitive Biology, Computer Science, Economics, Linguistics, Psychology, Philosophy, and Neuroscience. Linked through that list of topics, upon its selection the Cognitive Biology page offers a selection of reviews and articles with biological content ranging from cognitive ethology through evolutionary epistemology; cognition and art; evo-devo and cognitive science; animal learning; genes and cognition; cognition and animal welfare; etc.
A different application of the cognitive biology category is manifest in the 2009 publication of papers presented at a three-day interdisciplinary workshop on “The New Cognitive Sciences” held at the Konrad Lorenz Institute for Evolution and Cognition Research in 2006. The papers were listed under four headings, each representing a different domain of requisite cognitive ability: (i) space, (ii) qualities and objects, (iii) numbers and probabilities, and (iv) social entities. The workshop papers examined topics ranging from “Animals as Natural Geometers” and “Color Generalization by Birds” through “Evolutionary Biology of Limited Attention” and “A comparative Perspective on the Origin of Numerical Thinking” as well as “Neuroethology of Attention in Primates” and ten more with less colorful titles. “[O]n the last day of the workshop the participants agreed [that] the title ‘Cognitive Biology’ sounded like a potential candidate to capture the merging of the cognitive and the life sciences that the workshop aimed at representing.” Thus the publication of Tommasi, et al. (2009), Cognitive Biology: Evolutionary and Developmental Perspectives on Mind, Brain and Behavior.
A final example of categorical use comes from an author’s introduction to his 2011 publication on the subject, Cognitive Biology: Dealing with Information from Bacteria to Minds. After discussing the differences between the cognitive and biological sciences, as well as the value of one to the other, the author concludes: “Thus, the object of this book should be considered as an attempt at building a new discipline, that of cognitive biology, which endeavors to bridge these two domains.” There follows a detailed methodology illustrated by examples in biology anchored by concepts from cybernetics (e.g., self-regulatory systems) and quantum information theory (regarding probabilistic changes of state) with an invitation ""to consider system theory together with information theory as the formal tools that may ground biology and cognition as traditional mathematics grounds physics.”


== See also ==
Biosemiotics
Cognitive anthropology
Cognitive science of religion
Cognitive neuropsychology
Cognitive neuroscience
Cognitive psychology
Cognitive science
Embodied cognitive science
Embodied cognition
Evolutionary epistemology
Naturalized epistemology
Neuroepistemology
Spatial cognition


== References & notes ==


== Bibliography ==
Auletta, Gennaro (2011). Cognitive biology: dealing with information from bacteria to minds. Oxford University Press.
Baluška, František and Stefano Mancuso (2009). Deep evolutionary origins of neurobiology: Turning the essence of ‘neural’ upside-down. Commun Integr Biol.  Jan-Feb; 2(1): 60–65.
Bechtel, William (2013). Seminar description for ""Cognitive Biology"" in the ""Cognitive Science 200"" series for ""Fall, 2013"" at University of California, San Diego, <http://mechanism.ucsd.edu/teaching/f13/cs200/>. See also http://mechanism.ucsd.edu/teaching/f13/cs200/bacterialinformationprocessing.pdf.
Ben Jacob, Eshel, Yoash Shapira, and Alfred I. Tauber (2006). ""Seeking the foundations of cognition in bacteria: From Schrödinger's negative entropy to latent information."" Physica A: Statistical Mechanics and its Applications 359: 495-524.
Bird, Angela (2010). Review - Cognitive Biology: Evolutionary and Developmental Perspectives on Mind, Brain, and Behavior
by Luca, Tommasi, Mary A. Peterson and Lynn Nadel (Editors, MIT Press, 2009). Metapsychology Online Reviews, Volume 14, Issue 3.
Boden, Margaret A (2006). Mind as machine: A history of cognitive science. Vol. 1. Oxford University Press.
Boden, Margaret and Susan K Zaw (1980). “The case for a cognitive biology.” Proceedings of the Aristotelian Society, 54: 25–71.
Byrne, R. W., Bates, L. A., Moss, C. J. (2009). “Elephant cognition in primate perspective.” Comparative Cognition & Behavior Reviews, 4, 65-79. Retrieved from http://arquivo.pt/wayback/20160520170354/http://comparative-cognition-and-behavior-reviews.org/index.html  doi:10.3819/ccbr.2009.40009
Calvo, Paco, and Fred Keijzer  (2009). ""Cognition in plants."" Plant-environment interactions: Signaling & communication in plants: 247-266.
Chomsky, N. (1972). Problems of Knowledge and Freedom. London: Fontana.
Denton, Michael J., Craig J. Marshall, and Michael Legge (2002). ""The protein folds as platonic forms: new support for the pre-Darwinian conception of evolution by natural law."" Journal of Theoretical Biology 219.3: 325-342.
Emmeche, Claus. ""Life as an abstract phenomenon: Is artificial life possible?"" (1992). Pages 466-474 in Francisco J. Varela and Paul Bourgine (eds.):Toward a Practice of Autonomous Systems. Proceedings of the First European Conference on Artificial Life. The MIT Press.
Frankish, Keith, and William Ramsey, editors (2012). The Cambridge Handbook of Cognitive Science. Cambridge University Press.
Goodwin, Brian C (1976a). Analytical Physiology of Cells and Developing Organisms. London: Academic Press.
Goodwin, Brian C (1976b). ""On some relationships between embryogenesis and cognition."" Theoria to Theory 10: 33-44.
Goodwin, Brian C (1977). ""Cognitive biology."" Communication & Cognition. Vol 10(2), 87-91. This paper also appeared the same year in deMey, M, R. Pinxten, M. Poriau & F. Vandamme (Eds.),  CC77 International Workshop on the Cognitive Viewpoint, University of Ghent Press, London, pp. 396–400.
Goodwin, Brian C (1978). “A cognitive view of biological process.” J Soc Biol Structures 1:117-125
Griffin, Donald R. ""Prospects for a cognitive ethology."" Behavioral and Brain Sciences 1.04 (1978): 527-538.
Huber, Ludwig and Anna Wilkinson. ""Evolution of cognition: A comparative approach."" Chapter 8 in Sensory Perception. Springer Vienna, 2012. 135-152.
Kamil, Alan C. (1998). ""On the proper definition of cognitive ethology."" Animal cognition in nature. Academic Press, San Diego  1-28.
Kováč, Ladislav (1986a). “Úvod do kognitívnej biológie.” (Published in Slovak with an abstract in English, the title translates as an “Introduction to cognitive biology.”) Biol. listy 51: 172- 190. {Since old copies of Biologické listy are hard to find, see Kováč (2004a) for the republished version which is also in Slovak. Google translates it into English well enough, with some help from the reader using the Google translation tool.}
Kováč, Ladislav (1986b). The future of bioenergetics. EBEC Reports 4: 26 - 27.
Kováč, Ladislav (1987). “Overview: Bioenergetics between chemistry, genetics and physics.” Curr. Topics Bioenerg. 15: 331- 372.
Kováč, Ladislav (2000). ""Fundamental principles of cognitive biology."" Evolution and cognition 6.1: 51-69. Evolution and Cognition was published by the Konrad Lorenz Institute for Evolution and Cognition Research (KLI) from 1995-2004. That journal was succeeded by a journal titled Biological Theory: Integrating Development, Evolution and Cognition. The paper is archived at this URL < http://www.biocenter.sk/lkpublics_files/C-7.pdf >.
Kováč, Ladislav (2006a). “Life, chemistry, and cognition.” EMBO Reports  7, 562- 566
Kováč, Ladislav (2006b) “Princípy molekulárnej kognície.” Kognice an umělý život VI : 215-222. Translation: “Principles of Molecular Cognition.” Cognition and Artificial Life VI: pp. 215–222
Kovác, Ladislav (2007). ""Information and Knowledge in Biology: Time for Reappraisal.” Plant Signaling & Behavior 2:2, 65-73
Kovác, Ladislav (2008). ""Bioenergetics: A key to brain and mind."" Communicative & integrative biology 1.1: 114-122.
Lyon, Pamela (2006), ‘The biogenic approach to cognition’, Cognitive Processing 7(1), 11–29.
Lyon, Pamela (2013a). Foundations for a Cognitive Biology. Published on the homepage of the Cognitive Biology Project at the University of Adelaide. https://web.archive.org/web/20141018181532/http://www.hss.adelaide.edu.au/philosophy/cogbio/
Lyon, Pamela (2013b). Why Cognitive Biology? Published on an HTML page linked to Lyon (2013a). https://web.archive.org/web/20140714230036/http://www.hss.adelaide.edu.au/philosophy/cogbio/why/
Lyon, Pamela C and Jonathan P Opie (2007). “Prolegomena for a cognitive biology.” A conference paper presented at the Proceedings of the 2007 Meeting of International Society for the History, Philosophy and Social Studies of Biology, University of Exeter. Abstract at http://hdl.handle.net/2440/46578.
Lyon, Pamela, and Fred Keijzer (2007). ""The human stain."" Pages 132-165 in Wallace, Brendan editor. The mind, the body, and the world: psychology after cognitivism?. Imprint Academic, 2007
Mandler, George (2002). ""Origins of the cognitive (r)evolution"". Journal of the History of the Behavioral Sciences 38 (4): 339–353. doi:10.1002/jhbs.10066.PMID 12404267.
Margolis, Eric, Richard Samuels, and Stephen P. Stich, editors (2012). The Oxford Handbook of Philosophy of Cognitive Science. Oxford University Press.
Maturana, Humberto R.  (1970). “Biology of Cognition.” Biological Computer Laboratory Research Report BCL 9.0. Urbana IL: University of Illinois. Reprinted in: Autopoiesis and Cognition: The Realization of the Living. Dordecht: D. Reidel Publishing Co., 1980, pp. 5–58.
Miller, George A. ""The cognitive revolution: a historical perspective."" Trends in cognitive sciences 7.3 (2003): 141-144.
Prigogine, Ilya (1980). From Being to Becoming. Freeman, San Francisco.
Shapiro, J.A. (2007). “Bacteria are small but not stupid: cognition, natural genetic engineering and socio-bacteriology.” Stud. Hist. Phil. Biol. & Biomed. Sci., Vol. 38: 807–819.
Shapiro, J.A. (2011). Evolution: A View from the 21st Century, FT Press Science, NJ, USA.
Spetch, Marcia L., and Alinda Friedman (2006). ""Comparative cognition of object recognition."" Comparative Cognition & Behavior Reviews 1: 12-35.
Spitzer, Jan, and Bert Poolman (2009). ""The role of biomacromolecular crowding, ionic strength, and physicochemical gradients in the complexities of life's emergence."" Microbiology and Molecular Biology Reviews 73.2: 371-388.
Stahlberg, Rainer (2006) “Historical Overview on Plant Neurobiology.” Plant Signaling & Behavior 1:1, 6-8.
Stotz, Karola, and Colin Allen. ""From cell-surface receptors to higher learning: A whole world of experience."" In Philosophy of behavioral
biology, pp. 85–123. Springer Netherlands, 2012.
Tommasi, Luca, Mary A. Peterson, and Lynn Nadel, eds. (2009). Cognitive Biology: Evolutionary and Developmental Perspectives on Mind, Brain and Behavior. The MIT Press.
Tinbergen, N. (1963). On aims and methods in ethology. Zeitschrift für Tierpsychologie, 20, 410-433.
Van Duijn, Marc, Fred Keijzer, and Daan Franken. ""Principles of minimal cognition: Casting cognition as sensorimotor coordination."" Adaptive Behavior14.2 (2006): 157-170.
Von Eckardt, Barbara. What is cognitive science?. MIT press, 1995.
Wasserman, Edward A. (1993). ""Comparative cognition: Beginning the second century of the study of animal intelligence."" Psychological Bulletin 113.2: 211
Webster, Gerry, and Brian C. Goodwin (1982). ""The origin of species: a structuralist approach."" Journal of Social and Biological Structures 5.1: 15-47.
Webster, Gerry, and Brian Goodwin (1996). Form and transformation: generative and relational principles in biology. Cambridge University Press.
Whitehead, Alfred North. (1929). Process and Reality. Cambridge University Press.
WIRES (Wiley Interdisciplinary Reviews) http://wires.wiley.com/WileyCDA/ (2013). Wiley Online Library.  John Wiley & Sons, Inc.
Zentall, Thomas R., Edward A. Wasserman, Olga F. Lazareva, Roger KR Thompson, and Mary Jo Rattermann (2008). ""Concept learning in animals."" Comp Cogn Behav Rev 3: 13-45.


== External links ==
European Network for the Advancement of Artificial Cognitive Systems, Interaction and Robotics
Comparative Cognition and Behavior Reviews
Ladislav Kováč's Publications",104781,96,"Articles with short description, Branches of biology, CS1 maint: archived copy as title, Cognition, Cognitive psychology, Cognitive science, Plant cognition, Short description matches Wikidata, Webarchive template wayback links",1106218814,biology
https://en.wikipedia.org/wiki/Computer_science,Computer science,"Computer science is the study of computation, automation, and information. Computer science spans theoretical disciplines (such as algorithms, theory of computation, information theory, and automation) to practical disciplines (including the design and implementation of hardware and software). Computer science is generally considered an area of academic research and distinct from computer programming.Algorithms and data structures are central to computer science.
The theory of computation concerns abstract models of computation and general classes of problems that can be solved using them. The fields of cryptography and computer security involve studying the means for secure communication and for preventing security vulnerabilities. Computer graphics and computational geometry address the generation of images. Programming language theory considers different ways to describe computational processes, and database theory concerns the management of repositories of data. Human–computer interaction investigates the interfaces through which humans and computers interact, and software engineering focuses on the design and principles behind developing software. Areas such as operating systems, networks and embedded systems investigate the principles and design behind complex systems. Computer architecture describes the construction of computer components and computer-operated equipment. Artificial intelligence and machine learning aim to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. Within artificial intelligence, computer vision aims to understand and process image and video data, while natural language processing aims to understand and process textual and linguistic data.
The fundamental concern of computer science is determining what can and cannot be automated. The Turing Award is generally recognized as the highest distinction in computer science.","Computer science is the study of computation, automation, and information. Computer science spans theoretical disciplines (such as algorithms, theory of computation, information theory, and automation) to practical disciplines (including the design and implementation of hardware and software). Computer science is generally considered an area of academic research and distinct from computer programming.Algorithms and data structures are central to computer science.
The theory of computation concerns abstract models of computation and general classes of problems that can be solved using them. The fields of cryptography and computer security involve studying the means for secure communication and for preventing security vulnerabilities. Computer graphics and computational geometry address the generation of images. Programming language theory considers different ways to describe computational processes, and database theory concerns the management of repositories of data. Human–computer interaction investigates the interfaces through which humans and computers interact, and software engineering focuses on the design and principles behind developing software. Areas such as operating systems, networks and embedded systems investigate the principles and design behind complex systems. Computer architecture describes the construction of computer components and computer-operated equipment. Artificial intelligence and machine learning aim to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. Within artificial intelligence, computer vision aims to understand and process image and video data, while natural language processing aims to understand and process textual and linguistic data.
The fundamental concern of computer science is determining what can and cannot be automated. The Turing Award is generally recognized as the highest distinction in computer science.


== History ==

The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623. In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner. Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine. He started developing this machine in 1834, and ""in less than two years, he had sketched out many of the salient features of the modern computer"". ""A crucial step was the adoption of a punched card system derived from the Jacquard loom"" making it infinitely programmable. In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer. Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published the 2nd of the only two designs for mechanical analytical engines in history. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as ""Babbage's dream come true"".
During the 1940s, with the development of new and more powerful computing machines such as the Atanasoff–Berry computer and ENIAC, the term computer came to refer to the machines rather than their human predecessors. As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City. The renovated fraternity house on Manhattan's West Side was IBM's first laboratory devoted to pure science. The lab is the forerunner of IBM's Research Division, which today operates research facilities around the world. Ultimately, the close relationship between IBM and Columbia University was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946. Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s. The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science department in the United States was formed at Purdue University in 1962. Since practical computers became available, many applications of computing have become distinct areas of study in their own rights.


== Etymology ==

Although first proposed in 1956, the term ""computer science"" appears in a 1959 article in Communications of the ACM,
in which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921. Louis justifies the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.
His efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962. Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed. Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy, to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.
In the early days of computing, a number of terms for the practitioners of the field of computing were suggested in the Communications of the ACM—turingineer, turologist, flow-charts-man, applied meta-mathematician, and applied epistemologist. Three months later in the same journal, comptologist was suggested, followed next year by hypologist. The term computics has also been suggested. In Europe, terms derived from contracted translations of the expression ""automatic information"" (e.g. ""informazione automatica"" in Italian) or ""information and mathematics"" are often used, e.g. informatique (French), Informatik (German), informatica (Italian, Dutch), informática (Spanish, Portuguese), informatika (Slavic languages and Hungarian) or pliroforiki (πληροφορική, which means informatics) in Greek. Similar words have also been adopted in the UK (as in the School of Informatics, University of Edinburgh). ""In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain.""A folkloric quotation, often attributed to—but almost certainly not first formulated by—Edsger Dijkstra, states that ""computer science is no more about computers than astronomy is about telescopes."" The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been exchange of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as cognitive science, linguistics, mathematics, physics, biology, Earth science, statistics, philosophy, and logic.
Computer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science. Early computer science was strongly influenced by the work of mathematicians such as Kurt Gödel, Alan Turing, John von Neumann, Rózsa Péter and Alonzo Church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.The relationship between Computer Science and Software Engineering is a contentious issue, which is further muddied by disputes over what the term ""Software Engineering"" means, and how computer science is defined. David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.The academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.


== Philosophy ==


=== Epistemology of computer science ===
Despite the word ""science"" in its name, there is debate over whether or not computer science is a discipline of science, mathematics, or engineering. Allen Newell and Herbert A. Simon argued in 1975, Computer science is an empirical discipline. We would have called it an experimental science, but like astronomy, economics, and geology, some of its unique forms of observation and experience do not fit a narrow stereotype of the experimental method. Nonetheless, they are experiments. Each new machine that is built is an experiment. Actually constructing the machine poses a question to nature; and we listen for the answer by observing the machine in operation and analyzing it by all analytical and measurement means available. It has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs, but a problem remains in defining the laws and theorems of computer science (if any exist) and defining the nature of experiments in computer science. Proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering. They also argue that while empirical sciences observe what presently exists, computer science observes what is possible to exist and while scientists discover laws from observation, no proper laws have been found in computer science and it is instead concerned with creating phenomena.Proponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs can be deductively reasoned through mathematical formal methods. Computer scientists Edsger W. Dijkstra and Tony Hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems.


=== Paradigms of computer science ===
A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics. Peter Denning's working group argued that they are theory, abstraction (modeling), and design. Amnon H. Eden described them as the ""rationalist paradigm"" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the ""technocratic paradigm"" (which might be found in engineering approaches, most prominently in software engineering), and the ""scientific paradigm"" (which approaches computer-related artifacts from the empirical perspective of natural sciences, identifiable in some branches of artificial intelligence).
Computer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.


== Fields ==

As a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.CSAB, formerly called Computing Sciences Accreditation Board—which is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)—identifies four areas that it considers crucial to the discipline of computer science: theory of computation, algorithms and data structures, programming methodology and languages, and computer elements and architecture. In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel computation, distributed computation, human–computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science.
Computer science is no more about computers than astronomy is about telescopes.


=== Theoretical computer science ===

Theoretical Computer Science is mathematical and abstract in spirit, but it derives its motivation from the practical and everyday computation. Its aim is to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.


==== Theory of computation ====

According to Peter Denning, the fundamental question underlying computer science is, ""What can be automated?"" Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.
The famous P = NP? problem, one of the Millennium Prize Problems, is an open problem in the theory of computation.


==== Information and coding theory ====

Information theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.
Coding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.


==== Data structures and algorithms ====
Data structures and algorithms are the studies of commonly used computational methods and their computational efficiency.


==== Programming language theory and formal methods ====

Programming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.
Formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.


=== Applied computer science ===


==== Computer graphics and visualization ====

Computer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.


==== Image and sound processing ====

Information can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier - whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. What is the lower bound on the complexity of fast Fourier transform algorithms? is one of unsolved problems in theoretical computer science.


==== Computational science, finance and engineering ====

Scientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE, as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.


==== Social computing and human–computer interaction ====

Social computing is an area that is concerned with the intersection of social behavior and computational systems. Human–computer interaction research develops theories, principles, and guidelines for user interface designers.


==== Software engineering ====

Software engineering is the study of designing, implementing, and modifying the software in order to ensure it is of high quality, affordable, maintainable, and fast to build. It is a systematic approach to software design, involving the application of engineering practices to software. Software engineering deals with the organizing and analyzing of software—it doesn't just deal with the creation or manufacture of new software, but its internal arrangement and maintenance. For example software testing, systems engineering, technical debt and software development processes.


==== Artificial intelligence ====

Artificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question ""Can computers think?"", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.


=== Computer systems ===


==== Computer architecture and organization ====

Computer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory. Computer engineers study computational logic and design of computer hardware, from individual processor components, microcontrollers, personal computers to supercomputers and embedded systems. The term ""architecture"" in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks, Jr., members of the Machine Organization department in IBM's main research center in 1959.


==== Concurrent, parallel and distributed computing ====

Concurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other. A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the Parallel Random Access Machine model. When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.


==== Computer networks ====

This branch of computer science aims to manage networks between computers worldwide.


==== Computer security and cryptography ====

Computer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users.
Historical cryptography is the art of writing and deciphering secret messages. Modern cryptography is the scientific study of problems relating to distributed computations that can be attacked. Technologies studied in modern cryptography include symmetric and asymmetric encryption, digital signatures, cryptographic hash functions, key-agreement protocols, blockchain, zero-knowledge proofs, and garbled circuits.


==== Databases and data mining ====

A database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.


== Discoveries ==
The philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:
Gottfried Wilhelm Leibniz's, George Boole's, Alan Turing's, Claude Shannon's, and Samuel Morse's insight: there are only two objects that a computer has to deal with in order to represent ""anything"".All the information about any computable problem can be represented using only 0 and 1 (or any other bistable pair that can flip-flop between two easily distinguishable states, such as ""on/off"", ""magnetized/de-magnetized"", ""high-voltage/low-voltage"", etc.).
Alan Turing's insight: there are only five actions that a computer has to perform in order to do ""anything"".Every algorithm can be expressed in a language for a computer consisting of only five basic instructions:move left one location;
move right one location;
read symbol at current location;
print 0 at current location;
print 1 at current location.
Corrado Böhm and Giuseppe Jacopini's insight: there are only three ways of combining these actions (into more complex ones) that are needed in order for a computer to do ""anything"".Only three rules are needed to combine any set of basic instructions into more complex ones:
sequence: first do this, then do that;
 selection: IF such-and-such is the case, THEN do this, ELSE do that;
repetition: WHILE such-and-such is the case, DO this.
Note that the three rules of Boehm's and Jacopini's insight can be further simplified with the use of goto (which means it is more elementary than structured programming).


== Programming paradigms ==

Programming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:

Functional programming, a style of building the structure and elements of computer programs that treats computation as the evaluation of mathematical functions and avoids state and mutable data. It is a declarative programming paradigm, which means programming is done with expressions or declarations instead of statements.
Imperative programming, a programming paradigm that uses statements that change a program's state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates.
Object-oriented programming, a programming paradigm based on the concept of ""objects"", which may contain data, in the form of fields, often known as attributes; and code, in the form of procedures, often known as methods. A feature of objects is that an object's procedures can access and often modify the data fields of the object with which they are associated. Thus object-oriented computer programs are made out of objects that interact with one another.
Service-oriented programming, a programming paradigm that uses ""services"" as the unit of computer work, to design and implement integrated business applications and mission critical software programsMany languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.


== Research ==

Conferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications. One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.


== Education ==

Computer Science, known by its near synonyms, Computing, Computer Studies, has been taught in UK schools since the days of batch processing, mark sensitive cards and paper tape but usually to a select few students. In 1981, the BBC produced a micro-computer and classroom network and Computer Studies became common for GCE O level students (11–16-year-old), and Computer Science to A level students. Its importance was recognised, and it became a compulsory part of the National Curriculum, for Key Stage 3 & 4. In September 2014 it became an entitlement for all pupils over the age of 4.In the US, with 14,000 school districts deciding the curriculum, provision was fractured. According to a 2010 report by the Association for Computing Machinery (ACM) and Computer Science Teachers Association (CSTA), only 14 out of 50 states have adopted significant education standards for high school computer science. According to a 2021 report, only 51% of high schools in the US offer computer science.Israel, New Zealand, and South Korea have included computer science in their national secondary education curricula, and several others are following.


== See also ==


== Notes ==


== References ==


== Further reading ==


== External links ==

Computer science at Curlie
Scholarly Societies in Computer Science Archived June 23, 2011, at the Wayback Machine
What is Computer Science?
Best Papers Awards in Computer Science since 1996
Photographs of computer scientists by Bertrand Meyer
EECS.berkeley.edu


=== Bibliography and academic search engines ===
CiteSeerx (article): search engine, digital library and repository for scientific and academic papers with a focus on computer and information science.
DBLP Computer Science Bibliography (article): computer science bibliography website hosted at Universität Trier, in Germany.
The Collection of Computer Science Bibliographies (Collection of Computer Science Bibliographies)


=== Professional organizations ===
Association for Computing Machinery
IEEE Computer Society
Informatics Europe
AAAI
AAAS Computer Science


=== Misc ===
Computer Science—Stack Exchange: a community-run question-and-answer site for computer science
What is computer science Archived February 18, 2015, at the Wayback Machine
Is computer science science?
Computer Science (Software) Must be Considered as an Independent Discipline.",11512449,6814,"All Wikipedia articles written in American English, Articles with BNE identifiers, Articles with BNF identifiers, Articles with Curlie links, Articles with EMU identifiers, Articles with GND identifiers, Articles with HDS identifiers, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with NLK identifiers, Articles with short description, CS1 German-language sources (de), Computer science, Dynamic lists, Pages using Sister project links with hidden wikidata, Pages using Sister project links with wikidata namespace mismatch, Pages using multiple image with auto scaled images, Short description is different from Wikidata, Use American English from August 2022, Use mdy dates from October 2017, Webarchive template wayback links, Wikipedia indefinitely move-protected pages, Wikipedia pages semi-protected against vandalism",1133593379,cs
https://en.wikipedia.org/wiki/Ontology_(computer_science),Ontology (computer science),"In computer science and information science, an ontology encompasses a representation, formal naming, and definition of the categories, properties, and relations between the concepts, data, and entities that substantiate one, many, or all domains of discourse. More simply, an ontology is a way of showing the properties of a subject area and how they are related, by defining a set of concepts and categories that represent the subject.
Every academic discipline or field creates ontologies to limit complexity and organize data into information and knowledge. 
Each uses ontological assumptions to frame explicit theories, research and applications. New ontologies may improve problem solving within that domain. Translating research papers within every field is a problem made easier when experts from different countries maintain a controlled vocabulary of jargon between each of their languages.For instance, the definition and ontology of economics is a primary concern in Marxist economics, but also in other subfields of economics. An example of economics relying on information science occurs in cases where a simulation or model is intended to enable economic decisions, such as determining what capital assets are at risk and by how much (see risk management).
What ontologies in both computer science and philosophy have in common is the attempt to represent entities, ideas and events, with all their interdependent properties and relations, according to a system of categories. In both fields, there is considerable work on problems of ontology engineering (e.g., Quine and Kripke in philosophy, Sowa and Guarino in computer science), and debates concerning to what extent normative ontology is possible (e.g., foundationalism and coherentism in philosophy, BFO and Cyc in artificial intelligence).
Applied ontology is considered a successor to prior work in philosophy, however many current efforts are more concerned with establishing controlled vocabularies of narrow domains than first principles, the existence of fixed essences or whether enduring objects (e.g., perdurantism and endurantism) may be ontologically more primary than processes. Artificial intelligence has retained the most attention regarding applied ontology in subfields like natural language processing within machine translation and knowledge representation, but ontology editors are being used often in a range of fields like education without the intent to contribute to AI.","In computer science and information science, an ontology encompasses a representation, formal naming, and definition of the categories, properties, and relations between the concepts, data, and entities that substantiate one, many, or all domains of discourse. More simply, an ontology is a way of showing the properties of a subject area and how they are related, by defining a set of concepts and categories that represent the subject.
Every academic discipline or field creates ontologies to limit complexity and organize data into information and knowledge. 
Each uses ontological assumptions to frame explicit theories, research and applications. New ontologies may improve problem solving within that domain. Translating research papers within every field is a problem made easier when experts from different countries maintain a controlled vocabulary of jargon between each of their languages.For instance, the definition and ontology of economics is a primary concern in Marxist economics, but also in other subfields of economics. An example of economics relying on information science occurs in cases where a simulation or model is intended to enable economic decisions, such as determining what capital assets are at risk and by how much (see risk management).
What ontologies in both computer science and philosophy have in common is the attempt to represent entities, ideas and events, with all their interdependent properties and relations, according to a system of categories. In both fields, there is considerable work on problems of ontology engineering (e.g., Quine and Kripke in philosophy, Sowa and Guarino in computer science), and debates concerning to what extent normative ontology is possible (e.g., foundationalism and coherentism in philosophy, BFO and Cyc in artificial intelligence).
Applied ontology is considered a successor to prior work in philosophy, however many current efforts are more concerned with establishing controlled vocabularies of narrow domains than first principles, the existence of fixed essences or whether enduring objects (e.g., perdurantism and endurantism) may be ontologically more primary than processes. Artificial intelligence has retained the most attention regarding applied ontology in subfields like natural language processing within machine translation and knowledge representation, but ontology editors are being used often in a range of fields like education without the intent to contribute to AI.


== Etymology ==

The compound word ontology combines onto-, from the Greek ὄν, on (gen. ὄντος, ontos), i.e. ""being; that which is"", which is the present participle of the verb εἰμί, eimí, i.e. ""to be, I am"", and -λογία, -logia, i.e. ""logical discourse"", see classical compounds for this type of word formation.While the etymology is Greek, the oldest extant record of the word itself, the New Latin form ontologia, appeared in 1606 in the work Ogdoas Scholastica by Jacob Lorhard (Lorhardus) and in 1613 in the Lexicon philosophicum by Rudolf Göckel (Goclenius).
The first occurrence in English of ontology as recorded by the OED (Oxford English Dictionary, online edition, 2008) came in Archeologia Philosophica Nova or New Principles of Philosophy by Gideon Harvey.


== History ==

Ontologies arise out of the branch of philosophy known as metaphysics, which deals with questions like ""what exists?"" and ""what is the nature of reality?"". One of five traditional branches of philosophy, metaphysics is concerned with exploring existence through properties, entities and relations such as those between particulars and universals, intrinsic and extrinsic properties, or essence and existence. Metaphysics has been an ongoing topic of discussion since recorded history.
Since the mid-1970s, researchers in the field of artificial intelligence (AI) have recognized that knowledge engineering is the key to building large and powerful AI systems. AI researchers argued that they could create new ontologies as computational models that enable certain kinds of automated reasoning, which was only marginally successful. In the 1980s, the AI community began to use the term ontology to refer to both a theory of a modeled world and a component of knowledge-based systems. In particular, David Powers introduced the word ontology to AI to refer to real world or robotic grounding, publishing in 1990 literature reviews emphasizing grounded ontology in association with the call for papers for a AAAI Summer Symposium Machine Learning of Natural Language and Ontology, with an expanded version published in SIGART Bulletin and included as a preface to the proceedings.  Some researchers, drawing inspiration from philosophical ontologies, viewed computational ontology as a kind of applied philosophy.
In 1993, the widely cited web page and paper ""Toward Principles for the Design of Ontologies Used for Knowledge Sharing"" by Tom Gruber used ontology as a technical term in computer science closely related to earlier idea of semantic networks and taxonomies. Gruber introduced the term as a specification of a conceptualization: An ontology is a description (like a formal specification of a program) of the concepts and relationships that can formally exist for an agent or a community of agents. This definition is consistent with the usage of ontology as set of concept definitions, but more general. And it is a different sense of the word than its use in philosophy.
Attempting to distance ontologies from taxonomies and similar efforts in knowledge modeling that rely on classes and inheritance, Gruber stated (1993): Ontologies are often equated with taxonomic hierarchies of classes, class definitions, and the subsumption relation, but ontologies need not be limited to these forms. Ontologies are also not limited to conservative definitions — that is, definitions in the traditional logic sense that only introduce terminology and do not add any knowledge about the world. To specify a conceptualization, one needs to state axioms that do constrain the possible interpretations for the defined terms.
As refinement of Gruber's definition Feilmayr and Wöß (2016) stated: ""An ontology is a formal, explicit specification of a shared conceptualization that is characterized by high semantic expressiveness required for increased complexity.""


== Components ==

Contemporary ontologies share many structural similarities, regardless of the language in which they are expressed.  Most ontologies describe individuals (instances), classes (concepts), attributes and relations.  In this section each of these components is discussed in turn.
Common components of ontologies include:

Individuals
Instances or objects (the basic or ""ground level"" objects)
Classes
Sets, collections, concepts, classes in programming, types of objects or kinds of things
Attributes
Aspects, properties, features, characteristics or parameters that objects (and classes) can have
Relations
Ways in which classes and individuals can be related to one another
Function terms
Complex structures formed from certain relations that can be used in place of an individual term in a statement
Restrictions
Formally stated descriptions of what must be true in order for some assertion to be accepted as input
Rules
Statements in the form of an if-then (antecedent-consequent) sentence that describe the logical inferences that can be drawn from an assertion in a particular form
Axioms
Assertions (including rules) in a logical form that together comprise the overall theory that the ontology describes in its domain of application. This definition differs from that of ""axioms"" in generative grammar and formal logic.  In those disciplines, axioms include only statements asserted as a priori knowledge.  As used here, ""axioms"" also include the theory derived from axiomatic statements
Events
The changing of attributes or relationsOntologies are commonly encoded using ontology languages.


== Types ==


=== Domain ontology ===
A domain ontology (or domain-specific ontology) represents concepts which belong to a realm of the world, such as biology or politics. Each domain ontology typically models domain-specific definitions of terms. For example, the word card has many different meanings. An ontology about the domain of poker would model the ""playing card"" meaning of the word, while an ontology about the domain of computer hardware would model the ""punched card"" and ""video card"" meanings.
Since domain ontologies are written by different people, they represent concepts in very specific and unique ways, and are often incompatible within the same project. As systems that rely on domain ontologies expand, they often need to merge domain ontologies by hand-tuning each entity or using a combination of software merging and hand-tuning. This presents a challenge to the ontology designer. Different ontologies in the same domain arise due to different languages, different intended usage of the ontologies, and different perceptions of the domain (based on cultural background, education, ideology, etc.).
At present, merging ontologies that are not developed from a common upper ontology is a largely manual process and therefore time-consuming and expensive. Domain ontologies that use the same upper ontology to provide a set of basic elements with which to specify the meanings of the domain ontology entities can be merged with less effort. There are studies on generalized techniques for merging ontologies, but this area of research is still ongoing, and it's a recent event to see the issue sidestepped by having multiple domain ontologies using the same upper ontology like the OBO Foundry.


=== Upper ontology ===

An upper ontology (or foundation ontology) is a model of the commonly shared relations and objects that are generally applicable across a wide range of domain ontologies. It usually employs a core glossary that overarches the terms and associated object descriptions as they are used in various relevant domain ontologies.
Standardized upper ontologies available for use include BFO, BORO method, Dublin Core, GFO, Cyc, SUMO, UMBEL, the Unified Foundational Ontology (UFO), and DOLCE. WordNet has been considered an upper ontology by some and has been used as a linguistic tool for learning domain ontologies.


=== Hybrid ontology ===
The Gellish ontology is an example of a combination of an upper and a domain ontology.


== Visualization ==
A survey of ontology visualization methods is presented by Katifori et al. An updated survey of ontology visualization methods and tools was published by Dudás et al. The most established ontology visualization methods, namely indented tree and graph visualization are evaluated by Fu et al. A visual language for ontologies represented in OWL is specified by the Visual Notation for OWL Ontologies (VOWL).


== Engineering ==

Ontology engineering (also called ontology building)  is a set of tasks related to the development of ontologies for a particular domain. It is a subfield of knowledge engineering that studies the ontology development process, the ontology life cycle, the methods and methodologies for building ontologies, and the tools and languages that support them.Ontology engineering aims to make explicit the knowledge contained in software applications, and organizational procedures for a particular domain. Ontology engineering offers a direction for overcoming semantic obstacles, such as those related to the definitions of business terms and software classes. Known challenges with ontology engineering include:

Ensuring the ontology is current with domain knowledge and term use
Providing sufficient specificity and concept coverage for the domain of interest, thus minimizing the content completeness problem
Ensuring the ontology can support its use cases


=== Editors ===
Ontology editors are applications designed to assist in the creation or manipulation of ontologies. It is common for ontology editors to use one or more ontology languages.
Aspects of ontology editors include: visual navigation possibilities within the knowledge model, inference engines and information extraction; support for modules; the import and export of foreign knowledge representation languages for ontology matching; and the support of meta-ontologies such as OWL-S, Dublin Core, etc.


=== Learning ===

Ontology learning is the automatic or semi-automatic creation of ontologies, including extracting a domain's terms from natural language text. As building ontologies manually is extremely labor-intensive and time-consuming, there is great motivation to automate the process.  Information extraction and text mining have been explored to automatically link ontologies to documents, for example in the context of the BioCreative challenges.


=== Research ===

Epistemological assumptions, which in research asks ""What do you know? or ""How do you know it?"", creates the foundation researchers use when approaching a certain topic or area for potential research. As epistemology is directly linked to knowledge and how we come about accepting certain truths, individuals conducting academic research must understand what allows them to begin theory building. Simply, epistemological assumptions force researchers to question how they arrive at the knowledge they have.


== Languages ==

An ontology language is a formal language used to encode an ontology. There are a number of such languages for ontologies, both proprietary and standards-based:

Common Algebraic Specification Language is a general logic-based specification language developed within the IFIP working group 1.3 ""Foundations of System Specifications"" and is a de facto standard language for software specifications. It is now being applied to ontology specifications in order to provide modularity and structuring mechanisms.
Common logic is ISO standard 24707, a specification of a family of ontology languages that can be accurately translated into each other.
The Cyc project has its own ontology language called CycL, based on first-order predicate calculus with some higher-order extensions.
DOGMA (Developing Ontology-Grounded Methods and Applications) adopts the fact-oriented modeling approach to provide a higher level of semantic stability.
The Gellish language includes rules for its own extension and thus integrates an ontology with an ontology language.
IDEF5 is a software engineering method to develop and maintain usable, accurate, domain ontologies.
KIF is a syntax for first-order logic that is based on S-expressions.  SUO-KIF is a derivative version supporting the Suggested Upper Merged Ontology.
MOF and UML are standards of the OMG
Olog is a category theoretic approach to ontologies, emphasizing translations between ontologies using functors.
OBO, a language used for biological and biomedical ontologies.
OntoUML is an ontologically well-founded profile of UML for conceptual modeling of domain ontologies.
OWL is a language for making ontological statements, developed as a follow-on from RDF and RDFS, as well as earlier ontology language projects including OIL, DAML, and DAML+OIL. OWL is intended to be used over the World Wide Web, and all its elements (classes, properties and individuals) are defined as RDF resources, and identified by URIs.
Rule Interchange Format (RIF) and F-Logic combine ontologies and rules.
Semantic Application Design Language (SADL) captures a subset of the expressiveness of OWL, using an English-like language entered via an Eclipse Plug-in.
SBVR (Semantics of Business Vocabularies and Rules) is an OMG standard adopted in industry to build ontologies.
TOVE Project, TOronto Virtual Enterprise project


== Published examples ==
Arabic Ontology, a linguistic ontology for Arabic, which can be used as an Arabic Wordnet but with ontologically-clean content.
AURUM - Information Security Ontology, An ontology for information security knowledge sharing, enabling users to collaboratively understand and extend the domain knowledge body. It may serve as a basis for automated information security risk and compliance management.
BabelNet, a very large multilingual semantic network and ontology, lexicalized in many languages
Basic Formal Ontology, a formal upper ontology designed to support scientific research
BioPAX, an ontology for the exchange and interoperability of biological pathway (cellular processes) data
BMO, an e-Business Model Ontology based on a review of enterprise ontologies and business model literature
SSBMO, a Strongly Sustainable Business Model Ontology based on a review of the systems based natural and social science literature (including business).  Includes critique of and significant extensions to the Business Model Ontology (BMO).
CCO and GexKB, Application Ontologies (APO) that integrate diverse types of knowledge with the Cell Cycle Ontology (CCO) and the Gene Expression Knowledge Base (GexKB)
CContology (Customer Complaint Ontology), an e-business ontology to support online customer complaint management
CIDOC Conceptual Reference Model, an ontology for cultural heritage
COSMO, a Foundation Ontology (current version in OWL) that is designed to contain representations of all of the primitive concepts needed to logically specify the meanings of any domain entity.  It is intended to serve as a basic ontology that can be used to translate among the representations in other ontologies or databases.  It started as a merger of the basic elements of the OpenCyc and SUMO ontologies, and has been supplemented with other ontology elements (types, relations) so as to include representations of all of the words in the Longman dictionary defining vocabulary.
Computer Science Ontology, an automatically generated ontology of research topics in the field of Computer Science
Cyc, a large Foundation Ontology for formal representation of the universe of discourse
Disease Ontology, designed to facilitate the mapping of diseases and associated conditions to particular medical codes
DOLCE, a Descriptive Ontology for Linguistic and Cognitive Engineering
Drammar, ontology of drama
Dublin Core, a simple ontology for documents and publishing
Financial Industry Business Ontology (FIBO),  a business conceptual ontology for the financial industry
Foundational, Core and Linguistic Ontologies
Foundational Model of Anatomy, an ontology for human anatomy
Friend of a Friend, an ontology for describing persons, their activities and their relations to other people and objects
Gene Ontology for genomics
Gellish English dictionary, an ontology that includes a dictionary and taxonomy that includes an upper ontology and a lower ontology that focusses on industrial and business applications in engineering, technology and procurement.
Geopolitical ontology, an ontology describing geopolitical information created by Food and Agriculture Organization(FAO). The geopolitical ontology includes names in multiple languages (English, French, Spanish, Arabic, Chinese, Russian and Italian); maps standard coding systems (UN, ISO, FAOSTAT, AGROVOC, etc.); provides relations among territories (land borders, group membership, etc.); and tracks historical changes. In addition, FAO provides web services of geopolitical ontology and a module maker to download modules of the geopolitical ontology into different formats (RDF, XML, and EXCEL). See more information at FAO Country Profiles.
GAO (General Automotive Ontology) - an ontology for the automotive industry that includes 'car' extensions
GOLD, General Ontology for Linguistic Description
GUM (Generalized Upper Model), a linguistically motivated ontology for mediating between clients systems and natural language technology
IDEAS Group, a formal ontology for enterprise architecture being developed by the Australian, Canadian, UK and U.S. Defence Depts.
Linkbase, a formal representation of the biomedical domain, founded upon Basic Formal Ontology.
LPL, Landmark Pattern Language
NCBO Bioportal, biological and biomedical ontologies and associated tools to search, browse and visualise
NIFSTD Ontologies from the Neuroscience Information Framework: a modular set of ontologies for the neuroscience domain.
OBO-Edit, an ontology browser for most of the Open Biological and Biomedical Ontologies
OBO Foundry, a suite of interoperable reference ontologies in biology and biomedicine
OMNIBUS Ontology, an ontology of learning, instruction, and instructional design
Ontology for Biomedical Investigations, an open-access, integrated ontology of biological and clinical investigations
ONSTR, Ontology for Newborn Screening Follow-up and Translational Research, Newborn Screening Follow-up Data Integration Collaborative, Emory University, Atlanta.
Plant Ontology for plant structures and growth/development stages, etc.
POPE, Purdue Ontology for Pharmaceutical Engineering
PRO, the Protein Ontology of the Protein Information Resource, Georgetown University
ProbOnto, knowledge base and ontology of probability distributions.
Program abstraction taxonomy
Protein Ontology for proteomics
RXNO Ontology, for name reactions in chemistry
SCDO, the Sickle Cell Disease Ontology, facilitates data sharing and collaborations within the SDC community, amongst other applications (see list on SCDO website).
Sequence Ontology, for representing genomic feature types found on biological sequences
SNOMED CT (Systematized Nomenclature of Medicine—Clinical Terms)
Suggested Upper Merged Ontology, a formal upper ontology
Systems Biology Ontology (SBO), for computational models in biology
SWEET, Semantic Web for Earth and Environmental Terminology
SSN/SOSA, The Semantic Sensor Network Ontology (SSN) and Sensor, Observation, Sample, and Actuator Ontology (SOSA) are W3C Recommendation  and OGC Standards for describing sensors and their observations.
ThoughtTreasure ontology
TIME-ITEM, Topics for Indexing Medical Education
Uberon, representing animal anatomical structures
UMBEL, a lightweight reference structure of 20,000 subject concept classes and their relationships derived from OpenCyc
WordNet, a lexical reference system
YAMATO, Yet Another More Advanced Top-level OntologyThe W3C Linking Open Data community project coordinates attempts to converge different ontologies into worldwide Semantic Web.


== Libraries ==
The development of ontologies has led to the emergence of services providing lists or directories of ontologies called ontology libraries.
The following are libraries of human-selected ontologies.

COLORE is an open repository of first-order ontologies in Common Logic with formal links between ontologies in the repository.
DAML Ontology Library maintains a legacy of ontologies in DAML.
Ontology Design Patterns portal is a wiki repository of reusable components and practices for ontology design, and also maintains a list of exemplary ontologies.
Protégé Ontology Library contains a set of OWL, Frame-based and other format ontologies.
SchemaWeb is a directory of RDF schemata expressed in RDFS, OWL and DAML+OIL.The following are both directories and search engines.

OBO Foundry is a suite of interoperable reference ontologies in biology and biomedicine.
Bioportal (ontology repository of NCBO)
OntoSelect Ontology Library offers similar services for RDF/S, DAML and OWL ontologies.
Ontaria is a ""searchable and browsable directory of semantic web data"" with a focus on RDF vocabularies with OWL ontologies. (NB Project ""on hold"" since 2004).
Swoogle is a directory and search engine for all RDF resources available on the Web, including ontologies.
Open Ontology Repository initiative
ROMULUS is a foundational ontology repository aimed at improving semantic interoperability. Currently there are three foundational ontologies in the repository: DOLCE, BFO and GFO.


== Examples of applications ==
In general, ontologies can be used beneficially in several fields.

Enterprise applications. A more concrete example is SAPPHIRE (Health care) or Situational Awareness and Preparedness for Public Health Incidences and Reasoning Engines which is a semantics-based health information system capable of tracking and evaluating situations and occurrences that may affect public health.
Geographic information systems bring together data from different sources and benefit therefore from ontological metadata which helps to connect the semantics of the data.
Domain-specific ontologies are extremely important in biomedical research, which requires named entity disambiguation of various biomedical terms and abbreviations that have the same string of characters but represent different biomedical concepts. For example, CSF can represent Colony Stimulating Factor or Cerebral Spinal Fluid, both of which are represented by the same term, CSF, in biomedical literature. This is why a large number of public ontologies are related to the life sciences. Life science data science tools that fail to implement these types of biomedical ontologies will not be able to accurately determine causal relationships between concepts.


== See also ==

Related philosophical conceptsAlphabet of human thought
Characteristica universalis
Interoperability
Level of measurement
Metalanguage
Natural semantic metalanguage


== References ==


== Further reading ==
Oberle, D.; Guarino, N.; Staab, S. (2009). ""What is an ontology?"" (PDF). Staab & Studer 2009. pp. 1–17. doi:10.1007/978-3-540-92673-3_0. ISBN 978-3-540-70999-2.
Fensel, D.; van Harmelen, F.; Horrocks, I.; McGuinness, D.L.; Patel-Schneider, P.F. (2001). ""OIL: an ontology infrastructure for the Semantic Web"". IEEE Intelligent Systems. 16 (2): 38–45. doi:10.1109/5254.920598.
Gangemi, A.; Presutti, V. ""Ontology Design Patterns"" (PDF). Staab & Studer 2009.
Golemati, M.; Katifori, A.; Vassilakis, C.; Lepouras, G.; Halatsis, C. (2007). ""Creating an Ontology for the User Profile# Method and Applications"" (PDF). Proceedings of the First IEEE International Conference on Research Challenges in Information Science (RCIS), Morocco 2007. CiteSeerX 10.1.1.74.9399. Archived from the original (PDF) on 2008-12-17.
Mizoguchi, R. (2004). ""Tutorial on ontological engineering: Part 3: Advanced course of ontological engineering"" (PDF). New Gener Comput. 22: 193–220. doi:10.1007/BF03040960. S2CID 23747079. Archived from the original (PDF) on 2013-03-09. Retrieved 2009-06-08.
Gruber, T. R. (1993). ""A translation approach to portable ontology specifications"" (PDF). Knowledge Acquisition. 5 (2): 199–220. CiteSeerX 10.1.1.101.7493. doi:10.1006/knac.1993.1008.
Maedche, A.; Staab, S. (2001). ""Ontology learning for the Semantic Web"". IEEE Intelligent Systems. 16 (2): 72–79. doi:10.1109/5254.920602.
Noy, Natalya F.; McGuinness, Deborah L. (March 2001). ""Ontology Development 101: A Guide to Creating Your First Ontology"". Stanford Knowledge Systems Laboratory Technical Report KSL-01-05, Stanford Medical Informatics Technical Report SMI-2001-0880. Archived from the original on 2010-07-14. {{cite journal}}: Cite journal requires |journal= (help)
Chaminda Abeysiriwardana, Prabath; Kodituwakku, Saluka R (2012). ""Ontology Based Information Extraction for Disease Intelligence"". International Journal of Research in Computer Science. 2 (6): 7–19. arXiv:1211.3497. Bibcode:2012arXiv1211.3497C. doi:10.7815/ijorcs.26.2012.051. S2CID 11297019.
Razmerita, L.; Angehrn, A.; Maedche, A. (2003). ""Ontology-Based User Modeling for Knowledge Management Systems"". User Modeling 2003. Lecture Notes in Computer Science. Vol. 2702. Springer. pp. 213–7. CiteSeerX 10.1.1.102.4591. doi:10.1007/3-540-44963-9_29. ISBN 3-540-44963-9.
Soylu, A.; De Causmaecker, Patrick (2009). ""Merging model driven and ontology driven system development approaches pervasive computing perspective"". Proceedings of the 24th International Symposium on Computer and Information Sciences. pp. 730–5. doi:10.1109/ISCIS.2009.5291915. ISBN 978-1-4244-5021-3. S2CID 2267593.
Smith, B. (2008). ""Ontology (Science)"".  In Eschenbach, C.; Gruninger, M. (eds.). Formal Ontology in Information Systems, Proceedings of FOIS 2008. ISO Press. pp. 21–35. CiteSeerX 10.1.1.681.2599.
Staab, S.; Studer, R., eds. (2009). Handbook on Ontologies (2nd ed.). Springer. doi:10.1007/978-3-540-92673-3_0. ISBN 978-3-540-92673-3.
Uschold, Mike; Gruninger, M. (1996). ""Ontologies: Principles, Methods and Applications"". Knowledge Engineering Review. 11 (2): 93–136. CiteSeerX 10.1.1.111.5903. doi:10.1017/S0269888900007797. S2CID 2618234.
Pidcock, W. ""What are the differences between a vocabulary, a taxonomy, a thesaurus, an ontology, and a meta-model?"". Archived from the original on 2009-10-14.
Yudelson, M.; Gavrilova, T.; Brusilovsky, P. (2005). ""Towards User Modeling Meta-ontology"". User Modeling 2005. Lecture Notes in Computer Science. Vol. 3538. Springer. pp. 448–452. CiteSeerX 10.1.1.86.7079. doi:10.1007/11527886_62. ISBN 978-3-540-31878-1.
Movshovitz-Attias, Dana; Cohen, William W. (2012). ""Bootstrapping Biomedical Ontologies for Scientific Text using NELL"" (PDF). Proceedings of the 2012 Workshop on Biomedical Natural Language Processing. Association for Computational Linguistics. pp. 11–19. CiteSeerX 10.1.1.376.2874.


== External links ==

Knowledge Representation at Open Directory Project
Library of ontologies
GoPubMed using Ontologies for searching
ONTOLOG (a.k.a. ""Ontolog Forum"") - an Open, International, Virtual Community of Practice on Ontology, Ontological Engineering and Semantic Technology
Use of Ontologies in Natural Language Processing
Ontology Summit - an annual series of events (first started in 2006) that involves the ontology community and communities related to each year's theme chosen for the summit.
Standardization of Ontologies",217170,1524,"All articles with unsourced statements, Articles containing Ancient Greek (to 1453)-language text, Articles with BNF identifiers, Articles with FAST identifiers, Articles with GND identifiers, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with NDL identifiers, Articles with NKC identifiers, Articles with short description, Articles with unsourced statements from August 2018, Articles with unsourced statements from February 2021, Articles with unsourced statements from February 2022, CS1: long volume value, CS1 errors: missing periodical, CS1 maint: location, CS1 maint: postscript, Commons category link is locally defined, Information science, Knowledge bases, Knowledge engineering, Knowledge representation, Ontology (information science), Ontology editors, Semantic Web, Short description is different from Wikidata, Technical communication",1133332334,cs
https://en.wikipedia.org/wiki/Computer_graphics_(computer_science),Computer graphics (computer science),"Computer graphics is a sub-field of computer science which studies methods for digitally synthesizing and manipulating visual content.  Although the term often refers to the study of three-dimensional computer graphics, it also encompasses two-dimensional graphics and image processing. The individuals who serve as professional designers for computers graphics are known as ""Graphics Programmers"", who often are computer programmers with skills in computer graphics design.","Computer graphics is a sub-field of computer science which studies methods for digitally synthesizing and manipulating visual content.  Although the term often refers to the study of three-dimensional computer graphics, it also encompasses two-dimensional graphics and image processing. The individuals who serve as professional designers for computers graphics are known as ""Graphics Programmers"", who often are computer programmers with skills in computer graphics design. 


== Overview ==
Computer graphics studies the 
aesthetic manipulation of visual and geometric information using computational techniques.  It focuses on the mathematical and computational foundations of image generation and processing rather than purely aesthetic issues.  Computer graphics is often differentiated from the field of visualization, although the two fields have many similarities.
Connected studies include:

Applied mathematics
Computational geometry
Computational topology
Computer vision
Image processing
Information visualization
Scientific visualizationApplications of computer graphics include:

Print design
Digital art
Special effects
Video games
Visual effects


== History ==

There are several international conferences and journals where the most significant results in computer graphics are published. Among them are the SIGGRAPH and Eurographics conferences and the Association for Computing Machinery (ACM) Transactions on Graphics journal. The joint Eurographics and ACM SIGGRAPH symposium series features the major venues for the more specialized sub-fields: Symposium on Geometry Processing, Symposium on Rendering, Symposium on Computer Animation, and High Performance Graphics.As in the rest of computer science, conference publications in computer graphics are generally more significant than journal publications (and subsequently have lower acceptance rates).


== Subfields ==
A broad classification of major subfields in computer graphics might be:

Geometry: ways to represent and process surfaces
Animation: ways to represent and manipulate motion
Rendering: algorithms to reproduce light transport
Imaging: image acquisition or image editing


=== Geometry ===

The subfield of geometry studies the representation of three-dimensional objects in a discrete digital setting.  Because the appearance of an object depends largely on its exterior, boundary representations are most commonly used.  Two dimensional surfaces are a good representation for most objects, though they may be non-manifold.  Since surfaces are not finite, discrete digital approximations are used. Polygonal meshes (and to a lesser extent subdivision surfaces) are by far the most common representation, although point-based representations have become more popular recently (see for instance the Symposium on Point-Based Graphics). These representations are Lagrangian, meaning the spatial locations of the samples are independent.  Recently, Eulerian surface descriptions (i.e., where spatial samples are fixed) such as level sets have been developed into a useful representation for deforming surfaces which undergo many topological changes (with fluids being the most notable example).Geometry subfields include:

Implicit surface modeling – an older subfield which examines the use of algebraic surfaces, constructive solid geometry, etc., for surface representation.
Digital geometry processing – surface reconstruction, simplification, fairing, mesh repair, parameterization, remeshing, mesh generation, surface compression, and surface editing all fall under this heading.
Discrete differential geometry – a nascent field which defines geometric quantities for the discrete surfaces used in computer graphics.
Point-based graphics – a recent field which focuses on points as the fundamental representation of surfaces.
Subdivision surfaces
Out-of-core mesh processing – another recent field which focuses on mesh datasets that do not fit in main memory.


=== Animation ===
The subfield of animation studies descriptions for surfaces (and other phenomena) that move or deform over time.  Historically, most work in this field has focused on parametric and data-driven models, but recently physical simulation has become more popular as computers have become more powerful computationally.
Animation subfields include:

Performance capture
Character animation
Physical simulation (e.g. cloth modeling,  animation of fluid dynamics, etc.)


=== Rendering ===

Rendering generates images from a model.  Rendering may simulate light transport to create realistic images or it may create images that have a particular artistic style in non-photorealistic rendering.  The two basic operations in realistic rendering are transport (how much light passes from one place to another) and scattering (how surfaces interact with light).  See Rendering (computer graphics) for more information.
Rendering subfields include:

Transport describes how illumination in a scene gets from one place to another. Visibility is a major component of light transport.
Scattering: Models of scattering (how light interacts with the surface at a given point) and shading (how material properties vary across the surface) are used to describe the appearance of a surface.  In graphics these problems are often studied within the context of rendering since they can substantially affect the design of rendering algorithms.  Descriptions of scattering are usually given in terms of a bidirectional scattering distribution function (BSDF).  The latter issue addresses how different types of scattering are distributed across the surface (i.e., which scattering function applies where).  Descriptions of this kind are typically expressed with a program called a shader.  (Note that there is some confusion since the word ""shader"" is sometimes used for programs that describe local geometric variation.)
Non-photorealistic rendering
Physically based rendering – concerned with generating images according to the laws of geometric optics
Real-time rendering – focuses on rendering for interactive applications, typically using specialized hardware like GPUs
Relighting – recent area concerned with quickly re-rendering scenes


== Notable researchers ==


== Applications for their use ==
Bitmap Design / Image Editing

Adobe Photoshop
Corel Photo-Paint
GIMP
KritaVector drawing

Adobe Illustrator
CorelDRAW
Inkscape
Affinity Designer
SketchArchitecture

VariCAD
FreeCAD
AutoCAD
QCAD
LibreCAD
DataCAD
Corel DesignerVideo editing

Adobe Premiere Pro
Sony Vegas
Final Cut
DaVinci Resolve
Cinelerra
VirtualDubSculpting, Animation, and 3D Modeling

Blender 3D
Wings 3D
ZBrush
Sculptris
SolidWorks
Rhino3D
SketchUp
Houdini
3ds Max
Cinema 4D
Maya
HoudiniDigital composition

Nuke
Blackmagic Fusion
Adobe After Effects
NatronRendering

V-Ray
RedShift
RenderMan
Octane Render
Mantra
Lumion (Architectural visualization)Other applications examples

ACIS - geometric core
Autodesk Softimage
POV-Ray
Scribus
Silo
Hexagon
Lightwave


== See also ==


== References ==


== Further reading ==
Foley et al. Computer Graphics: Principles and Practice.
Shirley. Fundamentals of Computer Graphics.
Watt. 3D Computer Graphics.


== External links ==

A Critical History of Computer Graphics and Animation
History of Computer Graphics series of articles


=== Industry ===
Industrial labs doing ""blue sky"" graphics research include:

Adobe Advanced Technology Labs
MERL
Microsoft Research – Graphics
Nvidia ResearchMajor film studios notable for graphics research include:

ILM
PDI/Dreamworks Animation
Pixar",566072,1077,"Articles with GND identifiers, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with NDL identifiers, Articles with NKC identifiers, Articles with short description, Commons category link is on Wikidata, Computer graphics, Short description matches Wikidata, Webarchive template archiveis links, Webarchive template wayback links",1103503091,cs
https://en.wikipedia.org/wiki/Heuristic_(computer_science),Heuristic (computer science),"In mathematical optimization and computer science, heuristic (from Greek εὑρίσκω ""I find, discover"") is a technique designed for solving a problem more quickly when classic methods are too slow for finding an approximate solution, or when classic methods fail to find any exact solution.  This is achieved by trading optimality, completeness, accuracy, or precision for speed.  In a way, it can be considered a shortcut.
A heuristic function, also simply called a heuristic, is a function that ranks alternatives in search algorithms at each branching step based on available information to decide which branch to follow. For example, it may approximate the exact solution.

","In mathematical optimization and computer science, heuristic (from Greek εὑρίσκω ""I find, discover"") is a technique designed for solving a problem more quickly when classic methods are too slow for finding an approximate solution, or when classic methods fail to find any exact solution.  This is achieved by trading optimality, completeness, accuracy, or precision for speed.  In a way, it can be considered a shortcut.
A heuristic function, also simply called a heuristic, is a function that ranks alternatives in search algorithms at each branching step based on available information to decide which branch to follow. For example, it may approximate the exact solution.


== Definition and motivation ==
The objective of a heuristic is to produce a solution in a reasonable time frame that is good enough for solving the problem at hand.  This solution may not be the best of all the solutions to this problem, or it may simply approximate the exact solution. But it is still valuable because finding it does not require a prohibitively long time.
Heuristics may produce results by themselves, or they may be used in conjunction with optimization algorithms to improve their efficiency (e.g., they may be used to generate good seed values).
Results about NP-hardness in theoretical computer science make heuristics the only viable option for a variety of complex optimization problems that need to be routinely solved in real-world applications.
Heuristics underlie the whole field of Artificial Intelligence and the computer simulation of thinking, as they may be used in situations where there are no known algorithms.


== Trade-off ==
The trade-off criteria for deciding whether to use a heuristic for solving a given problem include the following:

Optimality: When several solutions exist for a given problem, does the heuristic guarantee that the best solution will be found? Is it actually necessary to find the best solution?
Completeness: When several solutions exist for a given problem, can the heuristic find them all? Do we actually need all solutions? Many heuristics are only meant to find one solution.
Accuracy and precision: Can the heuristic provide a confidence interval for the purported solution? Is the error bar on the solution unreasonably large?
Execution time: Is this the best known heuristic for solving this type of problem? Some heuristics converge faster than others. Some heuristics are only marginally quicker than classic methods, in which case the 'overhead' on calculating the heuristic might have negative impact.In some cases, it may be difficult to decide whether the solution found by the heuristic is good enough, because the theory underlying heuristics is not very elaborate.


== Examples ==


=== Simpler problem ===
One way of achieving the computational performance gain expected of a heuristic consists of solving a simpler problem whose solution is also a solution to the initial problem.


=== Travelling salesman problem ===
An example of approximation is described by Jon Bentley for solving the travelling salesman problem (TSP):

""Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city exactly once and returns to the origin city?""so as to select the order to draw using a pen plotter. TSP is known to be NP-hard so an optimal solution for even a moderate size problem is difficult to solve. Instead, the greedy algorithm can be used to give a good but not optimal solution (it is an approximation to the optimal answer) in a reasonably short amount of time. The greedy algorithm heuristic says to pick whatever is currently the best next step regardless of whether that prevents (or even makes impossible) good steps later. It is a heuristic in the sense that practice indicates it is a good enough solution, while theory indicates that there are better solutions (and even indicates how much better, in some cases).


=== Search ===
Another example of heuristic making an algorithm faster occurs in certain search problems. Initially, the heuristic tries every possibility at each step, like the full-space search algorithm. But it can stop the search at any time if the current possibility is already worse than the best solution already found. In such search problems, a heuristic can be used to try good choices first so that bad paths can be eliminated early (see alpha-beta pruning). In the case of best-first search algorithms, such as A* search, the heuristic improves the algorithm's convergence while maintaining its correctness as long as the heuristic is admissible.


=== Newell and Simon: heuristic search hypothesis ===
In their Turing Award acceptance speech, Allen Newell and Herbert A. Simon discuss the heuristic search hypothesis: a physical symbol system will repeatedly generate and modify known symbol structures until the created structure matches the solution structure. Each following step depends upon the step before it, thus the heuristic search learns what avenues to pursue and which ones to disregard by measuring how close the current step is to the solution. Therefore, some possibilities will never be generated as they are measured to be less likely to complete the solution.
A heuristic method can accomplish its task by using search trees. However, instead of generating all possible solution branches, a heuristic selects branches more likely to produce outcomes than other branches. It is selective at each decision point, picking branches that are more likely to produce solutions.


=== Antivirus software ===
Antivirus software often uses heuristic rules for detecting viruses and other forms of malware. Heuristic scanning looks for code and/or behavioral patterns common to a class or family of viruses, with different sets of rules for different viruses. If a file or executing process is found to contain matching code patterns and/or to be performing that set of activities, then the scanner infers that the file is infected. The most advanced part of behavior-based heuristic scanning is that it can work against highly randomized self-modifying/mutating (polymorphic) viruses that cannot be easily detected by simpler string scanning methods. Heuristic scanning has the potential to detect future viruses without requiring the virus to be first detected somewhere else, submitted to the virus scanner developer, analyzed, and a detection update for the scanner provided to the scanner's users.


== Pitfalls ==
Some heuristics have a strong underlying theory; they are either derived in a top-down manner from the theory or are arrived at based on either experimental or real world data. Others are just rules of thumb based on real-world observation or experience without even a glimpse of theory. The latter are exposed to a larger number of pitfalls.
When a heuristic is reused in various contexts because it has been seen to ""work"" in one context, without having been mathematically proven to meet a given set of requirements, it is possible that the current data set does not necessarily represent future data sets (see: overfitting) and that purported ""solutions"" turn out to be akin to noise.
Statistical analysis can be conducted when employing heuristics to estimate the probability of incorrect outcomes. To use a heuristic for solving a search problem or a knapsack problem, it is necessary to check that the heuristic is admissible. Given a heuristic function 
  
    
      
        h
        (
        
          v
          
            i
          
        
        ,
        
          v
          
            g
          
        
        )
      
    
    {\displaystyle h(v_{i},v_{g})}
   meant to approximate the true optimal distance 
  
    
      
        
          d
          
            ⋆
          
        
        (
        
          v
          
            i
          
        
        ,
        
          v
          
            g
          
        
        )
      
    
    {\displaystyle d^{\star }(v_{i},v_{g})}
   to the goal node 
  
    
      
        
          v
          
            g
          
        
      
    
    {\displaystyle v_{g}}
   in a directed graph 
  
    
      
        G
      
    
    {\displaystyle G}
   containing 
  
    
      
        n
      
    
    {\displaystyle n}
   total nodes or vertices labeled 
  
    
      
        
          v
          
            0
          
        
        ,
        
          v
          
            1
          
        
        ,
        ⋯
        ,
        
          v
          
            n
          
        
      
    
    {\displaystyle v_{0},v_{1},\cdots ,v_{n}}
  , ""admissible"" means roughly that the heuristic underestimates the cost to the goal or formally that 
  
    
      
        h
        (
        
          v
          
            i
          
        
        ,
        
          v
          
            g
          
        
        )
        ≤
        
          d
          
            ⋆
          
        
        (
        
          v
          
            i
          
        
        ,
        
          v
          
            g
          
        
        )
      
    
    {\displaystyle h(v_{i},v_{g})\leq d^{\star }(v_{i},v_{g})}
   for all 
  
    
      
        (
        
          v
          
            i
          
        
        ,
        
          v
          
            g
          
        
        )
      
    
    {\displaystyle (v_{i},v_{g})}
   where 
  
    
      
        
          i
          ,
          g
        
        ∈
        [
        0
        ,
        1
        ,
        .
        .
        .
        ,
        n
        ]
      
    
    {\displaystyle {i,g}\in [0,1,...,n]}
  .
If a heuristic is not admissible, it may never find the goal, either by ending up in a dead end of graph 
  
    
      
        G
      
    
    {\displaystyle G}
   or by skipping back and forth between two nodes 
  
    
      
        
          v
          
            i
          
        
      
    
    {\displaystyle v_{i}}
   and 
  
    
      
        
          v
          
            j
          
        
      
    
    {\displaystyle v_{j}}
   where 
  
    
      
        
          i
          ,
          j
        
        ≠
        g
      
    
    {\displaystyle {i,j}\neq g}
  .


== Etymology ==

The word ""heuristic"" came into usage in the early 19th century. It is formed irregularly from the Greek word heuriskein, meaning ""to find"".


== See also ==
Algorithm
Constructive heuristic
Genetic algorithm
Heuristic
Heuristic routing
Heuristic evaluation: Method for identifying usability problems in user interfaces.
Metaheuristic: Methods for controlling and tuning basic heuristic algorithms, usually with usage of memory and learning.
Matheuristics: Optimization algorithms made by the interoperation of metaheuristics and mathematical programming (MP) techniques.
Reactive search optimization: Methods using online machine learning principles for self-tuning of heuristics.
Recursion (computer science)
Macro (computer science)


== References ==",1475829,207,"Articles with short description, Heuristic algorithms, Short description is different from Wikidata",1128496356,cs
https://en.wikipedia.org/wiki/Integer_(computer_science),Integer (computer science),"In computer science, an integer is a datum of integral data type, a data type that represents some range of mathematical integers. Integral data types may be of different sizes and may or may not be allowed to contain negative values. Integers are commonly represented in a computer as a group of binary digits (bits). The size of the grouping varies so the set of integer sizes available varies between different types of computers. Computer hardware nearly always provides a way to represent a processor register or memory address as an integer.","In computer science, an integer is a datum of integral data type, a data type that represents some range of mathematical integers. Integral data types may be of different sizes and may or may not be allowed to contain negative values. Integers are commonly represented in a computer as a group of binary digits (bits). The size of the grouping varies so the set of integer sizes available varies between different types of computers. Computer hardware nearly always provides a way to represent a processor register or memory address as an integer.


== Value and representation ==
The value of an item with an integral type is the mathematical integer that it corresponds to. Integral types may be unsigned (capable of representing only non-negative integers) or signed (capable of representing negative integers as well).An integer value is typically specified in the source code of a program as a sequence of digits optionally prefixed with + or −. Some programming languages allow other notations, such as hexadecimal (base 16) or octal (base 8). Some programming languages also permit digit group separators.The internal representation of this datum is the way the value is stored in the computer's memory. Unlike mathematical integers, a typical datum in a computer has some minimal and maximum possible value.
The most common representation of a positive integer is a string of bits, using the binary numeral system. The order of the memory bytes storing the bits varies; see endianness. The width or precision of an integral type is the number of bits in its representation. An integral type with n bits can encode 2n numbers; for example an unsigned type typically represents the non-negative values 0 through 2n−1. Other encodings of integer values to bit patterns are sometimes used, for example binary-coded decimal or Gray code, or as printed character codes such as ASCII.
There are four well-known ways to represent signed numbers in a binary computing system. The most common is two's complement, which allows a signed integral type with n bits to represent numbers from −2(n−1) through 2(n−1)−1. Two's complement arithmetic is convenient because there is a perfect one-to-one correspondence between representations and values (in particular, no separate +0 and −0), and because addition, subtraction and multiplication do not need to distinguish between signed and unsigned types. Other possibilities include offset binary, sign-magnitude, and ones' complement.
Some computer languages define integer sizes in a machine-independent way; others have varying definitions depending on the underlying processor word size. Not all language implementations define variables of all integer sizes, and defined sizes may not even be distinct in a particular implementation. An integer in one programming language may be a different size in a different language or on a different processor.


== Common integral data types ==
Different CPUs support different integral data types. Typically, hardware will support both signed and unsigned types, but only a small, fixed set of widths.
The table above lists integral type widths that are supported in hardware by common processors. High level programming languages provide more possibilities. It is common to have a 'double width' integral type that has twice as many bits as the biggest hardware-supported type. Many languages also have bit-field types (a specified number of bits, usually constrained to be less than the maximum hardware-supported width) and range types (that can represent only the integers in a specified range).
Some languages, such as Lisp, Smalltalk, REXX, Haskell, Python, and Raku, support arbitrary precision integers (also known as infinite precision integers or bignums). Other languages that do not support this concept as a top-level construct may have libraries available to represent very large numbers using arrays of smaller variables, such as Java's BigInteger class or Perl's ""bigint"" package. These use as much of the computer's memory as is necessary to store the numbers; however, a computer has only a finite amount of storage, so they, too, can only represent a finite subset of the mathematical integers. These schemes support very large numbers; for example one kilobyte of memory could be used to store numbers up to 2466 decimal digits long.
A Boolean or Flag type is a type that can represent only two values: 0 and 1, usually identified with false and true respectively.  This type can be stored in memory using a single bit, but is often given a full byte for convenience of addressing and speed of access.
A four-bit quantity is known as a nibble (when eating, being smaller than a bite) or nybble (being a pun on the form of the word byte). One nibble corresponds to one digit in hexadecimal and holds one digit or a sign code in binary-coded decimal.


=== Bytes and octets ===

The term byte initially meant 'the smallest addressable unit of memory'. In the past, 5-, 6-, 7-, 8-, and 9-bit bytes have all been used. There have also been computers that could address individual bits ('bit-addressed machine'), or that could only address 16- or 32-bit quantities ('word-addressed machine'). The term byte was usually not used at all in connection with bit- and word-addressed machines.
The term octet always refers to an 8-bit quantity. It is mostly used in the field of computer networking, where computers with different byte widths might have to communicate.
In modern usage byte almost invariably means eight bits, since all other sizes have fallen into disuse; thus byte has come to be synonymous with octet.


=== Words ===

The term 'word' is used for a small group of bits that are handled simultaneously by processors of a particular architecture. The size of a word is thus CPU-specific. Many different word sizes have been used, including 6-, 8-, 12-, 16-, 18-, 24-, 32-, 36-, 39-, 40-, 48-, 60-, and 64-bit. Since it is architectural, the size of a word is usually set by the first CPU in a family, rather than the characteristics of a later compatible CPU. The meanings of terms derived from word, such as longword, doubleword, quadword, and halfword, also vary with the CPU and OS.Practically all new desktop processors are capable of using 64-bit words, though embedded processors with 8- and 16-bit word size are still common. The 36-bit word length was common in the early days of computers.
One important cause of non-portability of software is the incorrect assumption that all computers have the same word size as the computer used by the programmer. For example, if a programmer using the C language incorrectly declares as int a variable that will be used to store values greater than 215−1, the program will fail on computers with 16-bit integers. That variable should have been declared as long, which has at least 32 bits on any computer. Programmers may also incorrectly assume that a pointer can be converted to an integer without loss of information, which may work on (some) 32-bit computers, but fail on 64-bit computers with 64-bit pointers and 32-bit integers. This issue is resolved by C99 in stdint.h in the form of intptr_t.


=== Short integer ===
A short integer can represent a whole number that may take less storage, while having a smaller range, compared with a standard integer on the same machine.
In C, it is denoted by short. It is required to be at least 16 bits, and is often smaller than a standard integer, but this is not required. A conforming program can assume that it can safely store values between −(215−1) and 215−1, but it may not assume that the range is not larger. In Java, a short is always a 16-bit integer. In the Windows API, the datatype SHORT is defined as a 16-bit signed integer on all machines.


=== Long integer ===
A long integer can represent a whole integer whose range is greater than or equal to that of a standard integer on the same machine.
In C, it is denoted by long. It is required to be at least 32 bits, and may or may not be larger than a standard integer. A conforming program can assume that it can safely store values between −(231−1) and 231−1, but it may not assume that the range is not larger.


=== Long long ===

In the C99 version of the C programming language and the C++11 version of C++, a long long type is supported that has double the minimum capacity of the standard long. This type is not supported by compilers that require C code to be compliant with the previous C++ standard, C++03, because the long long type did not exist in C++03. For an ANSI/ISO compliant compiler, the minimum requirements for the specified ranges, that is, −(263−1) to 263−1 for signed and 0 to 264−1 for unsigned, must be fulfilled; however, extending this range is permitted. This can be an issue when exchanging code and data between platforms, or doing direct hardware access. Thus, there are several sets of headers providing platform independent exact width types. The C standard library provides stdint.h; this was introduced in C99 and C++11.


== Syntax ==
Literals for integers can be written as regular Arabic numerals, consisting of a sequence of digits and with negation indicated by a minus sign before the value. However, most programming languages disallow use of commas or spaces for digit grouping. Examples of integer literals are:

42
10000
-233000There are several alternate methods for writing integer literals in many programming languages:

Most programming languages, especially those influenced by C, prefix an integer literal with 0X or 0x to represent a hexadecimal value, e.g. 0xDEADBEEF. Other languages may use a different notation, e.g. some assembly languages append an H or h to the end of a hexadecimal value.
Perl, Ruby, Java, Julia, D, Go, Rust and Python (starting from version 3.6) allow embedded underscores for clarity, e.g. 10_000_000, and fixed-form Fortran ignores embedded spaces in integer literals. C (starting from C23) and C++ use single quotes for this purpose.
In C and C++, a leading zero indicates an octal value, e.g. 0755. This was primarily intended to be used with Unix modes; however, it has been criticized because normal integers may also lead with zero. As such, Python, Ruby, Haskell, and OCaml prefix octal values with 0O or 0o, following the layout used by hexadecimal values.
Several languages, including Java, C#, Scala, Python, Ruby, OCaml, C (starting from C23) and C++ can represent binary values by prefixing a number with 0B or 0b.


== See also ==
Arbitrary-precision arithmetic
Binary-coded decimal (BCD)
C data types
Integer overflow
Signed number representations


== Notes ==


== References ==",2044865,763,"Articles with short description, Computer arithmetic, Data types, Primitive types, Short description matches Wikidata, Webarchive template wayback links",1128516282,cs
https://en.wikipedia.org/wiki/Computer_science_and_engineering,Computer science and engineering,Computer Science and Engineering (CSE) is an academic program at many universities which comprises scientific and engineering aspects of computing. CSE is also a term often used in Europe to translate the name of engineering informatics academic programs. It is offered in both Undergraduate as well Postgraduate with specializations.,"Computer Science and Engineering (CSE) is an academic program at many universities which comprises scientific and engineering aspects of computing. CSE is also a term often used in Europe to translate the name of engineering informatics academic programs. It is offered in both Undergraduate as well Postgraduate with specializations.


== Academic courses ==
Academic programs vary between colleges, but typically include a combination of topics in computer science, computer engineering, and electrical engineering. Undergraduate courses usually include programming, algorithms and data structures, computer architecture, operating systems, computer networks, parallel computing, embedded systems, algorithms design, circuit analysis and electronics, digital logic and processor design, computer graphics, scientific computing, software engineering, database systems, digital signal processing, virtualization, computer simulations and games programming. CSE programs also include core subjects of theoretical computer science such as theory of computation, numerical methods, machine learning, programming theory and paradigms. Modern academic programs also cover emerging computing fields like image processing, data science, robotics, bio-inspired computing, computational biology, autonomic computing and artificial intelligence. Most CSE programs require introductory mathematical knowledge, hence the first year of study is dominated by mathematical courses, primarily discrete mathematics, mathematical analysis, linear algebra, probability, and statistics, as well as the basics of electrical and electronic engineering, physics, and electromagnetism.


== Example universities with CSE majors and departments ==
American International University-Bangladesh
American University of Beirut
Amrita Vishwa Vidyapeetham
Bangladesh University of Engineering and Technology
Bucknell University
Delft University of Technology
Indian Institute of Technology Kanpur
Indian Institute of Technology Bombay
Indian Institute of Technology Delhi
Indian Institute of Technology Madras
Lund University
Massachusetts Institute of Technology
North South University
Ohio State University
Santa Clara University
Seoul National University
UC Davis
UC Irvine
UCLA
UC Merced
UC San Diego
UC Santa Cruz
University of Michigan
University of New South Wales
University of Nevada
University of Notre Dame
University of Washington
Shahid Beheshti University


== See also ==
Computer science
Computer engineering
Computer graphics (computer science)
Bachelor of Technology


== References ==",294323,399,"Articles with short description, Computer engineering, Computer science education, Engineering academics, Engineering education, Short description is different from Wikidata",1131762653,cs
https://en.wikipedia.org/wiki/Theoretical_computer_science,Theoretical computer science,"Theoretical computer science (TCS) is a subset of general computer science and mathematics that focuses on mathematical aspects of computer science such as the theory of computation, lambda calculus, and type theory.
It is difficult to circumscribe the theoretical areas precisely. The ACM's Special Interest Group on Algorithms and Computation Theory (SIGACT) provides the following description:
TCS covers a wide variety of topics including algorithms, data structures, computational complexity, parallel and distributed computation, probabilistic computation, quantum computation, automata theory, information theory, cryptography, program semantics and verification, algorithmic game theory, machine learning, computational biology, computational economics, computational geometry, and computational number theory and algebra. Work in this field is often distinguished by its emphasis on mathematical technique and rigor.

","Theoretical computer science (TCS) is a subset of general computer science and mathematics that focuses on mathematical aspects of computer science such as the theory of computation, lambda calculus, and type theory.
It is difficult to circumscribe the theoretical areas precisely. The ACM's Special Interest Group on Algorithms and Computation Theory (SIGACT) provides the following description:
TCS covers a wide variety of topics including algorithms, data structures, computational complexity, parallel and distributed computation, probabilistic computation, quantum computation, automata theory, information theory, cryptography, program semantics and verification, algorithmic game theory, machine learning, computational biology, computational economics, computational geometry, and computational number theory and algebra. Work in this field is often distinguished by its emphasis on mathematical technique and rigor.


== History ==

While logical inference and mathematical proof had existed previously, in 1931 Kurt Gödel proved with his incompleteness theorem that there are fundamental limitations on what statements could be proved or disproved.
Information theory was added to the field with a 1948 mathematical theory of communication by Claude Shannon. In the same decade, Donald Hebb introduced a mathematical model of learning in the brain. With mounting biological data supporting this hypothesis with some modification, the fields of neural networks and parallel distributed processing were established. In 1971, Stephen Cook and, working independently, Leonid Levin, proved that there exist practically relevant problems that are NP-complete – a landmark result in computational complexity theory.
With the development of quantum mechanics in the beginning of the 20th century came the concept that mathematical operations could be performed on an entire particle wavefunction. In other words, one could compute functions on multiple states simultaneously. This led to the concept of a quantum computer in the latter half of the 20th century that took off in the 1990s when Peter Shor showed that such methods could be used to factor large numbers in polynomial time, which, if implemented, would render some modern public key cryptography algorithms like RSA insecure.Modern theoretical computer science research is based on these basic developments, but includes many other mathematical and interdisciplinary problems that have been posed, as shown below:


== Topics ==


=== Algorithms ===

An algorithm is a step-by-step procedure for calculations.  Algorithms are used for calculation, data processing, and automated reasoning.
An algorithm is an effective method expressed as a finite list of well-defined instructions for calculating a function.  Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing ""output"" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.


=== Automata theory ===

Automata theory is the study of abstract machines and automata, as well as the computational problems that can be solved using them. It is a theory in theoretical computer science, under discrete mathematics (a section of mathematics and also of computer science). Automata comes from the Greek word αὐτόματα meaning ""self-acting"".
Automata Theory is the study of self-operating virtual machines to help in the logical understanding of input and output process, without or with intermediate stage(s) of computation (or any function/process).


=== Coding theory ===

Coding theory is the study of the properties of codes and their fitness for a specific application. Codes are used for data compression, cryptography, error-correction and more recently also for network coding. Codes are studied by various scientific disciplines—such as information theory, electrical engineering,  mathematics, and computer science—for the purpose of designing efficient and reliable data transmission methods. This typically involves the removal of redundancy and the correction (or detection) of errors in the transmitted data.


=== Computational biology ===

Computational biology involves the development and application of data-analytical and theoretical methods, mathematical modeling and computational simulation techniques to the study of biological, behavioral, and social systems. The field is broadly defined and includes foundations in computer science, applied mathematics, animation, statistics, biochemistry, chemistry, biophysics, molecular biology, genetics, genomics, ecology, evolution, anatomy, neuroscience, and visualization.Computational biology is different from biological computation, which is a subfield of computer science and computer engineering using bioengineering and biology to build computers, but is similar to bioinformatics, which is an interdisciplinary science using computers to store and process biological data.


=== Computational complexity theory ===

Computational complexity theory is a branch of the theory of computation that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each other. A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an algorithm.
A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying the amount of resources needed to solve them, such as time and storage. Other complexity measures are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do.


=== Computational geometry ===

Computational geometry is a branch of computer science devoted to the study of algorithms that can be stated in terms of geometry. Some purely geometrical problems arise out of the study of computational geometric algorithms, and such problems are also considered to be part of computational geometry. 
The main impetus for the development of computational geometry as a discipline was progress in computer graphics and computer-aided design and manufacturing (CAD/CAM), but many problems in computational geometry are classical in nature, and may come from mathematical visualization.
Other important applications of computational geometry include robotics (motion planning and visibility problems), geographic information systems (GIS) (geometrical location and search, route planning), integrated circuit design (IC geometry design and verification), computer-aided engineering (CAE) (mesh generation), computer vision (3D reconstruction).


=== Computational learning theory ===

Theoretical results in machine learning mainly deal with a type of inductive learning called supervised learning.  In supervised learning, an algorithm is given samples that are labeled in some
useful way.  For example, the samples might be descriptions of mushrooms, and the labels could be whether or not the mushrooms are edible.  The algorithm takes these previously labeled samples and
uses them to induce a classifier.  This classifier is a function that assigns labels to samples including the samples that have never been previously seen by the algorithm.  The goal of the supervised learning algorithm is to optimize some measure of performance such as minimizing the number of mistakes made on new samples.


=== Computational number theory ===

Computational number theory, also known as algorithmic number theory, is the study of algorithms for performing number theoretic computations. The best known problem in the field is integer factorization.


=== Cryptography ===

Cryptography  is the practice and study of techniques for secure communication in the presence of third parties (called adversaries). More generally, it is about constructing and analyzing protocols that overcome the influence of adversaries and that are related to various aspects in information security such as data confidentiality, data integrity, authentication, and non-repudiation. Modern cryptography intersects the disciplines of mathematics, computer science, and electrical engineering. Applications of cryptography include ATM cards, computer passwords, and electronic commerce.
Modern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions, making such algorithms hard to break in practice by any adversary. It is theoretically possible to break such a system, but it is infeasible to do so by any known practical means. These schemes are therefore termed computationally secure; theoretical advances, e.g., improvements in integer factorization algorithms, and faster computing technology require these solutions to be continually adapted. There exist information-theoretically secure schemes that provably cannot be broken even with unlimited computing power—an example is the one-time pad—but these schemes are more difficult to implement than the best theoretically breakable but computationally secure mechanisms.


=== Data structures ===

A data structure is a particular way of organizing data in a computer so that it can be used efficiently.Different kinds of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. For example, databases use B-tree indexes for small percentages of data retrieval and compilers and databases use dynamic hash tables as look up tables.
Data structures provide a means to manage large amounts of data efficiently for uses such as large databases and internet indexing services. Usually, efficient data structures are key to designing efficient algorithms. Some formal design methods and programming languages emphasize data structures, rather than algorithms, as the key organizing factor in software design. Storing and retrieving can be carried out on data stored in both main memory and in secondary memory.


=== Distributed computation ===

Distributed computing studies distributed systems. A distributed system is a software system in which components located on networked computers communicate and coordinate their actions by passing messages. The components interact with each other in order to achieve a common goal. Three significant characteristics of distributed systems are: concurrency of components, lack of a global clock, and independent failure of components. Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to  peer-to-peer applications, and blockchain networks like Bitcoin.
A computer program that runs in a distributed system is called a distributed program, and distributed programming is the process of writing such programs. There are many alternatives for the message passing mechanism, including RPC-like connectors and message queues.  An important goal and challenge of distributed systems is location transparency.


=== Information-based complexity ===

Information-based complexity (IBC) studies optimal algorithms and computational complexity for continuous problems. IBC has studied continuous problems as path integration, partial differential equations, systems of ordinary differential equations, nonlinear equations, integral equations, fixed points, and very-high-dimensional integration.


=== Formal methods ===

Formal methods are a particular kind of mathematics based techniques for the specification, development and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.


=== Information theory ===

Information theory is a branch of applied mathematics, electrical engineering, and computer science involving the quantification of information.  Information theory was developed by Claude E. Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data. Since its inception it has broadened to find applications in many other areas, including statistical inference, natural language processing, cryptography, neurobiology, the evolution and function of molecular codes, model selection in statistics, thermal physics, quantum computing, linguistics, plagiarism detection, pattern recognition, anomaly detection and other forms of data analysis.Applications of fundamental topics of information theory include lossless data compression (e.g. ZIP files), lossy data compression (e.g. MP3s and JPEGs), and channel coding (e.g. for Digital Subscriber Line (DSL)).  The field is at the intersection of mathematics, statistics, computer science, physics, neurobiology, and electrical engineering. Its impact has been crucial to the success of the Voyager missions to deep space, the invention of the compact disc, the feasibility of mobile phones, the development of the Internet, the study of linguistics and of human perception, the understanding of black holes, and numerous other fields. Important sub-fields of information theory are source coding, channel coding, algorithmic complexity theory, algorithmic information theory, information-theoretic security, and measures of information.


=== Machine learning ===

Machine learning is a scientific discipline that deals with the construction and study of algorithms that can learn from data. Such algorithms operate by building a model based on inputs: 2  and using that to make predictions or decisions, rather than following only explicitly programmed instructions.
Machine learning can be considered a subfield of computer science and statistics. It has strong ties to artificial intelligence and optimization, which deliver methods, theory and application domains to the field. Machine learning is employed in a range of computing tasks where designing and programming explicit, rule-based algorithms is infeasible. Example applications include spam filtering, optical character recognition (OCR), search engines and computer vision. Machine learning is sometimes conflated with data mining, although that focuses more on exploratory data analysis. Machine learning and pattern recognition ""can be viewed as two facets of
the same field."": vii 


=== Parallel computation ===

Parallel computing is a form of computation in which many calculations are carried out simultaneously, operating on the principle that large problems can often be divided into smaller ones, which are then solved ""in parallel"". There are several different forms of parallel computing: bit-level, instruction level, data, and task parallelism. Parallelism has been employed for many years, mainly in high-performance computing, but interest in it has grown lately due to the physical constraints preventing frequency scaling. As power consumption (and consequently heat generation) by computers has become a concern in recent years, parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.Parallel computer programs are more difficult to write than sequential ones, because concurrency introduces several new classes of potential software bugs, of which race conditions are the most common. Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting good parallel program performance.
The maximum possible speed-up of a single program as a result of parallelization is known as Amdahl's law.


=== Program semantics ===

In programming language theory, semantics is the field concerned with the rigorous mathematical study of the meaning of programming languages. It does so by evaluating the meaning of syntactically legal strings defined by a specific programming language, showing the computation involved. In such a case that the evaluation would be of syntactically illegal strings, the result would be non-computation. Semantics describes the processes a computer follows when executing a program in that specific language. This can be shown by describing the relationship between the input and output of a program, or an explanation of how the program will execute on a certain platform, hence creating a model of computation.


=== Quantum computation ===

A quantum computer is a computation system that makes direct use of quantum-mechanical phenomena, such as superposition and entanglement, to perform operations on data. Quantum computers are different from digital computers based on transistors. Whereas digital computers require data to be encoded into binary digits (bits), each of which is always in one of two definite states (0 or 1), quantum computation uses qubits (quantum bits), which can be in superpositions of states. A theoretical model is the quantum Turing machine, also known as the universal quantum computer.  Quantum computers share theoretical similarities with non-deterministic and probabilistic computers; one example is the ability to be in more than one state simultaneously.  The field of quantum computing was first introduced by Yuri Manin in 1980 and Richard Feynman in 1982. A quantum computer with spins as quantum bits was also formulated for use as a quantum space–time in 1968.As of 2014, quantum computing is still in its infancy but experiments have been carried out in which quantum computational operations were executed on a very small number of qubits. Both practical and theoretical research continues, and many national governments and military funding agencies support quantum computing research to develop quantum computers for both civilian and national security purposes, such as cryptanalysis.


=== Symbolic computation ===

Computer algebra, also called symbolic computation or algebraic computation is a scientific area that refers to the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects. Although, properly speaking, computer algebra should be a subfield of scientific computing, they are generally considered as distinct fields because scientific computing is usually based on numerical computation with approximate floating point numbers, while symbolic computation emphasizes exact computation with expressions containing variables that have not any given value and are thus manipulated as symbols (therefore the name of symbolic computation).
Software applications that perform symbolic calculations are called computer algebra systems, with the term system alluding to the complexity of the main applications  that include, at least, a method to represent mathematical data in a computer, a user programming language (usually different from the language used for the implementation), a dedicated memory manager, a user interface for the input/output of mathematical expressions, a large set of routines to perform usual operations, like simplification of expressions, differentiation using chain rule, polynomial factorization, indefinite integration, etc.


=== Very-large-scale integration ===

Very-large-scale integration (VLSI) is the process of creating an integrated circuit (IC) by combining thousands of transistors into a single chip. VLSI began in the 1970s when complex semiconductor and communication technologies were being developed. The microprocessor is a VLSI device. Before the introduction of VLSI technology most ICs had a limited set of functions they could perform. An electronic circuit might consist of a CPU, ROM, RAM and other glue logic. VLSI allows IC makers to add all of these circuits into one chip.


== Organizations ==
European Association for Theoretical Computer Science
SIGACT
Simons Institute for the Theory of Computing


== Journals and newsletters ==
""Discrete Mathematics and Theoretical Computer Science""
Information and Computation
Theory of Computing (open access journal)
Formal Aspects of Computing
Journal of the ACM
SIAM Journal on Computing (SICOMP)
SIGACT News
Theoretical Computer Science
Theory of Computing Systems
International Journal of Foundations of Computer Science
Chicago Journal of Theoretical Computer Science (open access journal)
Foundations and Trends in Theoretical Computer Science
Journal of Automata, Languages and Combinatorics
Acta Informatica
Fundamenta Informaticae
ACM Transactions on Computation Theory
Computational Complexity
Journal of Complexity
ACM Transactions on Algorithms
Information Processing Letters
Open Computer Science (open access journal)


== Conferences ==
Annual ACM Symposium on Theory of Computing (STOC)
Annual IEEE Symposium on Foundations of Computer Science (FOCS)
Innovations in Theoretical Computer Science (ITCS)
Mathematical Foundations of Computer Science (MFCS)
International Computer Science Symposium in Russia (CSR)
ACM–SIAM Symposium on Discrete Algorithms (SODA)
IEEE Symposium on Logic in Computer Science (LICS)
Computational Complexity Conference (CCC)
International Colloquium on Automata, Languages and Programming (ICALP)
Annual Symposium on Computational Geometry (SoCG)
ACM Symposium on Principles of Distributed Computing (PODC)
ACM Symposium on Parallelism in Algorithms and Architectures (SPAA)
Annual Conference on Learning Theory (COLT)
Symposium on Theoretical Aspects of Computer Science (STACS)
European Symposium on Algorithms (ESA)
Workshop on Approximation Algorithms for Combinatorial Optimization Problems (APPROX)
Workshop on Randomization and Computation (RANDOM)
International Symposium on Algorithms and Computation (ISAAC)
International Symposium on Fundamentals of Computation Theory (FCT)
International Workshop on Graph-Theoretic Concepts in Computer Science (WG)


== See also ==
Formal science
Unsolved problems in computer science
List of important publications in theoretical computer science
Sun–Ni law


== Notes ==


== Further reading ==
Martin Davis, Ron Sigal, Elaine J. Weyuker, Computability, complexity, and languages: fundamentals of theoretical computer science, 2nd ed., Academic Press, 1994, ISBN 0-12-206382-1. Covers theory of computation, but also program semantics and quantification theory. Aimed at graduate students.


== External links ==
SIGACT directory of additional theory links
Theory Matters Wiki Theoretical Computer Science (TCS) Advocacy Wiki
List of academic conferences in the area of theoretical computer science at confsearch
Theoretical Computer Science - StackExchange, a Question and Answer site for researchers in theoretical computer science
Computer Science Animated
Theory of computation @ Massachusetts Institute of Technology",998839,394,"All articles containing potentially dated statements, All articles with unsourced statements, Articles containing potentially dated statements from 2014, Articles with GND identifiers, Articles with short description, Articles with unsourced statements from October 2014, Articles with unsourced statements from September 2017, CS1 Russian-language sources (ru), CS1 errors: missing periodical, Formal sciences, Harv and Sfn no-target errors, Short description matches Wikidata, Theoretical computer science, Webarchive template wayback links",1115029228,cs
https://en.wikipedia.org/wiki/Abstraction_(computer_science),Abstraction (computer science),"In software engineering and computer science, abstraction is:

The process of removing or generalizing physical, spatial, or temporal details or attributes in the study of objects or systems to focus attention on details of greater importance; it is similar in nature to the process of generalization;
the creation of abstract concept-objects by mirroring common features or attributes of various non-abstract objects or systems of study – the result of the process of abstraction.Abstraction, in general, is a fundamental concept in computer science and software development. The process of abstraction can also be referred to as modeling and is closely related to the concepts of theory and design. Models can also be considered types of abstractions per their generalization of aspects of reality.
Abstraction in computer science is closely related to abstraction in mathematics due to their common focus on building abstractions as objects, but is also related to other notions of abstraction used in other fields such as art.Abstractions may also refer to real-world objects and systems, rules of computational systems or rules of programming languages that carry or utilize features of abstraction itself, such as:

the usage of data types to perform data abstraction to separate usage from working representations of data structures within programs;
the concept of procedures, functions, or subroutines which represent a specific of implementing control flow in programs;
the rules commonly named ""abstraction"" that generalize expressions using free and bound variables in the various versions of lambda calculus;
the usage of S-expressions as an abstraction of data structures and programs in the Lisp programming language;
the process of reorganizing common behavior from non-abstract classes into ""abstract classes"" using inheritance to abstract over sub-classes as seen in the object-oriented C++ and Java programming languages.

","In software engineering and computer science, abstraction is:

The process of removing or generalizing physical, spatial, or temporal details or attributes in the study of objects or systems to focus attention on details of greater importance; it is similar in nature to the process of generalization;
the creation of abstract concept-objects by mirroring common features or attributes of various non-abstract objects or systems of study – the result of the process of abstraction.Abstraction, in general, is a fundamental concept in computer science and software development. The process of abstraction can also be referred to as modeling and is closely related to the concepts of theory and design. Models can also be considered types of abstractions per their generalization of aspects of reality.
Abstraction in computer science is closely related to abstraction in mathematics due to their common focus on building abstractions as objects, but is also related to other notions of abstraction used in other fields such as art.Abstractions may also refer to real-world objects and systems, rules of computational systems or rules of programming languages that carry or utilize features of abstraction itself, such as:

the usage of data types to perform data abstraction to separate usage from working representations of data structures within programs;
the concept of procedures, functions, or subroutines which represent a specific of implementing control flow in programs;
the rules commonly named ""abstraction"" that generalize expressions using free and bound variables in the various versions of lambda calculus;
the usage of S-expressions as an abstraction of data structures and programs in the Lisp programming language;
the process of reorganizing common behavior from non-abstract classes into ""abstract classes"" using inheritance to abstract over sub-classes as seen in the object-oriented C++ and Java programming languages.


== Rationale ==
Computing mostly operates independently of the concrete world. The hardware implements a model of computation that is interchangeable with others. The software is structured in architectures to enable humans to create the enormous systems by concentrating on a few issues at a time. These architectures are made of specific choices of abstractions. Greenspun's Tenth Rule is an aphorism on how such an architecture is both inevitable and complex.
A central form of abstraction in computing is language abstraction: new artificial languages are developed to express specific aspects of a system. Modeling languages help in planning. Computer languages can be processed with a computer. An example of this abstraction process is the generational development of programming languages from the machine language to the assembly language and the high-level language. Each stage can be used as a stepping stone for the next stage. The language abstraction continues for example in scripting languages and domain-specific programming languages.
Within a programming language, some features let the programmer create new abstractions. These include subroutines, modules, polymorphism, and software components. Some other abstractions such as software design patterns and architectural styles remain invisible to a translator and operate only in the design of a system.
Some abstractions try to limit the range of concepts a programmer needs to be aware of, by completely hiding the abstractions that they in turn are built on. The software engineer and writer Joel Spolsky has criticised these efforts by claiming that all abstractions are leaky – that they can never completely hide the details below; however, this does not negate the usefulness of abstraction.
Some abstractions are designed to inter-operate with other abstractions – for example, a programming language may contain a foreign function interface for making calls to the lower-level language.


== Abstraction features ==


=== Programming languages ===

Different programming languages provide different types of abstraction, depending on the intended applications for the language. For example:

In object-oriented programming languages such as C++, Object Pascal, or Java, the concept of abstraction has itself become a declarative statement – using the syntax function(parameters) = 0; (in C++) or the keywords abstract and interface (in Java). After such a declaration, it is the responsibility of the programmer to implement a class to instantiate the object of the declaration.
Functional programming languages commonly exhibit abstractions related to functions, such as lambda abstractions (making a term into a function of some variable) and higher-order functions (parameters are functions).
Modern members of the Lisp programming language family such as Clojure, Scheme and Common Lisp support macro systems to allow syntactic abstraction. Other programming languages such as Scala also have macros, or very similar metaprogramming features (for example, Haskell has Template Haskell, and OCaml has MetaOCaml). These can allow a programmer to eliminate boilerplate code, abstract away tedious function call sequences, implement new control flow structures, and implement Domain Specific Languages (DSLs), which allow domain-specific concepts to be expressed in concise and elegant ways. All of these, when used correctly, improve both the programmer's efficiency and the clarity of the code by making the intended purpose more explicit. A consequence of syntactic abstraction is also that any Lisp dialect and in fact almost any programming language can, in principle, be implemented in any modern Lisp with significantly reduced (but still non-trivial in most cases) effort when compared to ""more traditional"" programming languages such as Python, C or Java.


=== Specification methods ===

Analysts have developed various methods to formally specify software systems.  Some known methods include:

Abstract-model based method (VDM, Z);
Algebraic techniques (Larch, CLEAR, OBJ, ACT ONE, CASL);
Process-based techniques (LOTOS, SDL, Estelle);
Trace-based techniques (SPECIAL, TAM);
Knowledge-based techniques (Refine, Gist).


=== Specification languages ===

Specification languages generally rely on abstractions of one kind or another, since specifications are typically defined earlier in a project, (and at a more abstract level) than an eventual implementation. The UML specification language, for example, allows the definition of abstract classes, which in a waterfall project, remain abstract during the architecture and specification phase of the project.


== Control abstraction ==

Programming languages offer control abstraction as one of the main purposes of their use. Computer machines understand operations at the very low level such as moving some bits from one location of the memory to another location and producing the sum of two sequences of bits. Programming languages allow this to be done in the higher level. For example, consider this statement written in a Pascal-like fashion:

a := (1 + 2) * 5To a human, this seems a fairly simple and obvious calculation (""one plus two is three, times five is fifteen""). However, the low-level steps necessary to carry out this evaluation, and return the value ""15"", and then assign that value to the variable ""a"", are actually quite subtle and complex. The values need to be converted to binary representation (often a much more complicated task than one would think) and the calculations decomposed (by the compiler or interpreter) into assembly instructions (again, which are much less intuitive to the programmer: operations such as shifting a binary register left, or adding the binary complement of the contents of one register to another, are simply not how humans think about the abstract arithmetical operations of addition or multiplication). Finally, assigning the resulting value of ""15"" to the variable labeled ""a"", so that ""a"" can be used later, involves additional 'behind-the-scenes' steps of looking up a variable's label and the resultant location in physical or virtual memory, storing the binary representation of ""15"" to that memory location, etc.
Without control abstraction, a programmer would need to specify all the register/binary-level steps each time they simply wanted to add or multiply a couple of numbers and assign the result to a variable. Such duplication of effort has two serious negative consequences:

it forces the programmer to constantly repeat fairly common tasks every time a similar operation is needed
it forces the programmer to program for the particular hardware and instruction set


=== Structured programming ===

Structured programming involves the splitting of complex program tasks into smaller pieces with clear flow-control and interfaces between components, with a reduction of the complexity potential for side-effects.
In a simple program, this may aim to ensure that loops have single or obvious exit points and (where possible) to have single exit points from functions and procedures.
In a larger system, it may involve breaking down complex tasks into many different modules. Consider a system which handles payroll on ships and at shore offices:

The uppermost level may feature a menu of typical end-user operations.
Within that could be standalone executables or libraries for tasks such as signing on and off employees or printing checks.
Within each of those standalone components there could be many different source files, each containing the program code to handle a part of the problem, with only selected interfaces available to other parts of the program. A sign on program could have source files for each data entry screen and the database interface (which may itself be a standalone third party library or a statically linked set of library routines).
Either the database or the payroll application also has to initiate the process of exchanging data with between ship and shore, and that data transfer task will often contain many other components.These layers produce the effect of isolating the implementation details of one component and its assorted internal methods from the others. Object-oriented programming embraces and extends this concept.


== Data abstraction ==

Data abstraction enforces a clear separation between the abstract properties of a data type and the concrete details of its implementation. The abstract properties are those that are visible to client code that makes use of the data type—the interface to the data type—while the concrete implementation is kept entirely private, and indeed can change, for example to incorporate efficiency improvements over time. The idea is that such changes are not supposed to have any impact on client code, since they involve no difference in the abstract behaviour.
For example, one could define an abstract data type called lookup table which uniquely associates keys with values, and in which values may be retrieved by specifying their corresponding keys. Such a lookup table may be implemented in various ways: as a hash table, a binary search tree, or even a simple linear list of (key:value) pairs. As far as client code is concerned, the abstract properties of the type are the same in each case.
Of course, this all relies on getting the details of the interface right in the first place, since any changes there can have major impacts on client code. As one way to look at this: the interface forms a contract on agreed behaviour between the data type and client code; anything not spelled out in the contract is subject to change without notice.


== Manual data abstraction ==
While much of data abstraction occurs through computer science and automation, there are times when this process is done manually and without programming intervention. One way this can be understood is through data abstraction within the process of conducting a systematic review of the literature. In this methodology, data is abstracted by one or several abstractors when conducting a meta-analysis, with errors reduced through dual data abstraction followed by independent checking, known as adjudication.


== Abstraction in object oriented programming ==

In object-oriented programming theory, abstraction involves the facility to define objects that represent abstract ""actors"" that can perform work, report on and change their state, and ""communicate"" with other objects in the system. The term encapsulation refers to the hiding of state details, but extending the concept of data type from earlier programming languages to associate behavior most strongly with the data, and standardizing the way that different data types interact, is the beginning of abstraction.  When abstraction proceeds into the operations defined, enabling objects of different types to be substituted, it is called polymorphism. When it proceeds in the opposite direction, inside the types or classes, structuring them to simplify a complex set of relationships, it is called delegation or inheritance.
Various object-oriented programming languages offer similar facilities for abstraction, all to support a general strategy of polymorphism in object-oriented programming, which includes the substitution of one type for another in the same or similar role. Although not as generally supported, a configuration or image or package may predetermine a great many of these bindings at compile-time, link-time, or loadtime. This would leave only a minimum of such bindings to change at run-time.
Common Lisp Object System or Self, for example, feature less of a class-instance distinction and more use of delegation for polymorphism. Individual objects and functions are abstracted more flexibly to better fit with a shared functional heritage from Lisp.
C++ exemplifies another extreme: it relies heavily on templates and overloading and other static bindings at compile-time, which in turn has certain flexibility problems.
Although these examples offer alternate strategies for achieving the same abstraction, they do not fundamentally alter the need to support abstract nouns in code – all programming relies on an ability to abstract verbs as functions, nouns as data structures, and either as processes.
Consider for example a sample Java fragment to represent some common farm ""animals"" to a level of abstraction suitable to model simple aspects of their hunger and feeding. It defines an Animal class to represent both the state of the animal and its functions:

With the above definition, one could create objects of type Animal and call their methods like this:

In the above example, the class Animal is an abstraction used in place of an actual animal, LivingThing is a further abstraction (in this case a generalisation) of Animal.
If one requires a more differentiated hierarchy of animals – to differentiate, say, those who provide milk from those who provide nothing except meat at the end of their lives – that is an intermediary level of abstraction, probably DairyAnimal (cows, goats) who would eat foods suitable to giving good milk, and MeatAnimal (pigs, steers) who would eat foods to give the best meat-quality.
Such an abstraction could remove the need for the application coder to specify the type of food, so they could concentrate instead on the feeding schedule. The two classes could be related using inheritance or stand alone, and the programmer could define varying degrees of polymorphism between the two types. These facilities tend to vary drastically between languages, but in general each can achieve anything that is possible with any of the others. A great many operation overloads, data type by data type, can have the same effect at compile-time as any degree of inheritance or other means to achieve polymorphism. The class notation is simply a coder's convenience.


=== Object-oriented design ===

Decisions regarding what to abstract and what to keep under the control of the coder become the major concern of object-oriented design and domain analysis—actually determining the relevant relationships in the real world is the concern of object-oriented analysis or legacy analysis.
In general, to determine appropriate abstraction, one must make many small decisions about scope (domain analysis), determine what other systems one must cooperate with (legacy analysis), then perform a detailed object-oriented analysis which is expressed within project time and budget constraints as an object-oriented design. In our simple example, the domain is the barnyard, the live pigs and cows and their eating habits are the legacy constraints, the detailed analysis is that coders must have the flexibility to feed the animals what is available and thus there is no reason to code the type of food into the class itself, and the design is a single simple Animal class of which pigs and cows are instances with the same functions. A decision to differentiate DairyAnimal would change the detailed analysis but the domain and legacy analysis would be unchanged—thus it is entirely under the control of the programmer, and it is called an abstraction in object-oriented programming as distinct from abstraction in domain or legacy analysis.


== Considerations ==
When discussing formal semantics of programming languages, formal methods or abstract interpretation, abstraction refers to the act of considering a less detailed, but safe, definition of the observed program behaviors. For instance, one may observe only the final result of program executions instead of considering all the intermediate steps of executions. Abstraction is defined to a concrete (more precise) model of execution.
Abstraction may be exact or faithful with respect to a property if one can answer a question about the property equally well on the concrete or abstract model. For instance, if one wishes to know what the result of the evaluation of a mathematical expression involving only integers +, -, ×, is worth modulo n, then one needs only perform all operations modulo n (a familiar form of this abstraction is casting out nines).
Abstractions, however, though not necessarily exact, should be sound. That is, it should be possible to get sound answers from them—even though the abstraction may simply yield a result of undecidability. For instance, students in a class may be abstracted by their minimal and maximal ages; if one asks whether a certain person belongs to that class, one may simply compare that person's age with the minimal and maximal ages; if his age lies outside the range, one may safely answer that the person does not belong to the class; if it does not, one may only answer ""I don't know"".
The level of abstraction included in a programming language can influence its overall usability. The Cognitive dimensions framework includes the concept of abstraction gradient in a formalism. This framework allows the designer of a programming language to study the trade-offs between abstraction and other characteristics of the design, and how changes in abstraction influence the language usability.
Abstractions can prove useful when dealing with computer programs, because non-trivial properties of computer programs are essentially undecidable (see Rice's theorem). As a consequence, automatic methods for deriving information on the behavior of computer programs either have to drop termination (on some occasions, they may fail, crash or never yield out a result), soundness (they may provide false information), or precision (they may answer ""I don't know"" to some questions).
Abstraction is the core concept of abstract interpretation. Model checking generally takes place on abstract versions of the studied systems.


== Levels of abstraction ==

Computer science commonly presents levels (or, less commonly, layers) of abstraction, wherein each level represents a different model of the same information and processes, but with varying amounts of detail. Each level uses a system of expression involving a unique set of objects and compositions that apply only to a particular domain.

Each relatively abstract, ""higher"" level builds on a relatively concrete, ""lower"" level, which tends to provide an increasingly ""granular"" representation. For example, gates build on electronic circuits, binary on gates, machine language on binary, programming language on machine language, applications and operating systems on programming languages. Each level is embodied, but not determined, by the level beneath it, making it a language of description that is somewhat self-contained.


=== Database systems ===

Since many users of database systems lack in-depth familiarity with computer data-structures, database developers often hide complexity through the following levels:

Physical level: The lowest level of abstraction describes how a system actually stores data. The physical level describes complex low-level data structures in detail.
Logical level: The next higher level of abstraction describes what data the database stores, and what relationships exist among those data. The logical level thus describes an entire database in terms of a small number of relatively simple structures. Although implementation of the simple structures at the logical level may involve complex physical level structures, the user of the logical level does not need to be aware of this complexity. This is referred to as physical data independence. Database administrators, who must decide what information to keep in a database, use the logical level of abstraction.
View level: The highest level of abstraction describes only part of the entire database. Even though the logical level uses simpler structures, complexity remains because of the variety of information stored in a large database. Many users of a database system do not need all this information; instead, they need to access only a part of the database. The view level of abstraction exists to simplify their interaction with the system. The system may provide many views for the same database.


=== Layered architecture ===

The ability to provide a design of different levels of abstraction can

simplify the design considerably
enable different role players to effectively work at various levels of abstraction
support the portability of software artifacts (model-based ideally)Systems design and business process design can both use this. Some design processes specifically generate designs that contain various levels of abstraction.
Layered architecture partitions the concerns of the application into stacked groups (layers).
It is a technique used in designing computer software, hardware, and communications in which system or network components are isolated in layers so that changes can be made in one layer without affecting the others.


== See also ==
Abstraction principle (computer programming)
Abstraction inversion for an anti-pattern of one danger in abstraction
Abstract data type for an abstract description of a set of data
Algorithm for an abstract description of a computational procedure
Bracket abstraction for making a term into a function of a variable
Data modeling for structuring data independent of the processes that use it
Encapsulation for abstractions that hide implementation details
Greenspun's Tenth Rule for an aphorism about an (the?) optimum point in the space of abstractions
Higher-order function for abstraction where functions produce or consume other functions
Lambda abstraction for making a term into a function of some variable
List of abstractions (computer science)
Refinement for the opposite of abstraction in computing
Integer (computer science)
Heuristic (computer science)


== References ==


== Further reading ==


== External links ==
SimArch example of layered architecture for distributed simulation systems.",1421775,631,"Abstraction, Articles with example Java code, Articles with short description, Computer science, Data management, Object-oriented programming, Short description matches Wikidata, Use dmy dates from December 2019",1128541866,cs
https://en.wikipedia.org/wiki/Macro_(computer_science),Macro (computer science),"In computer programming, a macro (short for ""macro instruction""; from Greek  μακρο- 'long, large') is a rule or pattern that specifies how a certain input should be mapped to a replacement output. Applying a macro to an input is known as macro expansion. The input and output may be a sequence of lexical tokens or characters, or a syntax tree. Character macros are supported in software applications to make it easy to invoke common command sequences. Token and tree macros are supported in some programming languages to enable code reuse or to extend the language, sometimes for domain-specific languages.
Macros are used to make a sequence of computing instructions available to the programmer as a single program statement, making the programming task less tedious and less error-prone. (Thus, they are called ""macros"" because a ""big"" block of code can be expanded from a ""small"" sequence of characters.) Macros often allow positional or keyword parameters that dictate what the conditional assembler program generates and have been used to create entire programs or program suites according to such variables as operating system, platform or other factors. The term derives from ""macro instruction"", and such expansions were originally used in generating assembly language code.","In computer programming, a macro (short for ""macro instruction""; from Greek  μακρο- 'long, large') is a rule or pattern that specifies how a certain input should be mapped to a replacement output. Applying a macro to an input is known as macro expansion. The input and output may be a sequence of lexical tokens or characters, or a syntax tree. Character macros are supported in software applications to make it easy to invoke common command sequences. Token and tree macros are supported in some programming languages to enable code reuse or to extend the language, sometimes for domain-specific languages.
Macros are used to make a sequence of computing instructions available to the programmer as a single program statement, making the programming task less tedious and less error-prone. (Thus, they are called ""macros"" because a ""big"" block of code can be expanded from a ""small"" sequence of characters.) Macros often allow positional or keyword parameters that dictate what the conditional assembler program generates and have been used to create entire programs or program suites according to such variables as operating system, platform or other factors. The term derives from ""macro instruction"", and such expansions were originally used in generating assembly language code.


== Keyboard and mouse macros ==
Keyboard macros and mouse macros allow short sequences of keystrokes and mouse actions to transform into other, usually more time-consuming, sequences of keystrokes and mouse actions. In this way, frequently used or repetitive sequences of keystrokes and mouse movements can be automated. Separate programs for creating these macros are called macro recorders.
During the 1980s, macro programs – originally SmartKey, then SuperKey, KeyWorks, Prokey – were very popular, first as a means to automatically format screenplays, then for a variety of user input tasks. These programs were based on the TSR (terminate and stay resident) mode of operation and applied to all keyboard input, no matter in which context it occurred. They have to some extent fallen into obsolescence following the advent of mouse-driven user interfaces and the availability of keyboard and mouse macros in applications such as word processors and spreadsheets, making it possible to create application-sensitive keyboard macros.
Keyboard macros can be used in massively multiplayer online role-playing games (MMORPGs) to perform repetitive, but lucrative tasks, thus accumulating resources.  As this is done without human effort, it can skew the economy of the game. For this reason, use of macros is a violation of the TOS or EULA of most MMORPGs, and their administrators spend considerable effort to suppress them.


=== Application macros and scripting ===
Keyboard and mouse macros that are created using an application's built-in macro features are sometimes called application macros. They are created by carrying out the sequence once and letting the application record the actions. An underlying macro programming language, most commonly a scripting language, with direct access to the features of the application may also exist.
The programmers' text editor, Emacs, (short for ""editing macros"") follows this idea to a conclusion. In effect, most of the editor is made of macros. Emacs was originally devised as a set of macros in the editing language TECO; it was later ported to dialects of Lisp.
Another programmers' text editor, Vim (a descendant of vi), also has an implementation of keyboard macros. It can record into a register (macro) what a person types on the keyboard and it can be replayed or edited just like VBA macros for Microsoft Office. Vim also has a scripting language called Vimscript to create macros.
Visual Basic for Applications (VBA) is a programming language included in Microsoft Office from Office 97 through Office 2019 (although it was available in some components of Office prior to Office 97). However, its function has evolved from and replaced the macro languages that were originally included in some of these applications.
XEDIT, running on the Conversational Monitor System (CMS) component of VM, supports macros written in EXEC, EXEC2 and REXX, and some CMS commands were actually wrappers around XEDIT macros. The Hessling Editor (THE), a partial clone of XEDIT, supports Rexx macros using Regina and Open Object REXX (oorexx). Many common applications, and some on PCs, use Rexx as a scripting language.


==== Macro virus ====

VBA has access to most Microsoft Windows system calls and executes when documents are opened. This makes it relatively easy to write computer viruses in VBA, commonly known as macro viruses. In the mid-to-late 1990s, this became one of the most common types of computer virus. However, during the late 1990s and to date, Microsoft has been patching and updating its programs. In addition, current anti-virus programs immediately counteract such attacks.


== Parameterized macro ==
A parameterized macro is a macro that is able to insert given objects into its expansion. This gives the macro some of the power of a function.
As a simple example, in the C programming language, this is a typical macro that is not a parameterized macro:

 #define PI   3.14159

This causes PI to always be replaced with 3.14159 wherever it occurs. An example of a parameterized macro, on the other hand, is this:

 #define pred(x)  ((x)-1)

What this macro expands to depends on what argument x is passed to it. Here are some possible expansions:

 pred(2)    →  ((2)   -1)
 pred(y+2)  →  ((y+2) -1)
 pred(f(5)) →  ((f(5))-1)

Parameterized macros are a useful source-level mechanism for performing in-line expansion, but in languages such as C where they use simple textual substitution, they have a number of severe disadvantages over other mechanisms for performing in-line expansion, such as inline functions.
The parameterized macros used in languages such as Lisp, PL/I and Scheme, on the other hand, are much more powerful, able to make decisions about what code to produce based on their arguments; thus, they can effectively be used to perform run-time code generation.


== Text-substitution macros ==

Languages such as C and some assembly languages have rudimentary macro systems, implemented as preprocessors to the compiler or assembler. C preprocessor macros work by simple textual substitution at the token, rather than the character level. However, the macro facilities of more sophisticated assemblers, e.g., IBM High Level Assembler (HLASM) can't be implemented with a preprocessor; the code for assembling instructions and data is interspersed with the code for assembling macro invocations.
A classic use of macros is in the computer typesetting system TeX and its derivatives, where most of the functionality is based on macros.
MacroML is an experimental system that seeks to reconcile static typing and macro systems. Nemerle has typed syntax macros, and one productive way to think of these syntax macros is as a multi-stage computation.
Other examples:

m4 is a sophisticated stand-alone macro processor.
TRAC
Macro Extension TAL, accompanying Template Attribute Language
SMX: for web pages
ML/1 (Macro Language One)
The General Purpose Macrogenerator is a contextual pattern-matching macro processor, which could be described as a combination of regular expressions, EBNF and AWK
troff and nroff: for typesetting and formatting Unix manpages.
CMS EXEC: for command-line macros and application macros
EXEC 2 in Conversational Monitor System (CMS): for command-line macros and application macros
CLIST in IBM's Time Sharing Option (TSO): for command-line macros and application macros
REXX: for command-line macros and application macros in, e.g., AmigaOS, CMS, OS/2, TSO
SCRIPT: for formatting documents
Various shells for, e.g., LinuxSome major applications have been written as text macro invoked by other applications, e.g., by XEDIT in CMS.


=== Embeddable languages ===
Some languages, such as PHP, can be embedded in free-format text, or the source code of other languages. The mechanism by which the code fragments are recognised (for instance, being bracketed by <?php and ?>) is similar to a textual macro language, but they are much more powerful, fully featured languages.


== Procedural macros ==
Macros in the PL/I language are written in a subset of PL/I itself: the compiler executes ""preprocessor statements"" at compilation time, and the output of this execution forms part of the code that is compiled. The ability to use a familiar procedural language as the macro language gives power much greater than that of text substitution macros, at the expense of a larger and slower compiler.
Frame technology's frame macros have their own command syntax but can also contain text in any language. Each frame is both a generic component in a hierarchy of nested subassemblies, and a procedure for integrating itself with its subassembly frames (a recursive process that resolves integration conflicts in favor of higher level subassemblies). The outputs are custom documents, typically compilable source modules. Frame technology can avoid the proliferation of similar but subtly different components, an issue that has plagued software development since the invention of macros and subroutines.
Most assembly languages have less powerful procedural macro facilities, for example allowing a block of code to be repeated N times for loop unrolling; but these have a completely different syntax from the actual assembly language.


== Syntactic macros ==
Macro systems—such as the C preprocessor described earlier—that work at the level of lexical tokens cannot preserve the lexical structure reliably.
Syntactic macro systems work instead at the level of abstract syntax trees, and preserve the lexical structure of the original program. The most widely used implementations of syntactic macro systems are found in Lisp-like languages. These languages are especially suited for this style of macro due to their uniform, parenthesized syntax (known as S-expressions). In particular, uniform syntax makes it easier to determine the invocations of macros. Lisp macros transform the program structure itself, with the full language available to express such transformations. While syntactic macros are often found in Lisp-like languages, they are also available in other languages such as Prolog, Erlang, Dylan, Scala, Nemerle, Rust, Elixir, Nim, Haxe, and Julia. They are also available as third-party extensions to JavaScript, C# and Python.


=== Early Lisp macros ===
Before Lisp had macros, it had so-called FEXPRs, function-like operators whose inputs were not the values computed by the arguments but rather the syntactic forms of the arguments, and whose output were values to be used in the computation. In other words, FEXPRs were implemented at the same level as EVAL, and provided a window into the meta-evaluation layer. This was generally found to be a difficult model to reason about effectively.In 1963, Timothy Hart proposed adding macros to Lisp 1.5 in AI Memo 57: MACRO Definitions for LISP.


=== Anaphoric macros ===

An anaphoric macro is a type of programming macro that deliberately captures some form supplied to the macro which may be referred to by an anaphor (an expression referring to another). Anaphoric macros first appeared in Paul Graham's On Lisp and their name is a reference to linguistic anaphora—the use of words as a substitute for preceding words.


=== Hygienic macros ===

In the mid-eighties, a number of papers introduced the notion of hygienic macro expansion (syntax-rules), a pattern-based system where the syntactic environments of the macro definition and the macro use are distinct, allowing macro definers and users not to worry about inadvertent variable capture (cf. referential transparency). Hygienic macros have been standardized for Scheme in the R5RS, R6RS, and R7RS standards. A number of competing implementations of hygienic macros exist such as syntax-rules, syntax-case, explicit renaming, and syntactic closures. Both syntax-rules and syntax-case have been standardized in the Scheme standards.
Recently, Racket has combined the notions of hygienic macros with a ""tower of evaluators"", so that the syntactic expansion time of one macro system is the ordinary runtime of another block of code, and showed how to apply interleaved expansion and parsing in a non-parenthesized language.A number of languages other than Scheme either implement hygienic macros or implement partially hygienic systems. Examples include Scala, Rust, Elixir, Julia, Dylan, Nim, and Nemerle.


=== Applications ===
Evaluation order
Macro systems have a range of uses. Being able to choose the order of evaluation (see lazy evaluation and non-strict functions) enables the creation of new syntactic constructs (e.g. control structures) indistinguishable from those built into the language. For instance, in a Lisp dialect that has cond but lacks if, it is possible to define the latter in terms of the former using macros. For example, Scheme has both continuations and hygienic macros, which enables a programmer to design their own control abstractions, such as looping and early exit constructs, without the need to build them into the language.
Data sub-languages and domain-specific languages
Next, macros make it possible to define data languages that are immediately compiled into code, which means that constructs such as state machines can be implemented in a way that is both natural and efficient.
Binding constructs
Macros can also be used to introduce new binding constructs. The most well-known example is the transformation of let into the application of a function to a set of arguments.Felleisen conjectures that these three categories make up the primary legitimate uses of macros in such a system. Others have proposed alternative uses of macros, such as anaphoric macros in macro systems that are unhygienic or allow selective unhygienic transformation.
The interaction of macros and other language features has been a productive area of research. For example, components and modules are useful for large-scale programming, but the interaction of macros and these other constructs must be defined for their use together. Module and component-systems that can interact with macros have been proposed for Scheme and other languages with macros. For example, the Racket language extends the notion of a macro system to a syntactic tower, where macros can be written in languages including macros, using hygiene to ensure that syntactic layers are distinct and allowing modules to export macros to other modules.


== Macros for machine-independent software ==
Macros are normally used to map a short string (macro invocation) to a longer sequence of instructions. Another, less common, use of macros is to do the reverse: to map a sequence of instructions to a macro string. This was the approach taken by the STAGE2 Mobile Programming System, which used a rudimentary macro compiler (called SIMCMP) to map the specific instruction set of a given computer into machine-independent macros. Applications (notably compilers) written in these machine-independent macros can then be run without change on any computer equipped with the rudimentary macro compiler. The first application run in such a context is a more sophisticated and powerful macro compiler, written in the machine-independent macro language. This macro compiler is applied to itself, in a bootstrap fashion, to produce a compiled and much more efficient version of itself. The advantage of this approach is that complex applications can be ported from one computer to a very different computer with very little effort (for each target machine architecture, just the writing of the rudimentary macro compiler). The advent of modern programming languages, notably C, for which compilers are available on virtually all computers, has rendered such an approach superfluous. This was, however, one of the first instances (if not the first) of compiler bootstrapping.


== Assembly language ==
While macro instructions can be defined by a programmer for any set of native assembler program instructions, typically macros are associated with macro libraries delivered with the operating system allowing access to operating system functions such as

peripheral access by access methods (including macros such as OPEN, CLOSE, READ and WRITE)
operating system functions such as ATTACH, WAIT and POST for subtask creation and synchronization. Typically such macros expand into executable code, e.g., for the EXIT macroinstruction,
a list of define constant instructions, e.g., for the DCB macro—DTF (Define The File) for DOS—or a combination of code and constants, with the details of the expansion depending on the parameters of the macro instruction (such as a reference to a file and a data area for a READ instruction);
the executable code often terminated in either a branch and link register instruction to call a routine, or a supervisor call instruction to call an operating system function directly.
Generating a Stage 2 job stream for system generation in, e.g., OS/360. Unlike typical macros, sysgen stage 1 macros do not generate data or code to be loaded into storage, but rather use the PUNCH statement to output JCL and associated data.In older operating systems such as those used on IBM mainframes, full operating system functionality was only available to assembler language programs, not to high level language programs (unless assembly language subroutines were used, of course), as the standard macro instructions did not always have counterparts in routines available to high-level languages.


== History ==
In the mid-1950s, when assembly language programming was commonly used to write programs for digital computers, the use of macro instructions was initiated for two main purposes: to reduce the amount of program coding that had to be written by generating several assembly language statements from one macro instruction and to enforce program writing standards, e.g. specifying input/output commands in standard ways. Macro instructions were effectively a middle step between assembly language programming and the high-level programming languages that followed, such as FORTRAN and COBOL. Two of the earliest programming installations to develop ""macro languages"" for the IBM 705 computer were at Dow Chemical Corp. in Delaware and the Air Material Command, Ballistics Missile Logistics Office in California. A macro instruction written in the format of the target assembly language would be processed by a macro compiler, which was a pre-processor to the assembler, to generate one or more assembly language instructions to be processed next by the assembler program that would translate the assembly language instructions into machine language instructions.By the late 1950s the macro language was followed by the Macro Assemblers. This was a combination of both where one program served both functions, that of a macro pre-processor and an assembler in the same package.In 1959, Douglas E. Eastwood and Douglas McIlroy of Bell Labs introduced conditional and recursive macros into the popular SAP assembler, creating what is known as Macro SAP. McIlroy's 1960 paper was seminal in the area of extending any (including high-level) programming languages through macro processors.Macro Assemblers allowed assembly language programmers to implement their own macro-language and allowed limited portability of code between two machines running the same CPU but different operating systems, for example, early versions of MSDOS and CPM-86. The macro library would need to be written for each target machine but not the overall assembly language program. Note that more powerful macro assemblers allowed use of conditional assembly constructs in macro instructions that could generate different code on different machines or different operating systems, reducing the need for multiple libraries.In the 1980s and early 1990s, desktop PCs were only running at a few MHz and assembly language routines were commonly used to speed up programs written in C, Fortran, Pascal and others. These languages, at the time, used different calling conventions. Macros could be used to interface routines written in assembly language to the front end of applications written in almost any language. Again, the basic assembly language code remained the same, only the macro libraries needed to be written for each target language.In modern operating systems such as Unix and its derivatives, operating system access is provided through subroutines, usually provided by dynamic libraries. High-level languages such as C offer comprehensive access to operating system functions, obviating the need for assembler language programs for such functionality.


== See also ==
Anaphoric macros
Assembly language § Macros (the origin of the concept of macros)
Extensible programming
Hygienic macros
Programming by demonstration – Technique for teaching a computer or a robot new behaviors
String interpolation – Replacing placeholders in a string with values
Computer science and engineering
Computer science


== References ==


== External links ==
How to write Macro Instructions
Rochester Institute of Technology, Professors Powerpoint",1627579,1014,"All articles needing additional references, All articles with failed verification, All articles with unsourced statements, Articles needing additional references from June 2014, Articles with GND identifiers, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with failed verification from February 2020, Articles with short description, Articles with unsourced statements from April 2022, Articles with unsourced statements from February 2020, Automation software, Programming constructs, Short description is different from Wikidata, Source code",1128156999,cs
https://en.wikipedia.org/wiki/Paxos_(computer_science),Paxos (computer science),"Paxos is a family of protocols for solving consensus in a network of unreliable or fallible processors.
Consensus is the process of agreeing on one result among a group of participants.  This problem becomes difficult when the participants or their communications may experience failures.Consensus protocols are the basis for the state machine replication approach to distributed computing, as suggested by Leslie Lamport and surveyed by Fred Schneider.  State machine replication is a technique for converting an algorithm into a fault-tolerant, distributed implementation.  Ad-hoc techniques may leave important cases of failures unresolved.  The principled approach proposed by Lamport et al. ensures all cases are handled safely.
The Paxos protocol was first submitted in 1989 and named after a fictional legislative consensus system used on the Paxos island in Greece, where Lamport wrote that the parliament had to function ""even though legislators continually wandered in and out of the parliamentary Chamber"". It was later published as a journal article in 1998.The Paxos family of protocols includes a spectrum of trade-offs between the number of processors, number of message delays before learning the agreed value, the activity level of individual participants, number of messages sent, and types of failures.  Although no deterministic fault-tolerant consensus protocol can guarantee progress in an asynchronous network (a result proved in a paper by Fischer, Lynch and Paterson), Paxos guarantees safety (consistency), and the conditions that could prevent it from making progress are difficult to provoke.
Paxos is usually used where durability is required (for example, to replicate a file or a database), in which the amount of durable state could be large. The protocol attempts to make progress even during periods when some bounded number of replicas are unresponsive. There is also a mechanism to drop a permanently failed replica or to add a new replica.","Paxos is a family of protocols for solving consensus in a network of unreliable or fallible processors.
Consensus is the process of agreeing on one result among a group of participants.  This problem becomes difficult when the participants or their communications may experience failures.Consensus protocols are the basis for the state machine replication approach to distributed computing, as suggested by Leslie Lamport and surveyed by Fred Schneider.  State machine replication is a technique for converting an algorithm into a fault-tolerant, distributed implementation.  Ad-hoc techniques may leave important cases of failures unresolved.  The principled approach proposed by Lamport et al. ensures all cases are handled safely.
The Paxos protocol was first submitted in 1989 and named after a fictional legislative consensus system used on the Paxos island in Greece, where Lamport wrote that the parliament had to function ""even though legislators continually wandered in and out of the parliamentary Chamber"". It was later published as a journal article in 1998.The Paxos family of protocols includes a spectrum of trade-offs between the number of processors, number of message delays before learning the agreed value, the activity level of individual participants, number of messages sent, and types of failures.  Although no deterministic fault-tolerant consensus protocol can guarantee progress in an asynchronous network (a result proved in a paper by Fischer, Lynch and Paterson), Paxos guarantees safety (consistency), and the conditions that could prevent it from making progress are difficult to provoke.
Paxos is usually used where durability is required (for example, to replicate a file or a database), in which the amount of durable state could be large. The protocol attempts to make progress even during periods when some bounded number of replicas are unresponsive. There is also a mechanism to drop a permanently failed replica or to add a new replica.


== History ==
The topic predates the protocol. In 1988, Lynch, Dwork and Stockmeyer had demonstrated  the solvability of consensus in a broad family of ""partially synchronous"" systems.  Paxos has strong similarities to a protocol used for agreement in ""viewstamped replication"", first published by Oki and Liskov in 1988, in the context of distributed transactions.  Notwithstanding this prior work, Paxos offered a particularly elegant formalism, and included one of the earliest proofs of safety for a fault-tolerant distributed consensus protocol.
Reconfigurable state machines have strong ties to prior work on reliable group multicast protocols that support dynamic group membership, for example Birman's work in 1985 and 1987 on the virtually synchronous gbcast protocol. However, gbcast is unusual in supporting durability and addressing partitioning failures.
Most reliable multicast protocols lack these properties, which are required for implementations of the state machine replication model.
This point is elaborated in a paper by Lamport, Malkhi and Zhou.Paxos protocols are members of a theoretical class of solutions to a problem formalized as uniform agreement with crash failures.
Lower bounds for this problem have been proved by Keidar and Shraer.  Derecho, a C++ software library for cloud-scale state machine replication, offers a Paxos protocol that has been integrated with self-managed virtually synchronous membership.  This protocol matches the Keidar and Shraer optimality bounds, and maps efficiently to modern remote DMA (RDMA) datacenter hardware (but uses TCP if RDMA is not available).


== Assumptions ==
In order to simplify the presentation of Paxos, the following assumptions and definitions are made explicit.  Techniques to broaden the applicability are known in the literature, and are not covered in this article.


=== Processors ===
Processors operate at arbitrary speed.
Processors may experience failures.
Processors with stable storage may re-join the protocol after failures (following a crash-recovery failure model).
Processors do not collude, lie, or otherwise attempt to subvert the protocol. (That is, Byzantine failures don't occur. See Byzantine Paxos for a solution that tolerates failures that arise from arbitrary/malicious behavior of the processes.)


=== Network ===
Processors can send messages to any other processor.
Messages are sent asynchronously and may take arbitrarily long to deliver.
Messages may be lost, reordered, or duplicated.
Messages are delivered without corruption. (That is, Byzantine failures don't occur. See Byzantine Paxos for a solution which tolerates corrupted messages that arise from arbitrary/malicious behavior of the messaging channels.)


=== Number of processors ===
In general, a consensus algorithm can make progress using 
  
    
      
        n
        =
        2
        F
        +
        1
      
    
    {\displaystyle n=2F+1}
   processors, despite the simultaneous failure of any 
  
    
      
        F
      
    
    {\displaystyle F}
   processors: in other words, the number of non-faulty processes must be strictly greater than the number of faulty processes. However, using reconfiguration, a protocol may be employed which survives any number of total failures as long as no more than F fail simultaneously. For Paxos protocols, these reconfigurations can be handled as separate configurations.


== Roles ==
Paxos describes the actions of the processors by their roles in the protocol: client, acceptor, proposer, learner, and leader.  In typical implementations, a single processor may play one or more roles at the same time.  This does not affect the correctness of the protocol—it is usual to coalesce roles to improve the latency and/or number of messages in the protocol.

Client
The Client issues a request to the distributed system, and waits for a response.  For instance, a write request on a file in a distributed file server.
 Acceptor (Voters)
The Acceptors act as the fault-tolerant ""memory"" of the protocol. Acceptors are collected into groups called Quorums.  Any message sent to an Acceptor must be sent to a Quorum of Acceptors. Any message received from an Acceptor is ignored unless a copy is received from each Acceptor in a Quorum.
 Proposer
A Proposer advocates a client request, attempting to convince the Acceptors to agree on it, and acting as a coordinator to move the protocol forward when conflicts occur.
Learner
Learners act as the replication factor for the protocol.  Once a Client request has been agreed upon by the Acceptors, the Learner may take action (i.e.: execute the request and send a response to the client).  To improve availability of processing, additional Learners can be added.
 Leader
Paxos requires a distinguished Proposer (called the leader) to make progress.  Many processes may believe they are leaders, but the protocol only guarantees progress if one of them is eventually chosen.  If two processes believe they are leaders, they may stall the protocol by continuously proposing conflicting updates.  However, the safety properties are still preserved in that case.


=== Quorums ===
Quorums express the safety (or consistency) properties of Paxos by ensuring at least some surviving processor retains knowledge of the results.
Quorums are defined as subsets of the set of Acceptors such that any two Quorums share at least one member. Typically, a Quorum is any majority of participating Acceptors. For example, given the set of Acceptors {A,B,C,D}, a majority Quorum would be any three Acceptors:  {A,B,C}, {A,C,D}, {A,B,D}, {B,C,D}. More generally, arbitrary positive weights can be assigned to Acceptors; in that case, a Quorum can be defined as any subset of Acceptors with the summary weight greater than half of the total weight of all Acceptors.


=== Proposal number and agreed value ===
Each attempt to define an agreed value v is performed with proposals which may or may not be accepted by Acceptors. Each proposal is uniquely numbered for a given Proposer. So, e.g., each proposal may be of the form (n, v), where n is the unique identifier of the proposal and v is the actual proposed value. The value corresponding to a numbered proposal can be computed as part of running the Paxos protocol, but need not be.


== Safety and liveness properties ==
In order to guarantee safety (also called ""consistency""), Paxos defines three properties and ensures the first two are always held, regardless of the pattern of failures:

Validity (or non-triviality)
Only proposed values can be chosen and learned.
Agreement (or consistency, or safety)
No two distinct learners can learn different values (or there can't be more than one decided value)
Termination (or liveness)
If value C has been proposed, then eventually learner L will learn some value (if sufficient processors remain non-faulty).Note that Paxos is not guaranteed to terminate, and thus does not have the liveness property. This is supported by the Fischer Lynch Paterson impossibility result (FLP) which states that a consistency protocol can only have two of safety, liveness, and fault tolerance. As Paxos's point is to ensure fault tolerance and it guarantees safety, it cannot also guarantee liveness.


== Typical deployment ==
In most deployments of Paxos, each participating process acts in three roles; Proposer, Acceptor and Learner.  This reduces the message complexity significantly, without sacrificing correctness:

In Paxos, clients send commands to a leader. During normal operation, the leader receives a client's command, assigns it a new command number 
  
    
      
        i
      
    
    {\displaystyle i}
  , and then begins the 
  
    
      
        i
      
    
    {\displaystyle i}
  th instance of the consensus algorithm by sending messages to a set of acceptor processes.
By merging roles, the protocol ""collapses"" into an efficient client-master-replica style deployment, typical of the database community.  The benefit of the Paxos protocols (including implementations with merged roles) is the guarantee of its safety properties.
A typical implementation's message flow is covered in the section Multi-Paxos.


== Basic Paxos ==
This protocol is the most basic of the Paxos family. Each ""instance"" (or ""execution"") of the basic Paxos protocol decides on a single output value. The protocol proceeds over several rounds. A successful round has 2 phases: phase 1 (which is divided into parts a and b) and phase 2 (which is divided into parts a and b). See below the description of the phases. Remember that we assume an asynchronous model, so e.g. a processor may be in one phase while another processor may be in another. 


=== Phase 1 ===


==== Phase 1a: Prepare ====
A Proposer creates a message, which we call a ""Prepare"", identified with a number n. Note that n is not the value to be proposed and maybe agreed on, but just a number which uniquely identifies this initial message by the proposer (to be sent to the acceptors). The number n must be greater than any number used in any of the previous Prepare messages by this Proposer. Then, it sends the Prepare message containing n to at least a Quorum of Acceptors. Note that the Prepare message only contains the number n (that is, it does not have to contain e.g. the proposed value, often denoted by v). The Proposer decides who is in the Quorum. A Proposer should not initiate Paxos if it cannot communicate with at least a Quorum of Acceptors.


==== Phase 1b: Promise ====
Any of the Acceptors waits for a Prepare message from any of the Proposers. If an Acceptor receives a Prepare message, the Acceptor must look at the identifier number n of the just received Prepare message. There are two cases.If n is higher than every previous proposal number received, from any of the Proposers, by the Acceptor, then the Acceptor must return a message, which we call a ""Promise"", to the Proposer, to ignore all future proposals having a number less than n. If the Acceptor accepted a proposal at some point in the past, it must include the previous proposal number, say m, and the corresponding accepted value, say w, in its response to the Proposer.Otherwise (that is, n is less than or equal to any previous proposal number received from any Proposer by the Acceptor) the Acceptor can ignore the received proposal. It does not have to answer in this case for Paxos to work. However, for the sake of optimization, sending a denial (Nack) response would tell the Proposer that it can stop its attempt to create consensus with proposal n.


=== Phase 2 ===


==== Phase 2a: Accept ====
If a Proposer receives Promises from a Quorum of Acceptors, it needs to set a value v to its proposal. If any Acceptors had previously accepted any proposal, then they'll have sent their values to the Proposer, who now must set the value of its proposal, v, to the value associated with the highest proposal number reported by the Acceptors, let's call it z. If none of the Acceptors had accepted a proposal up to this point, then the Proposer may choose the value it originally wanted to propose, say x.The Proposer sends an Accept message, (n, v), to a Quorum of Acceptors with the chosen value for its proposal, v, and the proposal number n (which is the same as the number contained in the Prepare message previously sent to the Acceptors). So, the Accept message is either (n, v=z) or, in case none of the Acceptors previously accepted a value, (n, v=x).This Accept message should be interpreted as a ""request"", as in ""Accept this proposal, please!"".


==== Phase 2b: Accepted ====
If an Acceptor receives an Accept message, (n, v), from a Proposer, it must accept it if and only if it has not already promised (in Phase 1b of the Paxos protocol) to only consider proposals having an identifier greater than n.If the Acceptor has not already promised (in Phase 1b) to only consider proposals having an identifier greater than n, it should register the value v (of the just received Accept message) as the accepted value (of the Protocol), and send an Accepted message to the Proposer and every Learner (which can typically be the Proposers themselves).Else, it can ignore the Accept message or request.Note that consensus is achieved when a majority of Acceptors accept the same identifier number (rather than the same value).  Because each identifier is unique to a Proposer and only one value may be proposed per identifier, all Acceptors that accept the same identifier thereby accept the same value.  These facts result in a few counter-intuitive scenarios that do not impact correctness: Acceptors can accept multiple values, a value may achieve a majority across Acceptors (with different identifiers) only to later be changed, and Acceptors may continue to accept proposals after an identifier has achieved a majority.  However, the Paxos protocol guarantees that consensus is permanent and the chosen value is immutable.


=== When rounds fail ===
Rounds fail when multiple Proposers send conflicting Prepare messages, or when the Proposer does not receive a Quorum of responses (Promise or Accepted).  In these cases, another round must be started with a higher proposal number.


=== Paxos can be used to select a leader ===
Notice that a Proposer in Paxos could propose ""I am the leader,"" (or, for example, ""Proposer X is the leader""). Because of the agreement and validity guarantees of Paxos, if accepted by a Quorum, then the Proposer is now known to be the leader to all other nodes. This satisfies the needs of leader election because there is a single node believing it is the leader and a single node known to be the leader at all times.


=== Graphic representation of the flow of messages in the basic Paxos ===
The following diagrams represent several cases/situations of the application of the Basic Paxos protocol. Some cases show how the Basic Paxos protocol copes with the failure of certain (redundant) components of the distributed system.
Note that the values returned in the Promise message are ""null"" the first time a proposal is made (since no Acceptor has accepted a value before in this round).


==== Basic Paxos without failures ====
In the diagram below, there is 1 Client, 1 Proposer, 3 Acceptors (i.e. the Quorum size is 3) and 2 Learners (represented by the 2 vertical lines). This diagram represents the case of a first round, which is successful (i.e. no process in the network fails).

Here, V is the last of (Va, Vb, Vc).


==== Error cases in basic Paxos ====
The simplest error cases are the failure of an Acceptor (when a Quorum of Acceptors remains alive) and failure of a redundant Learner. In these cases, the protocol requires no ""recovery"" (i.e. it still succeeds): no additional rounds or messages are required, as shown below (in the next two diagrams/cases).


==== Basic Paxos when an Acceptor fails ====
In the following diagram, one of the Acceptors in the Quorum fails, so the Quorum size becomes 2. In this case, the Basic Paxos protocol still succeeds.

Client   Proposer      Acceptor     Learner
   |         |          |  |  |       |  |
   X-------->|          |  |  |       |  |  Request
   |         X--------->|->|->|       |  |  Prepare(1)
   |         |          |  |  !       |  |  !! FAIL !!
   |         |<---------X--X          |  |  Promise(1,{Va, Vb, null})
   |         X--------->|->|          |  |  Accept!(1,V)
   |         |<---------X--X--------->|->|  Accepted(1,V)
   |<---------------------------------X--X  Response
   |         |          |  |          |  |


==== Basic Paxos when a redundant learner fails ====
In the following case, one of the (redundant) Learners fails, but the Basic Paxos protocol still succeeds. 

Client Proposer         Acceptor     Learner
   |         |          |  |  |       |  |
   X-------->|          |  |  |       |  |  Request
   |         X--------->|->|->|       |  |  Prepare(1)
   |         |<---------X--X--X       |  |  Promise(1,{Va,Vb,Vc})
   |         X--------->|->|->|       |  |  Accept!(1,V)
   |         |<---------X--X--X------>|->|  Accepted(1,V)
   |         |          |  |  |       |  !  !! FAIL !!
   |<---------------------------------X     Response
   |         |          |  |  |       |


==== Basic Paxos when a Proposer fails ====
In this case, a Proposer fails after proposing a value, but before the agreement is reached. Specifically, it fails in the middle of the Accept message, so only one Acceptor of the Quorum receives the value. Meanwhile, a new Leader (a Proposer) is elected (but this is not shown in detail). Note that there are 2 rounds in this case (rounds proceed vertically, from the top to the bottom).

Client  Proposer        Acceptor     Learner
   |      |             |  |  |       |  |
   X----->|             |  |  |       |  |  Request
   |      X------------>|->|->|       |  |  Prepare(1)
   |      |<------------X--X--X       |  |  Promise(1,{Va, Vb, Vc})
   |      |             |  |  |       |  |
   |      |             |  |  |       |  |  !! Leader fails during broadcast !!
   |      X------------>|  |  |       |  |  Accept!(1,V)
   |      !             |  |  |       |  |
   |         |          |  |  |       |  |  !! NEW LEADER !!
   |         X--------->|->|->|       |  |  Prepare(2)
   |         |<---------X--X--X       |  |  Promise(2,{V, null, null})
   |         X--------->|->|->|       |  |  Accept!(2,V)
   |         |<---------X--X--X------>|->|  Accepted(2,V)
   |<---------------------------------X--X  Response
   |         |          |  |  |       |  |


==== Basic Paxos when multiple Proposers conflict ====
The most complex case is when multiple Proposers believe themselves to be Leaders.  For instance, the current leader may fail and later recover, but the other Proposers have already re-selected a new leader.  The recovered leader has not learned this yet and attempts to begin one round in conflict with the current leader. In the diagram below, 4 unsuccessful rounds are shown, but there could be more (as suggested at the bottom of the diagram).

Client   Proposer       Acceptor     Learner
   |      |             |  |  |       |  |
   X----->|             |  |  |       |  |  Request
   |      X------------>|->|->|       |  |  Prepare(1)
   |      |<------------X--X--X       |  |  Promise(1,{null,null,null})
   |      !             |  |  |       |  |  !! LEADER FAILS
   |         |          |  |  |       |  |  !! NEW LEADER (knows last number was 1)
   |         X--------->|->|->|       |  |  Prepare(2)
   |         |<---------X--X--X       |  |  Promise(2,{null,null,null})
   |      |  |          |  |  |       |  |  !! OLD LEADER recovers
   |      |  |          |  |  |       |  |  !! OLD LEADER tries 2, denied
   |      X------------>|->|->|       |  |  Prepare(2)
   |      |<------------X--X--X       |  |  Nack(2)
   |      |  |          |  |  |       |  |  !! OLD LEADER tries 3
   |      X------------>|->|->|       |  |  Prepare(3)
   |      |<------------X--X--X       |  |  Promise(3,{null,null,null})
   |      |  |          |  |  |       |  |  !! NEW LEADER proposes, denied
   |      |  X--------->|->|->|       |  |  Accept!(2,Va)
   |      |  |<---------X--X--X       |  |  Nack(3)
   |      |  |          |  |  |       |  |  !! NEW LEADER tries 4
   |      |  X--------->|->|->|       |  |  Prepare(4)
   |      |  |<---------X--X--X       |  |  Promise(4,{null,null,null})
   |      |  |          |  |  |       |  |  !! OLD LEADER proposes, denied
   |      X------------>|->|->|       |  |  Accept!(3,Vb)
   |      |<------------X--X--X       |  |  Nack(4)
   |      |  |          |  |  |       |  |  ... and so on ...


==== Basic Paxos where an Acceptor accepts Two Different Values ====
In the following case, one Proposer achieves acceptance of value V1 by one Acceptor before failing.  A new Proposer prepares the Acceptors that never accepted V1, allowing it to propose V2. Then V2 is accepted by all Acceptors, including the one that initially accepted V1. 

Proposer    Acceptor     Learner
 |  |       |  |  |       |  |
 X--------->|->|->|       |  |  Prepare(1)
 |<---------X--X--X       |  |  Promise(1,{null,null,null})
 x--------->|  |  |       |  |  Accept!(1,V1)
 |  |       X------------>|->|  Accepted(1,V1)
 !  |       |  |  |       |  |  !! FAIL !!
    |       |  |  |       |  |
    X--------->|->|       |  |  Prepare(2)
    |<---------X--X       |  |  Promise(2,{null,null})
    X------>|->|->|       |  |  Accept!(2,V2)
    |<------X--X--X------>|->|  Accepted(2,V2)
    |       |  |  |       |  |


==== Basic Paxos where a multi-identifier majority is insufficient ====
In the following case, one Proposer achieves acceptance of value V1 of one Acceptor before failing.  A new Proposer prepares the Acceptors that never accepted V1, allowing it to propose V2. This Proposer is able to get one Acceptor to accept V2 before failing.  A new Proposer finds a majority that includes the Acceptor that has accepted V1, and must propose it. The Proposer manages to get two Acceptors to accept it before failing.  At this point, three Acceptors have accepted V1, but not for the same identifier.  Finally, a new Proposer prepares the majority that has not seen the largest accepted identifier.  The value associated with the largest identifier in that majority is V2, so it must propose it.  This Proposer then gets all Acceptors to accept V2, achieving consensus. 

  Proposer           Acceptor        Learner
 |  |  |  |       |  |  |  |  |       |  |
 X--------------->|->|->|->|->|       |  |  Prepare(1)
 |<---------------X--X--X--X--X       |  |  Promise(1,{null,null,null,null,null})
 x--------------->|  |  |  |  |       |  |  Accept!(1,V1)
 |  |  |  |       X------------------>|->|  Accepted(1,V1)
 !  |  |  |       |  |  |  |  |       |  |  !! FAIL !!
    |  |  |       |  |  |  |  |       |  |
    X--------------->|->|->|->|       |  |  Prepare(2)
    |<---------------X--X--X--X       |  |  Promise(2,{null,null,null,null})
    X--------------->|  |  |  |       |  |  Accept!(2,V2)
    |  |  |       |  X--------------->|->|  Accepted(2,V2)
    !  |  |       |  |  |  |  |       |  |  !! FAIL !!
       |  |       |  |  |  |  |       |  | 
       X--------->|---->|->|->|       |  |  Prepare(3)
       |<---------X-----X--X--X       |  |  Promise(3,{V1,null,null,null})
       X--------------->|->|  |       |  |  Accept!(3,V1)
       |  |       |  |  X--X--------->|->|  Accepted(3,V1)
       !  |       |  |  |  |  |       |  |  !! FAIL !!
          |       |  |  |  |  |       |  |
          X------>|->|------->|       |  |  Prepare(4)
          |<------X--X--|--|--X       |  |  Promise(4,{V1(1),V2(2),null})
          X------>|->|->|->|->|       |  |  Accept!(4,V2)
          |       X--X--X--X--X------>|->|  Accepted(4,V2)


==== Basic Paxos where new Proposers cannot change an existing consensus ====
In the following case, one Proposer achieves acceptance of value V1 of two Acceptors before failing.  A new Proposer may start another round, but it is now impossible for that proposer to prepare a majority that doesn't include at least one Acceptor that has accepted V1.  As such, even though the Proposer doesn't see the existing consensus, the Proposer's only option is to propose the value already agreed upon.  New Proposers can continually increase the identifier to restart the process, but the consensus can never be changed. 

Proposer    Acceptor     Learner
 |  |       |  |  |       |  |
 X--------->|->|->|       |  |  Prepare(1)
 |<---------X--X--X       |  |  Promise(1,{null,null,null})
 x--------->|->|  |       |  |  Accept!(1,V1)
 |  |       X--X--------->|->|  Accepted(1,V1)
 !  |       |  |  |       |  |  !! FAIL !!
    |       |  |  |       |  |
    X--------->|->|       |  |  Prepare(2)
    |<---------X--X       |  |  Promise(2,{V1,null})
    X------>|->|->|       |  |  Accept!(2,V1)
    |<------X--X--X------>|->|  Accepted(2,V1)
    |       |  |  |       |  |


== Multi-Paxos ==
A typical deployment of Paxos requires a continuous stream of agreed values acting as commands to a distributed state machine.  If each command is the result of a single instance of the Basic Paxos protocol, a significant amount of overhead would result.
If the leader is relatively stable, phase 1 becomes unnecessary. Thus, it is possible to skip phase 1 for future instances of the protocol with the same leader.
To achieve this, the round number I is included along with each value which is incremented in each round by the same Leader.  Multi-Paxos reduces the failure-free message delay (proposal to learning) from 4 delays to 2 delays.


=== Graphic representation of the flow of messages in the Multi-Paxos ===


==== Multi-Paxos without failures ====
In the following diagram, only one instance (or ""execution"") of the basic Paxos protocol, with an initial Leader (a Proposer), is shown. Note that a Multi-Paxos consists of several instances of the basic Paxos protocol.

Client   Proposer      Acceptor     Learner
   |         |          |  |  |       |  | --- First Request ---
   X-------->|          |  |  |       |  |  Request
   |         X--------->|->|->|       |  |  Prepare(N)
   |         |<---------X--X--X       |  |  Promise(N,I,{Va,Vb,Vc})
   |         X--------->|->|->|       |  |  Accept!(N,I,V)
   |         |<---------X--X--X------>|->|  Accepted(N,I,V)
   |<---------------------------------X--X  Response
   |         |          |  |  |       |  |

where V = last of (Va, Vb, Vc).


==== Multi-Paxos when phase 1 can be skipped ====
In this case, subsequent instances of the basic Paxos protocol (represented by I+1) use the same leader, so the phase 1 (of these subsequent instances of the basic Paxos protocol), which consist of the Prepare and Promise sub-phases, is skipped. Note that the Leader should be stable, i.e. it should not crash or change.

Client   Proposer       Acceptor     Learner
   |         |          |  |  |       |  |  --- Following Requests ---
   X-------->|          |  |  |       |  |  Request
   |         X--------->|->|->|       |  |  Accept!(N,I+1,W)
   |         |<---------X--X--X------>|->|  Accepted(N,I+1,W)
   |<---------------------------------X--X  Response
   |         |          |  |  |       |  |


==== Multi-Paxos when roles are collapsed ====
A common deployment of the Multi-Paxos consists in collapsing the role of the Proposers, Acceptors and Learners to ""Servers"". So, in the end, there are only ""Clients"" and ""Servers"".
The following diagram represents the first ""instance"" of a basic Paxos protocol, when the roles of the Proposer, Acceptor and Learner are collapsed to a single role, called the ""Server"".

Client      Servers
   |         |  |  | --- First Request ---
   X-------->|  |  |  Request
   |         X->|->|  Prepare(N)
   |         |<-X--X  Promise(N, I, {Va, Vb})
   |         X->|->|  Accept!(N, I, Vn)
   |         X<>X<>X  Accepted(N, I)
   |<--------X  |  |  Response
   |         |  |  |


==== Multi-Paxos when roles are collapsed and the leader is steady ====
In the subsequent instances of the basic Paxos protocol, with the same leader as in the previous instances of the basic Paxos protocol, the phase 1 can be skipped.

Client      Servers
   X-------->|  |  |  Request
   |         X->|->|  Accept!(N,I+1,W)
   |         X<>X<>X  Accepted(N,I+1)
   |<--------X  |  |  Response
   |         |  |  |


== Optimisations ==
A number of optimisations can be performed to reduce the number of exchanged messages, to improve the performance of the protocol, etc. A few of these optimisations are reported below.

""We can save messages at the cost of an extra message delay by having a single distinguished learner that informs the other learners when it finds out that a value has been chosen. Acceptors then send Accepted messages only to the distinguished learner.  In most applications, the roles of leader and distinguished learner are performed by the same processor.""A leader can send its Prepare and Accept! messages just to a quorum of acceptors. As long as all acceptors in that quorum are working and can communicate with the leader and the learners, there is no need for acceptors not in the quorum to do anything.""Acceptors do not care what value is chosen. They simply respond to Prepare and Accept! messages to ensure that, despite failures, only a single value can be chosen. However, if an acceptor does learn what value has been chosen, it can store the value in stable storage and erase any other information it has saved there. If the acceptor later receives a Prepare or Accept! message, instead of performing its Phase1b or Phase2b action, it can simply inform the leader of the chosen value.""Instead of sending the value v, the leader can send a hash of v to some acceptors in its Accept! messages. A learner will learn that v is chosen if it receives Accepted messages for either v or its hash from a quorum of acceptors, and at least one of those messages contains v rather than its hash. However, a leader could receive Promise messages that tell it the hash of a value v that it must use in its Phase2a action without telling it the actual value of v. If that happens, the leader cannot execute its Phase2a action until it communicates with some process that knows v.""""A proposer can send its proposal only to the leader rather than to all coordinators. However, this requires that the result of the leader-selection algorithm be broadcast to the proposers, which might be expensive. So, it might be better to let the proposer send its proposal to all coordinators. (In that case, only the coordinators themselves need to know who the leader is.)""Instead of each acceptor sending Accepted messages to each learner, acceptors can send their Accepted messages to the leader and the leader can inform the learners when a value has been chosen. However, this adds an extra message delay.""Finally, observe that phase 1 is unnecessary for round 1 .. The leader of round 1 can begin the round by sending an Accept! message with any proposed value.""


== Cheap Paxos ==
Cheap Paxos extends Basic Paxos to tolerate F failures with F+1 main processors and F auxiliary processors by dynamically reconfiguring after each failure.
This reduction in processor requirements comes at the expense of liveness; if too many main processors fail in a short time, the system must halt until the auxiliary processors can reconfigure the system.  During stable periods, the auxiliary processors take no part in the protocol.

""With only two processors p and q, one processor cannot distinguish failure of the other processor from failure of the communication medium. A third processor is needed. However, that third processor does not have to participate in choosing the sequence of commands. It must take action only in case p or q fails, after which it does nothing while either p or q continues to operate the system by itself. The third processor can therefore be a small/slow/cheap one, or a processor primarily devoted to other tasks.""


=== Message flow: Cheap Multi-Paxos ===
An example involving three main acceptors, one auxiliary acceptor and quorum size of three, showing failure of one main processor and subsequent reconfiguration:

            {  Acceptors  }
Proposer     Main       Aux    Learner
|            |  |  |     |       |  -- Phase 2 --
X----------->|->|->|     |       |  Accept!(N,I,V)
|            |  |  !     |       |  --- FAIL! ---
|<-----------X--X--------------->|  Accepted(N,I,V)
|            |  |        |       |  -- Failure detected (only 2 accepted) --
X----------->|->|------->|       |  Accept!(N,I,V)  (re-transmit, include Aux)
|<-----------X--X--------X------>|  Accepted(N,I,V)
|            |  |        |       |  -- Reconfigure : Quorum = 2 --
X----------->|->|        |       |  Accept!(N,I+1,W) (Aux not participating)
|<-----------X--X--------------->|  Accepted(N,I+1,W)
|            |  |        |       |


== Fast Paxos ==
Fast Paxos generalizes Basic Paxos to reduce end-to-end message delays.  In Basic Paxos, the message delay from client request to learning is 3 message delays.  Fast Paxos allows 2 message delays, but requires that (1) the system be composed of 3f+ 1 acceptors to tolerate up to f faults (instead of the classic 2f+1), and (2) the Client to send its request to multiple destinations.
Intuitively, if the leader has no value to propose, then a client could send an Accept! message to the Acceptors directly.  The Acceptors would respond as in Basic Paxos, sending Accepted messages to the leader and every Learner achieving two message delays from Client to Learner.
If the leader detects a collision, it resolves the collision by sending Accept! messages for a new round which are Accepted as usual.  This coordinated recovery technique requires four message delays from Client to Learner.
The final optimization occurs when the leader specifies a recovery technique in advance, allowing the Acceptors to perform the collision recovery themselves.  Thus, uncoordinated collision recovery can occur in three message delays (and only two message delays if all Learners are also Acceptors).


=== Message flow: Fast Paxos, non-conflicting ===
Client    Leader         Acceptor      Learner
   |         |          |  |  |  |       |  |
   |         X--------->|->|->|->|       |  |  Any(N,I,Recovery)
   |         |          |  |  |  |       |  |
   X------------------->|->|->|->|       |  |  Accept!(N,I,W)
   |         |<---------X--X--X--X------>|->|  Accepted(N,I,W)
   |<------------------------------------X--X  Response(W)
   |         |          |  |  |  |       |  |


=== Message flow: Fast Paxos, conflicting proposals ===
Conflicting proposals with coordinated recovery.  Note: the protocol does not specify how to handle the dropped client request.

Client   Leader      Acceptor     Learner
 |  |      |        |  |  |  |      |  |
 |  |      |        |  |  |  |      |  |
 |  |      |        |  |  |  |      |  |  !! Concurrent conflicting proposals
 |  |      |        |  |  |  |      |  |  !!   received in different order
 |  |      |        |  |  |  |      |  |  !!   by the Acceptors
 |  X--------------?|-?|-?|-?|      |  |  Accept!(N,I,V)
 X-----------------?|-?|-?|-?|      |  |  Accept!(N,I,W)
 |  |      |        |  |  |  |      |  |
 |  |      |        |  |  |  |      |  |  !! Acceptors disagree on value
 |  |      |<-------X--X->|->|----->|->|  Accepted(N,I,V)
 |  |      |<-------|<-|<-X--X----->|->|  Accepted(N,I,W)
 |  |      |        |  |  |  |      |  |
 |  |      |        |  |  |  |      |  |  !! Detect collision & recover
 |  |      X------->|->|->|->|      |  |  Accept!(N+1,I,W)
 |  |      |<-------X--X--X--X----->|->|  Accepted(N+1,I,W)
 |<---------------------------------X--X  Response(W)
 |  |      |        |  |  |  |      |  |

Conflicting proposals with uncoordinated recovery. 

Client   Leader      Acceptor     Learner
 |  |      |        |  |  |  |      |  |
 |  |      X------->|->|->|->|      |  |  Any(N,I,Recovery)
 |  |      |        |  |  |  |      |  |
 |  |      |        |  |  |  |      |  |  !! Concurrent conflicting proposals
 |  |      |        |  |  |  |      |  |  !!   received in different order
 |  |      |        |  |  |  |      |  |  !!   by the Acceptors
 |  X--------------?|-?|-?|-?|      |  |  Accept!(N,I,V)
 X-----------------?|-?|-?|-?|      |  |  Accept!(N,I,W)
 |  |      |        |  |  |  |      |  |
 |  |      |        |  |  |  |      |  |  !! Acceptors disagree on value
 |  |      |<-------X--X->|->|----->|->|  Accepted(N,I,V)
 |  |      |<-------|<-|<-X--X----->|->|  Accepted(N,I,W)
 |  |      |        |  |  |  |      |  |
 |  |      |        |  |  |  |      |  |  !! Detect collision & recover
 |  |      |<-------X--X--X--X----->|->|  Accepted(N+1,I,W)
 |<---------------------------------X--X  Response(W)
 |  |      |        |  |  |  |      |  |


=== Message flow: Fast Paxos with uncoordinated recovery, collapsed roles ===
(merged Acceptor/Learner roles)

Client         Servers
 |  |         |  |  |  |
 |  |         X->|->|->|  Any(N,I,Recovery)
 |  |         |  |  |  |
 |  |         |  |  |  |  !! Concurrent conflicting proposals
 |  |         |  |  |  |  !!   received in different order
 |  |         |  |  |  |  !!   by the Servers
 |  X--------?|-?|-?|-?|  Accept!(N,I,V)
 X-----------?|-?|-?|-?|  Accept!(N,I,W)
 |  |         |  |  |  |
 |  |         |  |  |  |  !! Servers disagree on value
 |  |         X<>X->|->|  Accepted(N,I,V)
 |  |         |<-|<-X<>X  Accepted(N,I,W)
 |  |         |  |  |  |
 |  |         |  |  |  |  !! Detect collision & recover
 |  |         X<>X<>X<>X  Accepted(N+1,I,W)
 |<-----------X--X--X--X  Response(W)
 |  |         |  |  |  |


== Generalized Paxos ==
Generalized consensus explores the relationship between the operations of the replicated state machine and the consensus protocol that implements it. The main discovery involves optimizations of Paxos when conflicting proposals could be applied in any order.  i.e., when the proposed operations are commutative operations for the state machine. In such cases, the conflicting operations can both be accepted, avoiding the delays required for resolving conflicts and re-proposing the rejected operations.
This concept is further generalized into ever-growing sequences of commutative operations, some of which are known to be stable (and thus may be executed).  The protocol tracks these sequences ensuring that all proposed operations of one sequence are stabilized before allowing any operation non-commuting with them to become stable.


=== Example ===
In order to illustrate Generalized Paxos, the example below shows a message flow between two concurrently executing clients and a replicated state machine implementing read/write operations over two distinct registers A and B.

Note that  in this table indicates operations which are non-commutative.
A possible sequence of operations :

 <1:Read(A), 2:Read(B), 3:Write(B), 4:Read(B), 5:Read(A), 6:Write(A)> 
Since 5:Read(A) commutes with both 3:Write(B) and 4:Read(B), one possible permutation equivalent to the previous order is the following:

 <1:Read(A), 2:Read(B), 5:Read(A), 3:Write(B), 4:Read(B), 6:Write(A)> 
In practice, a commute occurs only when operations are proposed concurrently.


=== Message flow: Generalized Paxos (example) ===
Responses not shown. Note: message abbreviations differ from previous message flows due to specifics of the protocol, see  for a full discussion.

Client      Leader  Acceptor       Learner
 |  |         |      |  |  |         |  |  !! New Leader Begins Round
 |  |         X----->|->|->|         |  |  Prepare(N)
 |  |         |<-----X- X- X         |  |  Promise(N,null)
 |  |         X----->|->|->|         |  |  Phase2Start(N,null)
 |  |         |      |  |  |         |  | 
 |  |         |      |  |  |         |  |  !! Concurrent commuting proposals
 |  X------- ?|-----?|-?|-?|         |  |  Propose(ReadA)
 X-----------?|-----?|-?|-?|         |  |  Propose(ReadB)
 |  |         X------X-------------->|->|  Accepted(N,<ReadA,ReadB>)
 |  |         |<--------X--X-------->|->|  Accepted(N,<ReadB,ReadA>)
 |  |         |      |  |  |         |  |
 |  |         |      |  |  |         |  |  !! No Conflict, both accepted
 |  |         |      |  |  |         |  |  Stable = <ReadA, ReadB>
 |  |         |      |  |  |         |  |
 |  |         |      |  |  |         |  |  !! Concurrent conflicting proposals
 X-----------?|-----?|-?|-?|         |  |  Propose(<WriteB,ReadA>)
 |  X--------?|-----?|-?|-?|         |  |  Propose(ReadB)
 |  |         |      |  |  |         |  |
 |  |         X------X-------------->|->|  Accepted(N,<WriteB,ReadA> . <ReadB>)
 |  |         |<--------X--X-------->|->|  Accepted(N,<ReadB> . <WriteB,ReadA>)
 |  |         |      |  |  |         |  |
 |  |         |      |  |  |         |  |  !! Conflict detected, leader chooses
 |  |         |      |  |  |         |  |  commutative order:
 |  |         |      |  |  |         |  |  V = <ReadA, WriteB, ReadB>
 |  |         |      |  |  |         |  |
 |  |         X----->|->|->|         |  |  Phase2Start(N+1,V)
 |  |         |<-----X- X- X-------->|->|  Accepted(N+1,V)
 |  |         |      |  |  |         |  |  Stable = <ReadA, ReadB> .
 |  |         |      |  |  |         |  |           <ReadA, WriteB, ReadB>
 |  |         |      |  |  |         |  |
 |  |         |      |  |  |         |  | !! More conflicting proposals
 X-----------?|-----?|-?|-?|         |  |  Propose(WriteA)
 |  X--------?|-----?|-?|-?|         |  |  Propose(ReadA)
 |  |         |      |  |  |         |  |
 |  |         X------X-------------->|->|  Accepted(N+1,<WriteA> . <ReadA>)
 |  |         |<--------X- X-------->|->|  Accepted(N+1,<ReadA> . <WriteA>)
 |  |         |      |  |  |         |  |
 |  |         |      |  |  |         |  |  !! Leader chooses order:
 |  |         |      |  |  |         |  |  W = <WriteA, ReadA>
 |  |         |      |  |  |         |  |
 |  |         X----->|->|->|         |  |  Phase2Start(N+2,W)
 |  |         |<-----X- X- X-------->|->|  Accepted(N+2,W)
 |  |         |      |  |  |         |  |  Stable = <ReadA, ReadB> .
 |  |         |      |  |  |         |  |           <ReadA, WriteB, ReadB> .
 |  |         |      |  |  |         |  |           <WriteA, ReadA>
 |  |         |      |  |  |         |  |


=== Performance ===
The above message flow shows us that Generalized Paxos can leverage operation semantics to avoid collisions when the spontaneous ordering of the network fails. This allows the protocol to be in practice quicker than Fast Paxos. However, when a collision occurs, Generalized Paxos needs two additional round trips to recover. This situation is illustrated with operations WriteB and ReadB in the above schema.
In the general case, such round trips are unavoidable and come from the fact that multiple commands can be accepted during a round. This makes the protocol more expensive than Paxos when conflicts are frequent. Hopefully two possible refinements of Generalized Paxos are possible to improve recovery time.
First, if the coordinator is part of every quorum of acceptors (round N is said centered), then to recover at round N+1 from a collision at round N, the coordinator skips phase 1 and proposes at phase 2 the sequence it accepted last during round N. This reduces the cost of recovery to a single round trip.
Second, if both rounds N and N+1 use a unique and identical centered quorum, when an acceptor detects a collision at round N, it spontaneously proposes at round N+1 a sequence suffixing both (i) the sequence accepted at round N by the coordinator and (ii) the greatest non-conflicting prefix it accepted at round N. For instance, if the coordinator and the acceptor accepted respectively at round N <WriteB, ReadB>  and <ReadB, ReadA> , the acceptor will spontaneously accept <WriteB, ReadB, ReadA> at round N+1. With this variation, the cost of recovery is a single message delay which is obviously optimal. Notice here that the use of a unique quorum at a round does not harm liveness. This comes from the fact that any process in this quorum is a read quorum for the prepare phase of the next rounds.


== Byzantine Paxos ==
Paxos may also be extended to support arbitrary failures of the participants, including lying, fabrication of messages, collusion with other participants, selective non-participation, etc.  These types of failures are called Byzantine failures, after the solution popularized by Lamport.Byzantine Paxos introduced by Castro and Liskov adds an extra message (Verify) which acts to distribute knowledge and verify the actions of the other processors:


=== Message flow: Byzantine Multi-Paxos, steady state ===
Client   Proposer      Acceptor     Learner
   |         |          |  |  |       |  |
   X-------->|          |  |  |       |  |  Request
   |         X--------->|->|->|       |  |  Accept!(N,I,V)
   |         |          X<>X<>X       |  |  Verify(N,I,V) - BROADCAST
   |         |<---------X--X--X------>|->|  Accepted(N,V)
   |<---------------------------------X--X  Response(V)
   |         |          |  |  |       |  |

Fast Byzantine Paxos introduced by Martin and Alvisi removes this extra delay, since the client sends commands directly to the Acceptors.
Note the Accepted message in Fast Byzantine Paxos is sent to all Acceptors and all Learners, while Fast Paxos sends Accepted messages only to Learners):


=== Message flow: Fast Byzantine Multi-Paxos, steady state ===
Client    Acceptor     Learner
   |      |  |  |       |  |
   X----->|->|->|       |  |  Accept!(N,I,V)
   |      X<>X<>X------>|->|  Accepted(N,I,V) - BROADCAST
   |<-------------------X--X  Response(V)
   |      |  |  |       |  |

The failure scenario is the same for both protocols;  Each Learner waits to receive F+1 identical messages from different Acceptors.  If this does not occur, the Acceptors themselves will also be aware of it (since they exchanged each other's messages in the broadcast round), and correct Acceptors will re-broadcast the agreed value:


=== Message flow: Fast Byzantine Multi-Paxos, failure ===
Client    Acceptor     Learner
   |      |  |  !       |  |  !! One Acceptor is faulty
   X----->|->|->!       |  |  Accept!(N,I,V)
   |      X<>X<>X------>|->|  Accepted(N,I,{V,W}) - BROADCAST
   |      |  |  !       |  |  !! Learners receive 2 different commands
   |      |  |  !       |  |  !! Correct Acceptors notice error and choose
   |      X<>X<>X------>|->|  Accepted(N,I,V) - BROADCAST
   |<-------------------X--X  Response(V)
   |      |  |  !       |  |


== Adapting Paxos for RDMA networks ==
With the emergence of very high speed reliable datacenter networks that support remote DMA (RDMA), there has been substantial interest in optimizing Paxos to leverage hardware offloading, in which the network interface card and network routers provide reliability and network-layer congestion control, freeing the host CPU for other tasks.  The Derecho C++ Paxos library is an open-source Paxos implementation that explores this option.Derecho offers both a classic Paxos, with data durability across full shutdown/restart sequences, and vertical Paxos (atomic multicast), for in-memory replication and state-machine synchronization.  The Paxos protocols employed by Derecho needed to be adapted to maximize asynchronous data streaming and remove other sources of delay on the leader's critical path.  So doing enables Derecho to sustain the full bidirectional RDMA data rate.  In contrast, although traditional Paxos protocols can be migrated to an RDMA network by simply mapping the message send operations to native RDMA operations, doing so leaves round-trip delays on the critical path.  In high-speed RDMA networks, even small delays can be large enough to prevent utilization of the full potential bandwidth.


== Production use of Paxos ==
Google uses the Paxos algorithm in their Chubby distributed lock service in order to keep replicas consistent in case of failure.  Chubby is used by Bigtable which is now in production in Google Analytics and other products.
Google Spanner and Megastore use the Paxos algorithm internally.
The OpenReplica replication service uses Paxos to maintain replicas for an open access system that enables users to create fault-tolerant objects. It provides high performance through concurrent rounds and flexibility through dynamic membership changes.
IBM supposedly uses the Paxos algorithm in their IBM SAN Volume Controller product to implement a general purpose fault-tolerant virtual machine used to run the configuration and control components of the storage virtualization services offered by the cluster. (Original MIT & IBM research paper)
Microsoft uses Paxos in the Autopilot cluster management service from Bing, and in Windows Server Failover Clustering.
WANdisco have implemented Paxos within their DConE active-active replication technology.
XtreemFS uses a Paxos-based lease negotiation algorithm for fault-tolerant and consistent replication of file data and metadata.
Heroku uses Doozerd which implements Paxos for its consistent distributed data store.
Ceph uses Paxos as part of the monitor processes to agree which OSDs are up and in the cluster.
The Clustrix distributed SQL database uses Paxos for distributed transaction resolution.
Neo4j HA graph database implements Paxos, replacing Apache ZooKeeper from v1.9
Apache Cassandra NoSQL database uses Paxos for  Light Weight Transaction feature only
Amazon Elastic Container Services uses Paxos to maintain a consistent view of cluster state


== See also ==
Chandra–Toueg consensus algorithm
State machine
Raft


== References ==


== External links ==
Leslie Lamport's home page
Paxos Made Simple
Paxos Made Moderately Complex
Revisiting the Paxos Algorithm
Paxos Commit
Google Whitepaper: Chubby Distributed Lock Service
Google Whitepaper: Bigtable A Distributed Storage System for Structured Data
Survey of Paxos Algorithms (2007)
OpenReplica Open Replication Service
FTFile: Fault Tolerant File library
Isis2 library (the SafeSend primitive is a free, open source implementation of Paxos)
Mencius - Circular rotating Paxos for geo-distributed systems
WANdisco - Active-Active Replication solutions for Hadoop, Subversion & GIT
libpaxos, a collection of open source implementations of the Paxos algorithm
libpaxos-cpp, a C++ implementation of the paxos distributed consensus algorithm
JBP - Java Byzantine Paxos
erlpaxos, Paxos by Erlang
paxos - Straight-forward paxos implementation in Python & Java
Manhattan Paxos (mpaxos), Paxos in C, supporting multiple paxos groups and efficient transactions across them.
Clustering with Neo4j
HT-Paxos
PaxosStore, paxos implementation in WeChat
LWT in Cassandra
Google TechTalks: The Paxos Algorithm",1102229,599,"All articles needing additional references, Articles needing additional references from October 2018, CS1 errors: missing periodical, Distributed algorithms, Fault-tolerant computer systems, Webarchive template wayback links, Wikipedia articles needing clarification from October 2018",1130562313,cs
https://en.wikipedia.org/wiki/Polymorphism_(computer_science),Polymorphism (computer science),"In programming language theory and type theory, polymorphism is the provision of a single interface to entities of different types  or the use of a single symbol to represent multiple different types. The concept is borrowed from a principle in biology where an organism or species can have many different forms or stages.The most commonly recognized major classes of polymorphism are:

Ad hoc polymorphism: defines a common interface for an arbitrary set of individually specified types.
Parametric polymorphism: not specifying concrete types and instead use abstract symbols that can substitute for any type.
Subtyping (also called subtype polymorphism or inclusion polymorphism): when a name denotes instances of many different classes related by some common superclass.

","In programming language theory and type theory, polymorphism is the provision of a single interface to entities of different types  or the use of a single symbol to represent multiple different types. The concept is borrowed from a principle in biology where an organism or species can have many different forms or stages.The most commonly recognized major classes of polymorphism are:

Ad hoc polymorphism: defines a common interface for an arbitrary set of individually specified types.
Parametric polymorphism: not specifying concrete types and instead use abstract symbols that can substitute for any type.
Subtyping (also called subtype polymorphism or inclusion polymorphism): when a name denotes instances of many different classes related by some common superclass.


== History ==
Interest in polymorphic type systems developed significantly in the 1960s, with practical implementations beginning to appear by the end of the decade.  Ad hoc polymorphism and parametric polymorphism were originally described in Christopher Strachey's Fundamental Concepts in Programming Languages, where they are listed as ""the two main classes"" of polymorphism.  Ad hoc polymorphism was a feature of Algol 68, while parametric polymorphism was the core feature of ML's type system.
In a 1985 paper, Peter Wegner and Luca Cardelli introduced the term inclusion polymorphism to model subtypes and inheritance, citing Simula as the first programming language to implement it.


== Types ==


=== Ad hoc polymorphism ===

Christopher Strachey chose the term ad hoc polymorphism to refer to polymorphic functions that can be applied to arguments of different types, but that behave differently depending on the type of the argument to which they are applied (also known as function overloading or operator overloading). The term ""ad hoc"" in this context is not intended to be pejorative; it refers simply to the fact that this type of polymorphism is not a fundamental feature of the type system. In the Pascal / Delphi example below, the Add functions seem to work generically over various types when looking at the invocations, but are considered to be two entirely distinct functions by the compiler for all intents and purposes:

In dynamically typed languages the situation can be more complex as the correct function that needs to be invoked might only be determinable at run time.
Implicit type conversion has also been defined as a form of polymorphism, referred to as ""coercion polymorphism"".


=== Parametric polymorphism ===

Parametric polymorphism allows a function or a data type to be written generically, so that it can handle values uniformly without depending on their type. Parametric polymorphism is a way to make a language more expressive while still maintaining full static type-safety.
The concept of parametric polymorphism applies to both data types and functions. A function that can evaluate to or be applied to values of different types is known as a polymorphic function. A data type that can appear to be of a generalized type (e.g. a list with elements of arbitrary type) is designated polymorphic data type like the generalized type from which such specializations are made.
Parametric polymorphism is ubiquitous in functional programming, where it is often simply referred to as ""polymorphism"". The following example in Haskell shows a parameterized list data type and two parametrically polymorphic functions on them:

Parametric polymorphism is also available in several object-oriented languages. For instance, templates in C++ and D, or under the name generics in C#, Delphi, Java and Go:

John C. Reynolds (and later Jean-Yves Girard) formally developed this notion of polymorphism as an extension to lambda calculus (called the polymorphic lambda calculus or System F). Any parametrically polymorphic function is necessarily restricted in what it can do, working on the shape of the data instead of its value, leading to the concept of parametricity.


=== Subtyping ===

Some languages employ the idea of subtyping (also called subtype polymorphism or inclusion polymorphism) to restrict the range of types that can be used in a particular case of polymorphism. In these languages, subtyping allows a function to be written to take an object of a certain type T, but also work correctly, if passed an object that belongs to a type S that is a subtype of T (according to the Liskov substitution principle). This type relation is sometimes written S <: T. Conversely, T is said to be a supertype of S—written T :> S. Subtype polymorphism is usually resolved dynamically (see below).
In the following Java example we make cats and dogs subtypes of animals. The procedure letsHear() accepts an animal, but will also work correctly if a subtype is passed to it:

In another example, if Number, Rational, and Integer are types such that Number :> Rational and Number :> Integer, a function written to take a Number will work equally well when passed an Integer or Rational as when passed a Number. The actual type of the object can be hidden from clients into a black box, and accessed via object identity.
In fact, if the Number type is abstract, it may not even be possible to get your hands on an object whose most-derived type is Number (see abstract data type, abstract class). This particular kind of type hierarchy is known—especially in the context of the Scheme programming language—as a numerical tower, and usually contains many more types.
Object-oriented programming languages offer subtype polymorphism using subclassing (also known as inheritance). In typical implementations, each class contains what is called a virtual table—a table of functions that implement the polymorphic part of the class interface—and each object contains a pointer to the ""vtable"" of its class, which is then consulted whenever a polymorphic method is called. This mechanism is an example of:

late binding, because virtual function calls are not bound until the time of invocation;
single dispatch (i.e. single-argument polymorphism), because virtual function calls are bound simply by looking through the vtable provided by the first argument (the this object), so the runtime types of the other arguments are completely irrelevant.The same goes for most other popular object systems. Some, however, such as Common Lisp Object System, provide multiple dispatch, under which method calls are polymorphic in all arguments.
The interaction between parametric polymorphism and subtyping leads to the concepts of variance and bounded quantification.


=== Row polymorphism ===

Row polymorphism is a similar, but distinct concept from subtyping. It deals with structural types.  It allows the usage of all values whose types have certain properties, without losing the remaining type information.


=== Polytypism ===

A related concept is polytypism (or data type genericity). A polytypic function is more general than polymorphic, and in such a function, ""though one can provide fixed ad hoc cases for specific data types, an ad hoc combinator is absent"".


== Implementation aspects ==


=== Static and dynamic polymorphism ===

Polymorphism can be distinguished by when the implementation is selected: statically (at compile time) or dynamically (at run time, typically via a virtual function). This is known respectively as static dispatch and dynamic dispatch, and the corresponding forms of polymorphism are accordingly called static polymorphism and dynamic polymorphism.
Static polymorphism executes faster, because there is no dynamic dispatch overhead, but requires additional compiler support. Further, static polymorphism allows greater static analysis by compilers (notably for optimization), source code analysis tools, and human readers (programmers). Dynamic polymorphism is more flexible but slower—for example, dynamic polymorphism allows duck typing, and a dynamically linked library may operate on objects without knowing their full type.
Static polymorphism typically occurs in ad hoc polymorphism and parametric polymorphism, whereas dynamic polymorphism is usual for subtype polymorphism. However, it is possible to achieve static polymorphism with subtyping through more sophisticated use of template metaprogramming, namely the curiously recurring template pattern.
When polymorphism is exposed via a library, static polymorphism becomes impossible for dynamic libraries as there is no way of knowing what types the parameters are when the shared object is built. While languages like C++ and Rust use monomorphized templates, the Swift programming language makes extensive use of dynamic dispatch to build the application binary interface for these libraries by default. As a result, more code can be shared for a reduced system size at the cost of runtime overhead.


== See also ==
Duck typing for polymorphism without (static) types
Polymorphic code (computer virus terminology)
System F for a lambda calculus with parametric polymorphism.
Type class
Type theory
Virtual inheritance


== References ==


== External links ==
C++ examples of polymorphism
Objects and Polymorphism (Visual Prolog)
Polymorphism on MSDN
Polymorphism Java Documentation on Oracle",2297447,1052,"Articles with example C Sharp code, Articles with example Haskell code, Articles with example Java code, Articles with example Pascal code, Articles with short description, Data types, Functional programming, Generic programming, Object-oriented programming, Polymorphism (computer science), Programming language concepts, Short description is different from Wikidata, Type theory",1133175318,cs
https://en.wikipedia.org/wiki/Polling_(computer_science),Polling (computer science),"Polling, or polled operation, in computer science, refers to actively sampling the status of an external device by a client program as a synchronous activity. Polling is most often used in terms of input/output (I/O), and is also referred to as polled I/O or software-driven I/O. A good example of hardware implementation is a watchdog timer.","Polling, or polled operation, in computer science, refers to actively sampling the status of an external device by a client program as a synchronous activity. Polling is most often used in terms of input/output (I/O), and is also referred to as polled I/O or software-driven I/O. A good example of hardware implementation is a watchdog timer.


== Description ==
Polling is the process where the computer or controlling device waits for an external device to check for its readiness or state, often with low-level hardware. For example, when a printer is connected via a parallel port, the computer waits until the printer has received the next character. These processes can be as minute as only reading one bit. This is sometimes used synonymously with 'busy-wait' polling. In this situation, when an I/O operation is required, the computer does nothing other than check the status of the I/O device until it is ready, at which point the device is accessed. In other words, the computer waits until the device is ready. Polling also refers to the situation where a device is repeatedly checked for readiness, and if it is not, the computer returns to a different task. Although not as wasteful of CPU cycles as busy waiting, this is generally not as efficient as the alternative to polling, interrupt-driven I/O.
In a simple single-purpose system, even busy-wait is perfectly appropriate if no action is possible until the I/O access, but more often than not this was traditionally a consequence of simple hardware or non-multitasking operating systems.
Polling is often intimately involved with very low-level hardware. For example, polling a parallel printer port to check whether it is ready for another character involves examining as little as one bit of a byte. That bit represents, at the time of reading, whether a single wire in the printer cable is at low or high voltage. The I/O instruction that reads this byte directly transfers the voltage state of eight real world wires to the eight circuits (flip flops) that make up one byte of a CPU register.
Polling has the disadvantage that if there are too many devices to check, the time required to poll them can exceed the time available to service the I/O device.


=== Algorithm ===
Polling can be described in the following steps:
Host actions:

The host repeatedly reads the busy bit of the controller until it becomes clear (with a value of 0).
When clear, the host writes the command into the command register. If the host is sending output, it sets the write bit and writes a byte into the data-out register. If the host is receiving input, it reads the controller-written data from the data-in register, and sets the read bit to 0 as the next command.
The host sets the command-ready bit to 1.Controller actions:

When the controller notices that the command-ready bit is set, it sets the busy bit to 1.
The controller reads the command register. If the write bit inside is set, it reads from the data-out register and performs the necessary I/O operations on the device. If the read bit is set, data from the device is loaded into the data-in register for the host to read.
Once the operations are over, the controller clears the command-ready bit, clears the error bit to show the operation was successful, and clears the busy bit.


== Types ==
A polling cycle is the time in which each element is monitored once. The optimal polling cycle will vary according to several factors, including the desired speed of response and the overhead (e.g., processor time and bandwidth) of the polling.
In roll call polling, the polling device or process queries each element on a list in a fixed sequence. Because it waits for a response from each element, a timing mechanism is necessary to prevent lock-ups caused by non-responding elements. Roll call polling can be inefficient if the overhead for the polling messages is high, there are numerous elements to be polled in each polling cycle and only a few elements are active.
In hub polling, also referred to as token polling, each element polls the next element in some fixed sequence. This continues until the first element is reached, at which time the polling cycle starts all over again.
Polling can be employed in various computing contexts in order to control the execution or transmission sequence of the elements involved. For example, in multitasking operating systems, polling can be used to allocate processor time and other resources to the various competing processes.
In networks, polling is used to determine which nodes want to access the network. It is also used by routing protocols to retrieve routing information, as is the case with EGP (exterior gateway protocol).
An alternative to polling is the use of interrupts, which are signals generated by devices or processes to indicate that they need attention, want to communicate, etc. Although polling can be very simple, in many situations (e.g., multitasking operating systems) it is more efficient to use interrupts because it can reduce processor usage and/or bandwidth consumption.


== Poll message ==
A poll message is a control-acknowledgment message.
In a multidrop line arrangement (a central computer and different terminals in which the terminals share a single communication line to and from the computer), the system uses a master/slave polling arrangement whereby the central computer sends message (called polling message) to a specific terminal on the outgoing line. All terminals listen to the outgoing line, but only the terminal that is polled replies by sending any information that it has ready for transmission on the incoming line.In star networks, which, in its simplest form, consists of one central switch, hub, or computer that acts as a conduit to transmit messages, polling is not required to avoid chaos on the lines, but it is often used to allow the master to acquire input in an orderly fashion. These poll messages differ from those of the multidrop lines case because there are no site addresses needed, and each terminal only receives those polls that are directed to it.


== See also ==
Abstraction (computer science)
Asynchronous I/O
Bit banging
Infinite loop
Interrupt request (PC architecture)
Integer (computer science)
kqueue
Multiple asynchronous periodic polling
Pull technology
select (Unix)
Signal (IPC)


== References ==",937358,186,"All articles needing additional references, Articles needing additional references from January 2015, Articles with short description, Events (computing), Input/output, Short description is different from Wikidata",1119859475,cs
https://en.wikipedia.org/wiki/String_(computer_science),String (computer science),"In computer programming, a string is traditionally a sequence of characters, either as a literal constant or as some kind of variable. The latter may allow its elements to be mutated and the length changed, or it may be fixed (after creation). A string is generally considered as a data type and is often implemented as an array data structure of bytes (or words) that stores a sequence of elements, typically characters, using some character encoding. String may also denote more general arrays or other sequence (or list) data types and structures.
Depending on the programming language and precise data type used, a variable declared to be a string may either cause storage in memory to be statically allocated for a predetermined maximum length or employ dynamic allocation to allow it to hold a variable number of elements.
When a string appears literally in source code, it is known as a string literal or an anonymous string.In formal languages, which are used in mathematical logic and theoretical computer science, a string is a finite sequence of symbols that are chosen from a set called an alphabet.","In computer programming, a string is traditionally a sequence of characters, either as a literal constant or as some kind of variable. The latter may allow its elements to be mutated and the length changed, or it may be fixed (after creation). A string is generally considered as a data type and is often implemented as an array data structure of bytes (or words) that stores a sequence of elements, typically characters, using some character encoding. String may also denote more general arrays or other sequence (or list) data types and structures.
Depending on the programming language and precise data type used, a variable declared to be a string may either cause storage in memory to be statically allocated for a predetermined maximum length or employ dynamic allocation to allow it to hold a variable number of elements.
When a string appears literally in source code, it is known as a string literal or an anonymous string.In formal languages, which are used in mathematical logic and theoretical computer science, a string is a finite sequence of symbols that are chosen from a set called an alphabet.


== String datatypes ==

A string datatype is a datatype modeled on the idea of a formal string. Strings are such an important and useful datatype that they are implemented in nearly every programming language. In some languages they are available as primitive types and in others as composite types. The syntax of most high-level programming languages allows for a string, usually quoted in some way, to represent an instance of a string datatype; such a meta-string is called a literal or string literal.


=== String length ===
Although formal strings can have an arbitrary finite length, the length of strings in real languages is often constrained to an artificial maximum. In general, there are two types of string datatypes: fixed-length strings, which have a fixed maximum length to be determined at compile time and which use the same amount of memory whether this maximum is needed or not, and variable-length strings, whose length is not arbitrarily fixed and which can use varying amounts of memory depending on the actual requirements at run time (see Memory management). Most strings in modern programming languages are variable-length strings. Of course, even variable-length strings are limited in length – by the size of available computer memory. The string length can be stored as a separate integer (which may put another artificial limit on the length) or implicitly through a termination character, usually a character value with all bits zero such as in C programming language. See also ""Null-terminated"" below.


=== Character encoding ===
String datatypes have historically allocated one byte per character, and, although the exact character set varied by region, character encodings were similar enough that programmers could often get away with ignoring this, since characters a program treated specially (such as period and space and comma) were in the same place in all the encodings a program would encounter. These character sets were typically based on ASCII or EBCDIC. If text in one encoding was displayed on a system using a different encoding, text was often mangled, though often somewhat readable and some computer users learned to read the mangled text.
Logographic languages such as Chinese, Japanese, and Korean (known collectively as CJK) need far more than 256 characters (the limit of a one 8-bit byte per-character encoding) for reasonable representation. The normal solutions involved keeping single-byte representations for ASCII and using two-byte representations for CJK ideographs. Use of these with existing code led to problems with matching and cutting of strings, the severity of which depended on how the character encoding was designed. Some encodings such as the EUC family guarantee that a byte value in the ASCII range will represent only that ASCII character, making the encoding safe for systems that use those characters as field separators. Other encodings such as ISO-2022 and Shift-JIS do not make such guarantees, making matching on byte codes unsafe. These encodings also were not ""self-synchronizing"", so that locating character boundaries required backing up to the start of a string, and pasting two strings together could result in corruption of the second string.
Unicode has simplified the picture somewhat. Most programming languages now have a datatype for Unicode strings. Unicode's preferred byte stream format UTF-8 is designed not to have the problems described above for older multibyte encodings. UTF-8, UTF-16 and UTF-32 require the programmer to know that the fixed-size code units are different than the ""characters"", the main difficulty currently is incorrectly designed APIs that attempt to hide this difference (UTF-32 does make code points fixed-sized, but these are not ""characters"" due to composing codes).


=== Implementations ===

Some languages, such as C++, Perl and Ruby, normally allow the contents of a string to be changed after it has been created; these are termed mutable strings.  In other languages, such as Java, JavaScript, Lua, Python, and Go, the value is fixed and a new string must be created if any alteration is to be made; these are termed immutable strings. Some of these languages with immutable strings also provide another type that is mutable, such as Java and .NET's StringBuilder, the thread-safe Java StringBuffer, and the Cocoa NSMutableString. There are both advantages and disadvantages to immutability: although immutable strings may require inefficiently creating many copies, they are simpler and completely thread-safe.
Strings are typically implemented as arrays of bytes, characters, or code units, in order to allow fast access to individual units or substrings—including characters when they have a fixed length.  A few languages such as Haskell implement them as linked lists instead.
Some languages, such as Prolog and Erlang, avoid implementing a dedicated string datatype at all, instead adopting the convention of representing strings as lists of character codes.


=== Representations ===
Representations of strings depend heavily on the choice of character repertoire and the method of character encoding. Older string implementations were designed to work with repertoire and encoding defined by ASCII, or more recent extensions like the ISO 8859 series. Modern implementations often use the extensive repertoire defined by Unicode along with a variety of complex encodings such as UTF-8 and UTF-16.
The term byte string usually indicates a general-purpose string of bytes, rather than strings of only (readable) characters, strings of bits, or such. Byte strings often imply that bytes can take any value and any data can be stored as-is, meaning that there should be no value interpreted as a termination value.
Most string implementations are very similar to variable-length arrays with the entries storing the character codes of corresponding characters. The principal difference is that, with certain encodings, a single logical character may take up more than one entry in the array. This happens for example with UTF-8, where single codes (UCS code points) can take anywhere from one to four bytes, and single characters can take an arbitrary number of codes. In these cases, the logical length of the string (number of characters) differs from the physical length of the array (number of bytes in use). UTF-32 avoids the first part of the problem.


==== Null-terminated ====

The length of a string can be stored implicitly by using a special terminating character; often this is the null character (NUL), which has all bits zero, a convention used and perpetuated by the popular C programming language. Hence, this representation is commonly referred to as a C string. This representation of an n-character string takes n + 1 space (1 for the terminator), and is thus an implicit data structure.
In terminated strings, the terminating code is not an allowable character in any string. Strings with length field do not have this limitation and can also store arbitrary binary data.
An example of a null-terminated string stored in a 10-byte buffer, along with its ASCII (or more modern UTF-8) representation as 8-bit hexadecimal numbers is:

The length of the string in the above example, ""FRANK"", is 5 characters, but it occupies 6 bytes. Characters after the terminator do not form part of the representation; they may be either part of other data or just garbage. (Strings of this form are sometimes called ASCIZ strings, after the original assembly language directive used to declare them.)


==== Byte- and bit-terminated ====
Using a special byte other than null for terminating strings has historically appeared in both hardware and software, though sometimes with a value that was also a printing character. $ was used by many assembler systems, : used by CDC systems (this character had a value of zero), and the ZX80 used "" since this was the string delimiter in its BASIC language.
Somewhat similar, ""data processing"" machines like the IBM 1401 used a special word mark bit to delimit strings at the left, where the operation would start at the right. This bit had to be clear in all other parts of the string. This meant that, while the IBM 1401 had a seven-bit word, almost no-one ever thought to use this as a feature, and override the assignment of the seventh bit to (for example) handle ASCII codes.
Early microcomputer software relied upon the fact that ASCII codes do not use the high-order bit, and set it to indicate the end of a string. It must be reset to 0 prior to output.


==== Length-prefixed ====
The length of a string can also be stored explicitly, for example by prefixing the string with the length as a byte value. This convention is used in many Pascal dialects; as a consequence, some people call such a string a Pascal string or P-string. Storing the string length as byte limits the maximum string length to 255. To avoid such limitations, improved implementations of P-strings use 16-, 32-, or 64-bit words to store the string length. When the length field covers the address space, strings are limited only by the available memory.
If the length is bounded, then it can be encoded in constant space, typically a machine word, thus leading to an implicit data structure, taking n + k space, where k is the number of characters in a word (8 for 8-bit ASCII on a 64-bit machine, 1 for 32-bit UTF-32/UCS-4 on a 32-bit machine, etc.).
If the length is not bounded, encoding a length n takes log(n) space (see fixed-length code), so length-prefixed strings are a succinct data structure, encoding a string of length n in log(n) + n space.
In the latter case, the length-prefix field itself doesn't have fixed length, therefore the actual string data needs to be moved when the string grows such that the length field needs to be increased.
Here is a Pascal string stored in a 10-byte buffer, along with its ASCII / UTF-8 representation:


==== Strings as records ====
Many languages, including object-oriented ones, implement strings as records with an internal structure like:

However, since the implementation is usually hidden, the string must be accessed and modified through member functions. text is a pointer to a dynamically allocated memory area, which might be expanded as needed. See also string (C++).


==== Other representations ====
Both character termination and length codes limit strings: For example, C character arrays that contain null (NUL) characters cannot be handled directly by C string library functions: Strings using a length code are limited to the maximum value of the length code.
Both of these limitations can be overcome by clever programming.
It is possible to create data structures and functions that manipulate them that do not have the problems associated with character termination and can in principle overcome length code bounds. It is also possible to optimize the string represented using techniques from run length encoding (replacing repeated characters by the character value and a length) and Hamming encoding.
While these representations are common, others are possible. Using ropes makes certain string operations, such as insertions, deletions, and concatenations more efficient.
The core data structure in a text editor is the one that manages the string (sequence of characters) that represents the current state of the file being edited.
While that state could be stored in a single long consecutive array of characters, a typical text editor instead uses an alternative representation as its sequence data structure—a gap buffer, a linked list of lines, a piece table, or a rope—which makes certain string operations, such as insertions, deletions, and undoing previous edits, more efficient.


=== Security concerns ===
The differing memory layout and storage requirements of strings can affect the security of the program accessing the string data. String representations requiring a terminating character are commonly susceptible to buffer overflow problems if the terminating character is not present, caused by a coding error or an attacker deliberately altering the data. String representations adopting a separate length field are also susceptible if the length can be manipulated. In such cases, program code accessing the string data requires bounds checking to ensure that it does not inadvertently access or change data outside of the string memory limits.
String data is frequently obtained from user input to a program. As such, it is the responsibility of the program to validate the string to ensure that it represents the expected format. Performing limited or no validation of user input can cause a program to be vulnerable to code injection attacks.


== Literal strings ==

Sometimes, strings need to be embedded inside a text file that is both human-readable and intended for consumption by a machine.  This is needed in, for example, source code of programming languages, or in configuration files. In this case, the NUL character doesn't work well as a terminator since it is normally invisible (non-printable) and is difficult to input via a keyboard.  Storing the string length would also be inconvenient as manual computation and tracking of the length is tedious and error-prone.
Two common representations are:

Surrounded by quotation marks (ASCII 0x22 double quote ""str"" or ASCII 0x27 single quote 'str'), used by most programming languages. To be able to include special characters such as the quotation mark itself, newline characters, or non-printable characters, escape sequences are often available, usually prefixed with the backslash character (ASCII 0x5C).
Terminated by a newline sequence, for example in Windows INI files.


== Non-text strings ==
While character strings are very common uses of strings, a string in computer science may refer generically to any sequence of homogeneously typed data. A bit string or byte string, for example, may be used to represent non-textual binary data retrieved from a communications medium. This data may or may not be represented by a string-specific datatype, depending on the needs of the application, the desire of the programmer, and the capabilities of the programming language being used. If the programming language's string implementation is not 8-bit clean, data corruption may ensue.
C programmers draw a sharp distinction between a ""string"", aka a ""string of characters"", which by definition is always null terminated, vs. a ""byte string"" or ""pseudo string"" which may be stored in the same array but is often not null terminated.
Using C string handling functions on such a ""byte string"" often seems to work, but later leads to security problems.


== String processing algorithms ==

There are many algorithms for processing strings, each with various trade-offs. Competing algorithms can be analyzed with respect to run time, storage requirements, and so forth. The name stringology was coined in 1984 by computer scientist Zvi Galil for the theory of algorithms and data structures used for string processing.Some categories of algorithms include:

String searching algorithms for finding a given substring or pattern
String manipulation algorithms
Sorting algorithms
Regular expression algorithms
Parsing a string
Sequence miningAdvanced string algorithms often employ complex mechanisms and data structures, among them suffix trees and finite-state machines.


== Character string-oriented languages and utilities ==
Character strings are such a useful datatype that several languages have been designed in order to make string processing applications easy to write. Examples include the following languages:

awk
Icon
MUMPS
Perl
Rexx
Ruby
sed
SNOBOL
Tcl
TTMMany Unix utilities perform simple string manipulations and can be used to easily program some powerful string processing algorithms. Files and finite streams may be viewed as strings.
Some APIs like Multimedia Control Interface, embedded SQL or printf use strings to hold commands that will be interpreted.
Many scripting programming languages, including Perl, Python, Ruby, and Tcl employ regular expressions to facilitate text operations. Perl is particularly noted for its regular expression use, and many other languages and applications implement Perl compatible regular expressions.
Some languages such as Perl and Ruby support string interpolation, which permits arbitrary expressions to be evaluated and included in string literals.


== Character string functions ==

String functions are used to create strings or change the contents of a mutable string. They also are used to query information about a string. The set of functions and their names varies depending on the computer programming language.
The most basic example of a string function is the string length function – the function that returns the length of a string (not counting any terminator characters or any of the string's internal structural information) and does not modify the string. This function is often named length or len. For example, length(""hello world"") would return 11. Another common function is concatenation, where a new string is created by appending two strings, often this is the + addition operator.
Some microprocessor's instruction set architectures contain direct support for string operations, such as block copy (e.g. In intel x86m REPNZ MOVSB).


== Formal theory ==

Let Σ be a finite set of symbols (alternatively called characters), called the alphabet. No assumption is made about the nature of the symbols. A string (or word) over Σ is any finite sequence of symbols from Σ. For example, if Σ = {0, 1}, then 01011 is a string over Σ.
The length of a string s is the number of symbols in s (the length of the sequence) and can be any non-negative integer; it is often denoted as |s|.  The empty string is the unique string over Σ of length 0, and is denoted ε or λ.The set of all strings over Σ of length n is denoted Σn.  For example, if Σ = {0, 1}, then Σ2 = {00, 01, 10, 11}.  Note that Σ0 = {ε} for any alphabet Σ.
The set of all strings over Σ of any length is the Kleene closure of Σ and is denoted Σ*.  In terms of Σn,

  
    
      
        
          Σ
          
            ∗
          
        
        =
        
          ⋃
          
            n
            ∈
            
              N
            
            ∪
            {
            0
            }
          
        
        
          Σ
          
            n
          
        
      
    
    {\displaystyle \Sigma ^{*}=\bigcup _{n\in \mathbb {N} \cup \{0\}}\Sigma ^{n}}
  For example, if Σ = {0, 1}, then Σ* = {ε, 0, 1, 00, 01, 10, 11, 000, 001, 010, 011, ...}.  Although the set Σ* itself is countably infinite, each element of Σ* is a string of finite length.
A set of strings over Σ (i.e. any subset of Σ*) is called a formal language over Σ.  For example, if Σ = {0, 1}, the set of strings with an even number of zeros, {ε, 1, 00, 11, 001, 010, 100, 111, 0000, 0011, 0101, 0110, 1001, 1010, 1100, 1111, ...}, is a formal language over Σ.


=== Concatenation and substrings ===
Concatenation is an important binary operation on Σ*.  For any two strings s and t in Σ*, their concatenation is defined as the sequence of symbols in s followed by the sequence of characters in t, and is denoted st.  For example, if Σ = {a, b, ..., z}, s = bear, and t = hug, then st = bearhug and ts = hugbear.
String concatenation is an associative, but non-commutative operation. The empty string ε serves as the identity element; for any string s, εs = sε = s.  Therefore, the set Σ* and the concatenation operation form a monoid, the free monoid generated by Σ.  In addition, the length function defines a monoid homomorphism from Σ* to the non-negative integers (that is, a function 
  
    
      
        L
        :
        
          Σ
          
            ∗
          
        
        ↦
        
          N
        
        ∪
        {
        0
        }
      
    
    {\displaystyle L:\Sigma ^{*}\mapsto \mathbb {N} \cup \{0\}}
  , such that 
  
    
      
        L
        (
        s
        t
        )
        =
        L
        (
        s
        )
        +
        L
        (
        t
        )
        
        ∀
        s
        ,
        t
        ∈
        
          Σ
          
            ∗
          
        
      
    
    {\displaystyle L(st)=L(s)+L(t)\quad \forall s,t\in \Sigma ^{*}}
  ).
A string s is said to be a substring or factor of t if there exist (possibly empty) strings u and v such that t = usv.  The relation ""is a substring of"" defines a partial order on Σ*, the least element of which is the empty string.


=== Prefixes and suffixes ===
A string s is said to be a prefix of t if there exists a string u such that t = su. If u is nonempty, s is said to be a proper prefix of t. Symmetrically, a string s is said to be a suffix of t if there exists a string u such that t = us. If u is nonempty, s is said to be a proper suffix of t. Suffixes and prefixes are substrings of t. Both the relations ""is a prefix of"" and ""is a suffix of"" are prefix orders.


=== Reversal ===
The reverse of a string is a string with the same symbols but in reverse order. For example, if s = abc (where a, b, and c are symbols of the alphabet), then the reverse of s is cba. A string that is the reverse of itself (e.g., s = madam) is called a palindrome, which also includes the empty string and all strings of length 1.


=== Rotations ===
A string s = uv is said to be a rotation of t if t = vu. For example, if Σ = {0, 1} the string 0011001 is a rotation of 0100110, where u = 00110 and v = 01. As another example, the string abc has three different rotations, viz. abc itself (with u=abc, v=ε), bca (with u=bc, v=a), and cab (with u=c, v=ab).


=== Lexicographical ordering ===
It is often useful to define an ordering on a set of strings. If the alphabet Σ has a total order (cf. alphabetical order) one can define a total order on Σ* called lexicographical order. For example, if Σ = {0, 1} and 0 < 1, then the lexicographical order on Σ* includes the relationships ε < 0 < 00 < 000 < ... < 0001 < 001 < 01 < 010 < 011 < 0110 < 01111 < 1 < 10 < 100 < 101 < 111 < 1111 < 11111 ... The lexicographical order is total if the alphabetical order is, but isn't well-founded for any nontrivial alphabet, even if the alphabetical order is.
See Shortlex for an alternative string ordering that preserves well-foundedness.


=== String operations ===
A number of additional operations on strings commonly occur in the formal theory. These are given in the article on string operations.


=== Topology ===

Strings admit the following interpretation as nodes on a graph, where k is the number of symbols in Σ:

Fixed-length strings of length n can be viewed as the integer locations in an n-dimensional hypercube with sides of length k-1.
Variable-length strings (of finite length) can be viewed as nodes on a perfect k-ary tree.
Infinite strings (otherwise not considered here) can be viewed as infinite paths on a k-node complete graph.The natural topology on the set of fixed-length strings or variable-length strings is the discrete topology, but the natural topology on the set of infinite strings is the limit topology, viewing the set of infinite strings as the inverse limit of the sets of finite strings. This is the construction used for the p-adic numbers and some constructions of the Cantor set, and yields the same topology.
Isomorphisms between string representations of topologies can be found by normalizing according to the lexicographically minimal string rotation.


== See also ==
Binary-safe — a property of string manipulating functions treating their input as raw data stream
Bit array — a string of binary digits
C string handling — overview of C string handling
C++ string handling — overview of C++ string handling
Comparison of programming languages (string functions)
Connection string — passed to a driver to initiate a connection (e.g., to a database)
Empty string — its properties and representation in programming languages
Incompressible string — a string that cannot be compressed by any algorithm
Rope (data structure) — a data structure for efficiently manipulating long strings
String metric — notions of similarity between strings


== References ==",1755292,976,"Algorithms on strings, All articles needing additional references, Articles needing additional references from March 2015, Articles with short description, CS1 maint: unfit URL, Character encoding, Combinatorics on words, Commons category link is on Wikidata, Data types, Formal languages, Primitive types, Short description is different from Wikidata, String (computer science), Syntactic entities, Webarchive template wayback links, Wikipedia articles needing clarification from June 2015",1128124919,cs
https://en.wikipedia.org/wiki/Lock_(computer_science),Lock (computer science),"In computer science, a lock or mutex (from mutual exclusion) is a synchronization primitive: a mechanism that enforces limits on access to a resource when there are many threads of execution. A lock is designed to enforce a mutual exclusion concurrency control policy, and with a variety of possible methods there exists multiple unique implementations for different applications.

","In computer science, a lock or mutex (from mutual exclusion) is a synchronization primitive: a mechanism that enforces limits on access to a resource when there are many threads of execution. A lock is designed to enforce a mutual exclusion concurrency control policy, and with a variety of possible methods there exists multiple unique implementations for different applications.


== Types ==
Generally, locks are advisory locks, where each thread cooperates by acquiring the lock before accessing the corresponding data. Some systems also implement mandatory locks, where attempting unauthorized access to a locked resource will force an exception in the entity attempting to make the access.
The simplest type of lock is a binary semaphore. It provides exclusive access to the locked data. Other schemes also provide shared access for reading data. Other widely implemented access modes are exclusive, intend-to-exclude and intend-to-upgrade.
Another way to classify locks is by what happens when the lock strategy prevents the progress of a thread. Most locking designs block the execution of the thread requesting the lock until it is allowed to access the locked resource. With a spinlock, the thread simply waits (""spins"") until the lock becomes available. This is efficient if threads are blocked for a short time, because it avoids the overhead of operating system process re-scheduling. It is inefficient if the lock is held for a long time, or if the progress of the thread that is holding the lock depends on preemption of the locked thread.
Locks typically require hardware support for efficient implementation. This support usually takes the form of one or more atomic instructions such as ""test-and-set"", ""fetch-and-add"" or ""compare-and-swap"". These instructions allow a single process to test if the lock is free, and if free, acquire the lock in a single atomic operation.
Uniprocessor architectures have the option of using uninterruptible sequences of instructions—using special instructions or instruction prefixes to disable interrupts temporarily—but this technique does not work for multiprocessor shared-memory machines. Proper support for locks in a multiprocessor environment can require quite complex hardware or software support, with substantial synchronization issues.
The reason an atomic operation is required is because of concurrency, where more than one task executes the same logic. For example, consider the following C code:

The above example does not guarantee that the task has the lock, since more than one task can be testing the lock at the same time. Since both tasks will detect that the lock is free, both tasks will attempt to set the lock, not knowing that the other task is also setting the lock. Dekker's or Peterson's algorithm are possible substitutes if atomic locking operations are not available.
Careless use of locks can result in deadlock or livelock. A number of strategies can be used to avoid or recover from deadlocks or livelocks, both at design-time and at run-time. (The most common strategy is to standardize the lock acquisition sequences so that combinations of inter-dependent locks are always acquired in a specifically defined ""cascade"" order.)
Some languages do support locks syntactically. An example in C# follows:

The code lock(this) can lead to problems if the instance can be accessed publicly.Similar to Java, C# can also synchronize entire methods, by using the MethodImplOptions.Synchronized attribute.


== Granularity ==
Before being introduced to lock granularity, one needs to understand three concepts about locks:

lock overhead: the extra resources for using locks, like the memory space allocated for locks, the CPU time to initialize and destroy locks, and the time for acquiring or releasing locks. The more locks a program uses, the more overhead associated with the usage;
lock contention: this occurs whenever one process or thread attempts to acquire a lock held by another process or thread. The more fine-grained the available locks, the less likely one process/thread will request a lock held by the other. (For example, locking a row rather than the entire table, or locking a cell rather than the entire row);
deadlock: the situation when each of at least two tasks is waiting for a lock that the other task holds. Unless something is done, the two tasks will wait forever.There is a tradeoff between decreasing lock overhead and decreasing lock contention when choosing the number of locks in synchronization.
An important property of a lock is its granularity. The granularity is a measure of the amount of data the lock is protecting. In general, choosing a coarse granularity (a small number of locks, each protecting a large segment of data) results in less lock overhead when a single process is accessing the protected data, but worse performance when multiple processes are running concurrently. This is because of increased lock contention. The more coarse the lock, the higher the likelihood that the lock will stop an unrelated process from proceeding. Conversely, using a fine granularity (a larger number of locks, each protecting a fairly small amount of data) increases the overhead of the locks themselves but reduces lock contention. Granular locking where each process must hold multiple locks from a common set of locks can create subtle lock dependencies. This subtlety can increase the chance that a programmer will unknowingly introduce a deadlock.In a database management system, for example, a lock could protect, in order of decreasing granularity, part of a field, a field, a record, a data page, or an entire table. Coarse granularity, such as using table locks, tends to give the best performance for a single user, whereas fine granularity, such as record locks, tends to give the best performance for multiple users.


== Database locks ==

Database locks can be used as a means of ensuring transaction synchronicity. i.e. when making transaction processing concurrent (interleaving transactions), using 2-phased locks ensures that the concurrent execution of the transaction turns out equivalent to some serial ordering of the transaction. However, deadlocks become an unfortunate side-effect of locking in databases. Deadlocks are either prevented by pre-determining the locking order between transactions or are detected using waits-for graphs. An alternate to locking for database synchronicity while avoiding deadlocks involves the use of totally ordered global timestamps.
There are mechanisms employed to manage the actions of multiple concurrent users on a database—the purpose is to prevent lost updates and dirty reads. The two types of locking are pessimistic locking and optimistic locking:

Pessimistic locking: a user who reads a record with the intention of updating it places an exclusive lock on the record to prevent other users from manipulating it. This means no one else can manipulate that record until the user releases the lock. The downside is that users can be locked out for a very long time, thereby slowing the overall system response and causing frustration.Where to use pessimistic locking: this is mainly used in environments where data-contention (the degree of users request to the database system at any one time) is heavy; where the cost of protecting data through locks is less than the cost of rolling back transactions, if concurrency conflicts occur. Pessimistic concurrency is best implemented when lock times will be short, as in programmatic processing of records. Pessimistic concurrency requires a persistent connection to the database and is not a scalable option when users are interacting with data, because records might be locked for relatively large periods of time. It is not appropriate for use in Web application development.Optimistic locking: this allows multiple concurrent users access to the database whilst the system keeps a copy of the initial-read made by each user. When a user wants to update a record, the application determines whether another user has changed the record since it was last read. The application does this by comparing the initial-read held in memory to the database record to verify any changes made to the record. Any discrepancies between the initial-read and the database record violates concurrency rules and hence causes the system to disregard any update request. An error message is generated and the user is asked to start the update process again. It improves database performance by reducing the amount of locking required, thereby reducing the load on the database server. It works efficiently with tables that require limited updates since no users are locked out. However, some updates may fail. The downside is constant update failures due to high volumes of update requests from multiple concurrent users - it can be frustrating for users.Where to use optimistic locking: this is appropriate in environments where there is low contention for data, or where read-only access to data is required. Optimistic concurrency is used extensively in .NET to address the needs of mobile and disconnected applications, where locking data rows for prolonged periods of time would be infeasible. Also, maintaining record locks requires a persistent connection to the database server, which is not possible in disconnected applications.


== Disadvantages ==
Lock-based resource protection and thread/process synchronization have many disadvantages:

Contention: some threads/processes have to wait until a lock (or a whole set of locks) is released. If one of the threads holding a lock dies, stalls, blocks, or enters an infinite loop, other threads waiting for the lock may wait indefinitely until the computer is power cycled.
Overhead: the use of locks adds overhead for each access to a resource, even when the chances for collision are very rare. (However, any chance for such collisions is a race condition.)
Debugging: bugs associated with locks are time dependent and can be very subtle and extremely hard to replicate, such as deadlocks.
Instability: the optimal balance between lock overhead and lock contention can be unique to the problem domain (application) and sensitive to design, implementation, and even low-level system architectural changes. These balances may change over the life cycle of an application and may entail tremendous changes to update (re-balance).
Composability: locks are only composable (e.g., managing multiple concurrent locks in order to atomically delete item X from table A and insert X into table B) with relatively elaborate (overhead) software support and perfect adherence by applications programming to rigorous conventions.
Priority inversion: a low-priority thread/process holding a common lock can prevent high-priority threads/processes from proceeding. Priority inheritance can be used to reduce priority-inversion duration. The priority ceiling protocol can be used on uniprocessor systems to minimize the worst-case priority-inversion duration, as well as prevent deadlock.
Convoying: all other threads have to wait if a thread holding a lock is descheduled due to a time-slice interrupt or page fault.Some concurrency control strategies avoid some or all of these problems. For example, a funnel or serializing tokens can avoid the biggest problem: deadlocks. Alternatives to locking include non-blocking synchronization methods, like lock-free programming techniques and transactional memory. However, such alternative methods often require that the actual lock mechanisms be implemented at a more fundamental level of the operating software. Therefore, they may only relieve the application level from the details of implementing locks, with the problems listed above still needing to be dealt with beneath the application.
In most cases, proper locking depends on the CPU providing a method of atomic instruction stream synchronization (for example, the addition or deletion of an item into a pipeline requires that all contemporaneous operations needing to add or delete other items in the pipe be suspended during the manipulation of the memory content required to add or delete the specific item). Therefore, an application can often be more robust when it recognizes the burdens it places upon an operating system and is capable of graciously recognizing the reporting of impossible demands.


=== Lack of composability ===
One of lock-based programming's biggest problems is that ""locks don't compose"": it is hard to combine small, correct lock-based modules into equally correct larger programs without modifying the modules or at least knowing about their internals. Simon Peyton Jones (an advocate of software transactional memory) gives the following example of a banking application:
design a class Account that allows multiple concurrent clients to deposit or withdraw money to an account; and give an algorithm to transfer money from one account to another. The lock-based solution to the first part of the problem is:

class Account:
    member balance: Integer
    member mutex: Lock

    method deposit(n: Integer)
           mutex.lock()
           balance ← balance + n
           mutex.unlock()

    method withdraw(n: Integer)
           deposit(−n)

The second part of the problem is much more complicated. A transfer routine that is correct for sequential programs would be

function transfer(from: Account, to: Account, amount: integer)
    from.withdraw(amount)
    to.deposit(amount)

In a concurrent program, this algorithm is incorrect because when one thread is halfway through transfer, another might observe a state where amount has been withdrawn from the first account, but not yet deposited into the other account: money has gone missing from the system. This problem can only be fixed completely by taking locks on both account prior to changing any of the two accounts, but then the locks have to be taken according to some arbitrary, global ordering to prevent deadlock:

function transfer(from: Account, to: Account, amount: integer)
    if from < to    // arbitrary ordering on the locks
        from.lock()
        to.lock()
    else
        to.lock()
        from.lock()
    from.withdraw(amount)
    to.deposit(amount)
    from.unlock()
    to.unlock()

This solution gets more complicated when more locks are involved, and the transfer function needs to know about all of the locks, so they cannot be hidden.


== Language support ==

Programming languages vary in their support for synchronization:

Ada provides protected objects that have visible protected subprograms or entries as well as rendezvous.
The ISO/IEC C standard provides a standard mutual exclusion (locks) API since C11. The current ISO/IEC C++ standard supports threading facilities since C++11. The OpenMP standard is supported by some compilers, and allows critical sections to be specified using pragmas. The POSIX pthread API provides lock support. Visual C++ provides the synchronize attribute of methods to be synchronized, but this is specific to COM objects in the Windows architecture and Visual C++ compiler.  C and C++ can easily access any native operating system locking features.
C# provides the lock keyword on a thread to ensure its exclusive access to a resource.
VB.NET provides a SyncLock keyword like C#'s lock keyword.
Java provides the keyword synchronized to lock code blocks, methods or objects and libraries featuring concurrency-safe data structures.
Objective-C provides the keyword @synchronized to put locks on blocks of code and also provides the classes NSLock, NSRecursiveLock, and NSConditionLock along with the NSLocking protocol for locking as well.
PHP provides a file-based locking  as well as a Mutex class in the pthreads extension. 
Python provides a low-level mutex mechanism with a Lock class from the threading module.
The ISO/IEC Fortran standard (ISO/IEC 1539-1:2010) provides the lock_type derived type in the intrinsic module iso_fortran_env and the lock/unlock statements since Fortran 2008.
Ruby provides a low-level mutex object and no keyword.
Rust provides the Mutex<T> struct.
x86 assembly provides the LOCK prefix on certain operations to guarantee their atomicity.
Haskell implements locking via a mutable data structure called an MVar, which can either be empty or contain a value, typically a reference to a resource. A thread that wants to use the resource ‘takes’ the value of the MVar, leaving it empty, and puts it back when it is finished. Attempting to take a resource from an empty MVar results in the thread blocking until the resource is available. As an alternative to locking, an implementation of software transactional memory also exists.
Go provides a low-level Mutex object in standard's library sync package. It can be used for locking code blocks, methods or objects.


== See also ==
Critical section
Double-checked locking
File locking
Lock-free and wait-free algorithms
Monitor (synchronization)
Mutual exclusion
Read/write lock pattern
Semaphore (programming)


== References ==


== External links ==
Tutorial on Locks and Critical Sections",627531,368,"All articles with unsourced statements, Articles with example C Sharp code, Articles with short description, Articles with unsourced statements from July 2011, Articles with unsourced statements from November 2013, Concurrency control, Short description matches Wikidata, Software design patterns",1128688618,cs
https://en.wikipedia.org/wiki/Record_(computer_science),Record (computer science),"In computer science, a record (also called a structure,  struct, or compound data) is a basic data structure. Records in a database or spreadsheet are usually called ""rows"".A record is a collection of fields, possibly of different data types, typically in a fixed number and sequence. The fields of a record may also be called members, particularly in object-oriented programming; fields may also be called elements, though this risks confusion with the elements of a collection.
For example, a date could be stored as a record containing a numeric year field, a month field represented as a string, and a numeric day-of-month field. A personnel record might contain a name, a salary, and a rank. A Circle record might contain a center and a radius—in this instance, the center itself might be represented as a point record containing x and y coordinates.
Records are distinguished from arrays by the fact that their number of fields is determined in the definition of the record, and by the fact the records are a heterogenous data type; not all of the fields must contain the same type of data.A record type is a data type that describes such values and variables. Most modern computer languages allow the programmer to define new record types. The definition includes specifying the data type of each field and an identifier (name or label) by which it can be accessed.  In type theory, product types (with no field names) are generally preferred due to their simplicity, but proper record types are studied in languages such as System F-sub.  Since type-theoretical records may contain first-class function-typed fields in addition to data, they can express many features of object-oriented programming.
Records can exist in any storage medium, including main memory and mass storage devices such as magnetic tapes or hard disks.  Records are a fundamental component of most data structures, especially linked data structures.  Many computer files are organized as arrays of logical records, often grouped into larger physical records or blocks for efficiency.
The parameters of a function or procedure can often be viewed as the fields of a record variable; and the arguments passed to that function can be viewed as a record value that gets assigned to that variable at the time of the call. Also, in the call stack that is often used to implement procedure calls, each entry is an activation record or call frame, containing the procedure parameters and local variables, the return address, and other internal fields.
An object in object-oriented language is essentially a record that contains procedures specialized to handle that record; and object types are an elaboration of record types. Indeed, in most object-oriented languages, records are just special cases of objects, and are known as plain old data structures (PODSs), to contrast with objects that use OO features.
A record can be viewed as the computer analog of a mathematical tuple, although a tuple may or may not be considered a record, and vice versa, depending on conventions and the specific programming language.  In the same vein, a record type can be viewed as the computer language analog of the Cartesian product of two or more mathematical sets, or the implementation of an abstract product type in a specific language.","In computer science, a record (also called a structure,  struct, or compound data) is a basic data structure. Records in a database or spreadsheet are usually called ""rows"".A record is a collection of fields, possibly of different data types, typically in a fixed number and sequence. The fields of a record may also be called members, particularly in object-oriented programming; fields may also be called elements, though this risks confusion with the elements of a collection.
For example, a date could be stored as a record containing a numeric year field, a month field represented as a string, and a numeric day-of-month field. A personnel record might contain a name, a salary, and a rank. A Circle record might contain a center and a radius—in this instance, the center itself might be represented as a point record containing x and y coordinates.
Records are distinguished from arrays by the fact that their number of fields is determined in the definition of the record, and by the fact the records are a heterogenous data type; not all of the fields must contain the same type of data.A record type is a data type that describes such values and variables. Most modern computer languages allow the programmer to define new record types. The definition includes specifying the data type of each field and an identifier (name or label) by which it can be accessed.  In type theory, product types (with no field names) are generally preferred due to their simplicity, but proper record types are studied in languages such as System F-sub.  Since type-theoretical records may contain first-class function-typed fields in addition to data, they can express many features of object-oriented programming.
Records can exist in any storage medium, including main memory and mass storage devices such as magnetic tapes or hard disks.  Records are a fundamental component of most data structures, especially linked data structures.  Many computer files are organized as arrays of logical records, often grouped into larger physical records or blocks for efficiency.
The parameters of a function or procedure can often be viewed as the fields of a record variable; and the arguments passed to that function can be viewed as a record value that gets assigned to that variable at the time of the call. Also, in the call stack that is often used to implement procedure calls, each entry is an activation record or call frame, containing the procedure parameters and local variables, the return address, and other internal fields.
An object in object-oriented language is essentially a record that contains procedures specialized to handle that record; and object types are an elaboration of record types. Indeed, in most object-oriented languages, records are just special cases of objects, and are known as plain old data structures (PODSs), to contrast with objects that use OO features.
A record can be viewed as the computer analog of a mathematical tuple, although a tuple may or may not be considered a record, and vice versa, depending on conventions and the specific programming language.  In the same vein, a record type can be viewed as the computer language analog of the Cartesian product of two or more mathematical sets, or the implementation of an abstract product type in a specific language.


== Keys ==
A record may have zero or more keys. A key maps an expression to a value, or a set of values, in the record. A primary key is a key this is unique throughout all stored records; only one if this key exists. In other words, no duplicate may exist for any primary key. For example an employee file might contain employee number, name, department, and salary. The employee number will be unique in the organization and would be the primary key. Depending on the storage medium and file organization the employee number might be indexed—that is also stored in a separate file to make lookup faster. The department code is not necessarily unique; it may also be indexed, in which case it would be considered a secondary key, or alternate key. If it is not indexed the entire employee file would have to be scanned to produce a listing of all employees in a specific department. Keys are usually chosen in a way that minimizes the chances of multiple values being feasibly mapped to by one key. For example, the salary field would not normally be considered usable as a key since many employees will likely have the same salary. Indexing is one factor considered when designing a file.


== History ==

The concept of a record can be traced to various types of tables and ledgers used in accounting since remote times.  The modern notion of records in computer science, with fields of well-defined type and size, was already implicit in 19th century mechanical calculators, such as Babbage's Analytical Engine.

The original machine-readable medium used for data (as opposed to control) was punch card used for records in the 1890 United States Census: each punch card was a single record. Compare the journal entry from 1880 and the punch card from 1895. Records were well-established in the first half of the 20th century, when most data processing was done using punched cards. Typically each record of a data file would be recorded in one punched card, with specific columns assigned to specific fields. Generally, a record was the smallest unit that could be read in from external storage (e.g. card reader, tape or disk). The contents of punchcard-style records were originally called ""unit records"" because since punchcards had pre-determined document lengths. When storage systems became more advanced with the use of hard drives and magnetic tape, variable-length records became the standard. A variable-length record is a record in which the size of the record in bytes is approximately equal to the sum of the sizes of its fields. This was not possible to do before more advanced storage hardware was invented because all of the punchcards had to conform to pre-determined document lengths that the computer could read, since at the time the cards had to be physically fed into a machine. 
Most machine language implementations and early assembly languages did not have special syntax for records, but the concept was available (and extensively used) through the use of index registers, indirect addressing, and self-modifying code. Some early computers, such as the IBM 1620, had hardware support for delimiting records and fields, and special instructions for copying such records.
The concept of records and fields was central in some early file sorting and tabulating utilities, such as IBM's Report Program Generator (RPG).
COBOL was the first widespread programming language to support record types, and its record definition facilities were quite sophisticated at the time. The language allows for the definition of nested records with alphanumeric, integer, and fractional fields of arbitrary size and precision, as well as fields that automatically format any value assigned to them (e.g., insertion of currency signs, decimal points, and digit group separators). Each file is associated with a record variable where data is read into or written from. COBOL also provides a MOVE CORRESPONDING statement that assigns corresponding fields of two records according to their names.
The early languages developed for numeric computing, such as FORTRAN (up to FORTRAN IV) and Algol 60, did not have support for record types; but later versions of those languages, such as FORTRAN 77 and Algol 68 did add them. The original Lisp programming language too was lacking records (except for the built-in cons cell), but its S-expressions provided an adequate surrogate. The Pascal programming language was one of the first languages to fully integrate record types with other basic types into a logically consistent type system. The PL/I programming language provided for COBOL-style records. The C programming language initially provided the record concept as a kind of template (struct) that could be laid on top of a memory area, rather than a true record data type.  The latter were provided eventually (by the typedef declaration), but the two concepts are still distinct in the language. Most languages designed after Pascal (such as Ada, Modula, and Java), also supported records.
Although records are not often used in their original context anymore (i.e. being used solely for the purpose of containing data), records influenced  newer object oriented programming languages and relational database management systems. Since records provided more modularity in the way data was stored and handled, they are better suited at representing complex, real-world concepts than the primitive data types provided by default in languages. This influenced later languages such as C++, Python, JavaScript, and Objective-C which address the same modularity concerns of programmers. Objects in these languages are essentially records with the addition of methods and inheritance, which allow programmers to manipulate the way data behaves instead of only the contents of a record. Many programmers regard records as obsolete now since object-oriented languages have features that far surpass what records are capable of. On the other hand, many programmers argue that the low overhead and ability to use records in assembly language make records still relevant when programming with low levels of abstraction. to Today, the most popular languages on the TIOBE index, an indicator of the popularity of programming languages, have been influenced in some way by records due to the fact that they are object oriented. Query languages such as SQL and Object Query Language were also influenced by the concept of records. These languages allow the programmer to store sets of data, which are essentially records, in tables. This data can then be retrieved using a primary key. The tables themselves are also records which may have a foreign key: a key that references data in another table.   


== Operations ==
Declaration of a new record type, including the position, type, and (possibly) name of each field;
Declaration of variables and values as having a given record type;
Construction of a record value from given field values and (sometimes) with given field names;
Selection of a field of a record with an explicit name;
Assignment of a record value to a record variable;
Comparison of two records for equality;
Computation of a standard hash value for the record.The selection of a field from a record value yields a value.
Some languages may provide facilities that enumerate all fields of a record, or at least the fields that are references. This facility is needed to implement certain services such as debuggers, garbage collectors, and serialization. It requires some degree of type polymorphism.
In systems with record subtyping, operations on values of record type may also include:

Adding a new field to a record, setting the value of the new field.
Removing a field from a record.In such settings, a specific record type implies that a specific set of fields are present, but values of that type may contain additional fields. A record with fields x, y, and z would thus belong to the type of records with fields x and y, as would a record with fields x, y, and r. The rationale is that passing an (x,y,z) record to a function that expects an (x,y) record as argument should work, since that function will find all the fields it requires within the record. Many ways of practically implementing records in programming languages would have trouble with allowing such variability, but the matter is a central characteristic of record types in more theoretical contexts.


=== Assignment and comparison ===
Most languages allow assignment between records that have exactly the same record type (including same field types and names, in the same order). Depending on the language, however, two record data types defined separately may be regarded as distinct types even if they have exactly the same fields.
Some languages may also allow assignment between records whose fields have different names, matching each field value with the corresponding field variable by their positions within the record; so that, for example, a complex number with fields called real and imag can be assigned to a 2D point record variable with fields X and Y.  In this alternative, the two operands are still required to have the same sequence of field types.  Some languages may also require that corresponding types have the same size and encoding as well, so that the whole record can be assigned as an uninterpreted bit string.  Other languages may be more flexible in this regard, and require only that each value field can be legally assigned to the corresponding variable field; so that, for example, a short integer field can be assigned to a long integer field, or vice versa.
Other languages (such as COBOL) may match fields and values by their names, rather than positions.
These same possibilities apply to the comparison of two record values for equality.  Some languages may also allow order comparisons ('<'and '>'), using the lexicographic order based on the comparison of individual fields.PL/I allows both of the preceding types of assignment, and also allows structure expressions, such as a = a+1; where ""a"" is a record, or structure in PL/I terminology.


=== Algol 68's distributive field selection ===
In Algol 68, if Pts was an array of records, each with integer fields X and Y, one could write Y of Pts to obtain an array of integers, consisting of the Y fields of all the elements of Pts.  As a result, the statements Y of Pts[3] := 7 and (Y of Pts)[3] := 7 would have the same effect.


=== Pascal's ""with"" statement ===
In the Pascal programming language, the command with R do S would execute the command sequence S as if all the fields of record R had been declared as variables. Similarly to entering a different namespace in an object-oriented language like C#, it is no longer necessary to use the record name as a prefix to access the fields. So, instead of writing Pt.X := 5; Pt.Y := Pt.X + 3 one could write with Pt do begin X := 5; Y := X + 3 end.


== Representation in memory ==
The representation of records in memory varies depending on the programming languages. Usually the fields are stored in consecutive positions in memory, in the same order as they are declared in the record type. This may result in two or more fields stored into the same word of memory; indeed, this feature is often used in systems programming to access specific bits of a word. On the other hand, most compilers will add padding fields, mostly invisible to the programmer, in order to comply with alignment constraints imposed by the machine—say, that a floating point field must occupy a single word.
Some languages may implement a record as an array of addresses pointing to the fields (and, possibly, to their names and/or types). Objects in object-oriented languages are often implemented in rather complicated ways, especially in languages that allow multiple class inheritance.


== Self-defining records ==
A self-defining record is a type of record which contains information to identify the record type and to locate information within the record.  It may contain the offsets of elements; the elements can therefore be stored in any order or may be omitted. The information stored in a self-defining record can be interpreted as metadata for the record, which is similar to what one would expect to find in the UNIX metadata regarding a file, containing information such as the record's creation time and the size of the record in bytes. Alternatively, various elements of the record, each including an element identifier, can simply follow one another in any order.


== See also ==
Block (data storage)
Composite data type
Data hierarchy
Object composition
Passive data structure
Union type


== References ==",737745,338,"All articles needing additional references, All articles with unsourced statements, Articles needing additional references from September 2021, Articles with example Julia code, Articles with short description, Articles with unsourced statements from May 2009, Composite data types, Data types, Short description matches Wikidata",1132912169,cs
https://en.wikipedia.org/wiki/Data_(computer_science),Data (computer science),"In computer science, data (treated as singular, plural, or as a mass noun) is any sequence of one or more symbols; datum is a single symbol of data. Data requires interpretation to become information. Digital data is data that is represented using the binary number system of ones (1) and zeros (0), instead of analog representation. In modern (post-1960) computer systems, all data is digital. 
Data exists in three states: data at rest, data in transit and data in use.  Data within a computer, in most cases, moves as parallel data. Data moving to or from a computer, in most cases, moves as serial data. Data sourced from an analog device, such as a temperature sensor, may be converted to digital using an analog-to-digital converter. Data representing quantities, characters, or symbols on which operations are performed by a computer are stored and recorded on magnetic, optical, electronic, or mechanical recording media, and transmitted in the form of digital electrical or optical signals. Data pass in and out of computers via peripheral devices.
Physical computer memory elements consist of an address and a byte/word of data storage. Digital data are often stored in relational databases, like tables or SQL databases, and can generally be represented as abstract key/value pairs. Data can be organized in many different types of data structures, including arrays, graphs, and objects. Data structures can store data of many different types, including numbers, strings and even other data structures.","In computer science, data (treated as singular, plural, or as a mass noun) is any sequence of one or more symbols; datum is a single symbol of data. Data requires interpretation to become information. Digital data is data that is represented using the binary number system of ones (1) and zeros (0), instead of analog representation. In modern (post-1960) computer systems, all data is digital. 
Data exists in three states: data at rest, data in transit and data in use.  Data within a computer, in most cases, moves as parallel data. Data moving to or from a computer, in most cases, moves as serial data. Data sourced from an analog device, such as a temperature sensor, may be converted to digital using an analog-to-digital converter. Data representing quantities, characters, or symbols on which operations are performed by a computer are stored and recorded on magnetic, optical, electronic, or mechanical recording media, and transmitted in the form of digital electrical or optical signals. Data pass in and out of computers via peripheral devices.
Physical computer memory elements consist of an address and a byte/word of data storage. Digital data are often stored in relational databases, like tables or SQL databases, and can generally be represented as abstract key/value pairs. Data can be organized in many different types of data structures, including arrays, graphs, and objects. Data structures can store data of many different types, including numbers, strings and even other data structures.


== Characteristics ==
Metadata helps translate data to information. Metadata is data about the data. Metadata may be implied, specified or given.  
Data relating to physical events or processes will have a temporal component. This temporal component may be implied. This is the case when a device such as a temperature logger receives data from a temperature sensor. When the temperature is received it is assumed that the data has a temporal reference of now. So the device records the date, time and temperature together.  When the data logger communicates temperatures, it must also report the date and time as metadata for each temperature reading.
Fundamentally, computers follow a sequence of instructions they are given in the form of data.  A set of instructions to perform a given task (or tasks) is called a program. A program is data in the form of coded instructions to control the operation of a computer or other machine.  In the nominal case, the program, as executed by the computer, will consist of machine code.  The elements of storage manipulated by the program, but not actually executed by the central processing unit (CPU), are also data. At its most essential, a single datum is a value stored at a specific location. Therefore, it is possible for computer programs to operate on other computer programs, by manipulating their programmatic data.
To store data bytes in a file, they have to be serialized in a file format. Typically, programs are stored in special file types, different from those used for other data.  Executable files contain programs; all other files are also data files.  However, executable files may also contain data used by the program which is built into the program.  In particular, some executable files have a data segment, which nominally contains constants and initial values for variables, both of which can be considered data.
The line between program and data can become blurry.  An interpreter, for example, is a program.  The input data to an interpreter is itself a program, just not one expressed in native machine language.  In many cases, the interpreted program will be a human-readable text file, which is manipulated with a text editor program. Metaprogramming similarly involves programs manipulating other programs as data.  Programs like compilers, linkers, debuggers, program updaters, virus scanners and such use other programs as their data.
For example, a user might first instruct the operating system to load a word processor program from one file, and then use the running program to open and edit a document stored in another file.  In this example, the document would be considered data.  If the word processor also features a spell checker, then the dictionary (word list) for the spell checker would also be considered data.  The algorithms used by the spell checker to suggest corrections would be either machine code data or text in some interpretable programming language.
In an alternate usage, binary files (which are not human-readable) are sometimes called data as distinguished from human-readable text.The total amount of digital data in 2007 was estimated to be 281 billion gigabytes (281 exabytes).


== Data keys and values, structures and persistence ==
Keys in data provide the context for values.  Regardless of the structure of data, there is always a key component present. Keys in data and data-structures are essential for giving meaning to data values. Without a key that is directly or indirectly associated with a value, or collection of values in a structure, the values become meaningless and cease to be data. That is to say, there has to be a key component linked to a value component in order for it to be considered data.Data can be represented in computers in multiple ways, as per the following examples:


=== RAM ===
Random access memory (RAM) holds data that the CPU has direct access to. A CPU may only manipulate data within its processor registers or memory.  This is as opposed to data storage, where the CPU must direct the transfer of data between the storage device (disk, tape...) and memory. RAM is an array of linear contiguous locations that a processor may read or write by providing an address for the read or write operation. The processor may operate on any location in memory at any time in any order. In RAM the smallest element of data is the binary bit.  The capabilities and limitations of accessing RAM are processor specific. In general main memory is arranged as an array of locations beginning at address 0 (hexadecimal 0).   Each location can store usually 8 or 32 bits depending on the computer architecture.


=== Keys ===
Data keys need not be a direct hardware address in memory. Indirect, abstract and logical keys codes can be stored in association with values to form a data structure.  Data structures have predetermined offsets (or links or paths) from the start of the structure, in which data values are stored. Therefore, the data key consists of the key to the structure plus the offset (or links or paths) into the structure. When such a structure is repeated, storing variations of the data values and the data keys within the same repeating structure, the result can be considered to resemble a table, in which each element of the repeating structure is considered to be a column and each repetition of the structure is considered as a row of the table.  In such an organization of data, the data key is usually a value in one (or a composite of the values in several) of the columns.


=== Organised recurring data structures ===
The tabular view of repeating data structures is only one of many possibilities.  Repeating data structures can be organised hierarchically, such that nodes are linked to each other in a cascade of parent-child relationships.  Values and potentially more complex data-structures are linked to the nodes. Thus the nodal hierarchy provides the key for addressing the data structures associated with the nodes. This representation can be thought of as an inverted tree. E.g. modern computer operating system file systems are a common example; and XML is another.


=== Sorted or ordered data ===
Data has some inherent features when it is sorted on a key. All the values for subsets of the key appear together. When passing sequentially through groups of the data with the same key, or a subset of the key changes, this is referred to in data processing circles as a break, or a control break. It particularly facilitates the aggregation of data values on subsets of a key.


=== Peripheral storage ===
Until the advent of bulk non-volatile memory like flash, persistent data storage was traditionally achieved by writing the data to external block devices like magnetic tape and disk drives. These devices typically seek to a location on the magnetic media and then read or write blocks of data of a predetermined size. In this case, the seek location on the media, is the data key and the blocks are the data values. Early used raw disk data file-systems or disc operating systems reserved contiguous blocks on the disc drive for data files. In those systems, the files could be filled up, running out of data space before all the data had been written to them. Thus much unused data space was reserved unproductively to ensure adequate free space for each file. Later file-systems introduced partitions. They reserved blocks of disc data space for partitions and used the allocated blocks more economically, by dynamically assigning blocks of a partition to a file as needed. To achieve this, the file system had to keep track of which blocks were used or unused by data files in a catalog or file allocation table. Though this made better use of the disc data space, it resulted in fragmentation of files across the disc, and a concomitant performance overhead due additional seek time to read the data. Modern file systems reorganize fragmented files dynamically to optimize file access times. Further developments in file systems resulted in virtualization of disc drives i.e. where a logical drive can be defined as partitions from a number of physical drives.


=== Indexed data ===
Retrieving a small subset of data from a much larger set may imply inefficiently searching through the data sequentially.  Indexes are a way to copy out keys and location addresses from data structures in files, tables and data sets, then organize them using inverted tree structures to reduce the time taken to retrieve a subset of the original data. In order to do this, the key of the subset of data to be retrieved must be known before retrieval begins. The most popular indexes are the B-tree and the dynamic hash key indexing methods.  Indexing is overhead for filing and retrieving data. There are other ways of organizing indexes, e.g. sorting the keys and using a binary search algorithm.


=== Abstraction and indirection ===
Object-oriented programming uses two basic concepts for understanding data and software:The taxonomic rank-structure of classes, which is an example of a hierarchical data structure; and
at run time, the creation of references to in-memory data-structures of objects that have been instantiated from a class library.It is only after instantiation that an object of a specified class exists. After an object's reference is cleared, the object also ceases to exist. The memory locations where the object's data was stored are garbage and are reclassified as unused memory available for reuse.


=== Database data ===
The advent of databases introduced a further layer of abstraction for persistent data storage. Databases use metadata, and a structured query language protocol between client and server systems, communicating over a computer network, using a two phase commit logging system to ensure transactional completeness, when saving data.


=== Parallel distributed data processing ===
Modern scalable and high-performance data persistence technologies, such as Apache Hadoop, rely on massively parallel distributed data processing across many commodity computers on a high bandwidth network. In such systems, the data is distributed across multiple computers and therefore any particular computer in the system must be represented in the key of the data, either directly, or indirectly.  This enables the differentiation between two identical sets of data, each being processed on a different computer at the same time.


== See also ==


== References ==",17712,511,"All articles with unsourced statements, Articles with short description, Articles with unsourced statements from August 2021, Computer data, Short description matches Wikidata",1131150222,cs
https://en.wikipedia.org/wiki/Recursion_(computer_science),Recursion (computer science),"In computer science, recursion is a method of solving a computational problem where the solution depends on solutions to smaller instances of the same problem. Recursion solves such recursive problems by using functions that call themselves from within their own code. The approach can be applied to many types of problems, and recursion is one of the central ideas of computer science.
The power of recursion evidently lies in the possibility of defining an infinite set of objects by a finite statement.  In the same manner, an infinite number of computations can be described by a finite recursive program, even if this program contains no explicit repetitions.
Most computer programming languages support recursion by allowing a function to call itself from within its own code. Some functional programming languages (for instance, Clojure) do not define any looping constructs but rely solely on recursion to repeatedly call code. It is proved in computability theory that these recursive-only languages are Turing complete; this means that they are as powerful (they can be used to solve the same problems) as imperative languages based on control structures such as while and for.
Repeatedly calling a function from within itself may cause the call stack to have a size equal to the sum of the input sizes of all involved calls. It follows that, for problems that can be solved easily by iteration, recursion is generally less efficient, and, for large problems, it is fundamental to use optimization techniques such as tail call optimization.","In computer science, recursion is a method of solving a computational problem where the solution depends on solutions to smaller instances of the same problem. Recursion solves such recursive problems by using functions that call themselves from within their own code. The approach can be applied to many types of problems, and recursion is one of the central ideas of computer science.
The power of recursion evidently lies in the possibility of defining an infinite set of objects by a finite statement.  In the same manner, an infinite number of computations can be described by a finite recursive program, even if this program contains no explicit repetitions.
Most computer programming languages support recursion by allowing a function to call itself from within its own code. Some functional programming languages (for instance, Clojure) do not define any looping constructs but rely solely on recursion to repeatedly call code. It is proved in computability theory that these recursive-only languages are Turing complete; this means that they are as powerful (they can be used to solve the same problems) as imperative languages based on control structures such as while and for.
Repeatedly calling a function from within itself may cause the call stack to have a size equal to the sum of the input sizes of all involved calls. It follows that, for problems that can be solved easily by iteration, recursion is generally less efficient, and, for large problems, it is fundamental to use optimization techniques such as tail call optimization.


== Recursive functions and algorithms ==
A common algorithm design tactic is to divide a problem into sub-problems of the same type as the original, solve those sub-problems, and combine the results. This is often referred to as the divide-and-conquer method; when combined with a lookup table that stores the results of previously solved sub-problems (to avoid solving them repeatedly and incurring extra computation time), it can be referred to as dynamic programming or memoization.


=== Base case ===
A recursive function definition has one or more base cases, meaning input(s) for which the function produces a result trivially (without recurring), and one or more recursive cases, meaning input(s) for which the program recurs (calls itself).  For example, the factorial function can be defined recursively by the equations 0! = 1 and, for all n > 0, n! = n(n − 1)!. Neither equation by itself constitutes a complete definition; the first is the base case, and the second is the recursive case. Because the base case breaks the chain of recursion, it is sometimes also called the ""terminating case"".
The job of the recursive cases can be seen as breaking down complex inputs into simpler ones.  In a properly designed recursive function, with each recursive call, the input problem must be simplified in such a way that eventually the base case must be reached.  (Functions that are not intended to terminate under normal circumstances—for example, some system and server processes—are an exception to this.)  Neglecting to write a base case, or testing for it incorrectly, can cause an infinite loop.
For some functions (such as one that computes the series for e = 1/0! + 1/1! + 1/2! + 1/3! + ...) there is not an obvious base case implied by the input data; for these one may add a parameter (such as the number of terms to be added, in our series example) to provide a 'stopping criterion' that establishes the base case. Such an example is more naturally treated by corecursion, where successive terms in the output are the partial sums; this can be converted to a recursion by using the indexing parameter to say ""compute the nth term (nth partial sum)"".


== Recursive data types ==
Many computer programs must process or generate an arbitrarily large quantity of data.  Recursion is a technique for representing data whose exact size is unknown to the programmer: the programmer can specify this data with a self-referential definition.  There are two types of self-referential definitions: inductive and coinductive definitions.


=== Inductively defined data ===

An inductively defined recursive data definition is one that specifies how to construct instances of the data.  For example, linked lists can be defined inductively (here, using Haskell syntax):

The code above specifies a list of strings to be either empty, or a structure that contains a string and a list of strings.  The self-reference in the definition permits the construction of lists of any (finite) number of strings.
Another example of inductive definition is the natural numbers (or positive integers):

A natural number is either 1 or n+1, where n is a natural number.
Similarly recursive definitions are often used to model the structure of expressions and statements in programming languages.  Language designers often express grammars in a syntax such as Backus–Naur form; here is such a grammar, for a simple language of arithmetic expressions with multiplication and addition:

This says that an expression is either a number, a product of two expressions, or a sum of two expressions.  By recursively referring to expressions in the second and third lines, the grammar permits arbitrarily complicated arithmetic expressions such as (5 * ((3 * 6) + 8)), with more than one product or sum operation in a single expression.


=== Coinductively defined data and corecursion ===

A coinductive data definition is one that specifies the operations that may be performed on a piece of data; typically, self-referential coinductive definitions are used for data structures of infinite size.
A coinductive definition of infinite streams of strings, given informally, might look like this:

A stream of strings is an object s such that:
 head(s) is a string, and
 tail(s) is a stream of strings.

This is very similar to an inductive definition of lists of strings; the difference is that this definition specifies how to access the contents of the data structure—namely, via the accessor functions head and tail—and what those contents may be, whereas the inductive definition specifies how to create the structure and what it may be created from.
Corecursion is related to coinduction, and can be used to compute particular instances of (possibly) infinite objects.  As a programming technique, it is used most often in the context of lazy programming languages, and can be preferable to recursion when the desired size or precision of a program's output is unknown.  In such cases the program requires both a definition for an infinitely large (or infinitely precise) result, and a mechanism for taking a finite portion of that result.  The problem of computing the first n prime numbers is one that can be solved with a corecursive program (e.g. here).


== Types of recursion ==


=== Single recursion and multiple recursion ===
Recursion that contains only a single self-reference is known as single recursion, while recursion that contains multiple self-references is known as multiple recursion. Standard examples of single recursion include list traversal, such as in a linear search, or computing the factorial function, while standard examples of multiple recursion include tree traversal, such as in a depth-first search.
Single recursion is often much more efficient than multiple recursion, and can generally be replaced by an iterative computation, running in linear time and requiring constant space. Multiple recursion, by contrast, may require exponential time and space, and is more fundamentally recursive, not being able to be replaced by iteration without an explicit stack.
Multiple recursion can sometimes be converted to single recursion (and, if desired, thence to iteration). For example, while computing the Fibonacci sequence naively entails multiple iteration, as each value requires two previous values, it can be computed by single recursion by passing two successive values as parameters. This is more naturally framed as corecursion, building up from the initial values, while tracking two successive values at each step – see corecursion: examples. A more sophisticated example involves using a threaded binary tree, which allows iterative tree traversal, rather than multiple recursion.


=== Indirect recursion ===

Most basic examples of recursion, and most of the examples presented here, demonstrate direct recursion, in which a function calls itself.  Indirect recursion occurs when a function is called not by itself but by another function that it called (either directly or indirectly). For example, if f calls f, that is direct recursion, but if f calls g which calls f, then that is indirect recursion of f. Chains of three or more functions are possible; for example, function 1 calls function 2, function 2 calls function 3, and function 3 calls function 1 again.
Indirect recursion is also called mutual recursion, which is a more symmetric term, though this is simply a difference of emphasis, not a different notion. That is, if f calls g and then g calls f, which in turn calls g again, from the point of view of f alone, f is indirectly recursing, while from the point of view of g alone, it is indirectly recursing, while from the point of view of both, f and g are mutually recursing on each other. Similarly a set of three or more functions that call each other can be called a set of mutually recursive functions.


=== Anonymous recursion ===

Recursion is usually done by explicitly calling a function by name. However, recursion can also be done via implicitly calling a function based on the current context, which is particularly useful for anonymous functions, and is known as anonymous recursion.


=== Structural versus generative recursion ===

Some authors classify recursion as either ""structural"" or ""generative"".  The distinction is related to where a recursive procedure gets the data that it works on, and how it processes that data:

[Functions that consume structured data] typically decompose their arguments into their immediate structural components and then process those components. If one of the immediate components belongs to the same class of data as the input, the function is recursive. For that reason, we refer to these functions as (STRUCTURALLY) RECURSIVE  FUNCTIONS.
Thus, the defining characteristic of a structurally recursive function is that the argument to each recursive call is the content of a field of the original input.  Structural recursion includes nearly all tree traversals, including XML processing, binary tree creation and search, etc. By considering the algebraic structure of the natural numbers (that is, a natural number is either zero or the successor of a natural number), functions such as factorial may also be regarded as structural recursion.
Generative recursion is the alternative:

Many well-known recursive algorithms generate an entirely new piece of data from the given data and recur on it.  HtDP (How to Design Programs) refers to this kind as generative recursion.  Examples of generative recursion include: gcd, quicksort, binary search, mergesort, Newton's method, fractals, and adaptive integration.
This distinction is important in proving termination of a function.

All structurally recursive functions on finite (inductively defined) data structures can easily be shown to terminate, via structural induction: intuitively, each recursive call receives a smaller piece of input data, until a base case is reached.
Generatively recursive functions, in contrast, do not necessarily feed smaller input to their recursive calls, so proof of their termination is not necessarily as simple, and avoiding infinite loops requires greater care. These generatively recursive functions can often be interpreted as corecursive functions – each step generates the new data, such as successive approximation in Newton's method – and terminating this corecursion requires that the data eventually satisfy some condition, which is not necessarily guaranteed.
In terms of loop variants, structural recursion is when there is an obvious loop variant, namely size or complexity, which starts off finite and decreases at each recursive step.
By contrast, generative recursion is when there is not such an obvious loop variant, and termination depends on a function, such as ""error of approximation"" that does not necessarily decrease to zero, and thus termination is not guaranteed without further analysis.


== Implementation issues ==
In actual implementation, rather than a pure recursive function (single check for base case, otherwise recursive step), a number of modifications may be made, for purposes of clarity or efficiency. These include:

Wrapper function (at top)
Short-circuiting the base case, aka ""Arm's-length recursion"" (at bottom)
Hybrid algorithm (at bottom) – switching to a different algorithm once data is small enoughOn the basis of elegance, wrapper functions are generally approved, while short-circuiting the base case is frowned upon, particularly in academia. Hybrid algorithms are often used for efficiency, to reduce the overhead of recursion in small cases, and arm's-length recursion is a special case of this.


=== Wrapper function ===
A wrapper function is a function that is directly called but does not recurse itself, instead calling a separate auxiliary function which actually does the recursion.
Wrapper functions can be used to validate parameters (so the recursive function can skip these), perform initialization (allocate memory, initialize variables), particularly for auxiliary variables such as ""level of recursion"" or partial computations for memoization, and handle exceptions and errors. In languages that support nested functions, the auxiliary function can be nested inside the wrapper function and use a shared scope. In the absence of nested functions, auxiliary functions are instead a separate function, if possible private (as they are not called directly), and information is shared with the wrapper function by using pass-by-reference.


=== Short-circuiting the base case ===

Short-circuiting the base case, also known as arm's-length recursion, consists of checking the base case before making a recursive call – i.e., checking if the next call will be the base case, instead of calling and then checking for the base case. Short-circuiting is particularly done for efficiency reasons, to avoid the overhead of a function call that immediately returns. Note that since the base case has already been checked for (immediately before the recursive step), it does not need to be checked for separately, but one does need to use a wrapper function for the case when the overall recursion starts with the base case itself. For example, in the factorial function, properly the base case is 0! = 1, while immediately returning 1 for 1! is a short circuit, and may miss 0; this can be mitigated by a wrapper function. The box shows C code to shortcut factorial cases 0 and 1.
Short-circuiting is primarily a concern when many base cases are encountered, such as Null pointers in a tree, which can be linear in the number of function calls, hence significant savings for O(n) algorithms; this is illustrated below for a depth-first search. Short-circuiting on a tree corresponds to considering a leaf (non-empty node with no children) as the base case, rather than considering an empty node as the base case. If there is only a single base case, such as in computing the factorial, short-circuiting provides only O(1) savings.
Conceptually, short-circuiting can be considered to either have the same base case and recursive step, checking the base case only before the recursion, or it can be considered to have a different base case (one step removed from standard base case) and a more complex recursive step, namely ""check valid then recurse"", as in considering leaf nodes rather than Null nodes as base cases in a tree. Because short-circuiting has a more complicated flow, compared with the clear separation of base case and recursive step in standard recursion, it is often considered poor style, particularly in academia.


==== Depth-first search ====
A basic example of short-circuiting is given in depth-first search (DFS) of a binary tree; see binary trees section for standard recursive discussion.
The standard recursive algorithm for a DFS is:

base case: If current node is Null, return false
recursive step: otherwise, check value of current node, return true if match, otherwise recurse on childrenIn short-circuiting, this is instead:

check value of current node, return true if match,
otherwise, on children, if not Null, then recurse.In terms of the standard steps, this moves the base case check before the recursive step. Alternatively, these can be considered a different form of base case and recursive step, respectively. Note that this requires a wrapper function to handle the case when the tree itself is empty (root node is Null).
In the case of a perfect binary tree of height h, there are 2h+1−1 nodes and 2h+1 Null pointers as children (2 for each of the 2h leaves), so short-circuiting cuts the number of function calls in half in the worst case.
In C, the standard recursive algorithm may be implemented as:

The short-circuited algorithm may be implemented as:

Note the use of short-circuit evaluation of the Boolean && (AND) operators, so that the recursive call is made only if the node is valid (non-Null). Note that while the first term in the AND is a pointer to a node, the second term is a boolean, so the overall expression evaluates to a boolean. This is a common idiom in recursive short-circuiting. This is in addition to the short-circuit evaluation of the Boolean || (OR) operator, to only check the right child if the left child fails. In fact, the entire control flow of these functions can be replaced with a single Boolean expression in a return statement, but legibility suffers at no benefit to efficiency.


=== Hybrid algorithm ===
Recursive algorithms are often inefficient for small data, due to the overhead of repeated function calls and returns. For this reason efficient implementations of recursive algorithms often start with the recursive algorithm, but then switch to a different algorithm when the input becomes small. An important example is merge sort, which is often implemented by switching to the non-recursive insertion sort when the data is sufficiently small, as in the tiled merge sort. Hybrid recursive algorithms can often be further refined, as in Timsort, derived from a hybrid merge sort/insertion sort.


== Recursion versus iteration ==
Recursion and iteration are equally expressive: recursion can be replaced by iteration with an explicit call stack, while iteration can be replaced with tail recursion. Which approach is preferable depends on the problem under consideration and the language used. In imperative programming, iteration is preferred, particularly for simple recursion, as it avoids the overhead of function calls and call stack management, but recursion is generally used for multiple recursion. By contrast, in functional languages recursion is preferred, with tail recursion optimization leading to little overhead. Implementing an algorithm using iteration may not be easily achievable.
Compare the templates to compute xn defined by xn = f(n, xn-1) from xbase:

For an imperative language the overhead is to define the function, and for a functional language the overhead is to define the accumulator variable x.
For example, a factorial function may be implemented iteratively in C by assigning to a loop index variable and accumulator variable, rather than by passing arguments and returning values by recursion:


=== Expressive power ===
Most programming languages in use today allow the direct specification of recursive functions and procedures. When such a function is called, the program's runtime environment keeps track of the various instances of the function (often using a call stack, although other methods may be used). Every recursive function can be transformed into an iterative function by replacing recursive calls with iterative control constructs and simulating the call stack with a stack explicitly managed by the program.Conversely, all iterative functions and procedures that can be evaluated by a computer (see Turing completeness) can be expressed in terms of recursive functions; iterative control constructs such as while loops and for loops are routinely rewritten in recursive form in functional languages. However, in practice this rewriting depends on tail call elimination, which is not a feature of all languages. C, Java, and Python are notable mainstream languages in which all function calls, including tail calls, may cause stack allocation that would not occur with the use of looping constructs; in these languages, a working iterative program rewritten in recursive form may overflow the call stack, although tail call elimination may be a feature that is not covered by a language's specification, and different implementations of the same language may differ in tail call elimination capabilities.


=== Performance issues ===
In languages (such as C and Java) that favor iterative looping constructs, there is usually significant time and space cost associated with recursive programs, due to the overhead required to manage the stack and the relative slowness of function calls; in functional languages, a function call (particularly a tail call) is typically a very fast operation, and the difference is usually less noticeable.
As a concrete example, the difference in performance between recursive and iterative implementations of the ""factorial"" example above depends highly on the compiler used. In languages where looping constructs are preferred, the iterative version may be as much as several orders of magnitude faster than the recursive one. In functional languages, the overall time difference of the two implementations may be negligible; in fact, the cost of multiplying the larger numbers first rather than the smaller numbers (which the iterative version given here happens to do) may overwhelm any time saved by choosing iteration.


=== Stack space ===
In some programming languages, the maximum size of the call stack is much less than the space available in the heap, and recursive algorithms tend to require more stack space than iterative algorithms. Consequently, these languages sometimes place a limit on the depth of recursion to avoid stack overflows; Python is one such language. Note the caveat below regarding the special case of tail recursion.


=== Vulnerability ===
Because recursive algorithms can be subject to stack overflows, they may be vulnerable to pathological or malicious input. Some malware specifically targets a program's call stack and takes advantage of the stack's inherently recursive nature. Even in the absence of malware, a stack overflow caused by unbounded recursion can be fatal to the program, and exception handling logic may not prevent the corresponding process from being terminated.


=== Multiply recursive problems ===
Multiply recursive problems are inherently recursive, because of prior state they need to track. One example is tree traversal as in depth-first search; though both recursive and iterative methods are used, they contrast with list traversal and linear search in a list, which is a singly recursive and thus naturally iterative method. Other examples include divide-and-conquer algorithms such as Quicksort, and functions such as the Ackermann function. All of these algorithms can be implemented iteratively with the help of an explicit stack, but the programmer effort involved in managing the stack, and the complexity of the resulting program, arguably outweigh any advantages of the iterative solution.


=== Refactoring recursion ===
Recursive algorithms can be replaced with non-recursive counterparts. One method for replacing recursive algorithms is to simulate them using heap memory in place of stack memory. An alternative is to develop a replacement algorithm entirely based on non-recursive methods, which can be challenging. For example, recursive algorithms for matching wildcards, such as Rich Salz' wildmat algorithm, were once typical. Non-recursive algorithms for the same purpose, such as the Krauss matching wildcards algorithm, have been developed to avoid the drawbacks of recursion and have improved only gradually based on techniques such as collecting tests and profiling performance.


== Tail-recursive functions ==
Tail-recursive functions are functions in which all recursive calls are tail calls and hence do not build up any deferred operations. For example, the gcd function (shown again below) is tail-recursive.  In contrast, the factorial function (also below) is not tail-recursive; because its recursive call is not in tail position, it builds up deferred multiplication operations that must be performed after the final recursive call completes.  With a compiler or interpreter that treats tail-recursive calls as jumps rather than function calls, a tail-recursive function such as gcd will execute using constant space.  Thus the program is essentially iterative, equivalent to using imperative language control structures like the ""for"" and ""while"" loops.

The significance of tail recursion is that when making a tail-recursive call (or any tail call), the caller's return position need not be saved on the call stack; when the recursive call returns, it will branch directly on the previously saved return position. Therefore, in languages that recognize this property of tail calls, tail recursion saves both space and time.


== Order of execution ==
Consider these two functions:


=== Function 1 ===


=== Function 2 ===

Function 2 is function 1 with the lines swapped.
In the case of a function calling itself only once, instructions placed before the recursive call are executed once per recursion before any of the instructions placed after the recursive call. The latter are executed repeatedly after the maximum recursion has been reached. 
Also note that the order of the print statements is reversed, which is due to the way the functions and statements are stored on the call stack.


== Recursive procedures ==


=== Factorial ===
A classic example of a recursive procedure is the function used to calculate the factorial of a natural number:

  
    
      
        fact
        ⁡
        (
        n
        )
        =
        
          
            {
            
              
                
                  1
                
                
                  
                    
                      if 
                    
                  
                  n
                  =
                  0
                
              
              
                
                  n
                  ⋅
                  fact
                  ⁡
                  (
                  n
                  −
                  1
                  )
                
                
                  
                    
                      if 
                    
                  
                  n
                  >
                  0
                
              
            
            
          
        
      
    
    {\displaystyle \operatorname {fact} (n)={\begin{cases}1&{\mbox{if }}n=0\\n\cdot \operatorname {fact} (n-1)&{\mbox{if }}n>0\\\end{cases}}}
  The function can also be written as a recurrence relation:

  
    
      
        
          b
          
            n
          
        
        =
        n
        
          b
          
            n
            −
            1
          
        
      
    
    {\displaystyle b_{n}=nb_{n-1}}
  

  
    
      
        
          b
          
            0
          
        
        =
        1
      
    
    {\displaystyle b_{0}=1}
  This evaluation of the recurrence relation demonstrates the computation that would be performed in evaluating the pseudocode above:

This factorial function can also be described without using recursion by making use of the typical looping constructs found in imperative programming languages:

The imperative code above is equivalent to this mathematical definition using an accumulator variable t:

  
    
      
        
          
            
              
                fact
                ⁡
                (
                n
                )
              
              
                
                =
                
                  f
                  a
                  c
                  
                    t
                    
                      a
                      c
                      c
                    
                  
                
                ⁡
                (
                n
                ,
                1
                )
              
            
            
              
                
                  f
                  a
                  c
                  
                    t
                    
                      a
                      c
                      c
                    
                  
                
                ⁡
                (
                n
                ,
                t
                )
              
              
                
                =
                
                  
                    {
                    
                      
                        
                          t
                        
                        
                          
                            
                              if 
                            
                          
                          n
                          =
                          0
                        
                      
                      
                        
                          
                            f
                            a
                            c
                            
                              t
                              
                                a
                                c
                                c
                              
                            
                          
                          ⁡
                          (
                          n
                          −
                          1
                          ,
                          n
                          t
                          )
                        
                        
                          
                            
                              if 
                            
                          
                          n
                          >
                          0
                        
                      
                    
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\operatorname {fact} (n)&=\operatorname {fact_{acc}} (n,1)\\\operatorname {fact_{acc}} (n,t)&={\begin{cases}t&{\mbox{if }}n=0\\\operatorname {fact_{acc}} (n-1,nt)&{\mbox{if }}n>0\\\end{cases}}\end{aligned}}}
  The definition above translates straightforwardly to functional programming languages such as Scheme; this is an example of iteration implemented recursively.


=== Greatest common divisor ===
The Euclidean algorithm, which computes the greatest common divisor of two integers, can be written recursively.
Function definition:

  
    
      
        gcd
        (
        x
        ,
        y
        )
        =
        
          
            {
            
              
                
                  x
                
                
                  
                    
                      if 
                    
                  
                  y
                  =
                  0
                
              
              
                
                  gcd
                  (
                  y
                  ,
                  remainder
                  ⁡
                  (
                  x
                  ,
                  y
                  )
                  )
                
                
                  
                    
                      if 
                    
                  
                  y
                  >
                  0
                
              
            
            
          
        
      
    
    {\displaystyle \gcd(x,y)={\begin{cases}x&{\mbox{if }}y=0\\\gcd(y,\operatorname {remainder} (x,y))&{\mbox{if }}y>0\\\end{cases}}}
  Recurrence relation for greatest common divisor, where 
  
    
      
        x
        %
        y
      
    
    {\displaystyle x\%y}
   expresses the remainder of 
  
    
      
        x
        
          /
        
        y
      
    
    {\displaystyle x/y}
  :

  
    
      
        gcd
        (
        x
        ,
        y
        )
        =
        gcd
        (
        y
        ,
        x
        %
        y
        )
      
    
    {\displaystyle \gcd(x,y)=\gcd(y,x\%y)}
   if 
  
    
      
        y
        ≠
        0
      
    
    {\displaystyle y\neq 0}
  

  
    
      
        gcd
        (
        x
        ,
        0
        )
        =
        x
      
    
    {\displaystyle \gcd(x,0)=x}
  The recursive program above is tail-recursive; it is equivalent to an iterative algorithm, and the computation shown above shows the steps of evaluation that would be performed by a language that eliminates tail calls.  Below is a version of the same algorithm using explicit iteration, suitable for a language that does not eliminate tail calls.  By maintaining its state entirely in the variables x and y and using a looping construct, the program avoids making recursive calls and growing the call stack.

The iterative algorithm requires a temporary variable, and even given knowledge of the Euclidean algorithm it is more difficult to understand the process by simple inspection, although the two algorithms are very similar in their steps.


=== Towers of Hanoi ===

The Towers of Hanoi is a mathematical puzzle whose solution illustrates recursion. There are three pegs which can hold stacks of disks of different diameters. A larger disk may never be stacked on top of a smaller. Starting with n disks on one peg, they must be moved to another peg one at a time. What is the smallest number of steps to move the stack?
Function definition:

  
    
      
        hanoi
        ⁡
        (
        n
        )
        =
        
          
            {
            
              
                
                  1
                
                
                  
                    
                      if 
                    
                  
                  n
                  =
                  1
                
              
              
                
                  2
                  ⋅
                  hanoi
                  ⁡
                  (
                  n
                  −
                  1
                  )
                  +
                  1
                
                
                  
                    
                      if 
                    
                  
                  n
                  >
                  1
                
              
            
            
          
        
      
    
    {\displaystyle \operatorname {hanoi} (n)={\begin{cases}1&{\mbox{if }}n=1\\2\cdot \operatorname {hanoi} (n-1)+1&{\mbox{if }}n>1\\\end{cases}}}
  Recurrence relation for hanoi:

  
    
      
        
          h
          
            n
          
        
        =
        2
        
          h
          
            n
            −
            1
          
        
        +
        1
      
    
    {\displaystyle h_{n}=2h_{n-1}+1}
  

  
    
      
        
          h
          
            1
          
        
        =
        1
      
    
    {\displaystyle h_{1}=1}
  Example implementations:

Although not all recursive functions have an explicit solution, the Tower of Hanoi sequence can be reduced to an explicit formula.


=== Binary search ===
The binary search algorithm is a method of searching a sorted array for a single element by cutting the array in half with each recursive pass.  The trick is to pick a midpoint near the center of the array, compare the data at that point with the data being searched and then responding to one of three possible conditions: the data is found at the midpoint, the data at the midpoint is greater than the data being searched for, or the data at the midpoint is less than the data being searched for.
Recursion is used in this algorithm because with each pass a new array is created by cutting the old one in half.  The binary search procedure is then called recursively, this time on the new (and smaller) array.  Typically the array's size is adjusted by manipulating a beginning and ending index.  The algorithm exhibits a logarithmic order of growth because it essentially divides the problem domain in half with each pass.
Example implementation of binary search in C:


== Recursive data structures (structural recursion) ==

An important application of recursion in computer science is in defining dynamic data structures such as lists and trees.  Recursive data structures can dynamically grow to a theoretically infinite size in response to runtime requirements; in contrast, the size of a static array must be set at compile time.

""Recursive algorithms are particularly appropriate when the underlying problem or the data to be treated are defined in recursive terms.""

The examples in this section illustrate what is known as ""structural recursion"".  This term refers to the fact that the recursive procedures are acting on data that is defined recursively.

As long as a programmer derives the template from a data definition, functions employ structural recursion. That is, the recursions in a function's body consume some immediate piece of a given compound value.


=== Linked lists ===

Below is a C definition of a linked list node structure.  Notice especially how the node is defined in terms of itself.  The ""next"" element of struct node is a pointer to another struct node, effectively creating a list type.

Because the struct node data structure is defined recursively, procedures that operate on it can be implemented naturally as recursive procedures.  The list_print procedure defined below walks down the list until the list is empty (i.e., the list pointer has a value of NULL).  For each node it prints the data element (an integer).  In the C implementation, the list remains unchanged by the list_print procedure.


=== Binary trees ===

Below is a simple definition for a binary tree node.  Like the node for linked lists, it is defined in terms of itself, recursively.  There are two self-referential pointers: left (pointing to the left sub-tree) and right (pointing to the right sub-tree).

Operations on the tree can be implemented using recursion.  Note that because there are two self-referencing pointers (left and right), tree operations may require two recursive calls:

At most two recursive calls will be made for any given call to tree_contains as defined above.

The above example illustrates an in-order traversal of the binary tree.  A Binary search tree is a special case of the binary tree where the data elements of each node are in order.


=== Filesystem traversal ===
Since the number of files in a filesystem may vary, recursion is the only practical way to traverse and thus enumerate its contents.  Traversing a filesystem is very similar to that of tree traversal, therefore the concepts behind tree traversal are applicable to traversing a filesystem.  More specifically, the code below would be an example of a preorder traversal of a filesystem.

This code is both recursion and iteration - the files and directories are iterated, and each directory is opened recursively.
The ""rtraverse"" method is an example of direct recursion, whilst the ""traverse"" method is a wrapper function.
The ""base case"" scenario is that there will always be a fixed number of files and/or directories in a given filesystem.


== Time-efficiency of recursive algorithms ==
The time efficiency of recursive algorithms can be expressed in a recurrence relation of Big O notation.  They can (usually) then be simplified into a single Big-O term.


=== Shortcut rule (master theorem) ===

If the time-complexity of the function is in the form

Then the Big O of the time-complexity is thus:

If 
  
    
      
        f
        (
        n
        )
        =
        O
        (
        
          n
          
            
              log
              
                b
              
            
            ⁡
            a
            −
            ε
          
        
        )
      
    
    {\displaystyle f(n)=O(n^{\log _{b}a-\varepsilon })}
   for some constant 
  
    
      
        ε
        >
        0
      
    
    {\displaystyle \varepsilon >0}
  , then 
  
    
      
        T
        (
        n
        )
        =
        Θ
        (
        
          n
          
            
              log
              
                b
              
            
            ⁡
            a
          
        
        )
      
    
    {\displaystyle T(n)=\Theta (n^{\log _{b}a})}
  
If 
  
    
      
        f
        (
        n
        )
        =
        Θ
        (
        
          n
          
            
              log
              
                b
              
            
            ⁡
            a
          
        
        )
      
    
    {\displaystyle f(n)=\Theta (n^{\log _{b}a})}
  , then 
  
    
      
        T
        (
        n
        )
        =
        Θ
        (
        
          n
          
            
              log
              
                b
              
            
            ⁡
            a
          
        
        log
        ⁡
        n
        )
      
    
    {\displaystyle T(n)=\Theta (n^{\log _{b}a}\log n)}
  
If 
  
    
      
        f
        (
        n
        )
        =
        Ω
        (
        
          n
          
            
              log
              
                b
              
            
            ⁡
            a
            +
            ε
          
        
        )
      
    
    {\displaystyle f(n)=\Omega (n^{\log _{b}a+\varepsilon })}
   for some constant 
  
    
      
        ε
        >
        0
      
    
    {\displaystyle \varepsilon >0}
  , and if 
  
    
      
        a
        ⋅
        f
        (
        n
        
          /
        
        b
        )
        ≤
        c
        ⋅
        f
        (
        n
        )
      
    
    {\displaystyle a\cdot f(n/b)\leq c\cdot f(n)}
   for some constant c < 1 and all sufficiently large n, then 
  
    
      
        T
        (
        n
        )
        =
        Θ
        (
        f
        (
        n
        )
        )
      
    
    {\displaystyle T(n)=\Theta (f(n))}
  where a represents the number of recursive calls at each level of recursion, b represents by what factor smaller the input is for the next level of recursion (i.e. the number of pieces you divide the problem into), and f(n) represents the work that the function does independently of any recursion (e.g. partitioning, recombining) at each level of recursion.


== See also ==
Functional programming
Computational problem
Hierarchical and recursive queries in SQL
Kleene–Rosser paradox
Open recursion
Recursion
Sierpiński curve
McCarthy 91 function
μ-recursive functions
Primitive recursive functions
Tak (function)


== Notes ==


== References ==",2120665,1012,"All articles with unsourced statements, Articles with example pseudocode, Articles with short description, Articles with unsourced statements from November 2019, CS1 location test, Computability theory, Pages that use a deprecated format of the math tags, Programming idioms, Recursion, Short description is different from Wikidata, Subroutines, Theoretical computer science, Use dmy dates from March 2020, Wikipedia articles needing clarification from September 2020",1123455633,cs
https://en.wikipedia.org/wiki/State_(computer_science),State (computer science),"In information technology and computer science, a system is described as stateful if it is designed to remember preceding events or user interactions; the remembered information is called the state of the system.
The set of states a system can occupy is known as its state space. In a discrete system, the state space is countable and often finite. The system's internal behaviour or interaction with its environment consists of separately occurring individual actions or events, such as accepting input or producing output, that may or may not cause the system to change its state. Examples of such systems are digital logic circuits and components, automata and formal language, computer programs, and computers.  
The output of a digital circuit or deterministic computer program at any time is completely determined by its current inputs and its state.

","In information technology and computer science, a system is described as stateful if it is designed to remember preceding events or user interactions; the remembered information is called the state of the system.
The set of states a system can occupy is known as its state space. In a discrete system, the state space is countable and often finite. The system's internal behaviour or interaction with its environment consists of separately occurring individual actions or events, such as accepting input or producing output, that may or may not cause the system to change its state. Examples of such systems are digital logic circuits and components, automata and formal language, computer programs, and computers.  
The output of a digital circuit or deterministic computer program at any time is completely determined by its current inputs and its state.


== Digital logic circuit state ==
Digital logic circuits can be divided into two types: combinational logic, whose output signals are dependent only on its present input signals, and sequential logic, whose outputs are a function of both the current inputs and the past history of inputs. In sequential logic, information from past inputs is stored in electronic memory elements, such as flip-flops. The stored contents of these memory elements, at a given point in time, is collectively referred to as the circuit's state and contains all the information about the past to which the circuit has access.Since each binary memory element, such as a flip-flop, has only two possible states, one or zero, and there is a finite number of memory elements, a digital circuit has only a certain finite number of possible states. If N is the number of binary memory elements in the circuit, the maximum number of states a circuit can have is 2N.


== Program state ==
Similarly, a computer program stores data in variables, which represent storage locations in the computer's memory. The contents of these memory locations, at any given point in the program's execution, is called the program's state.A more specialized definition of state is used for computer programs that operate serially or sequentially on streams of data, such as parsers, firewalls, communication protocols and encryption. Serial programs operate on the incoming data characters or packets sequentially, one at a time. In some of these programs, information about previous data characters or packets received is stored in variables and used to affect the processing of the current character or packet. This is called a stateful protocol and the data carried over from the previous processing cycle is called the state. In others, the program has no information about the previous data stream and starts fresh with each data input; this is called a stateless protocol.
Imperative programming is a programming paradigm (way of designing a programming language) that describes computation in terms of the program state, and of the statements which change the program state. Changes of state are implicit, managed by the program runtime, so that a subroutine has visibility of the changes of state made by other parts of the program, known as side effects.
In declarative programming languages, the program describes the desired results and doesn't specify changes to the state directly.
In functional programming, state is usually represented with temporal logic as explicit variables that represent the program state at each step of a program execution: a state variable is passed as an input parameter of a state-transforming function, which returns the updated state as part of its return value. A pure functional subroutine only has visibility of changes of state represented by the state variables in its scope.


== Finite state machines ==
The output of a sequential circuit or computer program at any time is completely determined by its current inputs and current state. Since each binary memory element has only two possible states, 0 or 1, the total number of different states a circuit can assume is finite, and fixed by the number of memory elements. If there are N binary memory elements, a digital circuit can have at most 2N distinct states. The concept of state is formalized in an abstract mathematical model of computation called a finite state machine, used to design both sequential digital circuits and computer programs.


== Examples ==
An example of an everyday device that has a state is a television set. To change the channel of a TV, the user usually presses a ""channel up"" or ""channel down"" button on the remote control, which sends a coded message to the set. In order to calculate the new channel that the user desires, the digital tuner in the television must have stored in it the number of the current channel it is on. It then adds one or subtracts one from this number to get the number for the new channel, and adjusts the TV to receive that channel. This new number is then stored as the current channel. Similarly, the television also stores a number that controls the level of volume produced by the speaker. Pressing the ""volume up"" or ""volume down"" buttons increments or decrements this number, setting a new level of volume. Both the current channel and current volume numbers are part of the TV's state. They are stored in non-volatile memory, which preserves the information when the TV is turned off, so when it is turned on again the TV will return to its previous station and volume level.
As another example, the state of a microprocessor is the contents of all the memory elements in it: the accumulators, storage registers, data caches, and flags. When computers such as laptops go into a hibernation mode to save energy by shutting down the processor, the state of the processor is stored on the computer's hard disk, so it can be restored when the computer comes out of hibernation, and the processor can take up operations where it left off.


== See also ==
Data (computing)


== References ==",615530,193,"Articles with short description, Cognition, Models of computation, Short description is different from Wikidata",1120702470,cs
https://en.wikipedia.org/wiki/Garbage_collection_(computer_science),Garbage collection (computer science),"In computer science, garbage collection (GC) is a form of automatic memory management. The garbage collector attempts to reclaim memory which was allocated by the program, but is no longer referenced; such memory is called garbage. Garbage collection was invented by American computer scientist John McCarthy around 1959 to simplify manual memory management in Lisp.Garbage collection relieves the programmer from doing manual memory management, where the programmer specifies what objects to de-allocate and return to the memory system and when to do so. Other, similar techniques include stack allocation, region inference, and memory ownership, and combinations thereof. Garbage collection may take a significant proportion of a program's total processing time, and affect performance as a result.
Resources other than memory, such as network sockets, database handles, windows, file descriptors, and device descriptors, are not typically handled by garbage collection, but rather by other methods (e.g. destructors). Some such methods de-allocate memory as well.","In computer science, garbage collection (GC) is a form of automatic memory management. The garbage collector attempts to reclaim memory which was allocated by the program, but is no longer referenced; such memory is called garbage. Garbage collection was invented by American computer scientist John McCarthy around 1959 to simplify manual memory management in Lisp.Garbage collection relieves the programmer from doing manual memory management, where the programmer specifies what objects to de-allocate and return to the memory system and when to do so. Other, similar techniques include stack allocation, region inference, and memory ownership, and combinations thereof. Garbage collection may take a significant proportion of a program's total processing time, and affect performance as a result.
Resources other than memory, such as network sockets, database handles, windows, file descriptors, and device descriptors, are not typically handled by garbage collection, but rather by other methods (e.g. destructors). Some such methods de-allocate memory as well.


== Overview ==
Many programming languages require garbage collection, either as part of the language specification (e.g., RPL, Java, C#, D, Go, and most scripting languages) or effectively for practical implementation (e.g., formal languages like lambda calculus). These are said to be garbage-collected languages. Other languages, such as C and C++, were designed for use with manual memory management, but have garbage-collected implementations available. Some languages, like Ada, Modula-3, and C++/CLI, allow both garbage collection and manual memory management to co-exist in the same application by using separate heaps for collected and manually managed objects. Still others, like D, are garbage-collected but allow the user to manually delete objects or even disable garbage collection entirely when speed is required.
Although many languages integrate GC into their compiler and runtime system, post-hoc GC systems also exist, such as Automatic Reference Counting (ARC). Some of these post-hoc GC systems do not require recompilation. Post-hoc GC is sometimes called litter collection, to distinguish it from ordinary GC.


=== Advantages ===
GC frees the programmer from manually de-allocating memory. This helps avoid some kinds of errors:

Dangling pointers, which occur when a piece of memory is freed while there are still pointers to it, and one of those pointers is dereferenced. By then the memory may have been reassigned to another use, with unpredictable results.
Double free bugs, which occur when the program tries to free a region of memory that has already been freed, and perhaps already been allocated again.
Certain kinds of memory leaks, in which a program fails to free memory occupied by objects that have become unreachable, which can lead to memory exhaustion.


=== Disadvantages ===
GC uses computing resources to decide which memory to free. Therefore, the penalty for the convenience of not annotating object lifetime manually in the source code is overhead, which can impair program performance. A peer-reviewed paper from 2005 concluded that GC needs five times the memory to compensate for this overhead and to perform as fast as the same program using idealised explicit memory management. The comparison however is made to a program generated by inserting deallocation calls using an oracle, implemented by collecting traces from programs run under a profiler, and the program is only correct for one particular execution of the program. Interaction with memory hierarchy effects can make this overhead intolerable in circumstances that are hard to predict or to detect in routine testing. The impact on performance was given by Apple as a reason for not adopting garbage collection in iOS, despite it being the most desired feature.The moment when the garbage is actually collected can be unpredictable, resulting in stalls (pauses to shift/free memory) scattered throughout a session. Unpredictable stalls can be unacceptable in real-time environments, in transaction processing, or in interactive programs. Incremental, concurrent, and real-time garbage collectors address these problems, with varying trade-offs.


== Strategies ==


=== Tracing ===

Tracing garbage collection is the most common type of garbage collection, so much so that ""garbage collection"" often refers to tracing garbage collection, rather than other methods such as reference counting. The overall strategy consists of determining which objects should be garbage collected by tracing which objects are reachable by a chain of references from certain root objects, and considering the rest as garbage and collecting them. However, there are a large number of algorithms used in implementation, with widely varying complexity and performance characteristics.


=== Reference counting ===

Reference counting garbage collection is where each object has a count of the number of references to it. Garbage is identified by having a reference count of zero. An object's reference count is incremented when a reference to it is created, and decremented when a reference is destroyed. When the count reaches zero, the object's memory is reclaimed.As with manual memory management, and unlike tracing garbage collection, reference counting guarantees that objects are destroyed as soon as their last reference is destroyed, and usually only accesses memory which is either in CPU caches, in objects to be freed, or directly pointed to by those, and thus tends to not have significant negative side effects on CPU cache and virtual memory operation.
There are a number of disadvantages to reference counting; this can generally be solved or mitigated by more sophisticated algorithms:

Cycles
If two or more objects refer to each other, they can create a cycle whereby neither will be collected as their mutual references never let their reference counts become zero. Some garbage collection systems using reference counting (like the one in CPython) use specific cycle-detecting algorithms to deal with this issue. Another strategy is to use weak references for the ""backpointers"" which create cycles. Under reference counting, a weak reference is similar to a weak reference under a tracing garbage collector. It is a special reference object whose existence does not increment the reference count of the referent object. Furthermore, a weak reference is safe in that when the referent object becomes garbage, any weak reference to it lapses, rather than being permitted to remain dangling, meaning that it turns into a predictable value, such as a null reference.Space overhead (reference count)
Reference counting requires space to be allocated for each object to store its reference count. The count may be stored adjacent to the object's memory or in a side table somewhere else, but in either case, every single reference-counted object requires additional storage for its reference count. Memory space with the size of an unsigned pointer is commonly used for this task, meaning that 32 or 64 bits of reference count storage must be allocated for each object. On some systems, it may be possible to mitigate this overhead by using a tagged pointer to store the reference count in unused areas of the object's memory. Often, an architecture does not actually allow programs to access the full range of memory addresses that could be stored in its native pointer size; certain number of high bits in the address is either ignored or required to be zero. If an object reliably has a pointer at a certain location, the reference count can be stored in the unused bits of the pointer. For example, each object in Objective-C has a pointer to its class at the beginning of its memory; on the ARM64 architecture using iOS 7, 19 unused bits of this class pointer are used to store the object's reference count.Speed overhead (increment/decrement)
In naive implementations, each assignment of a reference and each reference falling out of scope often require modifications of one or more reference counters. However, in a common case when a reference is copied from an outer scope variable into an inner scope variable, such that the lifetime of the inner variable is bounded by the lifetime of the outer one, the reference incrementing can be eliminated. The outer variable ""owns"" the reference. In the programming language C++, this technique is readily implemented and demonstrated with the use of const references. Reference counting in C++ is usually implemented using ""smart pointers"" whose constructors, destructors and assignment operators manage the references. A smart pointer can be passed by reference to a function, which avoids the need to copy-construct a new smart pointer (which would increase the reference count on entry into the function and decrease it on exit). Instead the function receives a reference to the smart pointer which is produced inexpensively. The Deutsch-Bobrow method of reference counting capitalizes on the fact that most reference count updates are in fact generated by references stored in local variables. It ignores these references, only counting references in the heap, but before an object with reference count zero can be deleted, the system must verify with a scan of the stack and registers that no other reference to it still exists. A further substantial decrease in the overhead on counter updates can be obtained by update coalescing introduced by Levanoni and Petrank. Consider a pointer that in a given interval of the execution is updated several times. It first points to an object O1, then to an object O2, and so forth until at the end of the interval it points to some object On. A reference counting algorithm would typically execute rc(O1)--, rc(O2)++, rc(O2)--, rc(O3)++, rc(O3)--, ..., rc(On)++. But most of these updates are redundant. In order to have the reference count properly evaluated at the end of the interval it is enough to perform rc(O1)-- and rc(On)++. Levanoni and Petrank measured an elimination of more than 99% of the counter updates in typical Java benchmarks.Requires atomicity
When used in a multithreaded environment, these modifications (increment and decrement) may need to be atomic operations such as compare-and-swap, at least for any objects which are shared, or potentially shared among multiple threads. Atomic operations are expensive on a multiprocessor, and even more expensive if they have to be emulated with software algorithms. It is possible to avoid this issue by adding per-thread or per-CPU reference counts and only accessing the global reference count when the local reference counts become or are no longer zero (or, alternatively, using a binary tree of reference counts, or even giving up deterministic destruction in exchange for not having a global reference count at all), but this adds significant memory overhead and thus tends to be only useful in special cases (it is used, for example, in the reference counting of Linux kernel modules). Update coalescing by Levanoni and Petrank can be used to eliminate all atomic operations from the write-barrier. Counters are never updated by the program threads in the course of program execution. They are only modified by the collector which executes as a single additional thread with no synchronization. This method can be used as a stop-the-world mechanism for parallel programs, and also with a concurrent reference counting collector.Not real-time
Naive implementations of reference counting do not generally provide real-time behavior, because any pointer assignment can potentially cause a number of objects bounded only by total allocated memory size to be recursively freed while the thread is unable to perform other work. It is possible to avoid this issue by delegating the freeing of unreferenced objects to other threads, at the cost of extra overhead.


=== Escape analysis ===

Escape analysis is a compile-time technique that can convert heap allocations to stack allocations, thereby reducing the amount of garbage collection to be done. This analysis determines whether an object allocated inside a function is accessible outside of it. If a function-local allocation is found to be accessible to another function or thread, the allocation is said to ""escape"" and cannot be done on the stack. Otherwise, the object may be allocated directly on the stack and released when the function returns, bypassing the heap and associated memory management costs.


== Availability ==
Generally speaking, higher-level programming languages are more likely to have garbage collection as a standard feature. In some languages that do not have built in garbage collection, it can be added through a library, as with the Boehm garbage collector for C and C++.
Most functional programming languages, such as ML, Haskell, and APL, have garbage collection built in. Lisp is especially notable as both the first functional programming language and the first language to introduce garbage collection.Other dynamic languages, such as Ruby and Julia (but not Perl 5 or PHP before version 5.3, which both use reference counting), JavaScript and ECMAScript also tend to use GC. Object-oriented programming languages such as Smalltalk, RPL and Java usually provide integrated garbage collection. Notable exceptions are C++ and Delphi, which have destructors.


=== BASIC ===
BASIC and Logo have often used garbage collection for variable-length data types, such as strings and lists, so as not to burden programmers with memory management details. On the Altair 8800, programs with many string variables and little string space could cause long pauses due to garbage collection. Similarly the Applesoft BASIC interpreter's garbage collection algorithm repeatedly scans the string descriptors for the string having the highest address in order to compact it toward high memory, resulting in 
  
    
      
        O
        (
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle O(n^{2})}
   performance and pauses anywhere from a few seconds to a few minutes. A replacement garbage collector for Applesoft BASIC by Randy Wigginton identifies a group of strings in every pass over the heap, reducing collection time dramatically. BASIC.System, released with ProDOS in 1983, provides a windowing garbage collector for BASIC that is many times faster.


=== Objective-C ===
While the Objective-C traditionally had no garbage collection, with the release of OS X 10.5 in 2007 Apple introduced garbage collection for Objective-C 2.0, using an in-house developed runtime collector.
However, with the 2012 release of OS X 10.8, garbage collection was deprecated in favor of LLVM's automatic reference counter (ARC) that was introduced with OS X 10.7. Furthermore, since May 2015 Apple even forbids the usage of garbage collection for new OS X applications in the App Store. For iOS, garbage collection has never been introduced due to problems in application responsivity and performance; instead, iOS uses ARC.


=== Limited environments ===
Garbage collection is rarely used on embedded or real-time systems because of the usual need for very tight control over the use of limited resources. However, garbage collectors compatible with many limited environments have been developed. The Microsoft .NET Micro Framework, .NET nanoFramework and Java Platform, Micro Edition are embedded software platforms that, like their larger cousins, include garbage collection.


=== Java ===
Garbage collectors available in Java JDKs include:

G1
Parallel
Concurrent mark sweep collector (CMS)
Serial
C4 (Continuously Concurrent Compacting Collector)
Shenandoah
ZGC


=== Compile-time use ===
Compile-time garbage collection is a form of static analysis allowing memory to be reused and reclaimed based on invariants known during compilation.
This form of garbage collection has been studied in the Mercury programming language, and it saw greater usage with the introduction of LLVM's automatic reference counter (ARC) into Apple's ecosystem (iOS and OS X) in 2011.


=== Real-time systems ===
Incremental, concurrent, and real-time garbage collectors have been developed, for example by Henry Baker and by Henry Lieberman.In Baker's algorithm, the allocation is done in either half of a single region of memory. When it becomes half full, a garbage collection is performed which moves the live objects into the other half and the remaining objects are implicitly deallocated. The running program (the 'mutator') has to check that any object it references is in the correct half, and if not move it across, while a background task is finding all of the objects.Generational garbage collection schemes are based on the empirical observation that most objects die young. In generational garbage collection two or more allocation regions (generations) are kept, which are kept separate based on object's age. New objects are created in the ""young"" generation that is regularly collected, and when a generation is full, the objects that are still referenced from older regions are copied into the next oldest generation. Occasionally a full scan is performed.
Some high-level language computer architectures include hardware support for real-time garbage collection.
Most implementations of real-time garbage collectors use tracing. Such real-time garbage collectors meet hard real-time constraints when used with a real-time operating system.


== See also ==
Destructor (computer programming)
Dynamic dead-code elimination
Smart pointer
Virtual memory compression


== References ==


== Further reading ==
Jones, Richard; Hosking, Antony; Moss, J. Eliot B. (2011-08-16). The Garbage Collection Handbook: The Art of Automatic Memory Management. CRC Applied Algorithms and Data Structures Series. Chapman and Hall / CRC Press / Taylor & Francis Ltd. ISBN 978-1-4200-8279-1. (511 pages)
Jones, Richard; Lins, Rafael (1996-07-12). Garbage Collection: Algorithms for Automatic Dynamic Memory Management (1 ed.). Wiley. ISBN 978-0-47194148-4. (404 pages)
Schorr, Herbert; Waite, William M. (August 1967). ""An Efficient Machine-Independent Procedure for Garbage Collection in Various List Structures"" (PDF). Communications of the ACM. 10 (8): 501–506. doi:10.1145/363534.363554. S2CID 5684388. Archived (PDF) from the original on 2021-01-22.
Wilson, Paul R. (1992). ""Uniprocessor Garbage Collection Techniques"". Proceedings of the International Workshop on Memory Management (IWMM 92). Lecture Notes in Computer Science. Springer-Verlag. 637: 1–42. CiteSeerX 10.1.1.47.2438. doi:10.1007/bfb0017182. ISBN 3-540-55940-X.
Wilson, Paul R.; Johnstone, Mark S.; Neely, Michael; Boles, David (1995). ""Dynamic Storage Allocation: A Survey and Critical Review"". Proceedings of the International Workshop on Memory Management (IWMM 95). Lecture Notes in Computer Science (1 ed.). 986: 1–116. CiteSeerX 10.1.1.47.275. doi:10.1007/3-540-60368-9_19. ISBN 978-3-540-60368-9.


== External links ==

The Memory Management Reference
The Very Basics of Garbage Collection
Java SE 6 HotSpot™ Virtual Machine Garbage Collection Tuning
TinyGC - an independent implementation of the BoehmGC API
Conservative Garbage Collection Implementation for C Language
MeixnerGC - an incremental mark and sweep garbage collector for C++ using smart pointers",1866486,1296,"All articles needing additional references, All articles with unsourced statements, Articles needing additional references from April 2021, Articles needing additional references from July 2014, Articles with GND identifiers, Articles with example code, Articles with short description, Articles with unsourced statements from September 2019, Automatic memory management, Memory management, Short description matches Wikidata, Solid-state computer storage, Use dmy dates from April 2019, Use list-defined references from July 2022",1125070120,cs
https://en.wikipedia.org/wiki/Semantics_(computer_science),Semantics (computer science),"In programming language theory, semantics is the rigorous mathematical study of the meaning of programming languages. Semantics assigns computational meaning to valid strings in a programming language syntax.
Semantics describes the processes a computer follows when executing a program in that specific language. This can be shown by describing the relationship between the input and output of a program, or an explanation of how the program will be executed on a certain platform, hence creating a model of computation.

","In programming language theory, semantics is the rigorous mathematical study of the meaning of programming languages. Semantics assigns computational meaning to valid strings in a programming language syntax.
Semantics describes the processes a computer follows when executing a program in that specific language. This can be shown by describing the relationship between the input and output of a program, or an explanation of how the program will be executed on a certain platform, hence creating a model of computation.


== History ==
In 1967, Robert W. Floyd publishes the paper Assigning meanings to programs; his chief aim is ""a rigorous standard for proofs about computer programs, including proofs of correctness, equivalence, and termination"". Floyd further writes:

A semantic definition of a programming language, in our approach, is founded on a syntactic definition. It must specify which of the phrases in a syntactically correct program represent commands, and what conditions must be imposed on an interpretation in the neighborhood of each command.

In 1969, Tony Hoare publishes a paper on Hoare logic seeded by Floyd's ideas, now sometimes collectively called axiomatic semantics.In the 1970s, the terms operational semantics and denotational semantics emerged.


== Overview ==
The field of formal semantics encompasses all of the following:

The definition of semantic models
The relations between different semantic models
The relations between different approaches to meaning
The relation between computation and the underlying mathematical structures from fields such as logic, set theory, model theory, category theory, etc.It has close links with other areas of computer science such as programming language design, type theory, compilers and interpreters, program verification and model checking.


== Approaches ==
There are many approaches to formal semantics; these belong to three major classes:

Denotational semantics, whereby each phrase in the language is interpreted as a denotation, i.e. a conceptual meaning that can be thought of abstractly.  Such denotations are often mathematical objects inhabiting a mathematical space, but it is not a requirement that they should be so.  As a practical necessity, denotations are described using some form of mathematical notation, which can in turn be formalized as a denotational metalanguage.  For example, denotational semantics of functional languages often translate the language into domain theory. Denotational semantic descriptions can also serve as compositional translations from a programming language into the denotational metalanguage and used as a basis for designing compilers.
Operational semantics, whereby the execution of the language is described directly (rather than by translation).  Operational semantics loosely corresponds to interpretation, although again the ""implementation language"" of the interpreter is generally a mathematical formalism.  Operational semantics may define an abstract machine (such as the SECD machine), and give meaning to phrases by describing the transitions they induce on states of the machine.  Alternatively, as with the pure lambda calculus, operational semantics can be defined via syntactic transformations on phrases of the language itself;
Axiomatic semantics, whereby one gives meaning to phrases by describing the axioms that apply to them.  Axiomatic semantics makes no distinction between a phrase's meaning and the logical formulas that describe it; its meaning is exactly what can be proven about it in some logic.  The canonical example of axiomatic semantics is Hoare logic.Apart from the choice between denotational, operational, or axiomatic approaches, most variations in formal semantic systems arise from the choice of supporting mathematical formalism.


== Variations ==
Some variations of formal semantics include the following:

Action semantics is an approach that tries to modularize denotational semantics, splitting the formalization process in two layers (macro and microsemantics) and predefining three semantic entities (actions, data and yielders) to simplify the specification;
Algebraic semantics is a form of axiomatic semantics based on algebraic laws for describing and reasoning about program semantics in a formal manner. It also supports denotational semantics and operational semantics;
Attribute grammars define systems that systematically compute ""metadata"" (called attributes) for the various cases of the language's syntax.  Attribute grammars can be understood as a denotational semantics where the target language is simply the original language enriched with attribute annotations.  Aside from formal semantics, attribute grammars have also been used for code generation in compilers, and to augment regular or context-free grammars with context-sensitive conditions;
Categorical (or ""functorial"") semantics uses category theory as the core mathematical formalism. A categorical semantics is usually proven to correspond to some axiomatic semantics that gives a syntactic presentation of the categorical structures. Also, denotational semantics are often instances of a general categorical semantics;
Concurrency semantics is a catch-all term for any formal semantics that describes concurrent computations.  Historically important concurrent formalisms have included the actor model and process calculi;
Game semantics uses a metaphor inspired by game theory;
Predicate transformer semantics, developed by Edsger W. Dijkstra, describes the meaning of a program fragment as the function transforming a postcondition to the precondition needed to establish it.


== Describing relationships ==
For a variety of reasons, one might wish to describe the relationships between different formal semantics.  For example:

To prove that a particular operational semantics for a language satisfies the logical formulas of an axiomatic semantics for that language.  Such a proof demonstrates that it is ""sound"" to reason about a particular (operational) interpretation strategy using a particular (axiomatic) proof system.
To prove that operational semantics over a high-level machine is related by a simulation with the semantics over a low-level machine, whereby the low-level abstract machine contains more primitive operations than the high-level abstract machine definition of a given language. Such a proof demonstrates that the low-level machine ""faithfully implements"" the high-level machine.It is also possible to relate multiple semantics through abstractions via the theory of abstract interpretation.


== See also ==
Computational semantics
Formal semantics (logic)
Formal semantics (linguistics)
Ontology
Ontology (information science)
Semantic equivalence
Semantic technology


== References ==


== Further reading ==
Textbooks",551593,198,"All articles lacking in-text citations, Articles lacking in-text citations from August 2020, Articles with short description, CS1 errors: missing periodical, CS1 maint: bot: original URL status unknown, Formal methods, Formal specification languages, Logic in computer science, Programming language semantics, Short description is different from Wikidata",1131426159,cs
https://en.wikipedia.org/wiki/Glossary_of_computer_science,Glossary of computer science,"This glossary of computer science is a list of definitions of terms and concepts used in computer science, its sub-disciplines, and related fields, including terms relevant to software, data science, and computer programming.","This glossary of computer science is a list of definitions of terms and concepts used in computer science, its sub-disciplines, and related fields, including terms relevant to software, data science, and computer programming.


== A ==
abstract data type (ADT)
A mathematical model for data types in which a data type is defined by its behavior (semantics) from the point of view of a user of the data, specifically in terms of possible values, possible operations on data of this type, and the behavior of these operations. This contrasts with data structures, which are concrete representations of data from the point of view of an implementer rather than a user.

abstract method
One with only a signature and no implementation body. It is often used to specify that a subclass must provide an implementation of the method. Abstract methods are used to specify interfaces in some computer languages.

abstraction
1.  In software engineering and computer science, the process of removing physical, spatial, or temporal details or attributes in the study of objects or systems in order to more closely attend to other details of interest; it is also very similar in nature to the process of generalization.
2.  The result of this process: an abstract concept-object created by keeping common features or attributes to various concrete objects or systems of study.

agent architecture
A blueprint for software agents and intelligent control systems depicting the arrangement of components. The architectures implemented by intelligent agents are referred to as cognitive architectures.

agent-based model (ABM)
A class of computational models for simulating the actions and interactions of autonomous agents (both individual or collective entities such as organizations or groups) with a view to assessing their effects on the system as a whole. It combines elements of game theory, complex systems, emergence, computational sociology, multi-agent systems, and evolutionary programming. Monte Carlo methods are used to introduce randomness.

aggregate function
In database management, a function in which the values of multiple rows are grouped together to form a single value of more significant meaning or measurement, such as a sum, count, or max.

agile software development
An approach to software development under which requirements and solutions evolve through the collaborative effort of self-organizing and cross-functional teams and their customer(s)/end user(s). It advocates adaptive planning, evolutionary development, early delivery, and continual improvement, and it encourages rapid and flexible response to change.

algorithm
An unambiguous specification of how to solve a class of problems. Algorithms can perform calculation, data processing, and automated reasoning tasks. They are ubiquitous in computing technologies.

algorithm design
A method or mathematical process for problem-solving and for engineering algorithms. The design of algorithms is part of many solution theories of operation research, such as dynamic programming and divide-and-conquer. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, such as the template method pattern and decorator pattern.

algorithmic efficiency
A property of an algorithm which relates to the number of computational resources used by the algorithm. An algorithm must be analyzed to determine its resource usage, and the efficiency of an algorithm can be measured based on usage of different resources. Algorithmic efficiency can be thought of as analogous to engineering productivity for a repeating or continuous process.

American Standard Code for Information Interchange (ASCII)
A character encoding standard for electronic communications. ASCII codes represent text in computers, telecommunications equipment, and other devices. Most modern character-encoding schemes are based on ASCII, although they support many additional characters.

application programming interface (API)
A set of subroutine definitions, communication protocols, and tools for building software. In general terms, it is a set of clearly defined methods of communication among various components. A good API makes it easier to develop a computer program by providing all the building blocks, which are then put together by the programmer.

application software
Also simply application or app.
Computer software designed to perform a group of coordinated functions, tasks, or activities for the benefit of the user. Common examples of applications include word processors, spreadsheets, accounting applications, web browsers, media players, aeronautical flight simulators, console games, and photo editors. This contrasts with system software, which is mainly involved with managing the computer's most basic running operations, often without direct input from the user. The collective noun application software refers to all applications collectively.

array data structure
Also simply array.
A data structure consisting of a collection of elements (values or variables), each identified by at least one array index or key. An array is stored such that the position of each element can be computed from its index tuple by a mathematical formula. The simplest type of data structure is a linear array, also called a one-dimensional array.

artifact
One of many kinds of tangible by-products produced during the development of software. Some artifacts (e.g. use cases, class diagrams, and other Unified Modeling Language (UML) models, requirements, and design documents) help describe the function, architecture, and design of software. Other artifacts are concerned with the process of development itself—such as project plans, business cases, and risk assessments.

artificial intelligence (AI)
Also machine intelligence.
Intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and other animals. In computer science, AI research is defined as the study of ""intelligent agents"": devices capable of perceiving their environment and taking actions that maximize the chance of successfully achieving their goals. Colloquially, the term ""artificial intelligence"" is applied when a machine mimics ""cognitive"" functions that humans associate with other human minds, such as ""learning"" and ""problem solving"".

ASCII
See American Standard Code for Information Interchange.

assertion
In computer programming, a statement that a predicate (Boolean-valued function, i.e. a true–false expression) is always true at that point in code execution. It can help a programmer read the code, help a compiler compile it, or help the program detect its own defects. For the latter, some programs check assertions by actually evaluating the predicate as they run and if it is not in fact true – an assertion failure – the program considers itself to be broken and typically deliberately crashes or throws an assertion failure exception.

associative array
An associative array, map, symbol table, or dictionary is an abstract data type composed of a collection of (key, value) pairs, such that each possible key appears at most once in the collection.

Operations associated with this data type allow:the addition of a pair to the collection
the removal of a pair from the collection
the modification of an existing pair
the lookup of a value associated with a particular key

automata theory
The study of abstract machines and automata, as well as the computational problems that can be solved using them. It is a theory in theoretical computer science and discrete mathematics (a subject of study in both mathematics and computer science).

automated reasoning
An area of computer science and mathematical logic dedicated to understanding different aspects of reasoning. The study of automated reasoning helps produce computer programs that allow computers to reason completely, or nearly completely, automatically. Although automated reasoning is considered a sub-field of artificial intelligence, it also has connections with theoretical computer science, and even philosophy.


== B ==
bandwidth
The maximum rate of data transfer across a given path. Bandwidth may be characterized as network bandwidth, data bandwidth, or digital bandwidth.

Bayesian programming
A formalism and a methodology for having a technique to specify probabilistic models and solve problems when less than the necessary information is available.

benchmark
The act of running a computer program, a set of programs, or other operations, in order to assess the relative performance of an object, normally by running a number of standard tests and trials against it. The term benchmark is also commonly utilized for the purposes of elaborately designed benchmarking programs themselves.

best, worst and average case
Expressions of what the resource usage is at least, at most, and on average, respectively, for a given algorithm. Usually the resource being considered is running time, i.e. time complexity, but it could also be memory or some other resource. Best case is the function which performs the minimum number of steps on input data of n elements; worst case is the function which performs the maximum number of steps on input data of size n; average case is the function which performs an average number of steps on input data of n elements.

big data
A term used to refer to data sets that are too large or complex for traditional data-processing application software to adequately deal with. Data with many cases (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate.

big O notation
A mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity. It is a member of a family of notations invented by Paul Bachmann, Edmund Landau, and others, collectively called Bachmann–Landau notation or asymptotic notation.

binary number
In mathematics and digital electronics, a number expressed in the base-2 numeral system or binary numeral system, which uses only two symbols: typically 0 (zero) and 1 (one).

binary search algorithm
Also simply binary search, half-interval search, logarithmic search, or binary chop.
A search algorithm that finds the position of a target value within a sorted array.

binary tree
A tree data structure in which each node has at most two children, which are referred to as the left child and the right child. A recursive definition using just set theory notions is that a (non-empty) binary tree is a tuple (L, S, R), where L and R are binary trees or the empty set and S is a singleton set. Some authors allow the binary tree to be the empty set as well.

bioinformatics
An interdisciplinary field that combines biology, computer science, information engineering, mathematics, and statistics to develop methods and software tools for analyzing and interpreting biological data. Bioinformatics is widely used for in silico analyses of biological queries using mathematical and statistical techniques.

bit
A basic unit of information used in computing and digital communications; a portmanteau of binary digit. A binary digit can have one of two possible values, and may be physically represented with a two-state device. These state values are most commonly represented as either a 0or1.

bit rate (R)

Also bitrate.
In telecommunications and computing, the number of bits that are conveyed or processed per unit of time.

blacklist
Also block list.
In computing, a basic access control mechanism that allows through all elements (email addresses, users, passwords, URLs, IP addresses, domain names, file hashes, etc.), except those explicitly mentioned in a list of prohibited elements. Those items on the list are denied access. The opposite is a whitelist, which means only items on the list are allowed through whatever gate is being used while all other elements are blocked. A greylist contains items that are temporarily blocked (or temporarily allowed) until an additional step is performed.

BMP file format
Also bitmap image file, device independent bitmap (DIB) file format, or simply bitmap.
A raster graphics image file format used to store bitmap digital images independently of the display device (such as a graphics adapter), used especially on Microsoft Windows and OS/2 operating systems.

Boolean data type
A data type that has one of two possible values (usually denoted true and false), intended to represent the two truth values of logic and Boolean algebra. It is named after George Boole, who first defined an algebraic system of logic in the mid-19th century. The Boolean data type is primarily associated with conditional statements, which allow different actions by changing control flow depending on whether a programmer-specified Boolean condition evaluates to true or false. It is a special case of a more general logical data type (see probabilistic logic)—i.e. logic need not always be Boolean.

Boolean expression
An expression used in a programming language that returns a Boolean value when evaluated, that is one of true or false. A Boolean expression may be composed of a combination of the Boolean constants true or false, Boolean-typed variables, Boolean-valued operators, and Boolean-valued functions.

Boolean algebra
In mathematics and mathematical logic, the branch of algebra in which the values of the variables are the truth values true and false, usually denoted 1 and 0, respectively. Contrary to elementary algebra, where the values of the variables are numbers and the prime operations are addition and multiplication, the main operations of Boolean algebra are the conjunction and (denoted as ∧), the disjunction or (denoted as ∨), and the negation not (denoted as ¬). It is thus a formalism for describing logical relations in the same way that elementary algebra describes numeric relations.

byte
A unit of digital information that most commonly consists of eight bits, representing a binary number. Historically, the byte was the number of bits used to encode a single character of text in a computer and for this reason it is the smallest addressable unit of memory in many computer architectures.

booting
The procedures implemented in starting up a computer or computer appliance until it can be used. It can be initiated by hardware such as a button press or by a software command. After the power is switched on, the computer is relatively dumb and can read only part of its storage called read-only memory. There, a small program is stored called firmware. It does power-on self-tests and, most importantly, allows access to other types of memory like a hard disk and main memory. The firmware loads bigger programs into the computer's main memory and runs it.


== C ==
callback
Also a call-after function.
Any executable code that is passed as an argument to other code that is expected to ""call back"" (execute) the argument at a given time. This execution may be immediate, as in a synchronous callback, or it might happen at a later time, as in an asynchronous callback.

central processing unit (CPU)
The electronic circuitry within a computer that carries out the instructions of a computer program by performing the basic arithmetic, logic, controlling, and input/output (I/O) operations specified by the instructions. The computer industry has used the term ""central processing unit"" at least since the early 1960s. Traditionally, the term ""CPU"" refers to a processor, more specifically to its processing unit and control unit (CU), distinguishing these core elements of a computer from external components such as main memory and I/O circuitry.

character
A unit of information that roughly corresponds to a grapheme, grapheme-like unit, or symbol, such as in an alphabet or syllabary in the written form of a natural language.

cipher
Also cypher.
In cryptography, an algorithm for performing encryption or decryption—a series of well-defined steps that can be followed as a procedure.

class
In object-oriented programming, an extensible program-code-template for creating objects, providing initial values for state (member variables) and implementations of behavior (member functions or methods). In many languages, the class name is used as the name for the class (the template itself), the name for the default constructor of the class (a subroutine that creates objects), and as the type of objects generated by instantiating the class; these distinct concepts are easily conflated.

class-based programming
Also class-orientation.
A style of object-oriented programming (OOP) in which inheritance occurs via defining ""classes"" of objects, instead of via the objects alone (compare prototype-based programming).

client
A piece of computer hardware or software that accesses a service made available by a server. The server is often (but not always) on another computer system, in which case the client accesses the service by way of a network. The term applies to the role that programs or devices play in the client–server model.

cleanroom software engineering
A software development process intended to produce software with a certifiable level of reliability. The cleanroom process was originally developed by Harlan Mills and several of his colleagues including Alan Hevner at IBM. The focus of the cleanroom process is on defect prevention, rather than defect removal.

closure
Also lexical closure or function closure.
A technique for implementing lexically scoped name binding in a language with first-class functions. Operationally, a closure is a record storing a function together with an environment.

cloud computing
Shared pools of configurable computer system resources and higher-level services that can be rapidly provisioned with minimal management effort, often over the Internet.  Cloud computing relies on sharing of resources to achieve coherence and economies of scale, similar to a public utility.

code library
A collection of non-volatile resources used by computer programs, often for software development. These may include configuration data, documentation, help data, message templates, pre-written code and subroutines, classes, values or type specifications. In IBM's OS/360 and its successors they are referred to as partitioned data sets.

coding
Computer programming is the process of designing and building an executable computer program for accomplishing a specific computing task. Programming involves tasks such as analysis, generating algorithms, profiling algorithms' accuracy and resource consumption, and the implementation of algorithms in a chosen programming language (commonly referred to as coding). The source code of a program is written in one or more programming languages. The purpose of programming is to find a sequence of instructions that will automate the performance of a task for solving a given problem. The process of programming thus often requires expertise in several different subjects, including knowledge of the application domain, specialized algorithms, and formal logic.

coding theory
The study of the properties of codes and their respective fitness for specific applications. Codes are used for data compression, cryptography, error detection and correction, data transmission and data storage. Codes are studied by various scientific disciplines—such as information theory, electrical engineering, mathematics, linguistics, and computer science—for the purpose of designing efficient and reliable data transmission methods. This typically involves the removal of redundancy and the correction or detection of errors in the transmitted data.

cognitive science
The interdisciplinary, scientific study of the mind and its processes. It examines the nature, the tasks, and the functions of cognition (in a broad sense). Cognitive scientists study intelligence and behavior, with a focus on how nervous systems represent, process, and transform information. Mental faculties of concern to cognitive scientists include language, perception, memory, attention, reasoning, and emotion; to understand these faculties, cognitive scientists borrow from fields such as linguistics, psychology, artificial intelligence, philosophy, neuroscience, and anthropology.

collection
A collection or container is a grouping of some variable number of data items (possibly zero) that have some shared significance to the problem being solved and need to be operated upon together in some controlled fashion.  Generally, the data items will be of the same type or, in languages supporting inheritance, derived from some common ancestor type. A collection is a concept applicable to abstract data types, and does not prescribe a specific implementation as a concrete data structure, though often there is a conventional choice (see Container for type theory discussion).

comma-separated values (CSV)
A delimited text file that uses a comma to separate values. A CSV file stores tabular data (numbers and text) in plain text.  Each line of the file is a data record.  Each record consists of one or more fields, separated by commas. The use of the comma as a field separator is the source of the name for this file format.

compiler
A computer program that transforms computer code written in one programming language (the source language) into another programming language (the target language). Compilers are a type of translator that support digital devices, primarily computers. The name compiler is primarily used for programs that translate source code from a high-level programming language to a lower-level language (e.g. assembly language, object code, or machine code) to create an executable program.

computability theory
also known as recursion theory, is a branch of mathematical logic, of computer science, and of the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees. The field has since expanded to include the study of generalized computability and definability. In these areas, recursion theory overlaps with proof theory and effective descriptive set theory.

computation
Any type of calculation that includes both arithmetical and non-arithmetical steps and follows a well-defined model, e.g. an algorithm. The study of computation is paramount to the discipline of computer science.

computational biology
Involves the development and application of data-analytical and theoretical methods, mathematical modelling and computational simulation techniques to the study of biological, ecological, behavioural, and social systems. The field is broadly defined and includes foundations in biology, applied mathematics, statistics, biochemistry, chemistry, biophysics, molecular biology, genetics, genomics, computer science, and evolution.  Computational biology is different from biological computing, which is a subfield of computer science and computer engineering using bioengineering and biology to build computers.

computational chemistry
A branch of chemistry that uses computer simulation to assist in solving chemical problems. It uses methods of theoretical chemistry, incorporated into efficient computer programs, to calculate the structures and properties of molecules and solids.

computational complexity theory
A subfield of computational science which focuses on classifying computational problems according to their inherent difficulty, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.

computational model
A mathematical model in computational science that requires extensive computational resources to study the behavior of a complex system by computer simulation.

computational neuroscience
Also theoretical neuroscience or mathematical neuroscience.
A branch of neuroscience which employs mathematical models, theoretical analysis, and abstractions of the brain to understand the principles that govern the development, structure, physiology, and cognitive abilities of the nervous system.

computational physics
Is the study and implementation of numerical analysis to solve problems in physics for which a quantitative theory already exists. Historically, computational physics was the first application of modern computers in science, and is now a subset of computational science.

computational science
Also scientific computing and scientific computation (SC).
An interdisciplinary field that uses advanced computing capabilities to understand and solve complex problems. It is an area of science which spans many disciplines, but at its core it involves the development of computer models and simulations to understand complex natural systems.

computational steering
Is the practice of manually intervening with an otherwise autonomous computational process, to change its outcome.

computer
A device that can be instructed to carry out sequences of arithmetic or logical operations automatically via computer programming. Modern computers have the ability to follow generalized sets of operations, called programs. These programs enable computers to perform an extremely wide range of tasks.

computer architecture
A set of rules and methods that describe the functionality, organization, and implementation of computer systems. Some definitions of architecture define it as describing the capabilities and programming model of a computer but not a particular implementation. In other definitions computer architecture involves instruction set architecture design, microarchitecture design, logic design, and implementation.

computer data storage
Also simply storage or memory.
A technology consisting of computer components and recording media that are used to retain digital data. Data storage is a core function and fundamental component of all modern computer systems.: 15–16 

computer ethics
A part of practical philosophy concerned with how computing professionals should make decisions regarding professional and social conduct.

computer graphics
Pictures and films created using computers. Usually, the term refers to computer-generated image data created with the help of specialized graphical hardware and software. It is a vast and recently developed area of computer science.

computer network
Also data network.
A digital telecommunications network which allows nodes to share resources. In computer networks, computing devices exchange data with each other using connections (data links) between nodes. These data links are established over cable media such as wires or optic cables, or wireless media such as Wi-Fi.

computer program
Is a collection of instructions that can be executed by a computer to perform a specific task. 

computer programming
The process of designing and building an executable computer program for accomplishing a specific computing task. Programming involves tasks such as analysis, generating algorithms, profiling algorithms' accuracy and resource consumption, and the implementation of algorithms in a chosen programming language (commonly referred to as coding). The source code of a program is written in one or more programming languages. The purpose of programming is to find a sequence of instructions that will automate the performance of a task for solving a given problem. The process of programming thus often requires expertise in several different subjects, including knowledge of the application domain, specialized algorithms, and formal logic.

computer science
The theory, experimentation, and engineering that form the basis for the design and use of computers. It involves the study of algorithms that process, store, and communicate digital information. A computer scientist specializes in the theory of computation and the design of computational systems.

computer scientist
A person who has acquired the knowledge of computer science, the study of the theoretical foundations of information and computation and their application.

computer security
Also cybersecurity or information technology security (IT security).
The protection of computer systems from theft or damage to their hardware, software, or electronic data, as well as from disruption or misdirection of the services they provide.

computer vision
An interdisciplinary scientific field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.

computing
Is any goal-oriented activity requiring, benefiting from, or creating computing machinery. It includes study of algorithmic processes and development of both hardware and software. It has scientific, engineering, mathematical, technological and social aspects. Major computing fields include computer engineering, computer science, cybersecurity, data science, information systems, information technology and software engineering.

concatenation
In formal language theory and computer programming, string concatenation  is the operation of joining character strings end-to-end.  For example, the concatenation of ""snow"" and ""ball"" is ""snowball"". In certain formalisations of concatenation theory, also called string theory, string concatenation is a primitive notion.

Concurrency
The ability of different parts or units of a program, algorithm, or problem to be executed out-of-order or in partial order, without affecting the final outcome.  This allows for parallel execution of the concurrent units, which can significantly improve overall speed of the execution in multi-processor and multi-core systems. In more technical terms, concurrency refers to the decomposability property of a program, algorithm, or problem into order-independent or partially-ordered components or units.

conditional
Also conditional statement, conditional expression, and conditional construct.
A feature of a programming language which performs different computations or actions depending on whether a programmer-specified Boolean condition evaluates to true or false. Apart from the case of branch predication, this is always achieved by selectively altering the control flow based on some condition.

container
Is a class, a data structure, or an abstract data type (ADT) whose instances are collections of other objects. In other words, they store objects in an organized way that follows specific access rules. The size of the container depends on the number of objects (elements) it contains. Underlying (inherited) implementations of various container types may vary in size and complexity, and provide flexibility in choosing the right implementation for any given scenario.

continuation-passing style (CPS)
A style of functional programming in which control is passed explicitly in the form of a continuation. This is contrasted with direct style, which is the usual style of programming. Gerald Jay Sussman and Guy L. Steele, Jr. coined the phrase in AI Memo 349 (1975), which sets out the first version of the Scheme programming language.

control flow
Also flow of control.
The order in which individual statements, instructions or function calls of an imperative program are executed or evaluated. The emphasis on explicit control flow distinguishes an imperative programming language from a declarative programming language.

Creative Commons (CC)
An American non-profit organization devoted to expanding the range of creative works available for others to build upon legally and to share. The organization has released several copyright-licenses, known as Creative Commons licenses, free of charge to the public.

cryptography
Or cryptology,  is the practice and study of techniques for secure communication in the presence of third parties called adversaries. More generally, cryptography is about constructing and analyzing protocols that prevent third parties or the public from reading private messages; various aspects in information security such as data confidentiality, data integrity, authentication, and non-repudiation are central to modern cryptography. Modern cryptography exists at the intersection of the disciplines of mathematics, computer science, electrical engineering, communication science, and physics. Applications of cryptography include electronic commerce, chip-based payment cards, digital currencies, computer passwords, and military communications.

CSV
See comma-separated values.

cyberbullying
Also cyberharassment or online bullying.
A form of bullying or harassment using electronic means.

cyberspace
Widespread, interconnected digital technology.


== D ==
daemon
In multitasking computer operating systems, a daemon ( or ) is a computer program that runs as a background process, rather than being under the direct control of an interactive user. Traditionally, the process names of a daemon end with the letter d, for clarification that the process is in fact a daemon, and for differentiation between a daemon and a normal computer program. For example, syslogd is a daemon that implements system logging facility, and sshd is a daemon that serves incoming SSH connections.

data center
Also data centre.
A dedicated space used to house computer systems and associated components, such as telecommunications and data storage systems. It generally includes redundant or backup components and infrastructure for power supply, data communications connections, environmental controls (e.g. air conditioning and fire suppression) and various security devices.

database
An organized collection of data, generally stored and accessed electronically from a computer system. Where databases are more complex, they are often developed using formal design and modeling techniques.

data mining
Is a process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use. Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD. Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating. 

data science
An interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from data in various forms, both structured and unstructured, similar to data mining. Data science is a ""concept to unify statistics, data analysis, machine learning and their related methods"" in order to ""understand and analyze actual phenomena"" with data. It employs techniques and theories drawn from many fields within the context of mathematics, statistics, information science, and computer science.

data structure
A data organization, management, and storage format that enables efficient access and modification. More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data.

data type
Also simply type.
An attribute of data which tells the compiler or interpreter how the programmer intends to use the data. Most programming languages support common data types of real, integer, and Boolean. A data type constrains the values that an expression, such as a variable or a function, might take. This data type defines the operations that can be done on the data, the meaning of the data, and the way values of that type can be stored. A type of value from which an expression may take its value.

debugging
The process of finding and resolving defects or problems within a computer program that prevent correct operation of computer software or the system as a whole. Debugging tactics can involve interactive debugging, control flow analysis, unit testing, integration testing, log file analysis, monitoring at the application or system level, memory dumps, and profiling.

declaration
In computer programming, a language construct that specifies properties of an identifier: it declares what a word (identifier) ""means"". Declarations are most commonly used for functions, variables, constants, and classes, but can also be used for other entities such as enumerations and type definitions. Beyond the name (the identifier itself) and the kind of entity (function, variable, etc.), declarations typically specify the data type (for variables and constants), or the type signature (for functions); types may also include dimensions, such as for arrays. A declaration is used to announce the existence of the entity to the compiler; this is important in those strongly typed languages that require functions, variables, and constants, and their types, to be specified with a declaration before use, and is used in forward declaration. The term ""declaration"" is frequently contrasted with the term ""definition"", but meaning and usage varies significantly between languages.

digital data
In information theory and information systems, the discrete, discontinuous representation of information or works. Numbers and letters are commonly used representations.

digital signal processing (DSP)
The use of digital processing, such as by computers or more specialized digital signal processors, to perform a wide variety of signal processing operations.  The signals processed in this manner are a sequence of numbers that represent samples of a continuous variable in a domain such as time, space, or frequency.

discrete event simulation (DES)
A model of the operation of a system as a discrete sequence of events in time. Each event occurs at a particular instant in time and marks a change of state in the system. Between consecutive events, no change in the system is assumed to occur; thus the simulation can directly jump in time from one event to the next.

disk storage
(Also sometimes called drive storage) is a general category of storage mechanisms where data is recorded by various electronic, magnetic, optical, or mechanical changes to a surface layer of one or more rotating disks. A disk drive is a device implementing such a storage mechanism. Notable types are the hard disk drive (HDD) containing a non-removable disk, the  floppy disk drive (FDD) and its removable floppy disk, and various optical disc drives (ODD) and associated optical disc media.

distributed computing
A field of computer science that studies distributed systems. A distributed system is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another. The components interact with one another in order to achieve a common goal. Three significant characteristics of distributed systems are: concurrency of components, lack of a global clock, and independent failure of components. Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to peer-to-peer applications.

divide and conquer algorithm
An algorithm design paradigm based on multi-branched recursion. A divide-and-conquer algorithm works by recursively breaking down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly. The solutions to the sub-problems are then combined to give a solution to the original problem.

DNS
See Domain Name System.

documentation
Written text or illustration that accompanies computer software or is embedded in the source code. It either explains how it operates or how to use it, and may mean different things to people in different roles.

domain
Is the targeted subject area of a computer program. It is a term used in software engineering. Formally it represents the target subject of a specific programming project, whether narrowly or broadly defined.

Domain Name System (DNS)
A hierarchical and decentralized naming system for computers, services, or other resources connected to the Internet or to a private network. It associates various information with domain names assigned to each of the participating entities. Most prominently, it translates more readily memorized domain names to the numerical IP addresses needed for locating and identifying computer services and devices with the underlying network protocols. By providing a worldwide, distributed directory service, the Domain Name System has been an essential component of the functionality of the Internet since 1985.

double-precision floating-point format
A computer number format. It represents a wide dynamic range of numerical values by using a floating radix point.

download
In computer networks, to receive data from a remote system, typically a server such as a web server, an FTP server, an email server, or other similar systems. This contrasts with uploading, where data is sent to a remote server. A download is a file offered for downloading or that has been downloaded, or the process of receiving such a file.


== E ==
edge device
A device which provides an entry point into enterprise or service provider core networks. Examples include routers, routing switches, integrated access devices (IADs), multiplexers, and a variety of metropolitan area network (MAN) and wide area network (WAN) access devices.  Edge devices also provide connections into carrier and service provider networks. An edge device that connects a local area network to a high speed switch or backbone (such as an ATM switch) may be called an edge concentrator.

encryption
In cryptography, encryption is the process of encoding information. This process converts the original representation of the information, known as plaintext, into an alternative form known as ciphertext. Ideally, only authorized parties can decipher a ciphertext back to plaintext and access the original information. Encryption does not itself prevent interference but denies the intelligible content to a would-be interceptor. For technical reasons, an encryption scheme usually uses a pseudo-random encryption key generated by an algorithm. It is possible to decrypt the message without possessing the key, but, for a well-designed encryption scheme, considerable computational resources and skills are required. An authorized recipient can easily decrypt the message with the key provided by the originator to recipients but not to unauthorized users. Historically, various forms of encryption have been used to aid in cryptography. Early encryption techniques were often utilized in military messaging. Since then, new techniques have emerged and become commonplace in all areas of modern computing. Modern encryption schemes utilize the concepts of public-key and symmetric-key. Modern encryption techniques ensure security because modern computers are inefficient at cracking the encryption.

event
An action or occurrence recognized by software, often originating asynchronously from the external environment, that may be handled by the software. Because an event is an entity which encapsulates the action and the contextual variables triggering the action, the acrostic mnemonic ""Execution Variable Encapsulating Named Trigger"" is often used to clarify the concept.

event-driven programming
A programming paradigm in which the flow of the program is determined by events such as user actions (mouse clicks, key presses), sensor outputs, or messages from other programs or threads. Event-driven programming is the dominant paradigm used in graphical user interfaces and other applications (e.g. JavaScript web applications) that are centered on performing certain actions in response to user input. This is also true of programming for device drivers (e.g. P in USB device driver stacks).

evolutionary computing
A family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms. In technical terms, they are a family of population-based trial-and-error problem-solvers with a metaheuristic or stochastic optimization character.

executable
Also executable code, executable file, executable program, or simply executable.
Causes a computer ""to perform indicated tasks according to encoded instructions,"" as opposed to a data file that must be parsed by a program to be meaningful. The exact interpretation depends upon the use - while ""instructions"" is traditionally taken to mean machine code instructions for a physical CPU, in some contexts a file containing bytecode or scripting language instructions may also be considered executable.

executable module

execution
In computer and software engineering is the process by which a computer or  virtual machine executes the instructions of a computer program. Each instruction of a program is a description of a particular 
action which to be carried out in order for a specific problem to be solved; as instructions of a program and therefore the actions they describe are being carried out by an executing machine, specific effects are produced in accordance to the semantics of the instructions being executed. 

exception handling
The process of responding to the occurrence, during computation, of exceptions – anomalous or exceptional conditions requiring special processing – often disrupting the normal flow of program execution. It is provided by specialized programming language constructs, computer hardware mechanisms like interrupts, or operating system IPC facilities like signals.

Existence detection
An existence check before reading a file can catch and/or prevent a fatal error.

expression
In a programming language, a combination of one or more constants, variables, operators, and functions that the programming language interprets (according to its particular rules of precedence and of association) and computes to produce (""to return"", in a stateful environment) another value. This process, as for mathematical expressions, is called evaluation.

external library


== F ==
fault-tolerant computer system
A system designed around the concept of fault tolerance. In essence, they must be able to continue working to a level of satisfaction in the presence of errors or breakdowns.

feasibility study
An investigation which aims to objectively and rationally uncover the strengths and weaknesses of an existing business or proposed venture, opportunities and threats present in the natural environment, the resources required to carry through, and ultimately the prospects for success. In its simplest terms, the two criteria to judge feasibility are cost required and value to be attained.

field
Data that has several parts, known as a record, can be divided into fields. Relational databases arrange data as sets of database records, so called rows. Each record consists of several fields; the fields of all records form the columns.
Examples of fields: name, gender, hair colour. 

filename extension
An identifier specified as a suffix to the name of a computer file. The extension indicates a characteristic of the file contents or its intended use.

filter (software)
A computer program or subroutine to process a stream, producing another stream. While a single filter can be used individually, they are frequently strung together to form a pipeline.

floating point arithmetic
In computing, floating-point arithmetic (FP) is arithmetic using formulaic representation of real numbers as an approximation to support a trade-off between range and precision. For this reason, floating-point computation is often found in systems which include very small and very large real numbers, which require fast processing times. A number is, in general, represented approximately to a fixed number of significant digits (the significand) and scaled using an exponent in some fixed base; the base for the scaling is normally two, ten, or sixteen. A number that can be represented exactly is of the following form:

  
    
      
        
          significand
        
        ×
        
          
            base
          
          
            exponent
          
        
        ,
      
    
    {\displaystyle {\text{significand}}\times {\text{base}}^{\text{exponent}},}
  
where significand is an integer, base is an integer greater than or equal to two, and exponent is also an integer.
For example:

  
    
      
        1.2345
        =
        
          
            
              12345
              ⏟
            
          
          
            significand
          
        
        ×
        
          
            
              10
              ⏟
            
          
          
            base
          
        
        
        
        
        
        
        
          
          
            
              
                
                  
                    −
                    4
                  
                  ⏞
                
              
              
                exponent
              
            
          
        
        .
      
    
    {\displaystyle 1.2345=\underbrace {12345} _{\text{significand}}\times \underbrace {10} _{\text{base}}\!\!\!\!\!\!^{\overbrace {-4} ^{\text{exponent}}}.}
  for loop
Also for-loop. 
A control flow statement for specifying iteration, which allows code to be executed repeatedly. Various keywords are used to specify this statement: descendants of ALGOL use ""for"", while descendants of Fortran use ""do"". There are also other possibilities, e.g. COBOL uses ""PERFORM VARYING"".

formal methods
A set of mathematically based techniques for the specification, development, and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.

formal verification
The act of proving or disproving the correctness of intended algorithms underlying a system with respect to a certain formal specification or property, using formal methods of mathematics.

functional programming
A programming paradigm—a style of building the structure and elements of computer programs–that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data. It is a declarative programming paradigm in that programming is done with expressions or declarations instead of statements.


== G ==
game theory
The study of mathematical models of strategic interaction between rational decision-makers. It has applications in all fields of social science, as well as in logic and computer science. Originally, it addressed zero-sum games, in which each participant's gains or losses are exactly balanced by those of the other participants. Today, game theory applies to a wide range of behavioral relations, and is now an umbrella term for the science of logical decision making in humans, animals, and computers.

garbage in, garbage out (GIGO)
A term used to describe the concept that flawed or nonsense input data produces nonsense output or ""garbage"". It can also refer to the unforgiving nature of programming, in which a poorly written program might produce nonsensical behavior.

Graphics Interchange Format

gigabyte
A multiple of the unit byte for digital information. The prefix giga means 109 in the International System of Units (SI). Therefore, one gigabyte is 1000000000bytes.  The unit symbol for the gigabyte is GB.

global variable
In computer programming, a variable with global scope, meaning that it is visible (hence accessible) throughout the program, unless shadowed. The set of all global variables is known as the global environment or global state. In compiled languages, global variables are generally static variables, whose extent (lifetime) is the entire runtime of the program, though in interpreted languages (including command-line interpreters), global variables are generally dynamically allocated when declared, since they are not known ahead of time.

graph theory
In mathematics, the study of graphs, which are mathematical structures used to model pairwise relations between objects. A graph in this context is made up of vertices (also called nodes or points) which are connected by edges (also called links or lines). A distinction is made between undirected graphs, where edges link two vertices symmetrically, and directed graphs, where edges link two vertices asymmetrically.


== H ==
handle
In computer programming, a handle is an abstract reference to a resource that is used when application software references blocks of memory or objects that are managed by another system like a database or an operating system.

hard problem
Computational complexity theory focuses on classifying computational problems according to their inherent difficulty, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.

hash function
Any function that can be used to map data of arbitrary size to data of a fixed size. The values returned by a hash function are called hash values, hash codes, digests, or simply hashes. Hash functions are often used in combination with a hash table, a common data structure used in computer software for rapid data lookup. Hash functions accelerate table or database lookup by detecting duplicated records in a large file.

hash table
In computing, a hash table (hash map) is a data structure that implements an associative array abstract data type, a structure that can map keys to values. A hash table uses a hash function to compute an index into an array of buckets or slots, from which the desired value can be found.

heap
A specialized tree-based data structure which is essentially an almost complete tree that satisfies the heap property: if P is a parent node of C, then the key (the value) of P is either greater than or equal to (in a max heap) or less than or equal to (in a min heap) the key of C. The node at the ""top"" of the heap (with no parents) is called the root node.

heapsort
A comparison-based sorting algorithm. Heapsort can be thought of as an improved selection sort: like that algorithm, it divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest element and moving that to the sorted region. The improvement consists of the use of a heap data structure rather than a linear-time search to find the maximum.

human-computer interaction (HCI)
Researches the design and use of computer technology, focused on the interfaces between people (users) and computers. Researchers in the field of HCI both observe the ways in which humans interact with computers and design technologies that let humans interact with computers in novel ways. As a field of research, human–computer interaction is situated at the intersection of computer science, behavioral sciences, design, media studies, and several other fields of study.


== I ==
identifier
In computer languages, identifiers are tokens (also called symbols) which name language entities. Some of the kinds of entities an identifier might denote include variables, types, labels, subroutines,  and packages.

IDE
Integrated development environment.

image processing

imperative programming
A programming paradigm that uses statements that change a program's state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates.

incremental build model
A method of software development where the product is designed, implemented and tested incrementally (a little more is added each time) until the product is finished. It involves both development and maintenance. The product is defined as finished when it satisfies all of its requirements. This model combines the elements of the waterfall model with the iterative philosophy of prototyping.

information space analysis
A deterministic method, enhanced by machine intelligence, for locating and assessing resources for team-centric efforts.

information visualization

inheritance
In object-oriented programming, the mechanism of basing an object or class upon another object (prototype-based inheritance) or class (class-based inheritance), retaining similar implementation. Also defined as deriving new classes (sub classes) from existing ones (super class or base class) and forming them into a hierarchy of classes.

input/output (I/O)
Also informally io or IO. 
The communication between an information processing system, such as a computer, and the outside world, possibly a human or another information processing system. Inputs are the signals or data received by the system and outputs are the signals or data sent from it. The term can also be used as part of an action; to ""perform I/O"" is to perform an input or output operation.

insertion sort
A simple sorting algorithm that builds the final sorted array (or list) one item at a time.

instruction cycle
Also fetch–decode–execute cycle or simply fetch-execute cycle.
The cycle which the central processing unit (CPU) follows from boot-up until the computer has shut down in order to process instructions. It is composed of three main stages: the fetch stage, the decode stage, and the execute stage.

integer
A datum of integral data type, a data type that represents some range of mathematical integers. Integral data types may be of different sizes and may or may not be allowed to contain negative values. Integers are commonly represented in a computer as a group of binary digits (bits). The size of the grouping varies so the set of integer sizes available varies between different types of computers. Computer hardware, including virtual machines, nearly always provide a way to represent a processor register or memory address as an integer.

integrated development environment (IDE)
A software application that provides comprehensive facilities to computer programmers for software development. An IDE normally consists of at least a source code editor, build automation tools, and a debugger.

integration testing
(sometimes called integration and testing, abbreviated I&T) is the phase in software testing in which individual software modules are combined and tested as a group. Integration testing is conducted to evaluate the compliance of a system or component with specified functional requirements. It occurs after unit testing and before validation testing. Integration testing takes as its input modules that have been unit tested, groups them in larger aggregates, applies tests defined in an integration test plan to those aggregates, and delivers as its output the integrated system ready for system testing.

intellectual property (IP)
A category of legal property that includes intangible creations of the human intellect. There are many types of intellectual property, and some countries recognize more than others. The most well-known types are copyrights, patents, trademarks, and trade secrets.

intelligent agent
In artificial intelligence, an intelligent agent (IA) refers to an autonomous entity which acts, directing its activity towards achieving goals (i.e. it is an agent), upon an environment using observation through sensors and consequent actuators (i.e. it is intelligent). Intelligent agents may also learn or use knowledge to achieve their goals. They may be very simple or very complex. A reflex machine, such as a thermostat, is considered an example of an intelligent agent.

interface
A shared boundary across which two or more separate components of a computer system exchange information. The exchange can be between software, computer hardware, peripheral devices, humans, and combinations of these. Some computer hardware devices, such as a touchscreen, can both send and receive data through the interface, while others such as a mouse or microphone may only provide an interface to send data to a given system.

internal documentation
Computer software is said to have Internal Documentation if the notes on how and why various parts of code operate is included within the source code as comments.  It is often combined with meaningful variable names with the intention of providing potential future programmers a means of understanding the workings of the code. This contrasts with external documentation, where programmers keep their notes and explanations in a separate document.

internet
The global system of interconnected computer networks that use the Internet protocol suite (TCP/IP) to link devices worldwide. It is a network of networks that consists of private, public, academic, business, and government networks of local to global scope, linked by a broad array of electronic, wireless, and optical networking technologies.

internet bot
Also web robot, robot, or simply bot.
A software application that runs automated tasks (scripts) over the Internet. Typically, bots perform tasks that are both simple and structurally repetitive, at a much higher rate than would be possible for a human alone. The largest use of bots is in web spidering (web crawler), in which an automated script fetches, analyzes and files information from web servers at many times the speed of a human.

interpreter
A computer program that directly executes instructions written in a programming or scripting language, without requiring them to have been previously compiled into a machine language program.

invariant
One can encounter invariants that can be relied upon to be true during the execution of a program, or during some portion of it. It is a logical assertion that is always held to be true during a certain phase of execution. For example, a loop invariant is a condition that is true at the beginning and the end of every execution of a loop.

iteration
Is the repetition of a process in order to generate an outcome. The sequence will approach some end point or end value. Each repetition of the process is a single iteration, and the outcome of each iteration is then the starting point of the next iteration.  In mathematics and computer science, iteration (along with the related technique of recursion) is a standard element of algorithms.


== J ==
Java
A general-purpose programming language that is class-based, object-oriented(although not a pure OO language), and designed to have as few implementation dependencies as possible. It is intended to let application developers ""write once, run anywhere"" (WORA), meaning that compiled Java code can run on all platforms that support Java without the need for recompilation.


== K ==
kernel
The first section of an operating system to load into memory. As the center of the operating system, the kernel needs to be small, efficient, and loaded into a protected area in the memory so that it cannot be overwritten. It may be responsible for such essential tasks as disk drive management, file management, memory management, process management, etc.


== L ==
library (computing)
A collection of non-volatile resources used by computer programs, often for software development. These may include configuration data, documentation, help data, message templates, pre-written code and subroutines, classes, values, or type specifications.

linear search
Also sequential search.
A method for finding an element within a list. It sequentially checks each element of the list until a match is found or the whole list has been searched.

linked list
A linear collection of data elements, whose order is not given by their physical placement in memory. Instead, each element points to the next. It is a data structure consisting of a collection of nodes which together represent a sequence.

linker
 or link editor, is a computer utility program that takes one or more object files generated by a compiler or an assembler and combines them into a single executable file, library file, or another 'object' file.  A simpler version that writes its output directly to memory is called the loader, though loading is typically considered a separate process.

list
An abstract data type that represents a countable number of ordered values, where the same value may occur more than once. An instance of a list is a computer representation of the mathematical concept of a finite sequence; the (potentially) infinite analog of a list is a stream.: §3.5  Lists are a basic example of containers, as they contain other values. If the same value occurs multiple times, each occurrence is considered a distinct item.

loader
The part of an operating system that is responsible for loading programs and libraries. It is one of the essential stages in the process of starting a program, as it places programs into memory and prepares them for execution. Loading a program involves reading the contents of the executable file containing the program instructions into memory, and then carrying out other required preparatory tasks to prepare the executable for running. Once loading is complete, the operating system starts the program by passing control to the loaded program code.

logic error
In computer programming, a bug in a program that causes it to operate incorrectly, but not to terminate abnormally (or crash). A logic error produces unintended or undesired output or other behaviour, although it may not immediately be recognized as such.

logic programming
A type of programming paradigm which is largely based on formal logic. Any program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain.  Major logic programming language families include Prolog, answer set programming (ASP), and Datalog.


== M ==
machine learning (ML)
The scientific study of algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions, relying on patterns and inference instead. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as ""training data"", in order to make predictions or decisions without being explicitly programmed to perform the task.

machine vision (MV)
The technology and methods used to provide imaging-based automatic inspection and analysis for such applications as automatic inspection, process control, and robot guidance, usually in industry. Machine vision refers to many technologies, software and hardware products, integrated systems, actions, methods and expertise. Machine vision as a systems engineering discipline can be considered distinct from computer vision, a form of computer science. It attempts to integrate existing technologies in new ways and apply them to solve real world problems. The term is the prevalent one for these functions in industrial automation environments but is also used for these functions in other environments such as security and vehicle guidance.

mathematical logic
A subfield of mathematics exploring the applications of formal logic to mathematics.  It bears close connections to metamathematics, the foundations of mathematics, and theoretical computer science. The unifying themes in mathematical logic include the study of the expressive power of formal systems and the deductive power of formal proof systems.

matrix
In mathematics, a matrix, (plural matrices), is a rectangular array (see irregular matrix) of numbers, symbols, or expressions, arranged in rows and columns.

memory
Computer data storage, often called storage, is a technology consisting of computer components and recording media that are used to retain digital data. It is a core function and fundamental component of computers.: 15–16 

merge sort
Also mergesort.
An efficient, general-purpose, comparison-based sorting algorithm. Most implementations produce a stable sort, which means that the order of equal elements is the same in the input and output. Merge sort is a divide and conquer algorithm that was invented by John von Neumann in 1945. A detailed description and analysis of bottom-up mergesort appeared in a report by Goldstine and von Neumann as early as 1948.

method
In object-oriented programming (OOP), a procedure associated with a message and an object. An object consists of data and behavior. The data and behavior comprise an interface, which specifies how the object may be utilized by any of various consumers of the object.

methodology
In software engineering, a software development process is the process of dividing software development work into distinct phases to improve design, product management, and project management. It is also known as a software development life cycle (SDLC). The methodology may include the pre-definition of specific deliverables and artifacts that are created and completed by a project team to develop or maintain an application.

modem
Portmanteau of modulator-demodulator.
A hardware device that converts data into a format suitable for a transmission medium so that it can be transmitted from one computer to another (historically along telephone wires). A modem modulates one or more carrier wave signals to encode digital information for transmission and demodulates signals to decode the transmitted information. The goal is to produce a signal that can be transmitted easily and decoded reliably to reproduce the original digital data. Modems can be used with almost any means of transmitting analog signals from light-emitting diodes to radio. A common type of modem is one that turns the digital data of a computer into modulated electrical signal for transmission over telephone lines and demodulated by another modem at the receiver side to recover the digital data.


== N ==
natural language processing (NLP)
A subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.  Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.

node
Is a basic unit of a data structure, such as a linked list or tree data structure. Nodes contain data and also may link to other nodes. Links between nodes are often implemented by pointers.

number theory
A branch of pure mathematics devoted primarily to the study of the integers and integer-valued functions.

numerical analysis
The study of algorithms that use numerical approximation (as opposed to symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics).

numerical method
In numerical analysis, a numerical method is a mathematical tool designed to solve numerical problems. The implementation of a numerical method with an appropriate convergence check in a programming language is called a numerical algorithm.


== O ==
object
An object can be a variable, a data structure, a function, or a method, and as such, is a value in memory referenced by an identifier.  In the class-based object-oriented programming paradigm, object refers to a particular instance of a class, where the object can be a combination of variables, functions, and data structures.  In relational database management, an object can be a table or column, or an association between data and a database entity (such as relating a person's age to a specific person).

object code
Also object module.
The product of a compiler. In a general sense object code is a sequence of statements or instructions in a computer language, usually a machine code language (i.e., binary) or an intermediate language such as register transfer language (RTL). The term indicates that the code is the goal or result of the compiling process, with some early sources referring to source code as a ""subject program.""

object-oriented analysis and design (OOAD)
A technical approach for analyzing and designing an application, system, or business by applying object-oriented programming, as well as using visual modeling throughout the software development process to guide stakeholder communication and product quality.

object-oriented programming (OOP)
A programming paradigm based on the concept of ""objects"", which can contain data, in the form of fields (often known as attributes or properties), and code, in the form of procedures (often known as methods). A feature of objects is an object's procedures that can access and often modify the data fields of the object with which they are associated (objects have a notion of ""this"" or ""self""). In OOP, computer programs are designed by making them out of objects that interact with one another. OOP languages are diverse, but the most popular ones are class-based, meaning that objects are instances of classes, which also determine their types.

open-source software (OSS)
A type of computer software in which source code is released under a license in which the copyright holder grants users the rights to study, change, and distribute the software to anyone and for any purpose. Open-source software may be developed in a collaborative public manner. Open-source software is a prominent example of open collaboration.

operating system (OS)
System software that manages computer hardware, software resources, and provides common services for computer programs.

optical fiber
A flexible, transparent fiber made by drawing glass (silica) or plastic to a diameter slightly thicker than that of a human hair. Optical fibers are used most often as a means to transmit light between the two ends of the fiber and find wide usage in fiber-optic communications, where they permit transmission over longer distances and at higher bandwidths (data rates) than electrical cables. Fibers are used instead of metal wires because signals travel along them with less loss; in addition, fibers are immune to electromagnetic interference, a problem from which metal wires suffer.


== P ==
pair programming
An agile software development technique in which two programmers work together at one workstation. One, the driver, writes code while the other, the observer or navigator, reviews each line of code as it is typed in. The two programmers switch roles frequently.

parallel computing
A type of computation in which many calculations or the execution of processes are carried out simultaneously. Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism.

parameter
Also formal argument.
In computer programming, a special kind of variable, used in a subroutine to refer to one of the pieces of data provided as input to the subroutine. These pieces of data are the values of the arguments (often called actual arguments or actual parameters) with which the subroutine is going to be called/invoked. An ordered list of parameters is usually included in the definition of a subroutine, so that, each time the subroutine is called, its arguments for that call are evaluated, and the resulting values can be assigned to the corresponding parameters.

peripheral
Any auxiliary or ancillary device connected to or integrated within a computer system and used to send information to or retrieve information from the computer. An input device sends data or instructions to the computer; an output device provides output from the computer to the user; and an input/output device performs both functions.

pointer
Is an object in many programming languages that stores a memory address. This can be that of another value located in computer memory, or in some cases, that of memory-mapped computer hardware. A pointer references a location in memory, and obtaining the value stored at that location is known as dereferencing the pointer. As an analogy, a page number in a book's index could be considered a pointer to the corresponding page; dereferencing such a pointer would be done by flipping to the page with the given page number and reading the text found on that page. The actual format and content of a pointer variable is dependent on the underlying computer architecture.

postcondition
In computer programming, a condition or predicate that must always be true just after the execution of some section of code or after an operation in a formal specification. Postconditions are sometimes tested using assertions within the code itself. Often, postconditions are simply included in the documentation of the affected section of code.

precondition
In computer programming, a condition or predicate that must always be true just prior to the execution of some section of code or before an operation in a formal specification.  If a precondition is violated, the effect of the section of code becomes undefined and thus may or may not carry out its intended work.  Security problems can arise due to incorrect preconditions.

primary storage
(Also known as main memory, internal memory or prime memory), often referred to simply as memory, is the only one directly accessible to the CPU. The CPU continuously reads instructions stored there and executes them as required. Any data actively operated on is also stored there in uniform manner.

primitive data type

priority queue
An abstract data type which is like a regular queue or stack data structure, but where additionally each element has a ""priority"" associated with it. In a priority queue, an element with high priority is served before an element with low priority. In some implementations, if two elements have the same priority, they are served according to the order in which they were enqueued, while in other implementations, ordering of elements with the same priority is undefined.

procedural programming

procedure
In computer programming, a subroutine is a sequence of program instructions that performs a specific task, packaged as a unit. This unit can then be used in programs wherever that particular task should be performed.  Subroutines may be defined within programs, or separately in libraries that can be used by many programs.  In different programming languages, a subroutine may be called a routine, subprogram, function, method, or procedure. Technically, these terms all have different definitions. The generic, umbrella term  callable unit is sometimes used.

program lifecycle phase
Program lifecycle phases are the stages a computer program undergoes, from initial creation to deployment and execution. The phases are edit time, compile time, link time, distribution time, installation time, load time, and run time.

programming language
A formal language, which comprises a set of instructions that produce various kinds of output. Programming languages are used in computer programming to implement algorithms.

programming language implementation
Is a system for executing computer programs. There are two general approaches to programming language implementation: interpretation and compilation.

programming language theory
(PLT) is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and of their individual features.  It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, linguistics and even cognitive science.  It has become a well-recognized branch of computer science, and an active research area, with results published in numerous journals dedicated to PLT, as well as in general computer science and engineering publications.

Prolog
Is a logic programming language associated with artificial intelligence and computational linguistics.  Prolog has its roots in first-order logic, a formal logic, and unlike many other programming languages, Prolog is intended primarily as a declarative programming language: the program logic is expressed in terms of relations, represented as facts and rules.  A computation is initiated by running a query over these relations.

Python
Is an interpreted, high-level and general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.


== Q ==
quantum computing
The use of quantum-mechanical phenomena such as superposition and entanglement to perform computation. A quantum computer is used to perform such computation, which can be implemented theoretically or physically.: I-5 

queue
A collection in which the entities in the collection are kept in order and the principal (or only) operations on the collection are the addition of entities to the rear terminal position, known as enqueue, and removal of entities from the front terminal position, known as dequeue.

quicksort
Also partition-exchange sort.
An efficient sorting algorithm which serves as a systematic method for placing the elements of a random access file or an array in order.


== R ==
R programming language
R is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis.

radix
Also base.
In digital numeral systems, the number of unique digits, including the digit zero, used to represent numbers in a positional numeral system. For example, in the decimal/denary system (the most common system in use today) the radix (base number) is ten, because it uses the ten digits from 0 through 9, and all other numbers are uniquely specified by positional combinations of these ten base digits; in the binary system that is the standard in computing, the radix is two, because it uses only two digits, 0 and 1, to uniquely specify each number.

record
A record (also called a structure,  struct, or compound data) is a basic data structure. Records in a database or spreadsheet are usually called ""rows"".

recursion
Occurs when a thing is defined in terms of itself or of its type. Recursion is used in a variety of disciplines ranging from linguistics to logic. The most common application of recursion is in mathematics and computer science, where a function being defined is applied within its own definition. While this apparently defines an infinite number of instances (function values), it is often done in such a way that no infinite loop or infinite chain of references can occur.

reference
Is a value that enables a program to indirectly access a particular datum, such as a variable's value or a record, in the computer's memory or in some other storage device.  The reference is said to refer to the datum, and accessing the datum is called dereferencing the reference.

reference counting
A programming technique of storing the number of references, pointers, or handles to a resource, such as an object, a block of memory, disk space, and others. In garbage collection algorithms, reference counts may be used to deallocate objects which are no longer needed.

relational database
Is a digital database based on the relational model of data, as proposed by E. F. Codd in 1970.
A software system used to maintain relational databases is a relational database management system (RDBMS). Many relational database systems have an option of using the SQL (Structured Query Language) for querying and maintaining the database.

reliability engineering
A sub-discipline of systems engineering that emphasizes dependability in the lifecycle management of a product. Reliability describes the ability of a system or component to function under stated conditions for a specified period of time. Reliability is closely related to availability, which is typically described as the ability of a component or system to function at a specified moment or interval of time.

regression testing
(rarely non-regression testing) is re-running functional and non-functional tests to ensure that previously developed and tested software still performs after a change. If not, that would be called a regression. Changes that may require regression testing include bug fixes, software enhancements, configuration changes, and even substitution of electronic components. As regression test suites tend to grow with each found defect, test automation is frequently involved. Sometimes a change impact analysis is performed to determine an appropriate subset of tests (non-regression analysis).

requirements analysis
In systems engineering and software engineering, requirements analysis focuses on the tasks that determine the needs or conditions to meet the new or altered product or project, taking account of the possibly conflicting requirements of the various stakeholders, analyzing, documenting, validating and managing software or system requirements.

robotics
An interdisciplinary branch of engineering and science that includes mechanical engineering, electronic engineering, information engineering, computer science, and others. Robotics involves design, construction, operation, and use of robots, as well as computer systems for their perception, control, sensory feedback, and information processing. The goal of robotics is to design intelligent machines that can help and assist humans in their day-to-day lives and keep everyone safe.

round-off error
Also rounding error.
The difference between the result produced by a given algorithm using exact arithmetic and the result produced by the same algorithm using finite-precision, rounded arithmetic. Rounding errors are due to inexactness in the representation of real numbers and the arithmetic operations done with them. This is a form of quantization error. When using approximation equations or algorithms, especially when using finitely many digits to represent real numbers (which in theory have infinitely many digits), one of the goals of numerical analysis is to estimate computation errors. Computation errors, also called numerical errors, include both truncation errors and roundoff errors.

router
A networking device that forwards data packets between computer networks. Routers perform the traffic directing functions on the Internet.  Data sent through the internet, such as a web page or email, is in the form of data packets.   A packet is typically forwarded from one router to another router through the networks that constitute an internetwork (e.g. the Internet) until it reaches its destination node.

routing table
In computer networking a routing table, or routing information base (RIB), is a data table stored in a router or a network host that lists the routes to particular network destinations, and in some cases, metrics (distances) associated with those routes. The routing table contains information about the topology of the network immediately around it.

run time
Runtime, run time, or execution time is the final phase of a computer program's life cycle, in which the code is being executed on the computer's central processing unit (CPU) as machine code. In other words, ""runtime"" is the running phase of a program.

run time error
A runtime error is detected after or during the execution (running state) of a program, whereas a compile-time error is detected by the compiler before the program is ever executed. Type checking, register allocation, code generation, and code optimization are typically done at compile time, but may be done at runtime depending on the particular language and compiler. Many other runtime errors exist and are handled differently by different programming languages, such as division by zero errors, domain errors, array subscript out of bounds errors, arithmetic underflow errors, several types of underflow and overflow errors, and many other runtime errors generally considered as software bugs which may or may not be caught and handled by any particular computer language.


== S ==
search algorithm
Any algorithm which solves the search problem, namely, to retrieve information stored within some data structure, or calculated in the search space of a problem domain, either with discrete or continuous values.

secondary storage
Also known as external memory or auxiliary storage, differs from primary storage in that it is not directly accessible by the CPU. The computer usually uses its input/output channels to access secondary storage and transfer the desired data to primary storage. Secondary storage is non-volatile (retaining data when power is shut off). Modern computer systems typically have two orders of magnitude more secondary storage than primary storage because secondary storage is less expensive.

selection sort
Is an in-place comparison sorting algorithm. It has an O(n2) time complexity, which makes it inefficient on large lists, and generally performs worse than the similar insertion sort. Selection sort is noted for its simplicity and has performance advantages over more complicated algorithms in certain situations, particularly where auxiliary memory is limited.

semantics
In programming language theory, semantics is the field concerned with the rigorous mathematical study of the meaning of programming languages. It does so by evaluating the meaning of syntactically valid strings defined by a specific programming language, showing the computation involved. In such a case that the evaluation would be of syntactically invalid strings, the result would be non-computation. Semantics describes the processes a computer follows when executing a program in that specific language. This can be shown by describing the relationship between the input and output of a program, or an explanation of how the program will be executed on a certain platform, hence creating a model of computation.

sequence
In mathematics, a sequence is an enumerated collection of objects in which repetitions are allowed and order does matter.  Like a set, it contains members (also called elements, or terms).  The number of elements (possibly infinite) is called the length of the sequence.  Unlike a set, the same elements can appear multiple times at different positions in a sequence, and order does matter.  Formally, a sequence can be defined as a function whose domain is either the set of the natural numbers (for infinite sequences) or the set of the first n natural numbers (for a sequence of finite length n).

The position of an element in a sequence is its rank or index; it is the natural number for which the element is the image. The first element has index 0 or 1, depending on the context or a specific convention.  When a symbol is used to denote a sequence, the nth element of the sequence is denoted by this symbol with n as subscript; for example, the nth element of the Fibonacci sequence F is generally denoted Fn.

For example, (M, A, R, Y) is a sequence of letters with the letter 'M' first and 'Y' last.  This sequence differs from (A, R, M, Y).  Also, the sequence (1, 1, 2, 3, 5, 8), which contains the number 1 at two different positions, is a valid sequence.  Sequences can be finite, as in these examples, or infinite, such as the sequence of all even positive integers (2, 4, 6, ...).  In computing and computer science, finite sequences are sometimes called strings, words or lists, the different names commonly corresponding to different ways to represent them in computer memory; infinite sequences are called streams.  The empty sequence ( ) is included in most notions of sequence, but may be excluded depending on the context.

serializability
In concurrency control of databases, transaction processing (transaction management), and various transactional applications (e.g., transactional memory and software transactional memory), both centralized and distributed, a transaction schedule is serializable if its outcome (e.g., the resulting database state) is equal to the outcome of its transactions executed serially, i.e. without overlapping in time. Transactions are normally executed concurrently (they overlap), since this is the most efficient way. Serializability is the major correctness criterion for concurrent transactions' executions. It is considered the highest level of isolation between transactions, and plays an essential role in concurrency control. As such it is supported in all general purpose database systems. Strong strict two-phase locking (SS2PL) is a popular serializability mechanism utilized in most of the database systems (in various variants) since their early days in the 1970s.

serialization
Is the process of translating data structures or object state into a format that can be stored (for example, in a file or memory buffer) or transmitted (for example, across a network connection link) and reconstructed later (possibly in a different computer environment). When the resulting series of bits is reread according to the serialization format, it can be used to create a semantically identical clone of the original object. For many complex objects, such as those that make extensive use of references, this process is not straightforward. Serialization of object-oriented objects does not include any of their associated methods with which they were previously linked.

This process of serializing an object is also called marshalling an object in some situations.[2][3] The opposite operation, extracting a data structure from a series of bytes, is deserialization, (also called unserialization or unmarshalling).

service level agreement
(SLA), is a commitment between a service provider and a client. Particular aspects of the service – quality, availability, responsibilities – are agreed between the service provider and the service user. The most common component of an SLA is that the services should be provided to the customer as agreed upon in the contract. As an example, Internet service providers and telcos will commonly include service level agreements within the terms of their contracts with customers to define the level(s) of service being sold in plain language terms. In this case the SLA will typically have a technical definition in  mean time between failures (MTBF), mean time to repair or mean time to recovery (MTTR); identifying which party is responsible for reporting faults or paying fees; responsibility for various data rates; throughput; jitter; or similar measurable details.

set
Is an abstract data type that can store unique values, without any particular order. It is a computer implementation of the mathematical concept of a finite set. Unlike most other collection types, rather than retrieving a specific element from a set, one typically tests a value for membership in a set.

singleton variable
A variable that is referenced only once. May be used as a dummy argument in a function call, or when its address is assigned to another variable which subsequently accesses its allocated storage. Singleton variables sometimes occur because a mistake has been made – such as assigning a value to a variable and forgetting to use it later, or mistyping one instance of the variable name. Some compilers and lint-like tools flag occurrences of singleton variables.

soft computing

software
Computer software, or simply software, is a collection of data or computer instructions that tell the computer how to work. This is in contrast to physical hardware, from which the system is built and actually performs the work. In computer science and software engineering, computer software is all information processed by computer systems, programs and data. Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media. Computer hardware and software require each other and neither can be realistically used on its own.

software agent
Is a computer program that acts for a user or other program in a relationship of agency, which derives from the Latin agere (to do): an agreement to act on one's behalf. Such ""action on behalf of"" implies the authority to decide which, if any, action is appropriate. Agents are colloquially known as bots, from robot. They may be embodied, as when execution is paired with a robot body, or  as software such as a chatbot
executing on a phone (e.g. Siri)  or other computing device.  Software agents may be autonomous or work together with other agents or people.  Software agents interacting with people (e.g. chatbots, human-robot interaction environments) may possess human-like qualities such as natural language understanding and speech, personality or embody humanoid form (see Asimo).

software construction
Is a software engineering discipline. It is the detailed creation of working meaningful software through a combination of coding, verification, unit testing, integration testing, and debugging. It is linked to all the other software engineering disciplines, most strongly to software design and software testing.

software deployment
Is all of the activities that make a software system available for use.

software design
Is the process by which an agent creates a specification of a software artifact, intended to accomplish goals, using a set of primitive components and subject to constraints. Software design may refer to either ""all the activity involved in conceptualizing, framing, implementing, commissioning, and ultimately modifying complex systems"" or ""the activity following requirements specification and before programming, as ... [in] a stylized software engineering process.""

software development
Is the process of conceiving, specifying, designing, programming, documenting, testing, and bug fixing involved in creating and maintaining applications, frameworks, or other software components. Software development is a process of writing and maintaining the source code, but in a broader sense, it includes all that is involved between the conception of the desired software through to the final manifestation of the software, sometimes in a planned and structured process. Therefore, software development may include research, new development, prototyping, modification, reuse, re-engineering, maintenance, or any other activities that result in software products.

software development process
In software engineering, a software development process is the process of dividing software development work into distinct phases to improve design, product management, and project management.  It is also known as a software development life cycle (SDLC).  The methodology may include the pre-definition of specific deliverables and artifacts that are created and completed by a project team to develop or maintain an application.  Most modern development processes can be vaguely described as agile. Other methodologies include waterfall, prototyping, iterative and incremental development, spiral development, rapid application development, and extreme programming.

software engineering
Is the systematic application of engineering approaches to the development of software. Software engineering is a computing discipline.

software maintenance
In software engineering is the modification of a software product after delivery to correct faults, to improve performance or other attributes.

software prototyping
Is the activity of creating prototypes of software applications, i.e., incomplete versions of the software program being developed. It is an activity that can occur in software development and is comparable to prototyping as known from other fields, such as mechanical engineering or manufacturing.  A prototype typically simulates only a few aspects of, and may be completely different from, the final product. 

software requirements specification
(SRS), is a description of a software system to be  developed. The software requirements specification lays out functional and non-functional requirements, and it may include a set of use cases that describe user interactions that the software must provide to the user for perfect interaction.

software testing
Is an investigation conducted to provide stakeholders with information about the quality of the software product or service under test. Software testing can also provide an objective, independent view of the software to allow the business to appreciate and understand the risks of software implementation. Test techniques include the process of executing a program or application with the intent of finding software bugs (errors or other defects), and verifying that the software product is fit for use. 

sorting algorithm
Is an algorithm that puts elements of a list in a certain order. The most frequently used orders are numerical order and lexicographical order. Efficient sorting is important for optimizing the efficiency of other algorithms (such as search and merge algorithms) that require input data to be in sorted lists. Sorting is also often useful for canonicalizing data and for producing human-readable output. More formally, the output of any sorting algorithm must satisfy two conditions:

The output is in nondecreasing order (each element is no smaller than the previous element according to the desired total order);
The output is a permutation (a reordering, yet retaining all of the original elements) of the input.

Further, the input data is often stored in an array, which allows random access, rather than a list, which only allows sequential access; though many algorithms can be applied to either type of data after suitable modification.

source code
In computing, source code is any collection of code, with or without comments, written using a human-readable programming language, usually as plain text. The source code of a program is specially designed to facilitate the work of computer programmers, who specify the actions to be performed by a computer mostly by writing source code. The source code is often transformed by an assembler or compiler into binary machine code that can be executed by the computer. The machine code might then be stored for execution at a later time. Alternatively, source code may be interpreted and thus immediately executed.

spiral model
Is a risk-driven software development process model. Based on the unique risk patterns of a given project, the spiral model guides a team to adopt elements of one or more process models, such as incremental, waterfall, or evolutionary prototyping.

stack
Is an abstract data type that serves as a collection of elements, with two main principal operations:
push, which adds an element to the collection, and
pop, which removes the most recently added element that was not yet removed.
The order in which elements come off a stack gives rise to its alternative name, LIFO (last in, first out). Additionally, a peek operation may give access to the top without modifying the stack. The name ""stack"" for this type of structure comes from the analogy to a set of physical items stacked on top of each other. This structure makes it easy to take an item off the top of the stack, while getting to an item deeper in the stack may require taking off multiple other items first.

state
In information technology and computer science, a system is described as stateful if it is designed to remember preceding events or user interactions; the remembered information is called the state of the system.

statement
In computer programming, a statement is a syntactic unit of an imperative programming language that expresses some action to be carried out. A program written in such a language is formed by a sequence of one or more statements. A statement may have internal components (e.g., expressions).

storage
Computer data storage is a technology consisting of computer components and recording media that are used to retain digital data. It is a core function and fundamental component of computers.: 15–16 

stream
Is a sequence of data elements made available over time. A stream can be thought of as items on a conveyor belt being processed one at a time rather than in large batches.

string
In computer programming, a string is traditionally a sequence of characters, either as a literal constant or as some kind of variable. The latter may allow its elements to be mutated and the length changed, or it may be fixed (after creation). A string is generally considered as a data type and is often implemented as an array data structure of bytes (or words) that stores a sequence of elements, typically characters, using some character encoding. String may also denote more general arrays or other sequence (or list) data types and structures.

structured storage
A NoSQL (originally referring to ""non-SQL"" or ""non-relational"") database provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases. Such databases have existed since the late 1960s, but the name ""NoSQL"" was only coined in the early 21st century, triggered by the needs of Web 2.0 companies. NoSQL databases are increasingly used in big data and real-time web applications.  NoSQL systems are also sometimes called ""Not only SQL"" to emphasize that they may support SQL-like query languages or sit alongside SQL databases in polyglot-persistent architectures.

subroutine
In computer programming, a subroutine is a sequence of program instructions that performs a specific task, packaged as a unit. This unit can then be used in programs wherever that particular task should be performed.  Subroutines may be defined within programs, or separately in libraries that can be used by many programs.  In different programming languages, a subroutine may be called a routine, subprogram, function, method, or procedure. Technically, these terms all have different definitions. The generic, umbrella term  callable unit is sometimes used.

symbolic computation
In mathematics and computer science, computer algebra, also called symbolic computation or algebraic computation, is a scientific area that refers to the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects. Although computer algebra could be considered a subfield of scientific computing, they are generally considered as distinct fields because scientific computing is usually based on numerical computation with approximate floating point numbers, while symbolic computation emphasizes exact computation with expressions containing variables that have no given value and are manipulated as symbols.

syntax
The syntax of a computer language is the set of rules that defines the combinations of symbols that are considered to be correctly structured statements or expressions in that language. This applies both to programming languages, where the document represents source code, and to markup languages, where the document represents data.

syntax error
Is an error in the syntax of a sequence of characters or tokens that is intended to be written in compile-time. A program will not compile until all syntax errors are corrected. For interpreted languages, however, a syntax error may be detected during program execution, and an interpreter's error messages might not differentiate syntax errors from errors of other kinds. There is some disagreement as to just what errors are ""syntax errors"". For example, some would say that the use of an uninitialized variable's value in Java code is a syntax error, but many others would disagree and would classify this as a (static) semantic error.

system console
The system console, computer console, root console, operator's console, or simply console is the text entry and display device for system administration messages, particularly those from the BIOS or boot loader, the kernel, from the init system and from the system logger. It is a physical device consisting of a keyboard and a screen, and traditionally is a text terminal, but may also be a graphical terminal. System consoles are generalized to computer terminals, which are abstracted respectively by virtual consoles and terminal emulators. Today communication with system consoles is generally done abstractly, via the standard streams (stdin, stdout, and stderr), but there may be system-specific interfaces, for example those used by the system kernel.


== T ==
technical documentation
In engineering, any type of documentation that describes handling, functionality, and architecture of a technical product or a product under development or use. The intended recipient for product technical documentation is both the (proficient) end user as well as the administrator/service or maintenance technician. In contrast to a mere ""cookbook"" manual, technical documentation aims at providing enough information for a user to understand inner and outer dependencies of the product at hand.

third-generation programming language
A third-generation programming language (3GL) is a high-level computer programming language that tends to be more machine-independent and programmer-friendly than the machine code of the first-generation and assembly languages of the second-generation, while having a less specific focus to the fourth and fifth generations. Examples of common and historical third-generation programming languages are ALGOL, BASIC, C, COBOL, Fortran, Java, and Pascal.

top-down and bottom-up design

tree
A widely used abstract data type (ADT) that simulates a hierarchical tree structure, with a root value and subtrees of children with a parent node, represented as a set of linked nodes.

type theory
In mathematics, logic, and computer science, a type theory is any of a class of formal systems, some of which can serve as alternatives to set theory as a foundation for all mathematics. In type theory, every ""term"" has a ""type"" and operations are restricted to terms of a certain type.


== U ==
upload
In computer networks, to send data to a remote system such as a server or another client so that the remote system can store a copy. Contrast download.

Uniform Resource Locator (URL)
Colloquially web address.
A reference to a web resource that specifies its location on a computer network and a mechanism for retrieving it. A URL is a specific type of Uniform Resource Identifier (URI), although many people use the two terms interchangeably. URLs occur most commonly to reference web pages (http), but are also used for file transfer (ftp), email (mailto), database access (JDBC), and many other applications.

user
Is a person who utilizes a computer or network service. Users of computer systems and software products generally lack the technical expertise required to fully understand how they work. Power users use advanced features of programs, though they are not necessarily capable of computer programming and system administration.

user agent
Software (a software agent) that acts on behalf of a user, such as a web browser that ""retrieves, renders and facilitates end user interaction with Web content"". An email reader is a mail user agent.

user interface (UI)
The space where interactions between humans and machines occur. The goal of this interaction is to allow effective operation and control of the machine from the human end, whilst the machine simultaneously feeds back information that aids the operators' decision-making process. Examples of this broad concept of user interfaces include the interactive aspects of computer operating systems, hand tools, heavy machinery operator controls, and process controls. The design considerations applicable when creating user interfaces are related to or involve such disciplines as ergonomics and psychology.

user interface design
Also user interface engineering.
The design of user interfaces for machines and software, such as computers, home appliances, mobile devices, and other electronic devices, with the focus on maximizing usability and the user experience. The goal of user interface design is to make the user's interaction as simple and efficient as possible, in terms of accomplishing user goals (user-centered design).


== V ==
variable
In computer programming, a variable, or scalar, is a storage location (identified by a memory address) paired with an associated symbolic name (an identifier), which contains some known or unknown quantity of information referred to as a value. The variable name is the usual way to reference the stored value, in addition to referring to the variable itself, depending on the context. This separation of name and content allows the name to be used independently of the exact information it represents. The identifier in computer source code can be bound to a value during run time, and the value of the variable may therefore change during the course of program execution.

virtual machine (VM)
An emulation of a computer system. Virtual machines are based on computer architectures and attempt to provide the same functionality as a physical computer. Their implementations may involve specialized hardware, software, or a combination of both.

V-Model
A software development process that may be considered an extension of the waterfall model, and is an example of the more general V-model. Instead of moving down in a linear way, the process steps are bent upwards after the coding phase, to form the typical V shape. The V-Model demonstrates the relationships between each phase of the development life cycle and its associated phase of testing. The horizontal and vertical axes represent time or project completeness (left-to-right) and level of abstraction (coarsest-grain abstraction uppermost), respectively.


== W ==
waterfall model
A breakdown of project activities into linear sequential phases, where each phase depends on the deliverables of the previous one and corresponds to a specialisation of tasks.  The approach is typical for certain areas of engineering design. In software development, it tends to be among the less iterative and flexible approaches, as progress flows in largely one direction (""downwards"" like a waterfall) through the phases of conception, initiation, analysis, design, construction, testing, deployment and maintenance.

Waveform Audio File Format
Also WAVE or WAV due to its filename extension.
An audio file format standard, developed by Microsoft and IBM, for storing an audio bitstream on PCs. It is an application of the Resource Interchange File Format (RIFF) bitstream format method for storing data in ""chunks"", and thus is also close to the 8SVX and the AIFF format used on Amiga and Macintosh computers, respectively. It is the main format used on Microsoft Windows systems for raw and typically uncompressed audio. The usual bitstream encoding is the linear pulse-code modulation (LPCM) format.

web crawler
Also spider, spiderbot, or simply crawler.
An Internet bot that systematically browses the World Wide Web, typically for the purpose of Web indexing (web spidering).

Wi-Fi
A family of wireless networking technologies, based on the IEEE 802.11 family of standards, which are commonly used for local area networking of devices and Internet access. Wi‑Fi is a trademark of the non-profit Wi-Fi Alliance, which restricts the use of the term Wi-Fi Certified to products that successfully complete interoperability certification testing.


== X ==
XHTML
Abbreviaton of eXtensible HyperText Markup Language.
Part of the family of XML markup languages. It mirrors or extends versions of the widely used HyperText Markup Language (HTML), the language in which web pages are formulated.


== See also ==
Outline of computer science


== References ==


== Notes ==",395916,756,"All articles lacking reliable references, All articles with dead external links, All articles with unsourced statements, Articles lacking reliable references from June 2018, Articles with dead external links from March 2022, Articles with permanently dead external links, Articles with short description, Articles with unsourced statements from February 2018, CS1 German-language sources (de), CS1 errors: missing periodical, CS1 maint: multiple names: authors list, Computer science, Computers, Glossaries of computers, Glossaries of science, Harv and Sfn no-target errors, Short description matches Wikidata, Webarchive template archiveis links, Webarchive template wayback links, Wikipedia glossaries using description lists",1132222159,cs
https://en.wikipedia.org/wiki/Cohesion_(computer_science),Cohesion (computer science),"In computer programming, cohesion refers to the degree to which the elements inside a module belong together. In one sense, it is a measure of the strength of relationship between the methods and data of a class and some unifying purpose or concept served by that class. In another sense, it is a measure of the strength of relationship between the class's methods and data themselves.
Cohesion is an ordinal type of measurement and is usually described as “high cohesion” or “low cohesion”. Modules with high cohesion tend to be preferable, because high cohesion is associated with several desirable traits of software including robustness, reliability, reusability, and understandability. In contrast, low cohesion is associated with undesirable traits such as being difficult to maintain, test, reuse, or even understand.
Cohesion is often contrasted with coupling, a different concept. High cohesion often correlates with loose coupling, and vice versa. The software metrics of coupling and cohesion were invented by Larry Constantine in the late 1960s as part of Structured Design, based on characteristics of “good” programming practices that reduced maintenance and modification costs. Structured Design, cohesion and coupling were published in the article Stevens, Myers & Constantine (1974) and the book Yourdon & Constantine (1979); the latter two subsequently became standard terms in software engineering.","In computer programming, cohesion refers to the degree to which the elements inside a module belong together. In one sense, it is a measure of the strength of relationship between the methods and data of a class and some unifying purpose or concept served by that class. In another sense, it is a measure of the strength of relationship between the class's methods and data themselves.
Cohesion is an ordinal type of measurement and is usually described as “high cohesion” or “low cohesion”. Modules with high cohesion tend to be preferable, because high cohesion is associated with several desirable traits of software including robustness, reliability, reusability, and understandability. In contrast, low cohesion is associated with undesirable traits such as being difficult to maintain, test, reuse, or even understand.
Cohesion is often contrasted with coupling, a different concept. High cohesion often correlates with loose coupling, and vice versa. The software metrics of coupling and cohesion were invented by Larry Constantine in the late 1960s as part of Structured Design, based on characteristics of “good” programming practices that reduced maintenance and modification costs. Structured Design, cohesion and coupling were published in the article Stevens, Myers & Constantine (1974) and the book Yourdon & Constantine (1979); the latter two subsequently became standard terms in software engineering.


== High cohesion ==
In object-oriented programming, if the methods that serve a class tend to be similar in many aspects, then the class is said to have high cohesion. In a highly cohesive system, code readability and reusability is increased, while complexity is kept manageable.

Cohesion is increased if:

The functionalities embedded in a class, accessed through its methods, have much in common.
Methods carry out a small number of related activities, by avoiding coarsely grained or unrelated sets of data.
Related methods are in the same source file or otherwise grouped together; for example, in separate files but in the same sub-directory/folder.Advantages of high cohesion (or ""strong cohesion"") are:

Reduced module complexity (they are simpler, having fewer operations).
Increased system maintainability, because logical changes in the domain affect fewer modules, and because changes in one module require fewer changes in other modules.
Increased module reusability, because application developers will find the component they need more easily among the cohesive set of operations provided by the module.While in principle a module can have perfect cohesion by only consisting of a single, atomic element – having a single function, for example – in practice complex tasks are not expressible by a single, simple element. Thus a single-element module has an element that either is too complicated, in order to accomplish a task, or is too narrow, and thus tightly coupled to other modules. Thus cohesion is balanced with both unit complexity and coupling.


== Types of cohesion ==
Cohesion is a qualitative measure, meaning that the source code to be measured is examined using a rubric to determine a classification. Cohesion types, from the worst to the best, are as follows:

Coincidental cohesion (worst)
Coincidental cohesion is when parts of a module are grouped arbitrarily; the only relationship between the parts is that they have been grouped together (e.g., a “Utilities” class). Example:
Logical cohesion
Logical cohesion is when parts of a module are grouped because they are logically categorized to do the same thing even though they are different by nature (e.g., grouping all mouse and keyboard input handling routines or bundling all models, views, and controllers in separate folders in an MVC pattern).Temporal cohesion
Temporal cohesion is when parts of a module are grouped by when they are processed - the parts are processed at a particular time in program execution (e.g., a function which is called after catching an exception which closes open files, creates an error log, and notifies the user).Procedural cohesion
Procedural cohesion is when parts of a module are grouped because they always follow a certain sequence of execution (e.g., a function which checks file permissions and then opens the file).Communicational/informational cohesion
Communicational cohesion is when parts of a module are grouped because they operate on the same data (e.g., a module which operates on the same record of information).Sequential cohesion
Sequential cohesion is when parts of a module are grouped because the output from one part is the input to another part like an assembly line (e.g., a function which reads data from a file and processes the data).Functional cohesion (best)
Functional cohesion is when parts of a module are grouped because they all contribute to a single well-defined task of the module (e.g., Lexical analysis of an XML string). Example:
Perfect cohesion (atomic)
Example.
Although cohesion is a ranking type of scale, the ranks do not indicate a steady progression of improved cohesion. Studies by various people including Larry Constantine, Edward Yourdon, and Steve McConnell indicate that the first two types of cohesion are inferior; communicational and sequential cohesion are very good; and functional cohesion is superior.


== See also ==
Coupling (computer science)
List of object-oriented programming terms
Static code analysis


== References ==


== External links ==
Definitions of Cohesion metrics
Cohesion metrics
Measuring Cohesion in Python",916834,341,"All articles needing additional references, Articles needing additional references from October 2014, Articles with short description, Programming principles, Short description matches Wikidata, Software architecture, Software metrics",1126779758,cs
https://en.wikipedia.org/wiki/Outline_of_computer_science,Outline of computer science,"Computer science (also called computing science) is the study of the theoretical foundations of information and computation and their implementation and application in computer systems. One well known subject classification system for computer science is the ACM Computing Classification System devised by the Association for Computing Machinery.
Computer science can be described as all of the following:

Academic discipline
Science
Applied science

","Computer science (also called computing science) is the study of the theoretical foundations of information and computation and their implementation and application in computer systems. One well known subject classification system for computer science is the ACM Computing Classification System devised by the Association for Computing Machinery.
Computer science can be described as all of the following:

Academic discipline
Science
Applied science


== Subfields ==


=== Mathematical foundations ===
Coding theory – Useful in networking, programming, system development, and other areas where computers communicate with each other.
Game theory – Useful in artificial intelligence and cybernetics.
Discrete Mathematics
Graph theory – Foundations for data structures and searching algorithms.
Mathematical logic – Boolean logic and other ways of modeling logical queries; the uses and limitations of formal proof methods
Number theory – Theory of the integers.  Used in cryptography as well as a test domain in artificial intelligence.


=== Algorithms and data structures ===
Algorithms – Sequential and parallel computational procedures for solving a wide range of problems.
Data structures – The organization and manipulation of data.


=== Artificial intelligence ===
Outline of artificial intelligence

Artificial intelligence – The implementation and study of systems that exhibit an autonomous intelligence or behavior of their own.
Automated reasoning – Solving engines, such as used in Prolog, which produce steps to a result given a query on a fact and rule database, and automated theorem provers that aim to prove mathematical theorems with some assistance from a programmer.
Computer vision – Algorithms for identifying three-dimensional objects from a two-dimensional picture.
Soft computing, the use of inexact solutions for otherwise extremely difficult problems:
Machine learning - Development of models that are able to learn and adapt without following explicit instructions, by using algorithms and statistical models to analyse and draw inferences from patterns in data.
Evolutionary computing - Biologically inspired algorithms.
Natural language processing - Building systems and algorithms that analyze, understand, and generate natural (human) languages.
Robotics – Algorithms for controlling the behaviour of robots.


=== Communication and security ===
Networking – Algorithms and protocols for reliably communicating data across different shared or dedicated media, often including error correction.
Computer security – Practical aspects of securing computer systems and computer networks.
Cryptography – Applies results from complexity, probability, algebra and number theory to invent and break codes, and analyze the security of cryptographic protocols.


=== Computer architecture ===
Computer architecture – The design, organization, optimization and verification of a computer system, mostly about CPUs and Memory subsystem (and the bus connecting them).
Operating systems – Systems for managing computer programs and providing the basis of a usable system.


=== Computer graphics ===
Computer graphics – Algorithms both for generating visual images synthetically, and for integrating or altering visual and spatial information sampled from the real world.
Image processing – Determining information from an image through computation.
Information visualization – Methods for representing and displaying abstract data to facilitate human interaction for exploration and understanding.


=== Concurrent, parallel, and distributed systems ===
Parallel computing - The theory and practice of simultaneous computation; data safety in any multitasking or multithreaded environment.
Concurrency (computer science) – Computing using multiple concurrent threads of execution, devising algorithms for solving problems on multiple processors to achieve maximal speed-up compared to sequential execution.
Distributed computing – Computing using multiple computing devices over a network to accomplish a common objective or task and thereby reducing the latency involved in single processor contributions for any task.


=== Databases ===
Outline of databases

Relational databases – the set theoretic and algorithmic foundation of databases.
Structured Storage - non-relational databases such as NoSQL databases.
Data mining – Study of algorithms for searching and processing information in documents and databases; closely related to information retrieval.


=== Programming languages and compilers ===
Compiler theory – Theory of compiler design, based on Automata theory.
Programming language pragmatics – Taxonomy of programming languages, their strength and weaknesses. Various programming paradigms, such as object-oriented programming.
Programming language theory
Formal semantics – rigorous mathematical study of the meaning of programs.
Type theory – Formal analysis of the types of data, and the use of these types to understand properties of programs — especially program safety.


=== Scientific computing ===
Computational science – constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems.
Numerical analysis – Approximate numerical solution of mathematical problems such as root-finding, integration, the solution of ordinary differential equations; the approximation of special functions.
Symbolic computation – Manipulation and solution of expressions in symbolic form, also known as Computer algebra.
Computational physics – Numerical simulations of large non-analytic systems
Computational chemistry – Computational modelling of theoretical chemistry in order to determine chemical structures and properties
Bioinformatics and Computational biology – The use of computer science to maintain, analyse, store biological data and to assist in solving biological problems such as Protein folding, function prediction and Phylogeny.
Computational neuroscience – Computational modelling of neurophysiology.


=== Software engineering ===
Outline of software engineering

Formal methods – Mathematical approaches for describing and reasoning about software design.
Software engineering – The principles and practice of designing, developing, and testing programs, as well as proper engineering practices.
Algorithm design – Using ideas from algorithm theory to creatively design solutions to real tasks.
Computer programming – The practice of using a programming language to implement algorithms.
Human–computer interaction – The study and design of computer interfaces that people use.
Reverse engineering – The application of the scientific method to the understanding of arbitrary existing software.


=== Theory of computation ===

Automata theory – Different logical structures for solving problems.
Computability theory – What is calculable with the current models of computers. Proofs developed by Alan Turing and others provide insight into the possibilities of what may be computed and what may not.
List of unsolved problems in computer science
Computational complexity theory – Fundamental bounds (especially time and storage space) on classes of computations.
Quantum computing theory – Explores computational models involving quantum superposition of bits.


== History ==
History of computer science
List of pioneers in computer science


== Professions ==
Programmer (Software developer)
Teacher/Professor
Software engineer
Software architect
Software tester
Hardware engineer
Data analyst
Interaction designer
Network administrator
Data scientist


== Data and data structures ==
Data structure
Data type
Associative array and Hash table
Array
List
Tree
String
Matrix (computer science)
Database


== Programming paradigms ==
Imperative programming/Procedural programming
Functional programming
Logic programming
Object oriented programming
Class
Inheritance
Object


== See also ==
Abstraction
Big O notation
Closure
Compiler
Cognitive science


== External links ==

Outline of computer science at Curlie
ACM report on a recommended computer science curriculum (2008)
Directory of free university lectures in Computer Science
Collection of Computer Science Bibliographies
Photographs of computer scientists (Bertrand Meyer's gallery)",607857,350,"Articles with Curlie links, Articles with short description, Computer science, Computing-related lists, Outlines of sciences, Pages using Sister project links with default search, Short description is different from Wikidata, Wikipedia outlines",1133332039,cs
https://en.wikipedia.org/wiki/Consensus_(computer_science),Consensus (computer science),"A fundamental problem in distributed computing and multi-agent systems is to achieve overall system reliability in the presence of a number of faulty processes. This often requires coordinating processes to reach consensus, or agree on some data value that is needed during computation. Example applications of consensus include agreeing on what transactions to commit to a database in which order, state machine replication, and atomic broadcasts. Real-world applications often requiring consensus include cloud computing, clock synchronization, PageRank, opinion formation, smart power grids, state estimation, control of UAVs (and multiple robots/agents in general), load balancing, blockchain, and others.

","A fundamental problem in distributed computing and multi-agent systems is to achieve overall system reliability in the presence of a number of faulty processes. This often requires coordinating processes to reach consensus, or agree on some data value that is needed during computation. Example applications of consensus include agreeing on what transactions to commit to a database in which order, state machine replication, and atomic broadcasts. Real-world applications often requiring consensus include cloud computing, clock synchronization, PageRank, opinion formation, smart power grids, state estimation, control of UAVs (and multiple robots/agents in general), load balancing, blockchain, and others.


== Problem description ==
The consensus problem requires agreement among a number of processes (or agents) for a single data value. Some of the processes (agents) may fail or be unreliable in other ways, so consensus protocols must be fault tolerant or resilient. The processes must somehow put forth their candidate values, communicate with one another, and agree on a single consensus value.
The consensus problem is a fundamental problem in control of multi-agent systems. One approach to generating consensus is for all processes (agents) to agree on a majority value. In this context, a majority requires at least one more than half of available votes (where each process is given a vote). However, one or more faulty processes may skew the resultant outcome such that consensus may not be reached or reached incorrectly.
Protocols that solve consensus problems are designed to deal with limited numbers of faulty processes. These protocols must satisfy a number of requirements to be useful. For instance, a trivial protocol could have all processes output binary value 1. This is not useful and thus the requirement is modified such that the output must somehow depend on the input. That is, the output value of a consensus protocol must
be the input value of some process. Another requirement is that a process may decide upon an output value only once and this decision is irrevocable. A process is called correct in an execution if it does not experience a failure. A consensus protocol tolerating halting failures must satisfy the following properties.
Termination
Eventually, every correct process decides some value.
Integrity
If all the correct processes proposed the same value 
  
    
      
        v
      
    
    {\displaystyle v}
  , then any correct process must decide 
  
    
      
        v
      
    
    {\displaystyle v}
  .
Agreement
Every correct process must agree on the same value.Variations on the definition of integrity may be appropriate, according to the application. For example, a weaker type of integrity would be for the decision value to equal a value that some correct process proposed – not necessarily all of them. There is also a condition known as validity in the literature which refers to the property that a message sent by a process must be delivered.A protocol that can correctly guarantee consensus amongst n processes of which at most t fail is said to be t-resilient.
In evaluating the performance of consensus protocols two factors of interest are running time and message complexity. Running time is given in Big O notation in the number of rounds of message exchange as a function of some input parameters (typically the number of processes and/or the size of the input domain). Message complexity refers to the amount of message traffic that is generated by the protocol. Other factors may include memory usage and the size of messages.


== Models of computation ==
Varying models of computation may define a ""consensus problem"". Some models may deal with fully connected graphs, while others may deal with rings and trees. In some models message authentication is allowed, whereas in others processes are completely anonymous. Shared memory models in which processes communicate by accessing objects in shared memory are also an important area of research.


=== Communication channels with direct or transferable authentication ===
In most models of communication protocol participants communicate through authenticated channels. This means that messages are not anonymous, and receivers know the source of every message they receive.
Some models assume a stronger, transferable form of authentication, where each message is signed by the sender, so that a receiver knows not just the immediate source of every message, but the participant that initially created the message.
This stronger type of authentication is achieved by digital signatures, and when this stronger form of authentication is available, protocols can tolerate a larger number of faults.The two different authentication models are often called oral communication and written communication models. In an oral communication model, the immediate source of information is known, whereas in stronger, written communication models, every step along the receiver learns not just the immediate source of the message, but the communication history of the message.


=== Inputs and outputs of consensus ===
In the most traditional single-value consensus protocols such as Paxos, cooperating nodes agree on a single value such as an integer, which may be of variable size so as to encode useful metadata such as a transaction committed to a database.
A special case of the single-value consensus problem, called binary consensus, restricts the input, and hence the output domain, to a single binary digit {0,1}. While not highly useful by themselves, binary consensus protocols are often useful as building blocks in more general consensus protocols, especially for asynchronous consensus.
In multi-valued consensus protocols such as Multi-Paxos and Raft, the goal is to agree on not just a single value but a series of values over time, forming a progressively-growing history. While multi-valued consensus may be achieved naively by running multiple iterations of a single-valued consensus protocol in succession, many optimizations and other considerations such as reconfiguration support can make multi-valued consensus protocols more efficient in practice.


=== Crash and Byzantine failures ===
There are two types of failures a process may undergo, a crash failure or a Byzantine failure. A crash failure occurs when a process abruptly stops and does not resume. Byzantine failures are failures in which absolutely no conditions are imposed. For example, they may occur as a result of the malicious actions of an adversary. A process that experiences a Byzantine failure may send contradictory or conflicting data to other processes, or it may sleep and then resume activity after a lengthy delay. Of the two types of failures, Byzantine failures are far more disruptive.
Thus, a consensus protocol tolerating Byzantine failures must be resilient to every possible error that can occur.
A stronger version of consensus tolerating Byzantine failures is given by strengthening the Integrity constraint:

Integrity
If a correct process decides 
  
    
      
        v
      
    
    {\displaystyle v}
  , then 
  
    
      
        v
      
    
    {\displaystyle v}
   must have been proposed by some correct process.


=== Asynchronous and synchronous systems ===
The consensus problem may be considered in the case of asynchronous or synchronous systems. While real world communications are often inherently asynchronous, it is more practical and often easier to model synchronous systems, given that asynchronous systems naturally involve more issues than synchronous ones.
In synchronous systems, it is assumed that all communications proceed in rounds. In one round, a process may send all the messages it requires, while receiving all messages from other processes. In this manner, no message from one round may influence any messages sent within the same round.


==== The FLP impossibility result for asynchronous deterministic consensus ====
In a fully asynchronous message-passing distributed system, in which at least one process may have a crash failure, it has been proven in the famous FLP impossibility result that a deterministic algorithm for achieving consensus is impossible. This impossibility result derives from worst-case scheduling scenarios, which are unlikely to occur in practice except in adversarial situations such as an intelligent denial-of-service attacker in the network. In most normal situations, process scheduling has a degree of natural randomness.In an asynchronous model, some forms of failures can be handled by a synchronous consensus protocol. For instance, the loss of a communication link may be modeled as a process which has suffered a Byzantine failure.
Randomized consensus algorithms can circumvent the FLP impossibility result by achieving both safety and liveness with overwhelming probability, even under worst-case scheduling scenarios such as an intelligent denial-of-service attacker in the network.


=== Permissioned versus permissionless consensus ===
Consensus algorithms traditionally assume that the set of participating nodes is fixed and given at the outset: that is, that some prior (manual or automatic) configuration process has permissioned a particular known group of participants who can authenticate each other as members of the group.  In the absence of such a well-defined, closed group with authenticated members, a Sybil attack against an open consensus group can defeat even a Byzantine consensus algorithm, simply by creating enough virtual participants to overwhelm the fault tolerance threshold.
A permissionless consensus protocol, in contrast, allows anyone in the network to join dynamically and participate without prior permission, but instead imposes a different form of artificial cost or barrier to entry to mitigate the Sybil attack threat.  Bitcoin introduced the first permissionless consensus protocol using proof of work and a difficulty adjustment function, in which participants compete to solve cryptographic hash puzzles, and probabilistically earn the right to commit blocks and earn associated rewards in proportion to their invested computational effort.  Motivated in part by the high energy cost of this approach, subsequent permissionless consensus protocols have proposed or adopted other alternative participation rules for Sybil attack protection, such as proof of stake, proof of space, and proof of authority.


== Equivalency of agreement problems ==
Three agreement problems of interest are as follows.


=== Terminating Reliable Broadcast ===

A collection of 
  
    
      
        n
      
    
    {\displaystyle n}
   processes, numbered from 
  
    
      
        0
      
    
    {\displaystyle 0}
   to 
  
    
      
        n
        −
        1
        ,
      
    
    {\displaystyle n-1,}
   communicate by sending messages to one another. Process 
  
    
      
        0
      
    
    {\displaystyle 0}
   must transmit a value 
  
    
      
        v
      
    
    {\displaystyle v}
   to all processes such that:

if process 
  
    
      
        0
      
    
    {\displaystyle 0}
   is correct, then every correct process receives 
  
    
      
        v
      
    
    {\displaystyle v}
  
for any two correct processes, each process receives the same value.It is also known as The General's Problem.


=== Consensus ===
Formal requirements for a consensus protocol may include:

Agreement: All correct processes must agree on the same value.
Weak validity: For each correct process, its output must be the input of some correct process.
Strong validity: If all correct processes receive the same input value, then they must all output that value.
Termination: All processes must eventually decide on an output value


=== Weak Interactive Consistency ===
For n processes in a partially synchronous system (the system alternates between good and bad periods of synchrony), each process chooses a private value. The processes communicate with each other by rounds to determine a public value and generate a
consensus vector with the following requirements:
if a correct process sends 
  
    
      
        v
      
    
    {\displaystyle v}
  , then all correct processes receive either 
  
    
      
        v
      
    
    {\displaystyle v}
   or nothing (integrity property)
all messages sent in a round by a correct process are received in the same round by all correct processes (consistency property).It can be shown that variations of these problems are equivalent in that the solution for a problem in one type of model may be the solution for another problem in another type of model. For example, a solution to the Weak Byzantine General problem in a synchronous authenticated message passing model leads to a solution for Weak Interactive Consistency. An interactive consistency algorithm can solve the consensus problem by having each process choose the majority value in its consensus vector as its consensus value.


== Solvability results for some agreement problems ==
There is a t-resilient anonymous synchronous protocol which solves the Byzantine Generals problem, if 
  
    
      
        
          
            
              t
              n
            
          
        
        <
        
          
            
              1
              3
            
          
        
      
    
    {\displaystyle {\tfrac {t}{n}}<{\tfrac {1}{3}}}
    and the Weak Byzantine Generals case where 
  
    
      
        t
      
    
    {\displaystyle t}
   is the number of failures and 
  
    
      
        n
      
    
    {\displaystyle n}
   is the number of processes.
For systems with 
  
    
      
        n
      
    
    {\displaystyle n}
   processors, of which 
  
    
      
        f
      
    
    {\displaystyle f}
   are Byzantine, it has been shown that there exists no algorithm that solves the consensus problem for 
  
    
      
        n
        ≤
        3
        f
      
    
    {\displaystyle n\leq 3f}
   in the oral-messages model. The proof is constructed by first showing the impossibility for the three-node case 
  
    
      
        n
        =
        3
      
    
    {\displaystyle n=3}
   and using this result to argue about partitions of processors.  In the written-messages model there are protocols that can tolerate 
  
    
      
        n
        =
        f
        +
        1
      
    
    {\displaystyle n=f+1}
  .In a fully asynchronous system there is no consensus solution that can tolerate one or more crash failures even when only requiring the non triviality property. This result is sometimes called the FLP impossibility proof named after the authors Michael J. Fischer, Nancy Lynch, and Mike Paterson who were awarded a Dijkstra Prize for this significant work. The FLP result has been mechanically verified to hold even under fairness assumptions.   However, FLP does not state that consensus can never be reached: merely that under the model's assumptions, no algorithm can always reach consensus in bounded time. In practice it is highly unlikely to occur.


== Some consensus protocols ==
The Paxos consensus algorithm by Leslie Lamport, and variants of it such as Raft, are used pervasively in widely deployed distributed and cloud computing systems.  These algorithms are typically synchronous, dependent on an elected leader to make progress, and tolerate only crashes and not Byzantine failures.
An example of a polynomial time binary consensus protocol that tolerates Byzantine failures is the Phase King algorithm by Garay and Berman. The algorithm solves consensus in a synchronous message passing model with n processes and up to f failures, provided n > 4f.
In the phase king algorithm, there are f + 1 phases, with 2 rounds per phase.
Each process keeps track of its preferred output (initially equal to the process's own input value).  In the first round of each phase each process broadcasts its own preferred value to all other processes. It then receives the values from all processes and determines which value is the majority value and its count. In the second round of the phase, the process whose id matches the current phase number is designated the king of the phase. The king broadcasts the majority value it observed in the first round and serves as a tie breaker.  Each process then updates its preferred value as follows.  If the count of the majority value the process observed in the first round is greater than n/2 + f, the process changes its preference to that majority value; otherwise it uses the phase king's value. At the end of f + 1 phases the processes output their preferred values.
Google has implemented a distributed lock service library called Chubby.  Chubby maintains lock information in small files which are stored in a replicated database to achieve high availability in the face of failures. The database is implemented on top of a fault-tolerant log layer which is based on the Paxos consensus algorithm. In this scheme, Chubby clients communicate with the Paxos master in order to access/update the replicated log; i.e., read/write to the files.Many peer-to-peer online Real-time strategy games use a modified Lockstep protocol as a consensus protocol in order to manage game state between players in a game.  Each game action results in a game state delta broadcast to all other players in the game along with a hash of the total game state.  Each player validates the change by applying the delta to their own game state and comparing the game state hashes.  If the hashes do not agree then a vote is cast, and those players whose game state is in the minority are disconnected and removed from the game (known as a desync.)
Another well-known approach is called MSR-type algorithms which have been used widely from computer science to control theory.


=== [Permissionless consensus protocols] ===
Bitcoin uses proof of work, a difficulty adjustment function and a reorganization function to achieve permissionless consensus in its open peer-to-peer network. To extend Bitcoin's blockchain or distributed ledger, miners attempt to solve a cryptographic puzzle, where probability of finding a solution is proportional to the computational effort expended in hashes per second. The node that first solves such a puzzle has their proposed version of the next block of transactions added to the ledger and eventually accepted by all other nodes. As any node in the network can attempt to solve the proof-of-work problem, a Sybil attack is infeasible in principle unless the attacker has over 50% of the computational resources of the network.
Other cryptocurrencies (i.e. NEO, STRATIS, ...) use proof of stake, in which nodes compete to append blocks and earn associated rewards in proportion to stake, or existing cryptocurrency allocated and locked or staked for some time period. One advantage of a 'proof of stake' over a 'proof of work' system, is the high energy consumption demanded by the latter. As an example, Bitcoin mining (2018) is estimated to consume non-renewable energy sources at an amount similar to the entire nations of Czech Republic or Jordan.Some cryptocurrencies, such as Ripple, use a system of validating nodes to validate the ledger.
This system used by Ripple, called Ripple Protocol Consensus Algorithm (RPCA), works in rounds: 
Step 1: every server compiles a list of valid candidate transactions;
Step 2: each server amalgamates all candidates coming from its Unique Nodes List (UNL) and votes on their veracity;
Step 3: transactions passing the minimum threshold are passed to the next round;
Step 4: the final round requires 80% agreementOther participation rules used in permissionless consensus protocols to impose barriers to entry and resist sybil attacks include proof of authority, proof of space, proof of burn, or proof of elapsed time. These alternatives are again largely motivated by the high amount of computational energy consumed by the proof of work. Proof of space is used by cryptocoins such as Burstcoin.
Contrasting with the above permissionless participation rules, all of which reward participants in proportion to amount of investment in some action or resource, proof of personhood protocols aim to give each real human participant exactly one unit of voting power in permissionless consensus, regardless of economic investment.  Proposed approaches to achieving one-per-person distribution of consensus power for proof of personhood include physical pseudonym parties, social networks, pseudonymized government-issued identities, and biometrics.


== Consensus number ==
To solve the consensus problem in a shared-memory system, concurrent objects must be introduced. A concurrent object, or shared object, is a data structure which helps concurrent processes communicate to reach an agreement. Traditional implementations using critical sections face the risk of crashing if some process dies inside the critical section or sleeps for an intolerably long time. Researchers defined wait-freedom as the guarantee that the algorithm completes in a finite number of steps.
The consensus number of a concurrent object is defined to be the maximum number of processes in the system which can reach consensus by the given object in a wait-free implementation. Objects with a consensus number of 
  
    
      
        n
      
    
    {\displaystyle n}
   can implement any object with a consensus number of 
  
    
      
        n
      
    
    {\displaystyle n}
   or lower, but cannot implement any objects with a higher consensus number. The consensus numbers form what is called Herlihy's hierarchy of synchronization objects.
According to the hierarchy, read/write registers cannot solve consensus even in a 2-process system. Data structures like stacks and queues can only solve consensus between two processes. However, some concurrent objects are universal (notated in the table with 
  
    
      
        ∞
      
    
    {\displaystyle \infty }
  ), which means they can solve consensus among any number of processes and they can simulate any other objects through an operation sequence.


== See also ==
Uniform consensus
Quantum Byzantine agreement
Byzantine fault tolerance


== References ==


== Further reading ==
Herlihy, M.; Shavit, N. (1999). ""The topological structure of asynchronous computability"". Journal of the ACM. 46 (6): 858. CiteSeerX 10.1.1.78.1455. doi:10.1145/331524.331529. S2CID 5797174.
Saks, M.; Zaharoglou, F. (2000). ""Wait-Free k-Set Agreement is Impossible: The Topology of Public Knowledge"". SIAM Journal on Computing. 29 (5): 1449–1483. doi:10.1137/S0097539796307698.
Bashir, Imran. ""Blockchain Consensus."" Blockchain Consensus - An Introduction to Classical, Blockchain, and Quantum Consensus Protocols. ISBN 978-1-4842-8178-9 Apress, Berkeley, CA, 2022. doi:10.1007/978-1-4842-8179-6",687076,295,"Articles with short description, CS1 maint: uses authors parameter, Distributed computing problems, Fault-tolerant computer systems, Short description matches Wikidata",1133986723,cs
https://en.wikipedia.org/wiki/Scope_(computer_science),Scope (computer science),"In computer programming, the scope of a name binding (an association of a name to an entity, such as a variable) is the part of a program where the name binding is valid; that is, where the name can be used to refer to the entity. In other parts of the program, the name may refer to a different entity (it may have a different binding), or to nothing at all (it may be unbound). Scope helps prevent name collisions by allowing the same name to refer to different objects – as long as the names have separate scopes. The scope of a name binding is also known as the visibility of an entity, particularly in older or more technical literature—this is from the perspective of the referenced entity, not the referencing name.
The term ""scope"" is also used to refer to the set of all name bindings that are valid within a part of a program or at a given point in a program, which is more correctly referred to as context or environment.Strictly speaking and in practice for most programming languages, ""part of a program"" refers to a portion of source code (area of text), and is known as lexical scope. In some languages, however, ""part of a program"" refers to a portion of run time (time period during execution), and is known as dynamic scope. Both of these terms are somewhat misleading—they misuse technical terms, as discussed in the definition—but the distinction itself is accurate and precise, and these are the standard respective terms. Lexical scope is the main focus of this article, with dynamic scope understood by contrast with lexical scope.
In most cases, name resolution based on lexical scope is relatively straightforward to use and to implement, as in use one can read backwards in the source code to determine to which entity a name refers, and in implementation one can maintain a list of names and contexts when compiling or interpreting a program. Difficulties arise in name masking, forward declarations, and hoisting, while considerably subtler ones arise with non-local variables, particularly in closures.","In computer programming, the scope of a name binding (an association of a name to an entity, such as a variable) is the part of a program where the name binding is valid; that is, where the name can be used to refer to the entity. In other parts of the program, the name may refer to a different entity (it may have a different binding), or to nothing at all (it may be unbound). Scope helps prevent name collisions by allowing the same name to refer to different objects – as long as the names have separate scopes. The scope of a name binding is also known as the visibility of an entity, particularly in older or more technical literature—this is from the perspective of the referenced entity, not the referencing name.
The term ""scope"" is also used to refer to the set of all name bindings that are valid within a part of a program or at a given point in a program, which is more correctly referred to as context or environment.Strictly speaking and in practice for most programming languages, ""part of a program"" refers to a portion of source code (area of text), and is known as lexical scope. In some languages, however, ""part of a program"" refers to a portion of run time (time period during execution), and is known as dynamic scope. Both of these terms are somewhat misleading—they misuse technical terms, as discussed in the definition—but the distinction itself is accurate and precise, and these are the standard respective terms. Lexical scope is the main focus of this article, with dynamic scope understood by contrast with lexical scope.
In most cases, name resolution based on lexical scope is relatively straightforward to use and to implement, as in use one can read backwards in the source code to determine to which entity a name refers, and in implementation one can maintain a list of names and contexts when compiling or interpreting a program. Difficulties arise in name masking, forward declarations, and hoisting, while considerably subtler ones arise with non-local variables, particularly in closures.


== Definition ==
The strict definition of the (lexical) ""scope"" of a name (identifier) is unambiguous: lexical scope is ""the portion of source code in which a binding of a name with an entity applies"". This is virtually unchanged from its 1960 definition in the specification of ALGOL 60. Representative language specifications follow:

ALGOL 60 (1960)
The following kinds of quantities are distinguished: simple variables, arrays, labels, switches, and procedures. The scope of a quantity is the set of statements and expressions in which the declaration of the identifier associated with that quantity is valid.
C (2007)
An identifier can denote an object; a function; a tag or a member of a structure, union, or enumeration; a typedef name; a label name; a macro name; or a macro parameter. The same identifier can denote different entities at different points in the program. [...] For each different entity that an identifier designates, the identifier is visible (i.e., can be used) only within a region of program text called its scope.
Go (2013)
A declaration binds a non-blank identifier to a constant, type, variable, function, label, or package. [...] The scope of a declared identifier is the extent of source text in which the identifier denotes the specified constant, type, variable, function, label, or package.Most commonly ""scope"" refers to when a given name can refer to a given variable—when a declaration has effect—but can also apply to other entities, such as functions, types, classes, labels, constants, and enumerations.


=== Lexical scope vs. dynamic scope ===
A fundamental distinction in scope is what ""part of a program"" means. In languages with lexical scope (also called static scope), name resolution depends on the location in the source code and the lexical context (also called static context), which is defined by where the named variable or function is defined. In contrast, in languages with dynamic scope the name resolution depends upon the program state when the name is encountered which is determined by the execution context (also called runtime context, calling context or dynamic context). In practice, with lexical scope a name is resolved by searching the local lexical context, then if that fails, by searching the outer lexical context, and so on; whereas with dynamic scope, a name is resolved by searching the local execution context, then if that fails, by searching the outer execution context, and so on, progressing up the call stack.Most modern languages use lexical scope for variables and functions, though dynamic scope is used in some languages, notably some dialects of Lisp, some ""scripting"" languages, and some template languages.  Perl 5 offers both lexical and dynamic scope. Even in lexically scoped languages, scope for closures can be confusing to the uninitiated, as these depend on the lexical context where the closure is defined, not where it is called.
Lexical resolution can be determined at compile time, and is also known as early binding, while dynamic resolution can in general only be determined at run time, and thus is known as late binding.


=== Related concepts ===
In object-oriented programming, dynamic dispatch selects an object method at runtime, though whether the actual name binding is done at compile time or run time depends on the language. De facto dynamic scope is common in macro languages, which do not directly do name resolution, but instead expand in place.
Some programming frameworks like AngularJS use the term ""scope"" to mean something entirely different than how it is used in this article. In those frameworks the scope is just an object of the programming language that they use (JavaScript in case of AngularJS) that is used in certain ways by the framework to emulate dynamic scope in a language that uses lexical scope for its variables. Those AngularJS scopes can themselves be in context or not in context (using the usual meaning of the term) in any given part of the program, following the usual rules of variable scope of the language like any other object, and using their own inheritance and transclusion rules. In the context of AngularJS, sometimes the term ""$scope"" (with a dollar sign) is used to avoid confusion, but using the dollar sign in variable names is often discouraged by the style guides.


== Use ==
Scope is an important component of name resolution, which is in turn fundamental to language semantics. Name resolution (including scope) varies between programming languages, and within a programming language, varies by type of entity; the rules for scope are called scope rules (or scoping rules). Together with namespaces, scope rules are crucial in modular programming, so a change in one part of the program does not break an unrelated part.


== Overview ==

When discussing scope, there are three basic concepts: scope, extent, and context. ""Scope"" and ""context"" in particular are frequently confused: scope is a property of a name binding, while context is a property of a part of a program, that is either a portion of source code (lexical context or static context) or a portion of run time (execution context, runtime context, calling context or dynamic context). Execution context consists of lexical context (at the current execution point) plus additional runtime state such as the call stack. Strictly speaking, during execution a program enters and exits various name bindings' scopes, and at a point in execution name bindings are ""in context"" or ""not in context"", hence name bindings ""come into context"" or ""go out of context"" as the program execution enters or exits the scope. However, in practice usage is much looser.
Scope is a source-code level concept, and a property of name bindings, particularly variable or function name bindings—names in the source code are references to entities in the program—and is part of the behavior of a compiler or interpreter of a language. As such, issues of scope are similar to pointers, which are a type of reference used in programs more generally. Using the value of a variable when the name is in context but the variable is uninitialized is analogous to dereferencing (accessing the value of) a wild pointer, as it is undefined. However, as variables are not destroyed until they go out of context, the analog of a dangling pointer does not exist.
For entities such as variables, scope is a subset of lifetime (also known as extent)—a name can only refer to a variable that exists (possibly with undefined value), but variables that exist are not necessarily visible: a variable may exist but be inaccessible (the value is stored but not referred to within a given context), or accessible but not via the given name, in which case it is not in context (the program is ""out of the scope of the name""). In other cases ""lifetime"" is irrelevant—a label (named position in the source code) has lifetime identical with the program (for statically compiled languages), but may be in context or not at a given point in the program, and likewise for static variables—a static global variable is in context for the entire program, while a static local variable is only in context within a function or other local context, but both have lifetime of the entire run of the program.
Determining which entity a name refers to is known as name resolution or name binding (particularly in object-oriented programming), and varies between languages. Given a name, the language (properly, the compiler or interpreter) checks all entities that are in context for matches; in case of ambiguity (two entities with the same name, such as a global and local variable with the same name), the name resolution rules are used to distinguish them. Most frequently, name resolution relies on an ""inner-to-outer context"" rule, such as the Python LEGB (Local, Enclosing, Global, Built-in) rule: names implicitly resolves to the narrowest relevant context. In some cases name resolution can be explicitly specified, such as by the global and nonlocal keywords in Python; in other cases the default rules cannot be overridden.
When two identical names are in context at the same time, referring to different entities, one says that name masking is occurring, where the higher-priority name (usually innermost) is ""masking"" the lower-priority name. At the level of variables, this is known as variable shadowing. Due to the potential for logic errors from masking, some languages disallow or discourage masking, raising an error or warning at compile time or run time.
Various programming languages have various different scope rules for different kinds of declarations and names. Such scope rules have a large effect on language semantics and, consequently, on the behavior and correctness of programs. In languages like C++, accessing an unbound variable does not have well-defined semantics and may result in undefined behavior, similar to referring to a dangling pointer; and declarations or names used outside their scope will generate syntax errors.
Scopes are frequently tied to other language constructs and determined implicitly, but many languages also offer constructs specifically for controlling scope.


== Levels of scope ==
Scope can vary from as little as a single expression to as much as the entire program, with many possible gradations in between. The simplest scope rule is global scope—all entities are visible throughout the entire program. The most basic modular scope rule is two-level scope, with a global scope anywhere in the program, and local scope within a function. More sophisticated modular programming allows a separate module scope, where names are visible within the module (private to the module) but not visible outside it. Within a function, some languages, such as C, allow block scope to restrict scope to a subset of a function; others, notably functional languages, allow expression scope, to restrict scope to a single expression. Other scopes include file scope (notably in C) which behaves similarly to module scope, and block scope outside of functions (notably in Perl).
A subtle issue is exactly when a scope begins and ends. In some languages, such as C, a name's scope begins at the name declaration, and thus different names declared within a given block can have different scopes. This requires declaring functions before use, though not necessarily defining them, and requires forward declaration in some cases, notably for mutual recursion. In other languages, such as Python, a name's scope begins at the start of the relevant block where the name is declared (such as the start of a function), regardless of where it is defined, so all names within a given block have the same scope. In JavaScript, the scope of a name declared with let or const begins at the name declaration, and the scope of a name declared with var begins at the start of the function where the name is declared, which is known as variable hoisting. Behavior of names in context that have undefined value differs: in Python use of undefined names yields a runtime error, while in JavaScript undefined names declared with var are usable throughout the function because they are implicitly bound to the value undefined.


=== Expression scope ===
The scope of a name binding is an expression, which is known as expression scope. Expression scope is available in many languages, especially functional languages which offer a feature called let-expressions allowing a declaration's scope to be a single expression. This is convenient if, for example, an intermediate value is needed for a computation. For example, in Standard ML, if f() returns 12, then let val x = f() in x * x end is an expression that evaluates to 144, using a temporary variable named x to avoid calling f() twice. Some languages with block scope approximate this functionality by offering syntax for a block to be embedded into an expression; for example, the aforementioned Standard ML expression could be written in Perl as do { my $x = f(); $x * $x }, or in GNU C as ({ int x = f(); x * x; }).
In Python, auxiliary variables in generator expressions and list comprehensions (in Python 3) have expression scope.
In C, variable names in a function prototype have expression scope, known in this context as function protocol scope. As the variable names in the prototype are not referred to (they may be different in the actual definition)—they are just dummies—these are often omitted, though they may be used for generating documentation, for instance.


=== Block scope ===
The scope of a name binding is a block, which is known as block scope. Block scope is available in many, but not all, block-structured programming languages. This began with ALGOL 60, where ""[e]very declaration ... is valid only for that block."", and today is particularly associated with languages in the Pascal and C families and traditions. Most often this block is contained within a function, thus restricting the scope to a part of a function, but in some cases, such as Perl, the block may not be within a function.

A representative example of the use of block scope is the C code shown here, where two variables are scoped to the loop: the loop variable n, which is initialized once and incremented on each iteration of the loop, and the auxiliary variable n_squared, which is initialized at each iteration. The purpose is to avoid adding variables to the function scope that are only relevant to a particular block—for example, this prevents errors where the generic loop variable i has accidentally already been set to another value. In this example the expression n * n would generally not be assigned to an auxiliary variable, and the body of the loop would simply be written ret += n * n but in more complicated examples auxiliary variables are useful.
Blocks are primarily used for control flow, such as with if, while, and for loops, and in these cases block scope means the scope of variable depends on the structure of a function's flow of execution. However, languages with block scope typically also allow the use of ""naked"" blocks, whose sole purpose is to allow fine-grained control of variable scope. For example, an auxiliary variable may be defined in a block, then used (say, added to a variable with function scope) and discarded when the block ends, or a while loop might be enclosed in a block that initializes variables used inside the loop that should only be initialized once.
A subtlety of several programming languages, such as Algol 68 and C (demonstrated in this example and standardized since C99), is that block-scope variables can be declared not only within the body of the block, but also within the control statement, if any. This is analogous to function parameters, which are declared in the function declaration (before the block of the function body starts), and in scope for the whole function body. This is primarily used in for loops, which have an initialization statement separate from the loop condition, unlike while loops, and is a common idiom.
Block scope can be used for shadowing. In this example, inside the block the auxiliary variable could also have been called n, shadowing the parameter name, but this is considered poor style due to the potential for errors. Furthermore, some descendants of C, such as Java and C#, despite having support for block scope (in that a local variable can be made to go out of context before the end of a function), do not allow one local variable to hide another. In such languages, the attempted declaration of the second n would result in a syntax error, and one of the n variables would have to be renamed.
If a block is used to set the value of a variable, block scope requires that the variable be declared outside of the block. This complicates the use of conditional statements with single assignment. For example, in Python, which does not use block scope, one may initialize a variable as such:

where a is accessible after the if statement.
In Perl, which has block scope, this instead requires declaring the variable prior to the block:

Often this is instead rewritten using multiple assignment, initializing the variable to a default value. In Python (where it is not necessary) this would be:

while in Perl this would be:

In case of a single variable assignment, an alternative is to use the ternary operator to avoid a block, but this is not in general possible for multiple variable assignments, and is difficult to read for complex logic.
This is a more significant issue in C, notably for string assignment, as string initialization can automatically allocate memory, while string assignment to an already initialized variable requires allocating memory, a string copy, and checking that these are successful.

Some languages allow the concept of block scope to be applied, to varying extents, outside of a function. For example, in the Perl snippet at right, $counter is a variable name with block scope (due to the use of the my keyword), while increment_counter is a function name with global scope. Each call to increment_counter will increase the value of $counter by one, and return the new value. Code outside of this block can call increment_counter, but cannot otherwise obtain or alter the value of $counter. This idiom allows one to define closures in Perl.


=== Function scope ===
When the scope of variables declared within a function does not extend beyond that function, this is known as function scope. Function scope is available in most programming languages which offer a way to create a local variable in a function or subroutine: a variable whose scope ends (that goes out of context) when the function returns. In most cases the lifetime of the variable is the duration of the function call—it is an automatic variable, created when the function starts (or the variable is declared), destroyed when the function returns—while the scope of the variable is within the function, though the meaning of ""within"" depends on whether scope is lexical or dynamic. However, some languages, such as C, also provide for static local variables, where the lifetime of the variable is the entire lifetime of the program, but the variable is only in context when inside the function. In the case of static local variables, the variable is created when the program initializes, and destroyed only when the program terminates, as with a static global variable, but is only in context within a function, like an automatic local variable.
Importantly, in lexical scope a variable with function scope has scope only within the lexical context of the function: it goes out of context when another function is called within the function, and comes back into context when the function returns—called functions have no access to the local variables of calling functions, and local variables are only in context within the body of the function in which they are declared. By contrast, in dynamic scope, the scope extends to the execution context of the function: local variables stay in context when another function is called, only going out of context when the defining function ends, and thus local variables are in context of the function in which they are defined and all called functions. In languages with lexical scope and nested functions, local variables are in context for nested functions, since these are within the same lexical context, but not for other functions that are not lexically nested. A local variable of an enclosing function is known as a non-local variable for the nested function. Function scope is also applicable to anonymous functions.

For example, in the snippet of Python code on the right, two functions are defined: square and sum_of_squares. square computes the square of a number; sum_of_squares computes the sum of all squares up to a number. (For example, square(4) is 42 = 16, and sum_of_squares(4) is 02 + 12 + 22 + 32 + 42 = 30.)
Each of these functions has a variable named n that represents the argument to the function. These two n variables are completely separate and unrelated, despite having the same name, because they are lexically scoped local variables with function scope: each one's scope is its own, lexically separate function and thus, they don't overlap. Therefore, sum_of_squares can call square without its own n being altered. Similarly, sum_of_squares has variables named total and i; these variables, because of their limited scope, will not interfere with any variables named total or i that might belong to any other function. In other words, there is no risk of a name collision between these names and any unrelated names, even if they are identical.
No name masking is occurring: only one variable named n is in context at any given time, as the scopes do not overlap. By contrast, were a similar fragment to be written in a language with dynamic scope, the n in the calling function would remain in context in the called function—the scopes would overlap—and would be masked (""shadowed"") by the new n in the called function.
Function scope is significantly more complicated if functions are first-class objects and can be created locally to a function and then returned. In this case any variables in the nested function that are not local to it (unbound variables in the function definition, that resolve to variables in an enclosing context) create a closure, as not only the function itself, but also its context (of variables) must be returned, and then potentially called in a different context. This requires significantly more support from the compiler, and can complicate program analysis.


=== File scope ===
The scope of a name binding is a file, which is known as file scope. File scope is largely particular to C (and C++), where scope of variables and functions declared at the top level of a file (not within any function) is for the entire file—or rather for C, from the declaration until the end of the source file, or more precisely translation unit (internal linking). This can be seen as a form of module scope, where modules are identified with files, and in more modern languages is replaced by an explicit module scope. Due to the presence of include statements, which add variables and functions to the internal context and may themselves call further include statements, it can be difficult to determine what is in context in the body of a file.
In the C code snippet above, the function name sum_of_squares has file scope.


=== Module scope ===
The scope of a name binding is a module, which is known as module scope. Module scope is available in modular programming languages where modules (which may span various files) are the basic unit of a complex program, as they allow information hiding and exposing a limited interface. Module scope was pioneered in the Modula family of languages, and Python (which was influenced by Modula) is a representative contemporary example.
In some object-oriented programming languages that lack direct support for modules, such as C++, a similar structure is instead provided by the class hierarchy, where classes are the basic unit of the program, and a class can have private methods. This is properly understood in the context of dynamic dispatch rather than name resolution and scope, though they often play analogous roles. In some cases both these facilities are available, such as in Python, which has both modules and classes, and code organization (as a module-level function or a conventionally private method) is a choice of the programmer.


=== Global scope ===
The scope of a name binding is an entire program, which is known as global scope. Variable names with global scope—called global variables—are frequently considered bad practice, at least in some languages, due to the possibility of name collisions and unintentional masking, together with poor modularity, and function scope or block scope are considered preferable. However, global scope is typically used (depending on the language) for various other sorts of names, such as names of functions, names of classes and names of other data types. In these cases mechanisms such as namespaces are used to avoid collisions.


== Lexical scope vs. dynamic scope ==
The use of local variables — of variable names with limited scope, that only exist within a specific function — helps avoid the risk of a name collision between two identically named variables. However, there are two very different approaches to answering this question: What does it mean to be ""within"" a function?
In lexical scope (or lexical scoping; also called static scope or static scoping), if a variable name's scope is a certain function, then its scope is the program text of the function definition: within that text, the variable name exists, and is bound to the variable's value, but outside that text, the variable name does not exist. By contrast, in dynamic scope (or dynamic scoping), if a variable name's scope is a certain function, then its scope is the time-period during which the function is executing: while the function is running, the variable name exists, and is bound to its value, but after the function returns, the variable name does not exist. This means that if function f invokes a separately defined function g, then under lexical scope, function g does not have access to f's local variables (assuming the text of g is not inside the text of f), while under dynamic scope, function g does have access to f's local variables (since g is invoked during the invocation of f).

Consider, for example, the program on the right. The first line, x=1, creates a global variable x and initializes it to 1. The second line, function g() { echo $x ; x=2 ; }, defines a function g that prints out (""echoes"") the current value of x, and then sets x to 2 (overwriting the previous value). The third line, function f() { local x=3 ; g ; } defines a function f that creates a local variable x (hiding the identically named global variable) and initializes it to 3, and then calls g. The fourth line, f, calls f. The fifth line, echo $x, prints out the current value of x.

So, what exactly does this program print? It depends on the scope rules. If the language of this program is one that uses lexical scope, then g prints and modifies the global variable x (because g is defined outside f), so the program prints 1 and then 2. By contrast, if this language uses dynamic scope, then g prints and modifies f's local variable x (because g is called from within f), so the program prints 3 and then 1. (As it happens, the language of the program is Bash, which uses dynamic scope; so the program prints 3 and then 1. If the same code was run with ksh93 which uses lexical scope, the results would be different.)


== Lexical scope ==
With lexical scope, a name always refers to its lexical context. This is a property of the program text and is made independent of the runtime call stack by the language implementation. Because this matching only requires analysis of the static program text, this type of scope is also called static scope. Lexical scope is standard in all ALGOL-based languages such as Pascal, Modula-2 and Ada as well as in modern functional languages such as ML and Haskell. It is also used in the C language and its syntactic and semantic relatives, although with different kinds of limitations. Static scope allows the programmer to reason about object references such as parameters, variables, constants, types, functions, etc. as simple name substitutions. This makes it much easier to make modular code and reason about it, since the local naming structure can be understood in isolation. In contrast, dynamic scope forces the programmer to anticipate all possible execution contexts in which the module's code may be invoked.

For example, Pascal is lexically scoped. Consider the Pascal program fragment at right. The variable I is visible at all points, because it is never hidden by another variable of the same name. The char variable K is visible only in the main program because it is hidden by the real variable K visible in procedure B and C only. Variable L is also visible only in procedure B and C but it does not hide any other variable. Variable M is only visible in procedure C and therefore not accessible either from procedure B or the main program. Also, procedure C is visible only in procedure B and can therefore not be called from the main program.
There could have been another procedure C declared in the program outside of procedure B. The place in the program where ""C"" is mentioned then determines which of the two procedures named C it represents, thus precisely analogous with the scope of variables.
Correct implementation of lexical scope in languages with first-class nested functions is not trivial, as it requires each function value to carry with it a record of the values of the variables that it depends on (the pair of the function and this context is called a closure). Depending on implementation and computer architecture, variable lookup may become slightly inefficient when very deeply lexically nested functions are used, although there are well-known techniques to mitigate this. Also, for nested functions that only refer to their own arguments and (immediately) local variables, all relative locations can be known at compile time. No overhead at all is therefore incurred when using that type of nested function. The same applies to particular parts of a program where nested functions are not used, and, naturally, to programs written in a language where nested functions are not available (such as in the C language).


=== History ===
Lexical scope was first used in the early 1960s for the imperative language ALGOL 60 and has been picked up in most other imperative languages since then.Languages like Pascal and C have always had lexical scope, since they are both influenced by the ideas that went into ALGOL 60 and ALGOL 68 (although C did not include lexically nested functions).
Perl is a language with dynamic scope that added static scope afterwards.
The original Lisp interpreter (1960) used dynamic scope. Deep binding, which approximates static (lexical) scope, was introduced around 1962 in LISP 1.5 (via the Funarg device developed by Steve Russell, working under John McCarthy).
All early Lisps used dynamic scope, when based on interpreters. In 1982, Guy L. Steele Jr. and the Common LISP Group publish An overview of Common LISP, a short review of the history and the divergent implementations of Lisp up to that moment and a review of the features that a Common Lisp implementation should have. On page 102, we read:

Most LISP implementations are internally inconsistent in that by default the interpreter and compiler may assign different semantics to correct programs; this stems primarily from the fact that the interpreter assumes all variables to be dynamically scoped, while the compiler assumes all variables to be local unless forced to assume otherwise. This has been done for the sake of convenience and efficiency, but can lead to very subtle bugs. The definition of Common LISP avoids such anomalies by explicitly requiring the interpreter and compiler to impose identical semantics on correct programs.
Implementations of Common LISP were thus required to have lexical scope. Again, from An overview of Common LISP:

In addition, Common LISP offers the following facilities (most of which are borrowed from MacLisp, InterLisp or Lisp Machines Lisp): (...) Fully lexically scoped variables. The so-called ""FUNARG problem"" is completely solved, in both the downward and upward cases.
By the same year in which An overview of Common LISP was published (1982), initial designs (also by Guy L. Steele Jr.) of a compiled, lexically scoped Lisp, called Scheme had been published and compiler implementations were being attempted. At that time, lexical scope in Lisp was commonly feared to be inefficient to implement. In A History of T, Olin Shivers writes:

All serious Lisps in production use at that time were dynamically scoped. No one who hadn't carefully read the Rabbit thesis (written by Guy Lewis Steele Jr. in 1978) believed lexical scope would fly; even the few people who had read it were taking a bit of a leap of faith that this was going to work in serious production use.
The term ""lexical scope"" dates at least to 1967, while the term ""lexical scoping"" dates at least to 1970, where it was used in Project MAC to describe the scope rules of the Lisp dialect MDL (then known as ""Muddle"").


== Dynamic scope ==
With dynamic scope, a name refers to execution context.  In technical terms, this means that each name has a global stack of bindings. Introducing a local variable with name x pushes a binding onto the global x stack (which may have been empty), which is popped off when the control flow leaves the scope. Evaluating x in any context always yields the top binding. Note that this cannot be done at compile-time because the binding stack only exists at run-time, which is why this type of scope is called dynamic scope.
Dynamic scope is uncommon in modern languages.Generally, certain blocks are defined to create bindings whose lifetime is the execution time of the block; this adds some features of static scope to the dynamic scope process. However, since a section of code can be called from many different locations and situations, it can be difficult to determine at the outset what bindings will apply when a variable is used (or if one exists at all). This can be beneficial; application of the principle of least knowledge suggests that code avoid depending on the reasons for (or circumstances of) a variable's value, but simply use the value according to the variable's definition. This narrow interpretation of shared data can provide a very flexible system for adapting the behavior of a function to the current state (or policy) of the system. However, this benefit relies on careful documentation of all variables used this way as well as on careful avoidance of assumptions about a variable's behavior, and does not provide any mechanism to detect interference between different parts of a program. Some languages, like Perl and Common Lisp, allow the programmer to choose static or dynamic scope when defining or redefining a variable. Examples of languages that use dynamic scope include Logo, Emacs Lisp, LaTeX and the shell languages bash, dash, and PowerShell.
Dynamic scope is fairly easy to implement. To find an name's value, the program could traverse the runtime stack, checking each activation record (each function's stack frame) for a value for the name. In practice, this is made more efficient via the use of an association list, which is a stack of name/value pairs. Pairs are pushed onto this stack whenever declarations are made, and popped whenever variables go out of context. Shallow binding is an alternative strategy that is considerably faster, making use of a central reference table, which associates each name with its own stack of meanings. This avoids a linear search during run-time to find a particular name, but care should be taken to properly maintain this table. Note that both of these strategies assume a last-in-first-out (LIFO) ordering to bindings for any one variable; in practice all bindings are so ordered.
An even simpler implementation is the representation of dynamic variables with simple global variables. The local binding is performed by saving the original value in an anonymous location on the stack that is invisible to the program. When that binding scope terminates, the original value is restored from this location. In fact, dynamic scope originated in this manner. Early implementations of Lisp used this obvious strategy for implementing local variables, and the practice survives in some dialects which are still in use, such as GNU Emacs Lisp. Lexical scope was introduced into Lisp later. This is equivalent to the above shallow binding scheme, except that the central reference table is simply the global variable binding context, in which the current meaning of the variable is its global value. Maintaining global variables isn't complex. For instance, a symbol object can have a dedicated slot for its global value.
Dynamic scope provides an excellent abstraction for thread-local storage, but if it is used that way it cannot be based on saving and restoring a global variable. A possible implementation strategy is for each variable to have a thread-local key. When the variable is accessed, the thread-local key is used to access the thread-local memory location (by code generated by the compiler, which knows which variables are dynamic and which are lexical). If the thread-local key does not exist for the calling thread, then the global location is used. When a variable is locally bound, the prior value is stored in a hidden location on the stack. The thread-local storage is created under the variable's key, and the new value is stored there. Further nested overrides of the variable within that thread simply save and restore this thread-local location. When the initial, outermost override's context terminates, the thread-local key is deleted, exposing the global version of the variable once again to that thread.
With referential transparency the dynamic scope is restricted to the argument stack of the current function only, and coincides with the lexical scope.


=== Macro expansion ===

In modern languages, macro expansion in a preprocessor is a key example of de facto dynamic scope. The macro language itself only transforms the source code, without resolving names, but since the expansion is done in place, when the names in the expanded text are then resolved (notably free variables), they are resolved based on where they are expanded (loosely ""called""), as if dynamic scope were occurring.
The C preprocessor, used for macro expansion, has de facto dynamic scope, as it does not do name resolution by itself and it is independent of where the macro is defined. For example, the macro:

will expand to add a to the passed variable, with this name only later resolved by the compiler based on where the macro ADD_A is ""called"" (properly, expanded). Properly, the C preprocessor only does lexical analysis, expanding the macro during the tokenization stage, but not parsing into a syntax tree or doing name resolution.
For example, in the following code, the name a in the macro is resolved (after expansion) to the local variable at the expansion site:


== Qualified names ==
As we have seen, one of the key reasons for scope is that it helps prevent name collisions, by allowing identical names to refer to distinct things, with the restriction that the names must have separate scopes. Sometimes this restriction is inconvenient; when many different things need to be accessible throughout a program, they generally all need names with global scope, so different techniques are required to avoid name collisions.
To address this, many languages offer mechanisms for organizing global names. The details of these mechanisms, and the terms used, depend on the language; but the general idea is that a group of names can itself be given a name — a prefix — and, when necessary, an entity can be referred to by a qualified name consisting of the name plus the prefix. Normally such names will have, in a sense, two sets of scopes: a scope (usually the global scope) in which the qualified name is visible, and one or more narrower scopes in which the unqualified name (without the prefix) is visible as well. And normally these groups can themselves be organized into groups; that is, they can be nested.
Although many languages support this concept, the details vary greatly. Some languages have mechanisms, such as namespaces in C++ and C#, that serve almost exclusively to enable global names to be organized into groups. Other languages have mechanisms, such as packages in Ada and structures in Standard ML, that combine this with the additional purpose of allowing some names to be visible only to other members of their group. And object-oriented languages often allow classes or singleton objects to fulfill this purpose (whether or not they also have a mechanism for which this is the primary purpose). Furthermore, languages often meld these approaches; for example, Perl's packages are largely similar to C++'s namespaces, but optionally double as classes for object-oriented programming; and Java organizes its variables and functions into classes, but then organizes those classes into Ada-like packages.


== By language ==
Scope rules for representative languages follow.


=== C ===

In C, scope is traditionally known as linkage or visibility, particularly for variables. C is a lexically scoped language with global scope (known as external linkage), a form of module scope or file scope (known as internal linkage), and local scope (within a function); within a function scopes can further be nested via block scope. However, standard C does not support nested functions.
The lifetime and visibility of a variable are determined by its storage class. There are three types of lifetimes in C: static (program execution), automatic (block execution, allocated on the stack), and manual (allocated on the heap). Only static and automatic are supported for variables and handled by the compiler, while manually allocated memory must be tracked manually across different variables. There are three levels of visibility in C: external linkage (global), internal linkage (roughly file), and block scope (which includes functions); block scopes can be nested, and different levels of internal linkage is possible by use of includes. Internal linkage in C is visibility at the translation unit level, namely a source file after being processed by the C preprocessor, notably including all relevant includes.
C programs are compiled as separate object files, which are then linked into an executable or library via a linker. Thus name resolution is split across the compiler, which resolves names within a translation unit (more loosely, ""compilation unit"", but this is properly a different concept), and the linker, which resolves names across translation units; see linkage for further discussion.
In C, variables with block scope enter context when they are declared (not at the top of the block), go out of context if any (non-nested) function is called within the block, come back into context when the function returns, and go out of context at the end of the block. In the case of automatic local variables, they are also allocated on declaration and deallocated at the end of the block, while for static local variables, they are allocated at program initialization and deallocated at program termination.
The following program demonstrates a variable with block scope coming into context partway through the block, then exiting context (and in fact being deallocated) when the block ends:

The program outputs:

m
m
b
m

There are other levels of scope in C. Variable names used in a function prototype have function prototype visibility, and exit context at the end of the function prototype. Since the name is not used, this is not useful for compilation, but may be useful for documentation. Label names for GOTO statement have function scope, while case label names for switch statements have block scope (the block of the switch).


=== C++ ===
All the variables that we intend to use in a program must have been declared with its type specifier in an earlier
point in the code, like we did in the previous code at the beginning of the body of the function main when we
declared that a, b, and result were of type int.
A variable can be either of global or local scope. A global variable is a variable declared in the main body of the
source code, outside all functions, while a local variable is one declared within the body of a function or a block.
Modern versions allow nested lexical scope.


=== Swift ===
Swift has a similar rule for scopes with C++, but contains different access modifiers.


=== Go ===
Go is lexically scoped using blocks.


=== Java ===
Java is lexically scoped.
A Java class can contain three types of variables:
Local variables
are defined inside a method, or a particular block. These variables are local to where they were defined and lower levels. For example, a loop inside a method can use that method's local variables, but not the other way around. The loop's variables (local to that loop) are destroyed as soon as the loop ends.Member variables
also called fields are variables declared within the class, outside of any method. By default, these variables are available for all methods within that class and also for all classes in the package.Parameters
are variables in method declarations.In general, a set of brackets defines a particular scope, but variables at top level within a class can differ in their behavior depending on the modifier keywords used in their definition.
The following table shows the access to members permitted by each modifier.


=== JavaScript ===
JavaScript has simple scope rules, but variable initialization and name resolution rules can cause problems, and the widespread use of closures for callbacks means the lexical context of a function when defined (which is used for name resolution) can be very different from the lexical context when it is called (which is irrelevant for name resolution). JavaScript objects have name resolution for properties, but this is a separate topic.
JavaScript has lexical scope  nested at the function level, with the global context being the outermost context. This scope is used for both variables and for functions (meaning function declarations, as opposed to variables of function type). Block scope with the let and const keywords is standard since ECMAScript 6. Block scope can be produced by wrapping the entire block in a function and then executing it; this is known as the immediately-invoked function expression (IIFE) pattern.
While JavaScript scope is simple—lexical, function-level—the associated initialization and name resolution rules are a cause of confusion. Firstly, assignment to a name not in scope defaults to creating a new global variable, not a local one. Secondly, to create a new local variable one must use the var keyword; the variable is then created at the top of the function, with value undefined and the variable is assigned its value when the assignment expression is reached:

A variable with an Initialiser is assigned the value of its AssignmentExpression when the VariableStatement is executed, not when the variable is created.This is known as variable hoisting—the declaration, but not the initialization, is hoisted to the top of the function. Thirdly, accessing variables before initialization yields undefined, rather than a syntax error. Fourthly, for function declarations, the declaration and the initialization are both hoisted to the top of the function, unlike for variable initialization. For example, the following code produces a dialog with output undefined, as the local variable declaration is hoisted, shadowing the global variable, but the initialization is not, so the variable is undefined when used:

Further, as functions are first-class objects in JavaScript and are frequently assigned as callbacks or returned from functions, when a function is executed, the name resolution depends on where it was originally defined (the lexical context of the definition), not the lexical context or execution context where it is called. The nested scopes of a particular function (from most global to most local) in JavaScript, particularly of a closure, used as a callback, are sometimes referred to as the scope chain, by analogy with the prototype chain of an object.
Closures can be produced in JavaScript by using nested functions, as functions are first-class objects. Returning a nested function from an enclosing function includes the local variables of the enclosing function as the (non-local) lexical context of the returned function, yielding a closure. For example:

Closures are frequently used in JavaScript, due to being used for callbacks. Indeed, any hooking of a function in the local context as a callback or returning it from a function creates a closure if there are any unbound variables in the function body (with the context of the closure based on the nested scopes of the current lexical context, or ""scope chain""); this may be accidental. When creating a callback based on parameters, the parameters must be stored in a closure, otherwise it will accidentally create a closure that refers to the variables in the enclosing context, which may change.Name resolution of properties of JavaScript objects is based on inheritance in the prototype tree—a path to the root in the tree is called a prototype chain—and is separate from name resolution of variables and functions.


=== Lisp ===
Lisp dialects have various rules for scope.
The original Lisp used dynamic scope; it was Scheme, inspired by ALGOL, that introduced static (lexical) scope to the Lisp family.
Maclisp used dynamic scope by default in the interpreter and lexical scope by default in compiled code, though compiled code could access dynamic bindings by use of SPECIAL declarations for particular variables. However, Maclisp treated lexical binding more as an optimization than one would expect in modern languages, and it did not come with the closure feature one might expect of lexical scope in modern Lisps. A separate operation, *FUNCTION, was available to somewhat clumsily work around some of that issue.Common Lisp adopted lexical scope from Scheme, as did Clojure.
ISLISP has lexical scope for ordinary variables. It also has dynamic variables, but they are in all cases explicitly marked; they must be defined by a defdynamic special form, bound by a dynamic-let special form, and accessed by an explicit dynamic special form.Some other dialects of Lisp, like Emacs Lisp, still use dynamic scope by default. Emacs Lisp now has lexical scope available on a per-buffer basis.


=== Python ===
For variables, Python has function scope, module scope, and global scope. Names enter context at the start of a scope (function, module, or global scope), and exit context when a non-nested function is called or the scope ends. If a name is used prior to variable initialization, this raises a runtime exception. If a variable is simply accessed (not assigned to), name resolution follows the LEGB (Local, Enclosing, Global, Built-in) rule which resolves names to the narrowest relevant context. However, if a variable is assigned to, it defaults to declaring a variable whose scope starts at the start of the level (function, module, or global), not at the assignment. Both these rules can be overridden with a global or nonlocal (in Python 3) declaration prior to use, which allows accessing global variables even if there is a masking nonlocal variable, and assigning to global or nonlocal variables.
As a simple example, a function resolves a variable to the global scope:

Note that x is defined before f is called, so no error is raised, even though it is defined after its reference in the definition of f. Lexically this is a forward reference, which is allowed in Python.
Here assignment creates a new local variable, which does not change the value of the global variable:

Assignment to a variable within a function causes it to be declared local to the function, hence its scope is the entire function, and thus using it prior to this assignment raises an error. This differs from C, where the scope of the local variable start at its declaration. This code raises an error:

The default name resolution rules can be overridden with the global or nonlocal (in Python 3) keywords. In the below code, the global x declaration in g means that x resolves to the global variable. It thus can be accessed (as it has already been defined), and assignment assigns to the global variable, rather than declaring a new local variable. Note that no global declaration is needed in f—since it does not assign to the variable, it defaults to resolving to the global variable.

global can also be used for nested functions. In addition to allowing assignment to a global variable, as in an unnested function, this can also be used to access the global variable in the presence of a nonlocal variable:

For nested functions, there is also the nonlocal declaration, for assigning to a nonlocal variable, similar to using global in an unnested function:


=== R ===
R is a lexically scoped language, unlike other implementations of S where the values of free variables are determined by a set of global variables, while in R they are determined by the context in which the function was created. The scope contexts may be accessed using a variety of features (such as parent.frame()) which can simulate the experience of dynamic scope should the programmer desire.
There is no block scope:

Functions have access to scope they were created in:

Variables created or modified within a function stay there:

Variables created or modified within a function stay there unless assignment to enclosing scope is explicitly requested:

Although R has lexical scope by default, function scopes can be changed:


== See also ==
Closure (computer science)
Global variable
Local variable
Let expression
Non-local variable
Name binding
Name resolution (programming languages)
Variables (scope and extent)
Information hiding
Immediately-invoked function expressions in Javascript
Object lifetime


== Notes ==


== References ==",869127,700,"All articles needing additional references, All articles to be expanded, All articles with unsourced statements, Articles needing additional references from December 2008, Articles to be expanded from April 2013, Articles using small message boxes, Articles with example R code, Articles with short description, Articles with unsourced statements from June 2012, Articles with unsourced statements from March 2022, CS1 errors: missing periodical, Programming language concepts, Short description is different from Wikidata",1105603666,cs
https://en.wikipedia.org/wiki/Robustness_(computer_science),Robustness (computer science),"In computer science, robustness is the ability of a computer system to cope with errors during execution and cope with erroneous input. Robustness can encompass many areas of computer science, such as robust programming, robust machine learning, and Robust Security Network. Formal techniques, such as fuzz testing, are essential to showing robustness since this type of testing involves invalid or unexpected inputs. Alternatively, fault injection can be used to test robustness. Various commercial products perform robustness testing of software analysis.","In computer science, robustness is the ability of a computer system to cope with errors during execution and cope with erroneous input. Robustness can encompass many areas of computer science, such as robust programming, robust machine learning, and Robust Security Network. Formal techniques, such as fuzz testing, are essential to showing robustness since this type of testing involves invalid or unexpected inputs. Alternatively, fault injection can be used to test robustness. Various commercial products perform robustness testing of software analysis.


== Introduction ==
In general, building robust systems that encompass every point of possible failure is difficult because of the vast quantity of possible inputs and input combinations. Since all inputs and input combinations would require too much time to test, developers cannot run through all cases exhaustively. Instead, the developer will try to generalize such cases. For example, imagine inputting some integer values. Some selected inputs might consist of a negative number, zero, and a positive number. When using these numbers to test software in this way, the developer generalizes the set of all reals into three numbers. This is a more efficient and manageable method, but more prone to failure. Generalizing test cases is an example of just one technique to deal with failure—specifically, failure due to invalid user input. Systems generally may also fail due to other reasons as well, such as disconnecting from a network.
Regardless, complex systems should still handle any errors encountered gracefully. There are many examples of such successful systems. Some of the most robust systems are evolvable and can be easily adapted to new situations.


== Challenges ==
Programs and software are tools focused on a very specific task, and thus aren't generalized and flexible. However, observations in systems such as the internet or biological systems demonstrate adaptation to their environments. One of the ways biological systems adapt to environments is through the use of redundancy. Many organs are redundant in humans. The kidney is one such example. Humans generally only need one kidney, but having a second kidney allows room for failure. This same principle may be taken to apply to software, but there are some challenges.
When applying the principle of redundancy to computer science, blindly adding code is not suggested. Blindly adding code introduces more errors, makes the system more complex, and renders it harder to understand. Code that doesn't provide any reinforcement to the already existing code is unwanted. The new code must instead possess equivalent functionality, so that if a function is broken, another providing the same function can replace it, using manual or automated software diversity. To do so, the new code must know how and when to accommodate the failure point. This means more logic needs to be added to the system. But as a system adds more logic, components, and increases in size, it becomes more complex. Thus, when making a more redundant system, the system also becomes more complex and developers must consider balancing redundancy with complexity.
Currently, computer science practices do not focus on building robust systems. Rather, they tend to focus on scalability and efficiency. One of the main reasons why there is no focus on robustness today is because it is hard to do in a general way.


== Areas ==


=== Robust programming ===
Robust programming is a style of programming that focuses on handling unexpected termination and unexpected actions. It requires code to handle these terminations and actions gracefully by displaying accurate and unambiguous error messages. These error messages allow the user to more easily debug the program.


==== Principles ====
Paranoia
When building software, the programmer assumes users are out to break their code. The programmer also assumes that their own written code may fail or work incorrectly.Stupidity
The programmer assumes users will try incorrect, bogus and malformed inputs. As a consequence, the programmer returns to the user an unambiguous, intuitive error message that does not require looking up error codes. The error message should try to be as accurate as possible without being misleading to the user, so that the problem can be fixed with ease.Dangerous implements
Users should not gain access to libraries, data structures, or pointers to data structures. This information should be hidden from the user so that the user doesn't accidentally modify them and introduce a bug in the code. When such interfaces are correctly built, users use them without finding loopholes to modify the interface. The interface should already be correctly implemented, so the user does not need to make modifications. The user therefore focuses solely on their own code.Can't happen
Very often, code is modified and may introduce a possibility that an ""impossible"" case occurs. Impossible cases are therefore assumed to be highly unlikely instead. The developer thinks about how to handle the case that is highly unlikely, and implements the handling accordingly.


=== Robust machine learning ===
Robust machine learning typically refers to the robustness of machine learning algorithms. For a machine learning algorithm to be considered robust, either the testing error has to be consistent with the training error, or the performance is stable after adding some noise to the dataset. Recently, consistently with their rise in popularity, there has been an increasing interest in the robustness of neural networks. This is particularly due their vulnerability to adverserial attacks.


=== Robust network design ===
Robust network design is the study of network design in the face of variable or uncertain demands. In a sense, robustness in network design is broad just like robustness in software design because of the vast possibilities of changes or inputs.


=== Robust algorithms ===
There exists algorithms that tolerate errors in the input or during the computation. In that case, the computation eventually converges to the correct output. This phenomenon has been called ""correctness attraction"".


== See also ==
Defensive programming
Non-functional requirement


== References ==",436472,130,"Articles with short description, Reliability engineering, Short description is different from Wikidata, Software quality",1132265059,cs
https://en.wikipedia.org/wiki/Node_(computer_science),Node (computer science),"A node is a basic unit of a data structure, such as a linked list or tree data structure. Nodes contain data and also may link to other nodes. Links between nodes are often implemented by pointers.","A node is a basic unit of a data structure, such as a linked list or tree data structure. Nodes contain data and also may link to other nodes. Links between nodes are often implemented by pointers.


== Nodes and Trees ==

Nodes are often arranged into tree structures. A node represents the information contained in a single data structure. These nodes may contain a value or condition, or possibly serve as another independent data structure. Nodes are represented by a single parent node. The highest point on a tree structure is called a root node, which does not have a parent node, but serves as the parent or 'grandparent' of all of the nodes below it in the tree. The height of a node is determined by the total number of edges on the path from that node to the furthest leaf node, and the height of the tree is equal to the height of the root node. Node depth is determined by the distance between that particular node and the root node. The root node is said to have a depth of zero. Data can be discovered along these network paths.
An IP address uses this kind of system of nodes to define its location in a network.


=== Definitions ===
Child: A child node is a node extending from another node. For example, a computer with internet access could be considered a child node of a node representing the internet. The inverse relationship is that of a parent node. If node C is a child of node A, then A is the parent node of C.
Degree: the degree of a node is the number of children of the node.
Depth: the depth of node A is the length of the path from A to the root node. The root node is said to have depth 0.
Edge: the connection between nodes.
Forest: a set of trees.
Height: the height of node A is the length of the longest path through children to a leaf node.
Internal node: a node with at least one child.
Leaf node: a node with no children.
Root node: a node distinguished from the rest of the tree nodes. Usually, it is depicted as the highest node of the tree.
Sibling nodes: these are nodes connected to the same parent node.


== Markup languages ==
Another common use of node trees is in web development. In programming, XML is used to communicate information between computer programmers and computers alike. For this reason XML is used to create common communication protocols used in office productivity software, and serves as the base for the development of modern web markup languages like XHTML. Though similar in how it is approached by a programmer, HTML and CSS is typically the language used to develop website text and design. While XML, HTML and XHTML provide the language and expression, the DOM serves as a translator.


=== Node type ===
Different types of nodes in a tree are represented by specific interfaces. In other words, the node type is defined by how it communicates with other nodes. Each node has a node type property, which specifies the type of node, such as sibling or leaf.
For example, if the node type property is the constant properties for a node, this property specifies the type of the node. So if a node type property is the constant node ELEMENT_NODE, one can know that this node object is an object Element. This object uses the Element interface to define all the methods and properties of that particular node.
Different W3C World Wide Web Consortium node types and descriptions:

Document represents the entire document (the root-node of the DOM tree)
DocumentFragment represents a ""lightweight"" Document object, which can hold a portion of a document
DocumentType provides an interface to the entities defined for the document
ProcessingInstruction represents a processing instruction
EntityReference represents an entity reference
Element 	represents an element
Attr represents an attribute
Text represents textual content in an element or attribute
CDATASection represents a CDATA section in a document (text that will NOT be parsed by a parser)
Comment represents a comment
Entity represents an entity
Notation represents a notation declared in the DTD


=== Node object ===
A node object is represented by a single node in a tree. It can be an element node, attribute node, text node, or any type that is described in section ""node type"". All objects can inherit properties and methods for dealing with parent and child nodes, but not all of the objects have parent or child nodes. For example, with text nodes that cannot have child nodes, trying to add child nodes results in a DOM error.
Objects in the DOM tree may be addressed and manipulated by using methods on the objects. The public interface of a DOM is specified in its application programming interface (API). The history of the Document Object Model is intertwined with the history of the ""browser wars"" of the late 1990s between Netscape Navigator and Microsoft Internet Explorer, as well as with that of JavaScript and JScript,  the first scripting languages to be widely implemented in the layout engines of web browsers.


== See also ==
Vertex (graph theory)


== References ==


== External links ==
Data Trees as a Means of Presenting Complex Data Analysis by Sally Knipe
STL-like C++ tree class
Description of tree data structures from ideainfo.8m.com
WormWeb.org: Interactive Visualization of the C. elegans Cell Tree - Visualize the entire cell lineage tree of the nematode C. elegans (javascript)",658228,183,"Articles with short description, Graph data structures, Linked lists, Short description is different from Wikidata",1112075409,cs
https://en.wikipedia.org/wiki/Marshalling_(computer_science),Marshalling (computer science),"In computer science, marshalling or marshaling (US spelling) is the process of transforming the memory representation of an object into a data format suitable for storage or transmission. It is typically used when data must be moved between different parts of a computer program or from one program to another. 
Marshalling can be somewhat similar to or synonymous with serialization. Marshalling is describing an intent or process to transfer some object from a client to server, intent is to have the same object that is present in one running program, to be present in another running program, i.e. object on a client to be transferred to and present on the server. Serialization does not necessarily have this intent since it is only concerned about transforming data into a, for example, stream of bytes. One could say that marshalling might be done in some other way from serialization, but some form of serialization is usually used.It simplifies complex communications, because it uses composite objects in order to communicate instead of primitive objects. The inverse process of marshalling is called unmarshalling (or demarshalling, similar to deserialization). An unmarshalling interface takes a serialized object and transforms it into an internal data structure.
The accurate definition of marshalling differs across programming languages such as Python, Java, and .NET, and in some contexts, is used interchangeably with serialization.","In computer science, marshalling or marshaling (US spelling) is the process of transforming the memory representation of an object into a data format suitable for storage or transmission. It is typically used when data must be moved between different parts of a computer program or from one program to another. 
Marshalling can be somewhat similar to or synonymous with serialization. Marshalling is describing an intent or process to transfer some object from a client to server, intent is to have the same object that is present in one running program, to be present in another running program, i.e. object on a client to be transferred to and present on the server. Serialization does not necessarily have this intent since it is only concerned about transforming data into a, for example, stream of bytes. One could say that marshalling might be done in some other way from serialization, but some form of serialization is usually used.It simplifies complex communications, because it uses composite objects in order to communicate instead of primitive objects. The inverse process of marshalling is called unmarshalling (or demarshalling, similar to deserialization). An unmarshalling interface takes a serialized object and transforms it into an internal data structure.
The accurate definition of marshalling differs across programming languages such as Python, Java, and .NET, and in some contexts, is used interchangeably with serialization.


== Comparison with serialization ==
To ""serialize"" an object means to convert its state into a byte stream in such a way that the byte stream can be converted back into a copy of the object.
The term ""marshal"" is used for a specific type of ""serialization"" in the Python standard library – storing internal python objects:

The marshal module exists mainly to support reading and writing the “pseudo-compiled” code for Python modules of .pyc files. 
…

If you’re serializing and de-serializing Python objects, use the pickle module instead
In the Java-related RFC 2713, marshalling is used when serialising objects for remote invocation. An object that is marshalled records the state of the original object and it contains the codebase (codebase here refers to a list of URLs where the object code can be loaded from, and not source code). Hence, in order to convert the object state and codebase(s), unmarshalling must be done. The unmarshaller interface automatically converts the marshalled data containing codebase(s) into an executable Java object in JAXB. Any object that can be deserialized can be unmarshalled. However, the converse need not be true.

To ""marshal"" an object means to record its state and codebase(s) in such a way that when the marshalled object is ""unmarshalled,"" a copy of the original object is obtained, possibly by automatically loading the class definitions of the object.  You can marshal any object that is serializable or remote (that is, implements the java.rmi.Remote interface).  Marshalling is like serialization, except marshalling also records codebases. Marshalling is different from serialization in that marshalling treats remote objects specially.
…

Any object whose methods can be invoked [on an object in another Java virtual machine] must implement the java.rmi.Remote interface.  When such an object is invoked, its arguments are marshalled and sent from the local virtual machine to the remote one, where the arguments are unmarshalled and used.
In Microsoft .NET, marshalling is also used to refer to serialization when using remote calls:

When you marshal an object by value, a copy of the object is created and serialized to the server. Any method calls made on that object are done on the server


== Usage ==
Marshalling is used within implementations of different remote procedure call (RPC) mechanisms, where it is necessary to transport data between processes and/or between threads. In Microsoft's Component Object Model (COM), interface pointers must be marshalled when crossing COM apartment boundaries. In the .NET Framework, the conversion between an unmanaged type and a CLR type, as in the P/Invoke process, is also an example of an action that requires marshalling to take place.Additionally, marshalling is used extensively within scripts and applications that use the XPCOM technologies provided within the Mozilla application framework. The Mozilla Firefox browser is a popular application built with this framework, that additionally allows scripting languages to use XPCOM through XPConnect (Cross-Platform Connect).


=== Example ===
In the Microsoft Windows family of operating systems the entire set of device drivers for Direct3D are kernel-mode drivers. The user-mode portion of the API is handled by the DirectX runtime provided by Microsoft.
This is an issue because calling kernel-mode operations from user-mode requires performing a system call, and this inevitably forces the CPU to switch to ""kernel mode"". This is a slow operation, taking on the order of microseconds to complete. During this time, the CPU is unable to perform any operations. As such, minimizing the number of times this switching operation must be performed would optimize performance to a substantive degree.
Linux OpenGL drivers are split in two: a kernel-driver and a user-space driver. The user-space driver does all the translation of OpenGL commands into machine code to be submitted to the GPU. To reduce the number of system calls, the user-space driver implements marshalling. If the GPU's command buffer is full of rendering data, the API could simply store the requested rendering call in a temporary buffer and, when the command buffer is close to being empty, it can perform a switch to kernel-mode and add a number of stored commands all at once.


== Formats ==
XML is one means of transferring data between systems. Microsoft, for example, uses it as the basis of the file formats of the various components (Word, Excel, Access, PowerPoint, etc.) of the Microsoft Office suite (see Office Open XML). While this typically results in a verbose wire format, XML's fully-bracketed ""start-tag"", ""end-tag"" syntax allows provision of more accurate diagnostics and easier recovery from transmission or disk errors. In addition, because the tags occur repeatedly, one can use standard compression methods to shrink the content—all the Office file formats are created by zipping the raw XML. Alternative formats such as JSON (JavaScript Object Notation) are more concise, but correspondingly less robust for error recovery. 
Once the data is transferred to a program or an application, it needs to be converted back to an object for usage. Hence, unmarshalling is generally used in the receiver end of the implementations of Remote Method Invocation (RMI) and Remote procedure call (RPC) mechanisms to unmarshal transmitted objects in an executable form.


=== JAXB ===
JAXB or Java Architecture for XML Binding is the most common framework used by developers to marshal and unmarshal Java objects. JAXB provides for the interconversion between fundamental data types supported by Java and standard XML schema data types.


=== XmlSerializer ===
XmlSerializer is the framework used by C# developers to marshal and unmarshal C# objects. One of the advantages of C# over Java is that C# natively supports marshalling due to the inclusion of XmlSerializer class. Java, on the other hand requires a non-native glue code in the form of JAXB to support marshalling.


=== XML and executable representation ===
An example of unmarshalling is the conversion of an XML representation of an object to the default representation of the object in any programming language. Consider the following class.
XML representation of Student object:
Executable representation of Student object:
Unmarshalling converts the XML representation of Code Snippet 1 to the default executable Java representation of Code Snippet 2.


== Unmarshalling in Java ==


=== Unmarshaller in JAXB ===
The process of unmarshalling XML data into an executable Java object is taken care of by the in-built Unmarshaller class. The unmarshal methods defined in the Unmarshaller class are overloaded to accept XML from different types of input such as a File, FileInputStream, or URL. For example:


=== Unmarshalling XML Data ===
Unmarshal methods can deserialize an entire XML document or a small part of it. When the XML root element is globally declared, these methods utilize the JAXBContext's mapping of XML root elements to JAXB mapped classes to initiate the unmarshalling. If the mappings are not sufficient and the root elements are declared locally, the unmarshal methods use declaredType methods for the unmarshalling process. These two approaches can be understood below.


==== Unmarshal a global XML root element ====
The unmarshal method uses JAXBContext to unmarshal the XML data, when the root element is globally declared. The JAXBContext object always maintains a mapping of the globally declared XML element and its name to a JAXB mapped class. If the XML element name or its @xsi:type attribute matches the JAXB mapped class, the unmarshal method transforms the XML data using the appropriate JAXB mapped class. However, if the XML element name has no match, the unmarshal process will abort and throw an UnmarshalException. This can be avoided by using the unmarshal by declaredType methods.


==== Unmarshal a local XML root element ====
When the root element is not declared globally, the application assists the unmarshaller by application-provided mapping using declaredType parameters. By an order of precedence, even if the root name has a mapping to an appropriate JAXB class, the declaredType overrides the mapping. However, if the @xsi:type attribute of the XML data has a mapping to an appropriate JAXB class, then this takes precedence over declaredType parameter. The unmarshal methods by declaredType parameters always return a JAXBElement<declaredType> instance. The properties of this JAXBElement instance are set as follows:


== See also ==

Free and open-source graphics device driver#Software architecture
Component Object Model
CORBA
Pickle (Python)
Protocol Buffers
Java Architecture for XML Binding


== References ==",705361,185,"All articles with unsourced statements, Articles with unsourced statements from December 2018, CS1 errors: external links, Persistence, Remote procedure call",1134388870,cs
https://en.wikipedia.org/wiki/Computer_science_education,Computer science education,"Computer science education  or computing education is the art of teaching and learning the discipline of computer science, and computational thinking. As a subdiscipline of pedagogy it also addresses the wider impact of computer science in society through its intersection with philosophy, psychology, linguistics, natural sciences, and mathematics. In comparison to  science education and mathematics education, computer science (CS) education is a much younger field. In the history of computing, digital computers were only built from around the 1940s – although computation has been around for centuries since the invention of analog computers.Another differentiator of computer science education is that it has primarily only been taught at university level until recently, with some notable exceptions in Israel, Poland and the United Kingdom with the BBC Micro in the 1980s as part of Computer science education in the United Kingdom. Computer science has been a part of the school curricula from age 14 or age 16 in a few countries for a few decades, but has typically as an elective subject.
Primary and secondary computer science education is relatively new in the United States with many K-12 CS teachers facing obstacles to integrating CS instruction such as professional isolation, limited CS professional development resources, and low levels of CS teaching self-efficacy. According to a 2021 report, only 51% of high schools in the US offer computer science.
Elementary CS teachers in particular have lower CS teaching efficacy and have fewer chances to implement CS into their instruction than their middle and high school peers. Connecting CS teachers to resources and peers using methods such as Virtual Communities of Practice has been shown to help CS and STEM teachers improve their teaching self-efficacy and implement CS topics into student instruction.","Computer science education  or computing education is the art of teaching and learning the discipline of computer science, and computational thinking. As a subdiscipline of pedagogy it also addresses the wider impact of computer science in society through its intersection with philosophy, psychology, linguistics, natural sciences, and mathematics. In comparison to  science education and mathematics education, computer science (CS) education is a much younger field. In the history of computing, digital computers were only built from around the 1940s – although computation has been around for centuries since the invention of analog computers.Another differentiator of computer science education is that it has primarily only been taught at university level until recently, with some notable exceptions in Israel, Poland and the United Kingdom with the BBC Micro in the 1980s as part of Computer science education in the United Kingdom. Computer science has been a part of the school curricula from age 14 or age 16 in a few countries for a few decades, but has typically as an elective subject.
Primary and secondary computer science education is relatively new in the United States with many K-12 CS teachers facing obstacles to integrating CS instruction such as professional isolation, limited CS professional development resources, and low levels of CS teaching self-efficacy. According to a 2021 report, only 51% of high schools in the US offer computer science.
Elementary CS teachers in particular have lower CS teaching efficacy and have fewer chances to implement CS into their instruction than their middle and high school peers. Connecting CS teachers to resources and peers using methods such as Virtual Communities of Practice has been shown to help CS and STEM teachers improve their teaching self-efficacy and implement CS topics into student instruction. 


== Computing education research ==
Educational research on computing and teaching methods in computer science is usually known as Computing Education Research. The Association for Computing Machinery (ACM) runs a Special Interest Group (SIG) on Computer science education known as SIGCSE which celebrated its 50th anniversary in 2018, making it one of the oldest and longest running ACM Special Interest Groups.An outcome of computing education research are Parsons problems.


== Gender perspectives in computer science education ==
In many countries, there is a significant gender gap in computer science education. In 2015, 15.3% of computer science students graduating from non-doctoral granting institutions in the US were women while at doctoral granting institutions, the figure was 16.6%. The number of female PhD recipients in the US was 19.3% in 2018. In almost everywhere in the world, less than 20% of the computer science graduates are female. This problem mainly arises due to the lack of interests of girls in computing starting from the primary level. Despite numerous efforts by programs specifically designed to increase the ratio of women in this field, no significant improvement has been observed. Furthermore, a declining trend has been noticed in the involvement of women in past decades. The main reason for the failure of these programs is because almost all of them focused on girls in high school or higher levels of education. Researchers argue that by then women have already made up their mind and stereotypes start to form about computer scientists. Computer Science is perceived as a male dominated field, pursued by people who are nerdy and lack social skills. All these characteristics seem to be more damaging for a woman as compared to a man. Therefore, in order to break these stereotypes and to engage more women in computer science, it is crucial that there are special outreach programs designed to develop interest in girls starting at the middle school level and prepare them for a academic track towards the hard sciences.Evidently, there are a few countries in Asia and Africa where these stereotypes do not exist and women are encouraged for a career in science starting at the primary level, thus resulting in a gender gap that is virtually nonexistent. In 2011, women earned half of the computer science degrees in Malaysia. In 2001, 55 percent of computer science graduates in Guyana were women.


== References ==",67546,57,"All articles with unsourced statements, Articles with short description, Articles with unsourced statements from June 2021, Computer science education, Education by subject, Short description is different from Wikidata",1131499888,cs
https://en.wikipedia.org/wiki/Computer_engineering,Computer engineering,"Computer engineering (CoE or CpE) is a branch of electrical engineering and computer science that integrates several fields of computer science and electronic engineering required to develop computer hardware and software. Computer engineers not only require training in electronic engineering, software design, and hardware-software integration, but also in software engineering. It uses the techniques and principles of electrical engineering and computer science, but also covers areas such as artificial intelligence (AI), robotics, computer networks, computer architecture and operating systems. Computer engineers are involved in many hardware and software aspects of computing, from the design of individual microcontrollers, microprocessors, personal computers, and supercomputers, to circuit design. This field of engineering not only focuses on how computer systems themselves work, yet it also demands them to integrate into the larger picture. Robots are one of the applications of computer engineering.
Computer engineering usually deals with areas including writing software and firmware for embedded microcontrollers, designing VLSI chips, designing analog sensors, designing mixed signal circuit boards, and designing operating systems. Computer engineers are also suited for robotics research, which relies heavily on using digital systems to control and monitor electrical systems like motors, communications, and sensors.
In many institutions of higher learning, computer engineering students are allowed to choose areas of in-depth study in their junior and senior year because the full breadth of knowledge used in the design and application of computers is beyond the scope of an undergraduate degree. Other institutions may require engineering students to complete one or two years of general engineering before declaring computer engineering as their primary focus.

","Computer engineering (CoE or CpE) is a branch of electrical engineering and computer science that integrates several fields of computer science and electronic engineering required to develop computer hardware and software. Computer engineers not only require training in electronic engineering, software design, and hardware-software integration, but also in software engineering. It uses the techniques and principles of electrical engineering and computer science, but also covers areas such as artificial intelligence (AI), robotics, computer networks, computer architecture and operating systems. Computer engineers are involved in many hardware and software aspects of computing, from the design of individual microcontrollers, microprocessors, personal computers, and supercomputers, to circuit design. This field of engineering not only focuses on how computer systems themselves work, yet it also demands them to integrate into the larger picture. Robots are one of the applications of computer engineering.
Computer engineering usually deals with areas including writing software and firmware for embedded microcontrollers, designing VLSI chips, designing analog sensors, designing mixed signal circuit boards, and designing operating systems. Computer engineers are also suited for robotics research, which relies heavily on using digital systems to control and monitor electrical systems like motors, communications, and sensors.
In many institutions of higher learning, computer engineering students are allowed to choose areas of in-depth study in their junior and senior year because the full breadth of knowledge used in the design and application of computers is beyond the scope of an undergraduate degree. Other institutions may require engineering students to complete one or two years of general engineering before declaring computer engineering as their primary focus.


== History ==

Computer engineering began in 1939 when John Vincent Atanasoff and Clifford Berry began developing the world's first electronic digital computer through physics, mathematics, and electrical engineering. John Vincent Atanasoff was once a physics and mathematics teacher for Iowa State University and Clifford Berry a former graduate under electrical engineering and physics. Together, they created the Atanasoff-Berry computer, also known as the ABC which took five years to complete.
While the original ABC was dismantled and discarded in the 1940s a tribute was made to the late inventors, a replica of the ABC was made in 1997 where it took a team of researchers and engineers four years and $350,000 to build.The modern personal computer emerged in the 1970s, after several breakthroughs in semiconductor technology. These include the first working transistor by William Shockley, John Bardeen and Walter Brattain at Bell Labs in 1947, planar process by Jean Hoerni, the monolithic integrated circuit chip by Robert Noyce at Fairchild Semiconductor in 1959, the metal–oxide–semiconductor field-effect transistor (MOSFET, or MOS transistor) by Mohamed Atalla and Dawon Kahng at Bell Labs in 1959, and the single-chip microprocessor (Intel 4004) by Federico Faggin, Marcian Hoff, Masatoshi Shima and Stanley Mazor at Intel in 1971.


=== History of computer engineering education ===
The first computer engineering degree program in the United States was established in 1971 at Case Western Reserve University in Cleveland, Ohio. As of 2015, there were 250 ABET-accredited computer engineering programs in the U.S. In Europe, accreditation of computer engineering schools is done by a variety of agencies part of the EQANIE network. Due to increasing job requirements for engineers who can concurrently design hardware, software, firmware, and manage all forms of computer systems used in industry, some tertiary institutions around the world offer a bachelor's degree generally called computer engineering. Both computer engineering and electronic engineering programs include analog and digital circuit design in their curriculum. As with most engineering disciplines, having a sound knowledge of mathematics and science is necessary for computer engineers.


== Education ==
Computer engineering is referred to as computer science and engineering at some universities. Most entry-level computer engineering jobs require at least a bachelor's degree in computer engineering (or computer science and engineering). Typically one must learn an array of mathematics such as calculus, algebra and trigonometry and some computer science classes. Degrees in electronic or electric engineering also suffice due to the similarity of the two fields. Because hardware engineers commonly work with computer software systems, a strong background in computer programming is necessary. According to BLS, ""a computer engineering major is similar to electrical engineering but with some computer science courses added to the curriculum"". Some large firms or specialized jobs require a master's degree.
It is also important for computer engineers to keep up with rapid advances in technology. Therefore, many continue learning throughout their careers. This can be helpful, especially when it comes to learning new skills or improving existing ones. For example, as the relative cost of fixing a bug increases the further along it is in the software development cycle, there can be greater cost savings attributed to developing and testing for quality code as soon as possible in the process,  particularly before release.


== Profession: Computer engineer ==

A person with a profession in computer engineering is called a computer engineer.


== Applications and practice ==
There are two major focuses in computer engineering: hardware and software.


=== Computer hardware engineering ===

According to the BLS, Job Outlook employment for computer hardware engineers, the expected ten-year growth from 2019 to 2029 for computer hardware engineering was an estimated 2% and a total of 71,100 jobs. (""Slower than average"" in their own words when compared to other occupations)"". This is a decrease from the 2014 to 2024 BLS computer hardware engineering estimate of 3% and  a total of 77,700 jobs. "" and is down from 7% for the 2012 to 2022 BLS estimate and is further down from 9% in the BLS 2010 to 2020 estimate."" Today, computer hardware is somehow equal to electronic and computer engineering (ECE) and has been divided into many subcategories; the most significant is embedded system design.


=== Computer software engineering ===
According to the U.S. Bureau of Labor Statistics (BLS), ""computer applications software engineers and computer systems software engineers are projected to be among the faster than average growing occupations"" The expected ten-year growth as of 2014 for computer software engineering was an estimated seventeen percent and there was a total of 1,114,000 jobs that same year. This is down from the 2012 to 2022 BLS estimate of 22% for software developers. And, further down from the 30% 2010 to 2020 BLS estimate. In addition, growing concerns over cybersecurity add up to put computer software engineering high above the average rate of increase for all fields. However, some of the work will be outsourced in foreign countries. Due to this, job growth will not be as fast as during the last decade, as jobs that would have gone to computer software engineers in the United States would instead go to computer software engineers in countries such as India. In addition, the BLS Job Outlook for Computer Programmers, 2014–24 has an −8% (a decline, in their words), a Job Outlook, 2019-29 a -9% (Decline), and a 10% decline for 2021-2031 for those who program computers (i.e. embedded systems) who are not computer application developers. Furthermore, women in software fields has been declining over the years even faster than other engineering fields.


=== Computer engineering licensing and practice ===
Computer engineering is generally practiced within larger product development firms, and such practice may not be subject to licensing.  However, independent consultants who advertise computer engineering, just like any form of engineering, may be subject to state laws which restrict professional engineer practice to only those who have received the appropriate License.  National Council of Examiners for Engineering and Surveying (NCEES) first offered a Principles and Practice of Engineering Examination for computer engineering in 2003.


== Specialty areas ==
There are many specialty areas in the field of computer engineering.


=== Processor design ===

Processor design process involves choosing an instruction set and a certain execution paradigm (e.g. VLIW or RISC) and results in a microarchitecture, which might be described in e.g. VHDL or Verilog. CPU design is divided into design of the following components: datapaths (such as ALUs and pipelines), control unit: logic which controls the datapaths, memory components such as register files, caches, clock circuitry such as clock drivers, PLLs, clock distribution networks, pad transceiver circuitry, logic gate cell library which is used to implement the logic.


=== Coding, cryptography, and information protection ===

Computer engineers work in coding, cryptography, and information protection to develop new methods for protecting various information, such as digital images and music, fragmentation, copyright infringement and other forms of tampering. Examples include work on wireless communications, multi-antenna systems, optical transmission, and digital watermarking.


=== Communications and wireless networks ===

Those focusing on communications and wireless networks, work advancements in telecommunications systems and networks (especially wireless networks), modulation and error-control coding, and information theory. High-speed network design, interference suppression and modulation, design, and analysis of fault-tolerant system, and storage and transmission schemes are all a part of this specialty.


=== Compilers and operating systems ===

This specialty focuses on compilers and operating systems design and development. Engineers in this field develop new operating system architecture, program analysis techniques, and new techniques to assure quality. Examples of work in this field include post-link-time code transformation algorithm development and new operating system development.


=== Computational science and engineering ===

Computational science and engineering is a relatively new discipline. According to the Sloan Career Cornerstone Center, individuals working in this area, ""computational methods are applied to formulate and solve complex mathematical problems in engineering and the physical and the social sciences. Examples include aircraft design, the plasma processing of nanometer features on semiconductor wafers, VLSI circuit design, radar detection systems, ion transport through biological channels, and much more"".


=== Computer networks, mobile computing, and distributed systems ===

In this specialty, engineers build integrated environments for computing, communications, and information access. Examples include shared-channel wireless networks, adaptive resource management in various systems, and improving the quality of service in mobile and ATM environments. Some other examples include work on wireless network systems and fast Ethernet cluster wired systems.


=== Computer systems: architecture, parallel processing, and dependability ===

Engineers working in computer systems work on research projects that allow for reliable, secure, and high-performance computer systems. Projects such as designing processors for multi-threading and parallel processing are included in this field. Other examples of work in this field include the development of new theories, algorithms, and other tools that add performance to computer systems.Computer architecture includes CPU design, cache hierarchy layout, memory organization and load balancing.


=== Computer vision and robotics ===

In this specialty, computer engineers focus on developing visual sensing technology to sense an environment, representation of an environment, and manipulation of the environment. The gathered three-dimensional information is then implemented to perform a variety of tasks. These include improved human modeling, image communication, and human-computer interfaces, as well as devices such as special-purpose cameras with versatile vision sensors.


=== Embedded systems ===

Individuals working in this area design technology for enhancing the speed, reliability, and performance of systems. Embedded systems are found in many devices from a small FM radio to the space shuttle. According to the Sloan Cornerstone Career Center, ongoing developments in embedded systems include ""automated vehicles and equipment to conduct search and rescue, automated transportation systems, and human-robot coordination to repair equipment in space."" As of 2018, computer embedded systems specializations include system-on-chip design, architecture of edge computing and the Internet of things.


=== Integrated circuits, VLSI design, testing and CAD ===

This specialty of computer engineering requires adequate knowledge of electronics and electrical systems. Engineers working in this area work on enhancing the speed, reliability, and energy efficiency of next-generation very-large-scale integrated (VLSI) circuits and microsystems. An example of this specialty is work done on reducing the power consumption of VLSI algorithms and architecture.


=== Signal, image and speech processing ===

Computer engineers in this area develop improvements in human-computer interaction, including speech recognition and synthesis, medical and scientific imaging, or communications systems. Other work in this area includes computer vision development such as recognition of human facial features.


=== Quantum computing ===


== See also ==


=== Related fields ===


=== Associations ===
IEEE Computer Society
Association for Computing Machinery


== References ==


== External links ==
 Media related to Computer engineering at Wikimedia Commons",3406759,3041,"All articles containing potentially dated statements, All articles with unsourced statements, Articles containing potentially dated statements from 2015, Articles containing potentially dated statements from 2018, Articles with BNF identifiers, Articles with GND identifiers, Articles with J9U identifiers, Articles with LCCN identifiers, Articles with NLK identifiers, Articles with limited geographic scope from July 2018, Articles with short description, Articles with unsourced statements from April 2019, Articles with unsourced statements from June 2019, CS1 Spanish-language sources (es), Commons category link from Wikidata, Computer engineering, Electrical and computer engineering, Engineering disciplines, Short description matches Wikidata, United States-centric, Use mdy dates from September 2017, Wikipedia articles needing clarification from June 2019, Wikipedia pending changes protected pages",1133951129,cs
https://en.wikipedia.org/wiki/History_of_computer_science,History of computer science,"The history of computer science began long before the modern discipline of computer science, usually appearing in forms like mathematics or physics. Developments in previous centuries alluded to the discipline that we now know as computer science. This progression, from mechanical inventions and mathematical theories towards modern computer concepts and machines, led to the development of a major academic field, massive technological advancement across the Western world, and the basis of a massive worldwide trade and culture.","The history of computer science began long before the modern discipline of computer science, usually appearing in forms like mathematics or physics. Developments in previous centuries alluded to the discipline that we now know as computer science. This progression, from mechanical inventions and mathematical theories towards modern computer concepts and machines, led to the development of a major academic field, massive technological advancement across the Western world, and the basis of a massive worldwide trade and culture.


== Prehistory ==

The earliest known tool for use in computation was the abacus, developed in the period between 2700 and 2300 BCE in Sumer. The Sumerians' abacus consisted of a table of successive columns which delimited the successive orders of magnitude of their sexagesimal number system.: 11  Its original style of usage was by lines drawn in sand with pebbles. Abaci of a more modern design are still used as calculation tools today, such as the Chinese abacus.In the 5th century BC in ancient India, the grammarian Pāṇini formulated the grammar of Sanskrit in 3959 rules known as the Ashtadhyayi which was highly systematized and technical. Panini used metarules, transformations and recursions.The Antikythera mechanism is believed to be an early mechanical analog computer.  It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to circa 100 BC.Mechanical analog computer devices appeared again a thousand years later in the medieval Islamic world and were developed by Muslim astronomers, such as the mechanical geared astrolabe by Abū Rayhān al-Bīrūnī, and the torquetum by Jabir ibn Aflah. According to Simon Singh, Muslim mathematicians also made important advances in cryptography, such as the development of cryptanalysis and frequency analysis by Alkindus. Programmable machines were also invented by Muslim engineers, such as the automatic flute player by the Banū Mūsā brothers, and Al-Jazari's programmable castle clock, which is considered to be the first programmable analog computer. Technological artifacts of similar complexity appeared in 14th century Europe, with mechanical astronomical clocks.When John Napier discovered logarithms for computational purposes in the early 17th century, there followed a period of considerable progress by inventors and scientists in making calculating tools. In 1623 Wilhelm Schickard designed a calculating machine, but abandoned the project, when the prototype he had started building was destroyed by a fire in 1624. Around 1640, Blaise Pascal, a leading French mathematician, constructed a mechanical adding device based on a design described by Greek mathematician Hero of Alexandria. Then in 1672 Gottfried Wilhelm Leibniz invented the Stepped Reckoner which he completed in 1694.In 1837 Charles Babbage first described his Analytical Engine which is accepted as the first design for a modern computer. The analytical engine had expandable memory, an arithmetic unit, and logic processing capabilities able to interpret a programming language with loops and conditional branching. Although never built, the design has been studied extensively and is understood to be Turing equivalent. The analytical engine would have had a memory capacity of less than 1 kilobyte of memory and a clock speed of less than 10 Hertz.Considerable advancement in mathematics and electronics theory was required before the first modern computers could be designed.


== Binary logic ==

In 1702, Gottfried Wilhelm Leibniz developed logic in a formal, mathematical sense with his writings on the binary numeral system. In his system, the ones and zeros also represent true and false values or on and off states. But it took more than a century before George Boole published his Boolean algebra in 1854 with a complete system that allowed computational processes to be mathematically modeled.By this time, the first mechanical devices driven by a binary pattern had been invented. The industrial revolution had driven forward the mechanization of many tasks, and this included weaving. Punched cards controlled Joseph Marie Jacquard's loom in 1801, where a hole punched in the card indicated a binary one and an unpunched spot indicated a binary zero. Jacquard's loom was far from being a computer, but it did illustrate that machines could be driven by binary systems.


== Emergence of a discipline ==


=== Charles Babbage and Ada Lovelace ===

Charles Babbage is often regarded as one of the first pioneers of computing. Beginning in the 1810s, Babbage had a vision of mechanically computing numbers and tables. Putting this into reality, Babbage designed a calculator to compute numbers up to 8 decimal points long. Continuing with the success of this idea, Babbage worked to develop a machine that could compute numbers with up to 20 decimal places. By the 1830s, Babbage had devised a plan to develop a machine that could use punched cards to perform arithmetical operations. The machine would store numbers in memory units, and there would be a form of sequential control.  This means that one operation would be carried out before another in such a way that the machine would produce an answer and not fail. This machine was to be known as the “Analytical Engine”, which was the first true representation of what is the modern computer.

Ada Lovelace (Augusta Ada Byron) is credited as the pioneer of computer programming and is regarded as a mathematical genius. Lovelace began working with Charles Babbage as an assistant while Babbage was working on his ""Analytical Engine"", the first mechanical computer. During her work with Babbage, Ada Lovelace became the designer of the first computer algorithm, which had the ability to compute Bernoulli numbers, although this is arguable as Charles was the first to design the difference engine and consequently its corresponding difference based algorithms, making him the first computer algorithm designer.  Moreover, Lovelace's work with Babbage resulted in her prediction of future computers to not only perform mathematical calculations, but also manipulate symbols, mathematical or not. While she was never able to see the results of her work, as the ""Analytical Engine"" was not created in her lifetime, her efforts in later years, beginning in the 1840s, did not go unnoticed.


=== Contributions to Babbage's Analytical Engine during the first half of the 20th century ===
Following Babbage, although at first unaware of his earlier work, was Percy Ludgate, a clerk to a corn merchant in Dublin, Ireland. He independently designed a programmable mechanical computer, which he described in a work that was published in 1909. Two other inventors, Leonardo Torres y Quevedo and Vannevar Bush, also did follow on research based on Babbage's work. In his Essays on Automatics (1913) Torres y Quevedo designed a Babbage type of calculating machine that used electromechanical parts which included floating point number representations and built a prototype in 1920. Bush's paper Instrumental Analysis (1936) discussed using existing IBM punch card machines to implement Babbage's design. In the same year he started the Rapid Arithmetical Machine project to investigate the problems of constructing an electronic digital computer.


=== Charles Sanders Peirce and electrical switching circuits ===

In an 1886 letter, Charles Sanders Peirce described how logical operations could be carried out by electrical switching circuits. During 1880–81 he showed that NOR gates alone (or alternatively NAND gates alone) can be used to reproduce the functions of all the other logic gates, but this work on it was unpublished until 1933. The first published proof was by Henry M. Sheffer in 1913, so the NAND logical operation is sometimes called Sheffer stroke; the logical NOR is sometimes called Peirce's arrow. Consequently, these gates are sometimes called universal logic gates.Eventually, vacuum tubes replaced relays for logic operations. Lee De Forest's modification, in 1907, of the Fleming valve can be used as a logic gate. Ludwig Wittgenstein introduced a version of the 16-row truth table as proposition 5.101 of Tractatus Logico-Philosophicus (1921). Walther Bothe, inventor of the coincidence circuit, got part of the 1954 Nobel Prize in physics, for the first modern electronic AND gate in 1924. Konrad Zuse designed and built electromechanical logic gates for his computer Z1 (from 1935 to 1938).
Up to and during the 1930s, electrical engineers were able to build electronic circuits to solve mathematical and logic problems, but most did so in an ad hoc manner, lacking any theoretical rigor.  This changed with switching circuit theory in the 1930s. From 1934 to 1936, Akira Nakashima, Claude Shannon, and Viktor Shetakov published a series of papers showing that the two-valued Boolean algebra, can describe the operation of switching circuits. This concept, of utilizing the properties of electrical switches to do logic, is the basic concept that underlies all electronic digital computers. Switching circuit theory provided the mathematical foundations and tools for digital system design in almost all areas of modern technology.While taking an undergraduate philosophy class, Shannon had been exposed to Boole's work, and recognized that it could be used to arrange electromechanical relays (then used in telephone routing switches) to solve logic problems. His thesis became the foundation of practical digital circuit design when it became widely known among the electrical engineering community during and after World War II.


=== Alan Turing and the Turing machine ===

Before the 1920s, computers (sometimes computors) were human clerks that performed computations. They were usually under the lead of a physicist. Many thousands of computers were employed in commerce, government, and research establishments. Many of these clerks who served as human computers were women. Some performed astronomical calculations for calendars, others ballistic tables for the military.After the 1920s, the expression computing machine referred to any machine that performed the work of a human computer, especially those in accordance with effective methods of the Church-Turing thesis. The thesis states that a mathematical method is effective if it could be set out as a list of instructions able to be followed by a human clerk with paper and pencil, for as long as necessary, and without ingenuity or insight.
Machines that computed with continuous values became known as the analog kind. They used machinery that represented continuous numeric quantities, like the angle of a shaft rotation or difference in electrical potential.
Digital machinery, in contrast to analog, were able to render a state of a numeric value and store each individual digit. Digital machinery used difference engines or relays before the invention of faster memory devices.
The phrase computing machine gradually gave way, after the late 1940s, to just computer as the onset of electronic digital machinery became common. These computers were able to perform the calculations that were performed by the previous human clerks.
Since the values stored by digital machines were not bound to physical properties like analog devices, a logical computer, based on digital equipment, was able to do anything that could be described ""purely mechanical."" The theoretical Turing Machine, created by Alan Turing, is a hypothetical device theorized in order to study the properties of such hardware.
The mathematical foundations of modern computer science began to be laid by Kurt Gödel with his incompleteness theorem (1931). In this theorem, he showed that there were limits to what could be proved and disproved within a formal system. This led to work by Gödel and others to define and describe these formal systems, including concepts such as mu-recursive functions and lambda-definable functions.In 1936  Alan Turing and Alonzo Church independently, and also together, introduced the formalization of an algorithm, with limits on what can be computed, and a ""purely mechanical"" model for computing. This became the Church–Turing thesis, a hypothesis about the nature of mechanical calculation devices, such as electronic computers. The thesis states that any calculation that is possible can be performed by an algorithm running on a computer, provided that sufficient time and storage space are available.In 1936, Alan Turing also published his seminal work on the Turing machines, an abstract digital computing machine which is now simply referred to as the Universal Turing machine. This machine invented the principle of the modern computer and was the birthplace of the stored program concept that almost all modern day computers use. These hypothetical machines were designed to formally determine, mathematically, what can be computed, taking into account limitations on computing ability. If a Turing machine can complete the task, it is considered Turing computable.The Los Alamos physicist Stanley Frankel, has described John von Neumann's view of the fundamental importance of Turing's 1936 paper, in a letter:
 I know that in or about 1943 or ‘44 von Neumann was well aware of the fundamental importance of Turing's paper of 1936… Von Neumann introduced me to that paper and at his urging I studied it with care. Many people have acclaimed von Neumann as the ""father of the computer"" (in a modern sense of the term) but I am sure that he would never have made that mistake himself. He might well be called the midwife, perhaps, but he firmly emphasized to me, and to others I am sure, that the fundamental conception is owing to Turing...


=== Early computer hardware ===
The world's first electronic digital computer, the Atanasoff–Berry computer, was built on the Iowa State campus from 1939 through 1942 by John V. Atanasoff, a professor of physics and mathematics, and Clifford Berry, an engineering graduate student.
In 1941, Konrad Zuse developed the world's first functional program-controlled computer, the Z3. In 1998, it was shown to be Turing-complete in principle. Zuse also developed the S2 computing machine, considered the first process control computer. He founded one of the earliest computer businesses in 1941, producing the Z4, which became the world's first commercial computer.  In 1946, he designed the first high-level programming language, Plankalkül.In 1948, the Manchester Baby was completed; it was the world's first electronic digital computer that ran programs stored in its memory, like almost all modern computers. The influence on Max Newman of Turing's seminal 1936 paper on the Turing Machines and of his logico-mathematical contributions to the project, were both crucial to the successful development of the Baby.In 1950, Britain's National Physical Laboratory completed Pilot ACE, a small scale programmable computer, based on Turing's philosophy. With an operating speed of 1 MHz, the Pilot Model ACE was for some time the fastest computer in the world. Turing's design for ACE had much in common with today's RISC architectures and it called for a high-speed memory of roughly the same capacity as an early Macintosh computer, which was enormous by the standards of his day. Had Turing's ACE been built as planned and in full, it would have been in a different league from the other early computers.

The first actual computer bug was a moth. It was stuck in between the relays on the Harvard Mark II.
While the invention of the term 'bug' is often but erroneously attributed to Grace Hopper, a future rear admiral in the U.S. Navy, who supposedly logged the ""bug"" on September 9, 1945, most other accounts conflict at least with these details. According to these accounts, the actual date was September 9, 1947 when operators filed this 'incident' — along with the insect and the notation ""First actual case of bug being found"" (see software bug for details).


=== Shannon and information theory ===
Claude Shannon went on to found the field of information theory with his 1948 paper titled A Mathematical Theory of Communication, which applied probability theory to the problem of how to best encode the information a sender wants to transmit.  This work is one of the theoretical foundations for many areas of study, including data compression and cryptography.


=== Wiener and cybernetics ===
From experiments with anti-aircraft systems that interpreted radar images to detect enemy planes, Norbert Wiener coined the term cybernetics from the Greek word for ""steersman."" He published ""Cybernetics"" in 1948, which influenced artificial intelligence. Wiener also compared computation, computing machinery, memory devices, and other cognitive similarities with his analysis of brain waves.


=== John von Neumann and the von Neumann architecture ===

In 1946, a model for computer architecture was introduced and became known as Von Neumann architecture. Since 1950, the von Neumann model provided uniformity in subsequent computer designs. The von Neumann architecture was considered innovative as it introduced an idea of allowing machine instructions and data to share memory space.  The von Neumann model is composed of three major parts, the arithmetic logic unit (ALU), the memory, and the instruction processing unit (IPU). In von Neumann machine design, the IPU passes addresses to memory, and memory, in turn, is routed either back to the IPU if an instruction is being fetched or to the ALU if data is being fetched.Von Neumann's machine design uses a RISC (Reduced instruction set computing) architecture, which means the instruction set uses a total of 21 instructions to perform all tasks. (This is in contrast to CISC, complex instruction set computing, instruction sets which have more instructions from which to choose.)  With von Neumann architecture, main memory along with the accumulator (the register that holds the result of logical operations) are the two memories that are addressed. Operations can be carried out as simple arithmetic (these are performed by the ALU and include addition, subtraction, multiplication and division), conditional branches (these are more commonly seen now as if statements or while loops. The branches serve as go to statements), and logical moves between the different components of the machine, i.e., a move from the accumulator to memory or vice versa. Von Neumann architecture accepts fractions and instructions as data types. Finally, as the von Neumann architecture is a simple one, its register management is also simple. The architecture uses a set of seven registers to manipulate and interpret fetched data and instructions. These registers include the ""IR"" (instruction register), ""IBR"" (instruction buffer register), ""MQ"" (multiplier quotient register), ""MAR"" (memory address register), and ""MDR"" (memory data register).""  The architecture also uses a program counter (""PC"") to keep track of where in the program the machine is.


=== John McCarthy, Marvin Minsky and artificial intelligence ===

The term artificial intelligence was credited by John McCarthy to explain the research that they were doing for a proposal for the Dartmouth Summer Research. The naming of artificial intelligence also led to the birth of a new field in computer science. On August 31, 1955, a research project was proposed consisting of John McCarthy, Marvin L. Minsky, Nathaniel Rochester, and Claude E. Shannon. The official project began in 1956 that consisted of several significant parts they felt would help them better understand artificial intelligence's makeup.McCarthy and his colleagues' ideas behind automatic computers was while a machine is capable of completing a task, then the same should be confirmed with a computer by compiling a program to perform the desired results. They also discovered that the human brain was too complex to replicate, not by the machine itself but by the program. The knowledge to produce a program that sophisticated was not there yet.The concept behind this was looking at how humans understand our own language and structure of how we form sentences, giving different meaning and rule sets and comparing them to a machine process. The way computers can understand is at a hardware level. This language is written in binary (1s and 0's). This has to be written in a specific format that gives the computer the ruleset to run a particular hardware piece.Minsky's process determined how these artificial neural networks could be arranged to have similar qualities to the human brain. However, he could only produce partial results and needed to further the research into this idea.McCarthy and Shannon's idea behind this theory was to develop a way to use complex problems to determine and measure the machine's efficiency through mathematical theory and computations. However, they were only to receive partial test results.The idea behind self-improvement is how a machine would use self-modifying code to make itself smarter. This would allow for a machine to grow in intelligence and increase calculation speeds. The group believed they could study this if a machine could improve upon the process of completing a task in the abstractions part of their research.The group thought that research in this category could be broken down into smaller groups. This would consist of sensory and other forms of information about artificial intelligence. Abstractions in computer science can refer to mathematics and programming language.Their idea of computational creativity is how the program or a machine can be seen in having similar ways of human thinking. They wanted to see if a machine could take a piece of incomplete information and improve upon it to fill in the missing details as the human mind can do. If this machine could do this; they needed to think of how did the machine determine the outcome.


== See also ==
Computer museum
List of computer term etymologies, the origins of computer science words
List of pioneers in computer science
History of computing
History of computing hardware
History of software
History of personal computers
Timeline of algorithms
Timeline of women in computing
Timeline of computing 2020–present


== References ==


=== Sources ===
Evans, Claire L. (2018). Broad Band: The Untold Story of the Women Who Made the Internet. New York: Portfolio/Penguin. ISBN 9780735211759.
Grier, David Alan (2013). When Computers Were Human. Princeton: Princeton University Press. ISBN 9781400849369 – via Project MUSE.


== Further reading ==
Tedre, Matti (2014). The Science of Computing: Shaping a Discipline. Taylor and Francis / CRC Press. ISBN 978-1-4822-1769-8.
Kak, Subhash : Computing Science in Ancient India; Munshiram Manoharlal Publishers Pvt. Ltd (2001)
The Development of Computer Science: A Sociocultural Perspective Matti Tedre's Ph.D. Thesis, University of Joensuu (2006)
Ceruzzi, Paul E. (1998). A History of a Modern Computing. The MIT Press. ISBN 978-0-262-03255-1.
Copeland, B. Jack. ""The Modern History of Computing"".  In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.


== External links ==

Computer History Museum
Computers: From the Past to the Present
The First ""Computer Bug"" at the Naval History and Heritage Command Photo Archives.
Bitsavers, an effort to capture, salvage, and archive historical computer software and manuals from minicomputers and mainframes of the 1950s, 1960s, 1970s, and 1980s
Oral history interviews",1049877,1157,"All Wikipedia articles in need of updating, All accuracy disputes, Articles with disputed statements from April 2014, Articles with disputed statements from June 2022, Articles with short description, CS1: Julian–Gregorian uncertainty, Commons category link from Wikidata, History of computer science, History of computing, History of science by discipline, Short description matches Wikidata, Wikipedia articles in need of updating from September 2013",1122841097,cs
https://en.wikipedia.org/wiki/Foobar,Foobar,"The terms foobar (), foo, bar, baz, and others are used as metasyntactic variables and placeholder names in computer programming or computer-related documentation. They have been used to name entities such as variables, functions, and commands whose exact identity is unimportant and serve only to demonstrate a concept.","The terms foobar (), foo, bar, baz, and others are used as metasyntactic variables and placeholder names in computer programming or computer-related documentation. They have been used to name entities such as variables, functions, and commands whose exact identity is unimportant and serve only to demonstrate a concept.


== History and etymology ==

It is possible that foobar is a playful allusion to the World War II-era military slang FUBAR (Fucked Up Beyond All Recognition).According to an Internet Engineering Task Force RFC, the word FOO originated as a nonsense word with its earliest documented use in the 1930s comic Smokey Stover by Bill Holman. Holman states that he used the word due to having seen it on the bottom of a jade Chinese figurine in San Francisco Chinatown, purportedly signifying ""good luck"". If true, this is presumably related to the Chinese word fu (""福"", sometimes transliterated foo, as in foo dog), which can mean happiness or blessing.The first known use of the terms in print in a programming context appears in a 1965 edition of MIT's Tech Engineering News. The use of foo in a programming context is generally credited to the Tech Model Railroad Club (TMRC) of MIT from circa 1960. In the complex model system, there were scram switches located at numerous places around the room that could be thrown if something undesirable was about to occur, such as a train moving at full power towards an obstruction. Another feature of the system was a digital clock on the dispatch board. When someone hit a scram switch, the clock stopped and the display was replaced with the word ""FOO""; at TMRC the scram switches are, therefore, called ""Foo switches"". Because of this, an entry in the 1959 Dictionary of the TMRC Language went something like this: ""FOO: The first syllable of the misquoted sacred chant phrase 'foo mane padme hum.' Our first obligation is to keep the foo counters turning."" One book describing the MIT train room describes two buttons by the door labeled ""foo"" and ""bar"". These were general-purpose buttons and were often repurposed for whatever fun idea the MIT hackers had at the time, hence the adoption of foo and bar as general-purpose variable names. An entry in the Abridged Dictionary of the TMRC Language states:
Multiflush: stop-all-trains-button. Next best thing to the red door button. Also called FOO. Displays ""FOO"" on the clock when used.
Foobar was used as a variable name in the Fortran code of Colossal Cave Adventure (1977 Crowther and Woods version). The variable FOOBAR was used to contain the player's progress in saying the magic phrase ""Fee Fie Foe Foo"". Intel also used the term foo in their programming documentation in 1978.


== Examples in culture ==
Foo Camp is an annual hacker convention.
BarCamp, an international network of user-generated conferences
During the United States v. Microsoft Corp. trial, some evidence was presented that Microsoft had tried to use the Web Services Interoperability organization (WS-I) as a means to stifle competition, including e-mails in which top executives including Bill Gates referred to the WS-I using the codename ""foo"".
Foobar2000 is an audio player.
Google uses a web tool called ""foobar"" to recruit new employees.
The Foo Bar is a pub in the Leiden University's faculty of Mathematics and Computer Science.


== See also ==
Alice and Bob
Foo fighter
Foo was here
xyzzy
Category:Variable (computer science)
Fu (character)


== References ==


== External links ==

The Jargon File entry on ""foobar"", catb.org
RFC 1639 – FTP Operation Over Big Address Records (FOOBAR)",1842765,1082,"All articles with specifically marked weasel-worded phrases, Articles containing Chinese-language text, Articles with example C code, Articles with short description, Articles with specifically marked weasel-worded phrases from November 2010, Computer programming folklore, Placeholder names, Short description matches Wikidata, Use dmy dates from February 2020",1127396671,cs
https://en.wikipedia.org/wiki/Persistence_(computer_science),Persistence (computer science),"In computer science, persistence refers to the characteristic of state of a system that outlives (persists more than) the process that created it. This is achieved in practice by storing the state as data in computer data storage. Programs have to transfer data to and from storage devices and have to provide mappings from the native programming-language data structures to the storage device data structures.Picture editing programs or word processors, for example, achieve state persistence by saving their documents to files.","In computer science, persistence refers to the characteristic of state of a system that outlives (persists more than) the process that created it. This is achieved in practice by storing the state as data in computer data storage. Programs have to transfer data to and from storage devices and have to provide mappings from the native programming-language data structures to the storage device data structures.Picture editing programs or word processors, for example, achieve state persistence by saving their documents to files.


== Orthogonal or transparent persistence ==
Persistence is said to be ""orthogonal"" or ""transparent"" when it is implemented as an intrinsic property of the execution environment of a program. An orthogonal persistence environment does not require any specific actions by programs running in it to retrieve or save their state.
Non-orthogonal persistence requires data to be written and read to and from storage using specific instructions in a program, resulting in the use of persist as a transitive verb: On completion, the program persists the data.
The advantage of orthogonal persistence environments is simpler and less error-prone programs.The term ""persistent"" was first introduced by Atkinson and Morrison in the sense of orthogonal persistence: they used an adjective rather than a verb to emphasize persistence as a property of the data, as distinct from an imperative action performed by a program. The use of the transitive verb ""persist"" (describing an action performed by a program) is a back-formation.  


=== Adoption ===
Orthogonal persistence is widely adopted in operating systems for hibernation and in platform virtualization systems such as VMware and VirtualBox for state saving.
Research prototype languages such as PS-algol, Napier88, Fibonacci and pJama, successfully demonstrated the concepts along with the advantages to programmers.


== Persistence techniques ==


=== System images ===

Using system images is the simplest persistence strategy. Notebook hibernation is an example of orthogonal persistence using a system image because it does not require any actions by the programs running on the machine. An example of non-orthogonal persistence using a system image is a simple text editing program executing specific instructions to save an entire document to a file.
Shortcomings: Requires enough RAM to hold the entire system state. State changes made to a system after its last image was saved are lost in the case of a system failure or shutdown. Saving an image for every single change would be too time-consuming for most systems, so images are not used as the single persistence technique for critical systems.


=== Journals ===

Using journals is the second simplest persistence technique. Journaling is the process of storing events in a log before each one is applied to a system. Such logs are called journals.
On startup, the journal is read and each event is reapplied to the system, avoiding data loss in the case of system failure or shutdown.
The entire ""Undo/Redo"" history of user commands in a picture editing program, for example, when written to a file, constitutes a journal capable of recovering the state of an edited picture at any point in time.
Journals are used by journaling file systems, prevalent systems and database management systems where they are also called ""transaction logs"" or ""redo logs"".
Shortcomings: When journals are used exclusively, the entire (potentially large) history of all system events must be reapplied on every system startup. As a result, journals are often combined with other persistence techniques.


=== Dirty writes ===
This technique is the writing to storage of only those portions of system state that have been modified (are dirty) since their last write. Sophisticated document editing applications, for example, will use dirty writes to save only those portions of a document that were actually changed since the last save.
Shortcomings: This technique requires state changes to be intercepted within a program. This is achieved in a non-transparent way by requiring specific storage-API calls or in a transparent way with automatic program transformation. This results in code that is slower than native code and more complicated to debug.


== Persistence layers ==
Any software layer that makes it easier for a program to persist its state is generically called a persistence layer. Most persistence layers will not achieve persistence directly but will use an underlying database management system.


== System prevalence ==

System prevalence is a technique that combines system images and transaction journals, mentioned above, to overcome their limitations.
Shortcomings: A prevalent system must have enough RAM to hold the entire system state.


== Database management systems (DBMSs) ==

DBMSs use a combination of the dirty writes and transaction journaling techniques mentioned above. They provide not only persistence but also other services such as queries, auditing and access control.


== Persistent operating systems ==
Persistent operating systems are operating systems that remain persistent even after a crash or unexpected shutdown. Operating systems that employ this ability include

KeyKOS
EROS, the successor to KeyKOS
CapROS, revisions of EROS
Coyotos, successor to EROS
Multics with its single-level store
Phantom
IBM System/38
Grasshopper OS [1]
Lua OS
tahrpuppy-6.0.5


== See also ==
Persistent data
Persistent data structure
Persistent identifier
Persistent memory
Copy-on-write
CRUD
Java Data Objects
Java Persistence API
System Prevalence
Orthogonality
Service Data Object
Snapshot (computer storage)


== References ==",523868,229,"All articles needing additional references, All articles with unsourced statements, Articles needing additional references from September 2014, Articles with GND identifiers, Articles with short description, Articles with unsourced statements from April 2015, Computer programming, Computing terminology, Models of computation, Persistence, Short description is different from Wikidata",1034040358,cs
https://en.wikipedia.org/wiki/Assignment_(computer_science),Assignment (computer science),"In computer programming, an assignment statement sets and/or re-sets the value stored in the storage location(s) denoted by a variable name; in other words, it copies a value into the variable. In most imperative programming languages, the assignment statement (or expression) is a fundamental construct.
Today, the most commonly used notation for this operation is x = expr (originally Superplan 1949–51, popularized by Fortran 1957 and C). The second most commonly used notation is x := expr (originally ALGOL 1958, popularised by Pascal). Many other notations are also in use. In some languages, the symbol used is regarded as an operator (meaning that the assignment statement as a whole returns a value). Other languages define assignment as a statement (meaning that it cannot be used in an expression).
Assignments typically allow a variable to hold different values at different times during its life-span and scope. However, some languages (primarily strictly functional languages) do not allow that kind of ""destructive"" reassignment, as it might imply changes of non-local state. The purpose is to enforce referential transparency, i.e. functions that do not depend on the state of some variable(s), but produce the same results for a given set of parametric inputs at any point in time. Modern programs in other languages also often use similar strategies, although less strict, and only in certain parts, in order to reduce complexity, normally in conjunction with complementing methodologies such as data structuring, structured programming and object orientation.","In computer programming, an assignment statement sets and/or re-sets the value stored in the storage location(s) denoted by a variable name; in other words, it copies a value into the variable. In most imperative programming languages, the assignment statement (or expression) is a fundamental construct.
Today, the most commonly used notation for this operation is x = expr (originally Superplan 1949–51, popularized by Fortran 1957 and C). The second most commonly used notation is x := expr (originally ALGOL 1958, popularised by Pascal). Many other notations are also in use. In some languages, the symbol used is regarded as an operator (meaning that the assignment statement as a whole returns a value). Other languages define assignment as a statement (meaning that it cannot be used in an expression).
Assignments typically allow a variable to hold different values at different times during its life-span and scope. However, some languages (primarily strictly functional languages) do not allow that kind of ""destructive"" reassignment, as it might imply changes of non-local state. The purpose is to enforce referential transparency, i.e. functions that do not depend on the state of some variable(s), but produce the same results for a given set of parametric inputs at any point in time. Modern programs in other languages also often use similar strategies, although less strict, and only in certain parts, in order to reduce complexity, normally in conjunction with complementing methodologies such as data structuring, structured programming and object orientation.


== Semantics ==
An assignment operation is a process in imperative programming in which different values are associated with a particular variable name as time passes. The program, in such model, operates by changing its state using successive assignment statements. Primitives of imperative programming languages rely on assignment to do iteration. At the lowest level, assignment is implemented using machine operations such as MOVE or STORE.Variables are containers for values. It is possible to put a value into a variable and later replace it with a new one. An assignment operation modifies the current state of the executing program. Consequently, assignment is dependent on the concept of variables. In an assignment:

The expression is evaluated in the current state of the program.
The variable is assigned the computed value, replacing the prior value of that variable.Example: Assuming that a is a numeric variable, the assignment a := 2*a means that the content of the variable a is doubled after the execution of the statement.
An example segment of C code:

In this sample, the variable x is first declared as an int, and is then assigned the value of 10. Notice that the declaration and assignment occur in the same statement. In the second line, y is declared without an assignment. In the third line, x is reassigned the value of 23. Finally, y is assigned the value of 32.4.
For an assignment operation, it is necessary that the value of the expression is well-defined (it is a valid rvalue) and that the variable represents a modifiable entity (it is a valid modifiable (non-const) lvalue). In some languages, typically dynamic ones, it is not necessary to declare a variable prior to assigning it a value. In such languages, a variable is automatically declared the first time it is assigned to, with the scope it is declared in varying by language.


== Single assignment ==

Any assignment that changes an existing value (e.g. x := x + 1) is disallowed in purely functional languages. In functional programming, assignment is discouraged in favor of single assignment, also called initialization. Single assignment is an example of name binding and differs from assignment as described in this article in that it can only be done once, usually when the variable is created; no subsequent reassignment is allowed.
An evaluation of expression does not have a side effect if it does not change an observable state of the machine, and produces same values for same input. Imperative assignment can introduce side effects while destroying and making the old value unavailable while substituting it with a new one, and is referred to as destructive assignment for that reason in LISP and functional programming, similar to destructive updating.
Single assignment is the only form of assignment available in purely functional languages, such as Haskell, which do not have variables in the sense of imperative programming languages but rather named constant values possibly of compound nature with their elements progressively defined on-demand. Purely functional languages can provide an opportunity for computation to be performed in parallel, avoiding the von Neumann bottleneck of sequential one step at a time execution, since values are independent of each other.Impure functional languages provide both single assignment as well as true assignment (though true assignment is typically used with less frequency than in imperative programming languages). For example, in Scheme, both single assignment (with let) and true assignment (with set!) can be used on all variables, and specialized primitives are provided for destructive update inside lists, vectors, strings, etc. In OCaml, only single assignment is allowed for variables, via the let name = value syntax; however destructive update can be used on elements of arrays and strings with separate <- operator, as well as on fields of records and objects that have been explicitly declared mutable (meaning capable of being changed after their initial declaration) by the programmer.
Functional programming languages that use single assignment include Clojure (for data structures, not vars), Erlang (it accepts multiple assignment if the values are equal, in contrast to Haskell), F#, Haskell, JavaScript (for constants), Lava, OCaml, Oz (for dataflow variables, not cells), Racket (for some data structures like lists, not symbols), SASL, Scala (for vals), SISAL, Standard ML. Non-backtracking Prolog code can be considered explicit single-assignment, explicit in a sense that its (named) variables can be in explicitly unassigned state, or be set exactly once. In Haskell, by contrast, there can be no unassigned variables, and every variable can be thought of as being implicitly set, when it is created, to its value (or rather to a computational object that will produce its value on demand).


== Value of an assignment ==
In some programming languages, an assignment statement returns a value, while in others it does not.
In most expression-oriented programming languages (for example, C), the assignment statement returns the assigned value, allowing such idioms as x = y = a, in which the assignment statement y = a returns the value of a, which is then assigned to x. In a statement such as while ((ch = getchar()) != EOF) {…}, the return value of a function is used to control a loop while assigning that same value to a variable.
In other programming languages, Scheme for example, the return value of an assignment is undefined and such idioms are invalid.
In Haskell, there is no variable assignment; but operations similar to assignment (like assigning to a field of an array or a field of a mutable data structure) usually evaluate to the unit type, which is represented as (). This type has only one possible value, therefore containing no information. It is typically the type of an expression that is evaluated purely for its side effects.


== Variant forms of assignment ==
Certain use patterns are very common, and thus often have special syntax to support them. These are primarily syntactic sugar to reduce redundancy in the source code, but also assists readers of the code in understanding the programmer's intent, and provides the compiler with a clue to possible optimization.


=== Augmented assignment ===

The case where the assigned value depends on a previous one is so common that many imperative languages, most notably C and the majority of its descendants, provide special operators called augmented assignment, like *=, so a = 2*a can instead be written as a *= 2. Beyond syntactic sugar, this assists the task of the compiler by making clear that in-place modification of the variable a is possible.


=== Chained assignment ===
A statement like w = x = y = z is called a chained assignment in which the value of z is assigned to multiple variables w, x, and y. Chained assignments are often used to initialize multiple variables, as in
a = b = c = d = f = 0
Not all programming languages support chained assignment. Chained assignments are equivalent to a sequence of assignments, but the evaluation strategy differs between languages. For simple chained assignments, like initializing multiple variables, the evaluation strategy does not matter, but if the targets (l-values) in the assignment are connected in some way, the evaluation strategy affects the result.
In some programming languages (C for example), chained assignments are supported because assignments are expressions, and have values. In this case chain assignment can be implemented by having a right-associative assignment, and assignments happen right-to-left. For example, i = arr[i] = f() is equivalent to arr[i] = f(); i = arr[i]. In C++ they are also available for values of class types by declaring the appropriate return type for the assignment operator.
In Python, assignment statements are not expressions and thus do not have a value. Instead, chained assignments are a series of statements with multiple targets for a single expression. The assignments are executed left-to-right so that i = arr[i] = f() evaluates the expression f(), then assigns the result to the leftmost target, i, and then assigns the same result to the next target, arr[i], using the new value of i. This is essentially equivalent to tmp = f(); i = tmp; arr[i] = tmp though no actual variable is produced for the temporary value.


=== Parallel assignment ===
Some programming languages, such as APL, Common Lisp, Go, JavaScript (since 1.7), PHP, Maple, Lua, occam 2, Perl, Python, REBOL, Ruby, and PowerShell allow several variables to be assigned in parallel, with syntax like:

a, b := 0, 1

which simultaneously assigns 0 to a and 1 to b. This is most often known as parallel assignment; it was introduced in CPL in 1963, under the name simultaneous assignment, and is sometimes called multiple assignment, though this is confusing when used with ""single assignment"", as these are not opposites. If the right-hand side of the assignment is a single variable (e.g. an array or structure), the feature is called unpacking or destructuring assignment:
var list := {0, 1}
a, b := list

The list will be unpacked so that 0 is assigned to a and 1 to b. Furthermore,

a, b := b, a

swaps the values of a and b. In languages without parallel assignment, this would have to be written to use a temporary variable

var t := a
a := b
b := t

since a := b; b := a leaves both a and b with the original value of b.
Some languages, such as Go and Python, combine parallel assignment, tuples, and automatic tuple unpacking to allow multiple return values from a single function, as in this Python example,

while other languages, such as C# and Rust, shown here, require explicit tuple construction and deconstruction with parentheses:

This provides an alternative to the use of output parameters for returning multiple values from a function. This dates to CLU (1974), and CLU helped popularize parallel assignment generally.
C# additionally allows generalized deconstruction assignment with implementation defined by the expression on the right-hand side, as the compiler searches for an appropriate instance or extension Deconstruct method on the expression, which must have output parameters for the variables being assigned to. For example, one such method that would give the class it appears in the same behavior as the return value of f() above would be

In C and C++, the comma operator is similar to parallel assignment in allowing multiple assignments to occur within a single statement, writing a = 1, b = 2 instead of a, b = 1, 2. 
This is primarily used in for loops, and is replaced by parallel assignment in other languages such as Go.
However, the above C++ code does not ensure perfect simultaneity, since the right side of the following code a = b, b = a+1 is evaluated after the left side. In languages such as Python, a, b = b, a+1 will assign the two variables concurrently, using the initial value of a to compute the new b.


== Assignment versus equality ==

The use of the equals sign = as an assignment operator has been frequently criticized, due to the conflict with equals as comparison for equality. This results both in confusion by novices in writing code, and confusion even by experienced programmers in reading code. The use of equals for assignment dates back to Heinz Rutishauser's language Superplan, designed from 1949 to 1951, and was particularly popularized by Fortran:

A notorious example for a bad idea was the choice of the equal sign to denote assignment. It goes back to Fortran in 1957 and has blindly been copied by armies of language designers. Why is it a bad idea? Because it overthrows a century old tradition to let “=” denote a comparison for equality, a predicate which is either true or false. But Fortran made it to mean assignment, the enforcing of equality. In this case, the operands are on unequal footing: The left operand (a variable) is to be made equal to the right operand (an expression). x = y does not mean the same thing as y = x.
Beginning programmers sometimes confuse assignment with the relational operator for equality, as ""="" means equality in mathematics, and is used for assignment in many languages. But assignment alters the value of a variable, while equality testing tests whether two expressions have the same value.
In some languages, such as BASIC, a single equals sign (""="") is used for both the assignment operator and the equality relational operator, with context determining which is meant. Other languages use different symbols for the two operators. For example:

In ALGOL and Pascal, the assignment operator is a colon and an equals sign ("":="") while the equality operator is a single equals (""="").
In C, the assignment operator is a single equals sign (""="") while the equality operator is a pair of equals signs (""=="").
In R, the assignment operator is basically <-, as in x <- value, but a single equals sign can be used in certain contexts.The similarity in the two symbols can lead to errors if the programmer forgets which form (""="", ""=="", "":="") is appropriate, or mistypes ""="" when ""=="" was intended. This is a common programming problem with languages such as C (including one famous attempt to backdoor the Linux kernel), where the assignment operator also returns the value assigned (in the same way that a function returns a value), and can be validly nested inside expressions. If the intention was to compare two values in an if statement, for instance, an assignment is quite likely to return a value interpretable as Boolean true, in which case the then clause will be executed, leading the program to behave unexpectedly. Some language processors (such as gcc) can detect such situations, and warn the programmer of the potential error.


== Notation ==

The two most common representations for the copying assignment are equals sign (=) and colon-equals (:=). Both forms may semantically denote either an assignment statement or an assignment operator (which also has a value), depending on language and/or usage.

Other possibilities include a left arrow or a keyword, though there are other, rarer, variants:

Mathematical pseudo code assignments are generally depicted with a left-arrow.
Some platforms put the expression on the left and the variable on the right:

Some expression-oriented languages, such as Lisp and Tcl, uniformly use prefix (or postfix) syntax for all statements, including assignment.


== See also ==
Assignment operator in C++
Operator (programming)
Name binding
Unification (computing)
Immutable object
Const-correctness


== Notes ==


== References ==",475319,527,"Articles with example C code, Articles with short description, Assignment operations, CS1 errors: requires URL, Programming language concepts, Short description matches Wikidata, Webarchive template wayback links",1132355460,cs
https://en.wikipedia.org/wiki/Bachelor_of_Computer_Science,Bachelor of Computer Science,"The Bachelor of Computer Science (abbreviated BCompSc or BCS) is a bachelor's degree awarded by some universities for completion of an undergraduate program in computer science. In general, computer science degree programs emphasize the mathematical and theoretical foundations of computing.

","The Bachelor of Computer Science (abbreviated BCompSc or BCS) is a bachelor's degree awarded by some universities for completion of an undergraduate program in computer science. In general, computer science degree programs emphasize the mathematical and theoretical foundations of computing.


== Typical requirements ==
Because computer science is a wide field, courses required to earn a bachelor of computer science degree vary. A typical list of course requirements includes topics such as:
Computer programming
Programming paradigms
Algorithms
Data structures
Logic & Computation
Computer architectureSome schools may place more emphasis on mathematics and require additional courses such as:
Linear algebra
Calculus
Probability theory and statistics
Combinatorics and discrete mathematics
Differential calculus and mathematicsBeyond the basic set of computer science courses, students can typically choose additional courses from a variety of different fields, such as:
Theory of computation
Operating systems
Numerical computation
Compilers, compiler design
Real-time computing
Distributed systems
Computer networking
Data communication
Computer graphics
Artificial intelligence
Human-computer interaction
Information theory
Software testing
Information assuranceSome schools allow students to specialize in a certain area of computer science.


== Related degrees ==
Bachelor of Software Engineering
Bachelor of Science in Information Technology
Bachelor of Computing
Bachelor of Information Technology
Bachelor of Computer Information Systems


== See also ==
Computer science
Computer science and engineering


== References ==",1154301,275,"All articles needing additional references, All articles that may contain original research, Articles needing additional references from July 2017, Articles that may contain original research from December 2021, Articles with short description, Bachelor's degrees, Computer science education, Computer science educators, Short description is different from Wikidata, Webarchive template wayback links",1113699293,cs
https://en.wikipedia.org/wiki/Input_(computer_science),Input (computer science),"In computer science, the general meaning of input is to provide or give something to the computer, in other words, when a computer or device is receiving a command or signal from outer sources, the event is referred to as input to the device.
Some computer devices can also be categorized as input devices  because we use these devices to send instructions to the computer, some common examples of computer input devices are:

Mouse
Keyboard
Touchscreen
Microphone
Webcam
Softcam
Touchpad
Trackpad
Image scanner
TrackballAlso some internal components of computer are input components to other components, like the power-on button of a computer is input component for the processor or the power supply, because it takes user input and sends it to other components for further processing.
In many computer languages the keyword ""input"" is used as a special keyword or function, such as in Visual Basic or Python. The command ""input"" is used to give the machine the data it has to process.

","In computer science, the general meaning of input is to provide or give something to the computer, in other words, when a computer or device is receiving a command or signal from outer sources, the event is referred to as input to the device.
Some computer devices can also be categorized as input devices  because we use these devices to send instructions to the computer, some common examples of computer input devices are:

Mouse
Keyboard
Touchscreen
Microphone
Webcam
Softcam
Touchpad
Trackpad
Image scanner
TrackballAlso some internal components of computer are input components to other components, like the power-on button of a computer is input component for the processor or the power supply, because it takes user input and sends it to other components for further processing.
In many computer languages the keyword ""input"" is used as a special keyword or function, such as in Visual Basic or Python. The command ""input"" is used to give the machine the data it has to process.


== See also ==
Input method
Input device
Input/output


== References ==",185186,59,"Articles with short description, Input/output, Short description is different from Wikidata",1029542255,cs
https://en.wikipedia.org/wiki/Thrashing_(computer_science),Thrashing (computer science),"In computer science, thrashing occurs when a computer's virtual memory resources are overused, leading to a constant state of paging and page faults, inhibiting most application-level processing. This causes the performance of the computer to degrade or collapse. The situation can continue indefinitely until either the user closes some running applications or the active processes free up additional virtual memory resources.
After completing initialization, most programs operate on a small number of code and data pages compared to the total memory the program requires. The pages most frequently accessed are called the working set.
When the working set is a small percentage of the system's total number of pages, virtual memory systems work most efficiently and an insignificant amount of computing is spent resolving page faults. As the working set grows, resolving page faults remains manageable until the growth reaches a critical point. Then faults go up dramatically and the time spent resolving them overwhelms time spent on the computing the program was written to do. This condition is referred to as thrashing. Thrashing occurs on a program that works with huge data structures, as its large working set causes continual page faults that drastically slow down the system. Satisfying page faults may require freeing pages that will soon have to be re-read from disk. 
The term is also used for various similar phenomena, particularly movement between other levels of the memory hierarchy, where a process progresses slowly because significant time is being spent acquiring resources.
""Thrashing"" is also used in contexts other than virtual memory systems; for example, to describe cache issues in computing or silly window syndrome in networking.","In computer science, thrashing occurs when a computer's virtual memory resources are overused, leading to a constant state of paging and page faults, inhibiting most application-level processing. This causes the performance of the computer to degrade or collapse. The situation can continue indefinitely until either the user closes some running applications or the active processes free up additional virtual memory resources.
After completing initialization, most programs operate on a small number of code and data pages compared to the total memory the program requires. The pages most frequently accessed are called the working set.
When the working set is a small percentage of the system's total number of pages, virtual memory systems work most efficiently and an insignificant amount of computing is spent resolving page faults. As the working set grows, resolving page faults remains manageable until the growth reaches a critical point. Then faults go up dramatically and the time spent resolving them overwhelms time spent on the computing the program was written to do. This condition is referred to as thrashing. Thrashing occurs on a program that works with huge data structures, as its large working set causes continual page faults that drastically slow down the system. Satisfying page faults may require freeing pages that will soon have to be re-read from disk. 
The term is also used for various similar phenomena, particularly movement between other levels of the memory hierarchy, where a process progresses slowly because significant time is being spent acquiring resources.
""Thrashing"" is also used in contexts other than virtual memory systems; for example, to describe cache issues in computing or silly window syndrome in networking.


== Overview ==
Virtual memory works by treating a portion of secondary storage such as a computer hard disk as an additional layer of the cache hierarchy.  Virtual memory is notable for allowing processes to use more memory than is physically present in main memory and for enabling virtual machines.  Operating systems supporting virtual memory assign processes a virtual address space and each process refers to addresses in its execution context by a so-called virtual address.  In order to access data such as code or variables at that address, the process must translate the address to a physical address in a process known as virtual address translation.  In effect, physical main memory becomes a cache for virtual memory which is in general stored on disk in memory pages.
Programs are allocated a certain number of pages as needed by the operating system.  Active memory pages exist in both RAM and on disk.  Inactive pages are removed from the cache and written to disk when the main memory becomes full.
If processes are utilizing all main memory and need additional memory pages, a cascade of severe cache misses known as page faults will occur, often leading to a noticeable lag in operating system responsiveness. This process together with the futile, repetitive page swapping that occurs are known as ""thrashing"". This frequently leads to high, runaway CPU utilization that can grind the system to a halt. In modern computers, thrashing may occur in the paging system (if there is not sufficient physical memory or the disk access time is overly long), or in the I/O communications subsystem (especially in conflicts over internal bus access), etc.
Depending on the configuration and algorithms involved, the throughput and latency of a system may degrade by multiple orders of magnitude.  Thrashing is a state in which the CPU performs 'productive' work less, and 'swapping' more. The overall memory access time may increase since the higher level memory is only as fast as the next lower level in the memory hierarchy. The CPU is busy in swapping pages so much that it can not respond to users' programs and interrupts as much as required. Thrashing occurs when there are too many pages in memory, and each page refers to another page. The real memory shortens in capacity to have all the pages in it, so it uses 'virtual memory'. When each page in execution demands that page that is not currently in real memory (RAM) it places some pages on virtual memory and adjusts the required page on RAM. If the CPU is too busy in doing this task, thrashing occurs.


=== Causes ===
In virtual memory systems, thrashing may be caused by programs or workloads that present insufficient locality of reference: if the working set of a program or a workload cannot be effectively held within physical memory, then constant data swapping, i.e., thrashing, may occur. The term was first used during the tape operating system days to describe the sound the tapes made when data was being rapidly written to and read.
A worst-case scenario of this sort on the IBM System/370 series mainframe computer could be an execute instruction crossing a page boundary that points to a move instruction itself also crossing a page boundary, itself pointing to a source and a target that each cross page boundaries.  The total number of pages thus involved in this particular instruction is eight, and all eight pages must be simultaneously present in memory.  If any one of the eight pages can't be swapped in (for example to make room for any of the other pages), the instruction will fault, and every attempt to restart it will fail until all eight pages can be swapped in.
A system thrashing is often a result of a sudden spike in page demanding from a small number of running programs. Swap-token is a lightweight and dynamic thrashing protection mechanism. The basic idea is to set a token in the system, which is randomly given to a process that has page faults when thrashing happens. The process that has the token is given a privilege to allocate more physical memory pages to build its working set, which is expected to quickly finish its execution and to release the memory pages to other processes. A time stamp is used to handover the token one by one. The first version of swap-token is implemented in Linux. The second version is called preempt swap-token. In this updated swap-token implementation, a priority counter is set for each process to track the number of swap-out pages. The token is always given to the process with a high priority, which has a high number of swap-out pages. The length of the time stamp is not a constant but is determined by the priority: the higher the number of swap-out pages of a process, the longer the time stamp for it will be.


== Other uses ==
Thrashing is best known in the context of memory and storage, but analogous phenomena occur for other resources, including:

Cache thrashingWhere main memory is accessed in a pattern that leads to multiple main memory locations competing for the same cache lines, resulting in excessive cache misses.  This is most problematic for caches that have low associativity.

TLB thrashingWhere the translation lookaside buffer (TLB) acting as a cache for the memory management unit (MMU) which translates virtual addresses to physical addresses is too small for the working set of pages. TLB thrashing can occur even if instruction cache or data cache thrashing are not occurring, because these are cached in different sizes. Instructions and data are cached in small blocks (cache lines), not entire pages, but address lookup is done at the page level. Thus even if the code and data working sets fit into cache, if the working sets are fragmented across many pages, the virtual address working set may not fit into TLB, causing TLB thrashing.

Heap thrashingFrequent garbage collection, due to failure to allocate memory for an object, due to insufficient free memory or insufficient contiguous free memory due to memory fragmentation is referred to as heap thrashing.
Process thrashingA similar phenomenon occurs for processes: when the process working set cannot be coscheduled – so not all interacting processes are scheduled to run at the same time – they experience ""process thrashing"" due to being repeatedly scheduled and unscheduled, progressing only slowly.


== See also ==

Page replacement algorithm – Algorithm for virtual memory implementation
Congestion collapse
Resource contention – Conflict over access to a shared resource
Out of memory – State of computer operation where no additional memory can be allocated
Software aging – Tendency of software to fail due to external changes or prolonged operation


== References ==",721869,327,"All articles lacking in-text citations, All pages needing cleanup, Articles lacking in-text citations from February 2010, Articles needing cleanup from January 2015, Articles with multiple maintenance issues, Articles with short description, Cleanup tagged articles with a reason field from January 2015, Short description is different from Wikidata, Virtual memory, Wikipedia pages needing cleanup from January 2015",1120237074,cs
https://en.wikipedia.org/wiki/Decomposition_(computer_science),Decomposition (computer science),"Decomposition in computer science, also known as factoring, is breaking a complex problem or system into parts that are easier to conceive, understand, program, and maintain.","Decomposition in computer science, also known as factoring, is breaking a complex problem or system into parts that are easier to conceive, understand, program, and maintain.


== Overview ==
There are different types of decomposition defined in computer sciences:

In structured programming, algorithmic decomposition breaks a process down into well-defined steps.
Structured analysis breaks down a software system from the system context level to system functions and data entities as described by Tom DeMarco.
Object-oriented decomposition, on the other hand, breaks a large system down into progressively smaller classes or objects that are responsible for some part of the problem domain.
According to Booch, algorithmic decomposition is a necessary part of object-oriented analysis and design, but object-oriented systems start with and emphasize decomposition into objects.More generally, functional decomposition in computer science is a technique for mastering the complexity of the function of a model. A functional model of a system is thereby replaced by a series of functional models of subsystems.


== Decomposition topics ==


=== Decomposition paradigm ===
A decomposition  paradigm in computer programming is a strategy for organizing a program as a number of parts, and it usually implies a specific way to organize a program text. Usually the aim of using a decomposition paradigm is to optimize some metric related to program complexity, for example the modularity of the program or its maintainability.
Most decomposition paradigms suggest breaking down a program into parts so as to minimize the static dependencies among those parts, and to maximize the cohesiveness of each part. Some popular decomposition paradigms are the procedural, modules, abstract data type and object oriented ones.
The concept of decomposition paradigm is entirely independent and different from that of model of computation, but the two are often confused, most often in the cases of the functional model of computation being confused with procedural decomposition, and of the actor model of computation being confused with object oriented decomposition.


=== Decomposition diagram ===

		
		
		
A decomposition diagram shows a complex, process, organization, data subject area, or other type of object broken down into lower level, more detailed components. For example, decomposition diagrams may represent organizational structure or functional decomposition into processes. Decomposition diagrams  provide a logical hierarchical decomposition of a system.


== See also ==
Code refactoring
Component-based software engineering
Dynamization
Duplicate code
Event partitioning
How to Solve It
Integrated Enterprise Modeling
Personal information management
Readability
Subroutine


== References ==


== External links ==

Object Oriented Analysis and Design
On the Criteria To Be Used in Decomposing Systems into Modules",369735,104,"All articles needing additional references, Articles needing additional references from November 2008, Commons category link is on Wikidata, Decomposition methods, Software design",1115604298,cs
https://en.wikipedia.org/wiki/Statement_(computer_science),Statement (computer science),"In computer programming, a statement is a syntactic unit of an imperative programming language that expresses some action to be carried out. A program written in such a language is formed by a sequence of one or more statements. A statement may have internal components (e.g., expressions).
Many programming languages (e.g. Ada, Algol 60, C, Java, Pascal) make a distinction between statements and definitions/declarations. A definition or declaration specifies the data on which a program is to operate, while a statement specifies the actions to be taken with that data.
Statements which cannot contain other statements are simple; those which can contain other statements are compound.The appearance of a statement (and indeed a program) is determined by its syntax or grammar. The meaning of a statement is determined by its semantics.","In computer programming, a statement is a syntactic unit of an imperative programming language that expresses some action to be carried out. A program written in such a language is formed by a sequence of one or more statements. A statement may have internal components (e.g., expressions).
Many programming languages (e.g. Ada, Algol 60, C, Java, Pascal) make a distinction between statements and definitions/declarations. A definition or declaration specifies the data on which a program is to operate, while a statement specifies the actions to be taken with that data.
Statements which cannot contain other statements are simple; those which can contain other statements are compound.The appearance of a statement (and indeed a program) is determined by its syntax or grammar. The meaning of a statement is determined by its semantics.


== Simple statements ==
Simple statements are complete in themselves; these include assignments, subroutine calls, and a few statements which may significantly affect the program flow of control (e.g. goto, return, stop/halt). In some languages, input and output, assertions, and exits are handled by special statements, while other languages use calls to predefined subroutines.

assignment
Fortran: variable = expression
Pascal, Algol 60, Ada: variable := expression;
C, C#, C++, PHP, Java: variable = expression;
call
Fortran: CALL subroutine name(parameters)
C, C++, Java, PHP, Pascal, Ada: subroutine name(parameters);
assertion
C, C++, PHP: assert(relational expression);
Java: assert relational expression;
goto
Fortran: GOTO numbered-label
Algol 60: goto label;
C, C++, PHP, Pascal: goto label;
return
Fortran: RETURN value
C, C++, Java, PHP: return value;
stop/halt/exit
Fortran: STOP number
C, C++: exit(expression)
PHP: exit number;


== Compound statements ==
Compound statements may contain (sequences of) statements, nestable to any reasonable depth, and generally involve tests to decide whether or not to obey or repeat these contained statements.

Notation for the following examples:
<statement> is any single statement (could be simple or compound).
<sequence> is any sequence of zero or more <statements>
Some programming languages provide a general way of grouping statements together, so that any single <statement> can be replaced by a group:Algol 60: begin <sequence> end
Pascal: begin <sequence> end
C, PHP, Java: { <sequence> }Other programming languages have a different special terminator on each kind of compound statement, so that one or more statements are automatically treated as a group:
Ada: if test then <sequence> end if;Many compound statements are loop commands or choice commands. In theory only one of each of these types of commands is required. In practice there are various special cases which occur quite often; these may make a program easier to understand, may make programming easier, and can often be implemented much more efficiently.  There are many subtleties not mentioned here; see the linked articles for details.

count-controlled loop:
Algol 60: for index := 1 step 1 until limit do <statement> ;
Pascal: for index := 1 to limit do <statement> ;
C, Java:  for ( index = 1; index <= limit; index += 1) <statement> ;
Ada: for index in 1..limit loop <sequence> end loop
Fortran 90:condition-controlled loop with test at start of loop:
Algol 60: for index := expression while test do <statement> ;
Pascal: while test do <statement> ;
C, Java: while (test) <statement> ;
Ada: while test loop <sequence> end loop
Fortran 90: 
condition-controlled loop with test at end of loop:
Pascal: repeat <sequence> until test;  { note reversed test}
C, Java: do { <sequence> } while (test) ;
Ada: loop <sequence> exit when test; end loop;
condition-controlled loop with test in the middle of the loop:
C: do { <sequence> if (test) break; <sequence> } while (true) ;
Ada: loop <sequence> exit when test; <sequence> end loop;
if-statement simple situation:
Algol 60:if test then <unconditional statement> ;
Pascal:if test then <statement> ;
C, Java: if (test) <statement> ;
Ada: if test then <sequence> end if;
Fortran 77+: if-statement two-way choice:
Algol 60:if test then <unconditional statement> else <statement> ;
Pascal:if test then <statement> else <statement> ;
C, Java: it (test) <statement> else <statement> ;
Ada: if test then <sequence> else <sequence> end if;
Fortran 77+: 
case/switch statement multi-way choice:
Pascal: case c of 'a': alert(); 'q': quit(); end;
Ada: case c is when 'a' => alert(); when 'q' => quit(); end case;
C, Java: switch (c) { case 'a': alert(); break; case 'q': quit(); break; }
Exception handling:
Ada:  begin protected code except when exception specification => exception handler
Java: try { protected code } catch (exception specification) { exception handler } finally { cleanup }
Python:  try: protected code except exception specification: exception handler else: no exceptions  finally: cleanup


== Syntax ==

Apart from assignments and subroutine calls, most languages start each statement with a special word (e.g. goto, if, while, etc.) as shown in the above examples.  Various methods have been used to describe the form of statements in different languages; the more formal methods tend to be more precise:

Algol 60 used Backus–Naur form (BNF) which set a new level for language grammar specification.
Up until Fortran 77, the language was described in English prose with examples,  From Fortran 90 onwards, the language was described using a variant of BNF.
Cobol used a two-dimensional metalanguage.
Pascal used both syntax diagrams and equivalent BNF.BNF uses recursion to express repetition, so various extensions have been proposed to allow direct indication of repetition.


=== Statements and keywords ===
Some programming language grammars reserve keywords or mark them specially, and do not allow them to be used as identifiers. This often leads to grammars which are easier to parse, requiring less lookahead.


==== No distinguished keywords ====
Fortran and PL/1 do not have reserved keywords, allowing statements like:

in PL/1:
IF IF = THEN THEN ...    (the second IF and the first THEN are variables).
in Fortran:
IF (A) X = 10...              conditional statement (with other variants)
IF (A) = 2               assignment to a subscripted variable named IFAs spaces were optional up to Fortran 95, a typo could completely change the meaning of a statement:DO 10 I = 1,5            start of a loop with I running from 1 to 5
DO 10 I = 1.5            assignment of the value 1.5 to the variable DO10I


==== Flagged words ====

In Algol 60 and Algol 68, special tokens were distinguished explicitly: for publication, in boldface e.g. begin; for programming, with some special marking, e.g., a flag ('begin), quotation marks ('begin'), or underlined (begin on the Elliott 503). This is called ""stropping"".
Tokens that are part of the language syntax thus do not conflict with programmer-defined names.


==== Reserved keywords ====

Certain names are reserved as part of the programming language and can not be used as programmer-defined names.
The majority of the most popular programming languages use reserved keywords. Early examples include FLOW-MATIC (1953) and COBOL (1959). Since 1970 other examples include Ada, C, C++, Java, and Pascal.  The number of reserved words depends on the language: C has about 30 while COBOL has about 400.


== Semantics ==

Semantics is concerned with the meaning of a program. The standards documents for many programming languages use BNF or some equivalent to express the syntax/grammar in a fairly formal and precise way, but the semantics/meaning of the program is generally described using examples and English prose.  This can result in ambiguity.  In some language descriptions the meaning of compound statements is defined by the use of 'simpler' constructions, e.g. a while loop can be defined by a combination of tests, jumps, and labels, using if and goto.
The semantics article describes several mathematical/logical formalisms which have been used to specify  semantics in a precise way; these are generally more complicated than BNF, and no single approach is generally accepted as the way to go.  Some approaches effectively define an interpreter for the language, some use formal logic to reason about a program, some attach affixes to syntactic entities to ensure consistency, etc.


== Expressions ==
A distinction is often made between statements, which are executed, and expressions, which are evaluated. Expressions always evaluate to a value, which statements do not. However, expressions are often used as part of a larger statement.
In most programming languages, a statement can consist little more than an expression, usually by following the expression with a statement terminator (semicolon). In such a case, while the expression evaluates to a value, the complete statement does not (the expression's value is discarded). For instance, in C, C++, C#, and many similar languages, x = y + 1 is an expression that will set x to the value of y plus one, and the whole expression itself will evaluate to the same value that x is set to. However, x = y + 1; (note the semicolon at the end) is a statement that will still set x to the value of y plus one because the expression within the statement is still evaluated, but the result of the expression is discarded, and the statement itself does not evaluate to any value.Expressions can also be contained within other expressions. For instance, the expression x = y + 1 contains the expression y + 1, which in turn contains the values y and 1, which are also technically expressions.
Although the previous examples show assignment expressions, some languages do not implement assignment as an expression, but rather as a statement. A notable example of this is Python, where = is not an operator, but rather just a separator in the assignment statement. Although Python allows multiple assignments as each assignment were an expression, this is simply a special case of the assignment statement built into the language grammar rather than a true expression.


== Extensibility ==
Most languages have a fixed set of statements defined by the language, but there have been experiments with extensible languages that allow the programmer to define new statements.


== See also ==
Comparison of Programming Languages - Statements
Control flow
Expression (contrast)


== References ==


== External links ==
PC ENCYCLOPEDIA: Definition of: program statement",501374,316,"All articles with bare URLs for citations, Articles with GND identifiers, Articles with PDF format bare URLs for citations, Articles with bare URLs for citations from October 2022, Articles with short description, Programming language concepts, Short description is different from Wikidata, Statements",1121251236,cs
https://en.wikipedia.org/wiki/Reference_(computer_science),Reference (computer science),"In computer programming, a reference is a value that enables a program to indirectly access a particular data, such as a variable's value or a record, in the computer's memory or in some other storage device.  The reference is said to refer to the datum, and accessing the datum is called dereferencing the reference. A reference is distinct from the datum itself.
A reference is an abstract data type and may be implemented in many ways. Typically, a reference refers to data stored in memory on a given system, and its internal value is the memory address of the data, i.e. a reference is implemented as a pointer. For this reason a reference is often said to ""point to"" the data. Other implementations include an offset (difference) between the datum's address and some fixed ""base"" address, an index, unique key, or identifier used in a lookup operation into an array or table, an operating system handle, a physical address on a storage device, or a network address such as a URL.","In computer programming, a reference is a value that enables a program to indirectly access a particular data, such as a variable's value or a record, in the computer's memory or in some other storage device.  The reference is said to refer to the datum, and accessing the datum is called dereferencing the reference. A reference is distinct from the datum itself.
A reference is an abstract data type and may be implemented in many ways. Typically, a reference refers to data stored in memory on a given system, and its internal value is the memory address of the data, i.e. a reference is implemented as a pointer. For this reason a reference is often said to ""point to"" the data. Other implementations include an offset (difference) between the datum's address and some fixed ""base"" address, an index, unique key, or identifier used in a lookup operation into an array or table, an operating system handle, a physical address on a storage device, or a network address such as a URL.


== Formal representation ==
A reference R is a value that admits one operation, dereference(R), which yields a value. Usually the reference is typed so that it returns values of a specific type, e.g.:

Often the reference also admits an assignment operation store(R, x), meaning it is an abstract variable.


== Use ==
References are widely used in programming, especially to efficiently pass large or mutable data as arguments to procedures, or to share such data among various uses.  In particular, a reference may point to a variable or record that contains references to other data.  This idea is the basis of indirect addressing and of many linked data structures, such as linked lists. References increase flexibility in where objects can be stored, how they are allocated, and how they are passed between areas of code. As long as one can access a reference to the data, one can access the data through it, and the data itself need not be moved. They also make sharing of data between different code areas easier; each keeps a reference to it.
References can cause significant complexity in a program, partially due to the possibility of dangling and wild references and partially because the topology of data with references is a directed graph, whose analysis can be quite complicated. Nonetheless, references are still simpler to analyze than pointers due to the absence of pointer arithmetic.
The mechanism of references, if varying in implementation, is a fundamental programming language feature common to nearly all modern programming languages. Even some languages that support no direct use of references have some internal or implicit use. For example, the call by reference calling convention can be implemented with either explicit or implicit use of references.


== Examples ==
Pointers are the most primitive type of reference. Due to their intimate relationship with the underlying hardware, they are one of the most powerful and efficient types of references. However, also due to this relationship, pointers require a strong understanding by the programmer of the details of memory architecture. Because pointers store a memory location's address, instead of a value directly, inappropriate use of pointers can lead to undefined behavior in a program, particularly due to dangling pointers or wild pointers. Smart pointers are opaque data structures that act like pointers but can only be accessed through particular methods.
A handle is an abstract reference, and may be represented in various ways. A common example are file handles (the FILE data structure in the C standard I/O library), used to abstract file content. It usually represents both the file itself, as when requesting a lock on the file, and a specific position within the file's content, as when reading a file.
In distributed computing, the reference may contain more than an address or identifier; it may also include an embedded specification of the network protocols used to locate and access the referenced object, the way information is encoded or serialized. Thus, for example, a WSDL description of a remote web service can be viewed as a form of reference; it includes a complete specification of how to locate and bind to a particular web service. A reference to a live distributed object is another example: it is a complete specification for how to construct a small software component called a proxy that will subsequently engage in a peer-to-peer interaction, and through which the local machine may gain access to data that is replicated or exists only as a weakly consistent message stream. In all these cases, the reference includes the full set of instructions, or a recipe, for how to access the data; in this sense, it serves the same purpose as an identifier or address in memory.
If we have a set of keys K and a set of data objects D, any well-defined (single-valued) function from K to D ∪ {null} defines a type of reference, where null is the image of a key not referring to anything meaningful.
An alternative representation of such a function is a directed graph called a reachability graph. Here, each datum is represented by a vertex and there is an edge from u to v if the datum in u refers to the datum in v. The maximum out-degree is one. These graphs are valuable in garbage collection, where they can be used to separate accessible from inaccessible objects.


== External and internal storage ==
In many data structures, large, complex objects are composed of smaller objects. These objects are typically stored in one of two ways:

With internal storage, the contents of the smaller object are stored inside the larger object.
With external storage, the smaller objects are allocated in their own location, and the larger object only stores references to them.Internal storage is usually more efficient, because there is a space cost for the references and dynamic allocation metadata, and a time cost associated with dereferencing a reference and with allocating the memory for the smaller objects. Internal storage also enhances locality of reference by keeping different parts of the same large object close together in memory. However, there are a variety of situations in which external storage is preferred:

If the data structure is recursive, meaning it may contain itself. This cannot be represented in the internal way.
If the larger object is being stored in an area with limited space, such as the stack, then we can prevent running out of storage by storing large component objects in another memory region and referring to them using references.
If the smaller objects may vary in size, it is often inconvenient or expensive to resize the larger object so that it can still contain them.
References are often easier to work with and adapt better to new requirements.Some languages, such as Java, Smalltalk, Python, and Scheme, do not support internal storage.  In these languages, all objects are uniformly accessed through references.


== Language support ==


=== Assembly ===
In assembly language, it is typical to express references using either raw memory addresses or indexes into tables. These work, but are somewhat tricky to use, because an address tells you nothing about the value it points to, not even how large it is or how to interpret it; such information is encoded in the program logic. The result is that misinterpretations can occur in incorrect programs, causing bewildering errors.


=== Lisp ===
One of the earliest opaque references was that of the Lisp language cons cell, which is simply a record containing two references to other Lisp objects, including possibly other cons cells. This simple structure is most commonly used to build singly linked lists, but can also be used to build simple binary trees and so-called ""dotted lists"", which terminate not with a null reference but a value.


=== C/C++ ===

The pointer is still one of the most popular types of references today. It is similar to the assembly representation of a raw address, except that it carries a static datatype which can be used at compile-time to ensure that the data it refers to is not misinterpreted. However, because C has a weak type system which can be violated using casts (explicit conversions between various pointer types and between pointer types and integers), misinterpretation is still possible, if more difficult. Its successor C++ tried to increase type safety of pointers with new cast operators, a reference type &, and smart pointers in its standard library, but still retained the ability to circumvent these safety mechanisms for compatibility.


=== Fortran ===
Fortran does not have an explicit representation of references, but does use them implicitly in its call-by-reference calling semantics. A Fortran reference is best thought of as an alias of another object, such as a scalar variable or a row or column of an array. There is no syntax to dereference the reference or manipulate the contents of the referent directly. Fortran references can be null. As in other languages, these references facilitate the processing of dynamic structures, such as linked lists, queues, and trees.


=== Object-oriented languages ===
A number of object-oriented languages languages such as Eiffel, Java, C#, and Visual Basic have adopted a much more opaque type of reference, usually referred to as simply a reference. These references have types like C pointers indicating how to interpret the data they reference, but they are typesafe in that they cannot be interpreted as a raw address and unsafe conversions are not permitted. References are extensively used to access and assign objects. References are also used in function/method calls or message passing, and reference counts are frequently used to perform garbage collection of unused objects.


=== Functional languages ===
In Standard ML, OCaml, and many other functional languages, most values are persistent: they cannot be modified by assignment. Assignable ""reference cells"" provide mutable variables, data that can be modified. Such reference cells can hold any value, and so are given the polymorphic type α ref, where α is to be replaced with the type of value pointed to. These mutable references can be pointed to different objects over their lifetime. For example, this permits building of circular data structures. The reference cell is functionally equivalent to a mutable array of length 1.
To preserve safety and efficient implementations, references cannot be type-cast in ML, nor can pointer arithmetic be performed. It is important to note that in the functional paradigm, many structures that would be represented using pointers in a language like C are represented using other facilities, such as the powerful algebraic datatype mechanism. The programmer is then able to enjoy certain properties (such as the guarantee of  immutability) while programming, even though the compiler often uses machine pointers ""under the hood"".


=== Perl/PHP ===
Perl supports hard references, which function similarly to those in other languages, and symbolic references, which are just string values that contain the names of variables. When a value that is not a hard reference is dereferenced, Perl considers it to be a symbolic reference and gives the variable with the name given by the value. PHP has a similar feature in the form of its $$var syntax.


== See also ==
Reference type
Abstraction (computer science)
Autovivification
Bounded pointer
Linked data
Magic cookie
Variable (programming)
Weak reference


== References ==


== External links ==

Pointer Fun With Binky Introduction to pointers in a 3-minute educational video – Stanford Computer Science Education Library",569494,346,"All articles needing additional references, Articles needing additional references from November 2009, Articles with short description, Data types, Primitive types, Programming language concepts, Short description is different from Wikidata",1118902981,cs
https://en.wikipedia.org/wiki/Unification_(computer_science),Unification (computer science),"In logic and computer science, unification is an algorithmic process of solving equations between symbolic expressions.
Depending on which expressions (also called terms) are allowed to occur in an equation set (also called unification problem), and which expressions are considered equal, several frameworks of unification are distinguished. If higher-order variables, that is, variables representing functions, are allowed in an expression, the process is called higher-order unification, otherwise first-order unification. If a solution is required to make both sides of each equation literally equal, the process is called syntactic or free unification, otherwise semantic or equational unification, or E-unification, or unification modulo theory.
A solution of a unification problem is denoted as a substitution, that is, a mapping assigning a symbolic value to each variable of the problem's expressions. A unification algorithm should compute for a given problem a complete and minimal substitution set, i.e., a set containing all of the problem's solutions and no redundant members. Depending on the framework, a complete and minimal substitution set may have at most one, at most finitely many, or possibly infinitely many members, or may not exist at all. In some frameworks it is generally impossible to decide whether any solution exists. For first-order syntactical unification, Martelli and Montanari gave an algorithm that reports unsolvability or computes a complete and minimal singleton substitution set containing the so-called most general unifier.
For example, using x,y,z as variables, the singleton equation set { cons(x,cons(x,nil)) = cons(2,y) } is a syntactic first-order unification problem that has the substitution { x ↦ 2, y ↦ cons(2,nil) } as its only solution.
The syntactic first-order unification problem { y = cons(2,y) } has no solution over the set of finite terms; however, it has the single solution { y ↦ cons(2,cons(2,cons(2,...))) } over the set of infinite trees.
The semantic first-order unification problem { a⋅x = x⋅a } has each substitution of the form { x ↦ a⋅...⋅a } as a solution in a semigroup, i.e. if (⋅) is considered associative; the same problem, viewed in an abelian group, where  (⋅) is considered also commutative, has any substitution at all as a solution.
The singleton set { a = y(x) } is a syntactic second-order unification problem, since y is a function variable.
One solution is { x ↦ a, y ↦ (identity function) }; another one is { y ↦ (constant function mapping each value to a), x ↦ (any value) }.
A unification algorithm was first discovered by Jacques Herbrand, while a first formal investigation can be attributed to John Alan Robinson, who used first-order syntactical unification as a basic building block of his resolution procedure for first-order logic, a great step forward in automated reasoning technology, as it eliminated one source of combinatorial explosion: searching for instantiation of terms. Today, automated reasoning is still the main application area of unification.
Syntactical first-order unification is used in logic programming and programming language type system implementation, especially in Hindley–Milner based type inference algorithms.
Semantic unification is used in SMT solvers, term rewriting algorithms and cryptographic protocol analysis.
Higher-order unification is used in proof assistants, for example Isabelle and Twelf, and restricted forms of higher-order unification (higher-order pattern unification) are used in some programming language implementations, such as lambdaProlog, as higher-order patterns are expressive, yet their associated unification procedure retains theoretical properties closer to first-order unification.","In logic and computer science, unification is an algorithmic process of solving equations between symbolic expressions.
Depending on which expressions (also called terms) are allowed to occur in an equation set (also called unification problem), and which expressions are considered equal, several frameworks of unification are distinguished. If higher-order variables, that is, variables representing functions, are allowed in an expression, the process is called higher-order unification, otherwise first-order unification. If a solution is required to make both sides of each equation literally equal, the process is called syntactic or free unification, otherwise semantic or equational unification, or E-unification, or unification modulo theory.
A solution of a unification problem is denoted as a substitution, that is, a mapping assigning a symbolic value to each variable of the problem's expressions. A unification algorithm should compute for a given problem a complete and minimal substitution set, i.e., a set containing all of the problem's solutions and no redundant members. Depending on the framework, a complete and minimal substitution set may have at most one, at most finitely many, or possibly infinitely many members, or may not exist at all. In some frameworks it is generally impossible to decide whether any solution exists. For first-order syntactical unification, Martelli and Montanari gave an algorithm that reports unsolvability or computes a complete and minimal singleton substitution set containing the so-called most general unifier.
For example, using x,y,z as variables, the singleton equation set { cons(x,cons(x,nil)) = cons(2,y) } is a syntactic first-order unification problem that has the substitution { x ↦ 2, y ↦ cons(2,nil) } as its only solution.
The syntactic first-order unification problem { y = cons(2,y) } has no solution over the set of finite terms; however, it has the single solution { y ↦ cons(2,cons(2,cons(2,...))) } over the set of infinite trees.
The semantic first-order unification problem { a⋅x = x⋅a } has each substitution of the form { x ↦ a⋅...⋅a } as a solution in a semigroup, i.e. if (⋅) is considered associative; the same problem, viewed in an abelian group, where  (⋅) is considered also commutative, has any substitution at all as a solution.
The singleton set { a = y(x) } is a syntactic second-order unification problem, since y is a function variable.
One solution is { x ↦ a, y ↦ (identity function) }; another one is { y ↦ (constant function mapping each value to a), x ↦ (any value) }.
A unification algorithm was first discovered by Jacques Herbrand, while a first formal investigation can be attributed to John Alan Robinson, who used first-order syntactical unification as a basic building block of his resolution procedure for first-order logic, a great step forward in automated reasoning technology, as it eliminated one source of combinatorial explosion: searching for instantiation of terms. Today, automated reasoning is still the main application area of unification.
Syntactical first-order unification is used in logic programming and programming language type system implementation, especially in Hindley–Milner based type inference algorithms.
Semantic unification is used in SMT solvers, term rewriting algorithms and cryptographic protocol analysis.
Higher-order unification is used in proof assistants, for example Isabelle and Twelf, and restricted forms of higher-order unification (higher-order pattern unification) are used in some programming language implementations, such as lambdaProlog, as higher-order patterns are expressive, yet their associated unification procedure retains theoretical properties closer to first-order unification.


== Formal definition ==


=== Prerequisites ===
Formally, a unification approach presupposes

An infinite set 
  
    
      
        V
      
    
    {\displaystyle V}
   of variables. For higher-order unification, it is convenient to choose 
  
    
      
        V
      
    
    {\displaystyle V}
   disjoint from the set of lambda-term bound variables.
A set 
  
    
      
        T
      
    
    {\displaystyle T}
   of terms such that 
  
    
      
        V
        ⊆
        T
      
    
    {\displaystyle V\subseteq T}
  . For first-order unification, 
  
    
      
        T
      
    
    {\displaystyle T}
   is usually the set of first-order terms (terms built from variable and function symbols).  For higher-order unification 
  
    
      
        T
      
    
    {\displaystyle T}
   consists of first-order terms and lambda terms (terms containing some higher-order variables).
A mapping vars: 
  
    
      
        T
        →
      
    
    {\displaystyle T\rightarrow }
   
  
    
      
        
          P
        
      
    
    {\displaystyle \mathbb {P} }
  
  
    
      
        (
        V
        )
      
    
    {\displaystyle (V)}
  , assigning to each term 
  
    
      
        t
      
    
    {\displaystyle t}
   the set 
  
    
      
        
          vars
        
        (
        t
        )
        ⊊
        V
      
    
    {\displaystyle {\text{vars}}(t)\subsetneq V}
   of free variables occurring in 
  
    
      
        t
      
    
    {\displaystyle t}
  .
An equivalence relation 
  
    
      
        ≡
      
    
    {\displaystyle \equiv }
   on 
  
    
      
        T
      
    
    {\displaystyle T}
  , indicating which terms are considered equal. For first-order E-unification, 
  
    
      
        ≡
      
    
    {\displaystyle \equiv }
   reflects the background knowledge about certain function symbols; for example, if 
  
    
      
        ⊕
      
    
    {\displaystyle \oplus }
   is considered commutative, 
  
    
      
        t
        ≡
        u
      
    
    {\displaystyle t\equiv u}
   if 
  
    
      
        u
      
    
    {\displaystyle u}
   results from 
  
    
      
        t
      
    
    {\displaystyle t}
   by swapping the arguments of 
  
    
      
        ⊕
      
    
    {\displaystyle \oplus }
   at some (possibly all) occurrences.  In the most typical case that there is no background knowledge at all, then only literally, or syntactically, identical terms are considered equal. In this case, ≡ is called the free theory (because it is a free object), the empty theory (because the set of equational sentences, or the background knowledge, is empty), the theory of uninterpreted functions (because unification is done on uninterpreted terms), or the theory of constructors (because all function symbols just build up data terms, rather than operating on them). For higher-order unification, usually 
  
    
      
        t
        ≡
        u
      
    
    {\displaystyle t\equiv u}
   if 
  
    
      
        t
      
    
    {\displaystyle t}
   and 
  
    
      
        u
      
    
    {\displaystyle u}
   are alpha equivalent.


=== Substitution ===

A substitution is a mapping 
  
    
      
        σ
        :
        V
        →
        T
      
    
    {\displaystyle \sigma :V\rightarrow T}
   from variables to terms; the notation 
  
    
      
        {
        
          x
          
            1
          
        
        ↦
        
          t
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          x
          
            k
          
        
        ↦
        
          t
          
            k
          
        
        }
      
    
    {\displaystyle \{x_{1}\mapsto t_{1},...,x_{k}\mapsto t_{k}\}}
   refers to a substitution mapping each variable 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   to the term 
  
    
      
        
          t
          
            i
          
        
      
    
    {\displaystyle t_{i}}
  , for 
  
    
      
        i
        =
        1
        ,
        .
        .
        .
        ,
        k
      
    
    {\displaystyle i=1,...,k}
  , and every other variable to itself. Applying that substitution to a term 
  
    
      
        t
      
    
    {\displaystyle t}
   is written in postfix notation as 
  
    
      
        t
        {
        
          x
          
            1
          
        
        ↦
        
          t
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          x
          
            k
          
        
        ↦
        
          t
          
            k
          
        
        }
      
    
    {\displaystyle t\{x_{1}\mapsto t_{1},...,x_{k}\mapsto t_{k}\}}
  ; it means to (simultaneously) replace every occurrence of each variable 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   in the term 
  
    
      
        t
      
    
    {\displaystyle t}
   by 
  
    
      
        
          t
          
            i
          
        
      
    
    {\displaystyle t_{i}}
  . The result 
  
    
      
        t
        τ
      
    
    {\displaystyle t\tau }
   of applying a substitution 
  
    
      
        τ
      
    
    {\displaystyle \tau }
   to a term 
  
    
      
        t
      
    
    {\displaystyle t}
   is called an instance of that term 
  
    
      
        t
      
    
    {\displaystyle t}
  .
As a first-order example, applying the substitution { x ↦ h(a,y), z ↦ b } to the term 


=== Generalization, specialization ===
If a term 
  
    
      
        t
      
    
    {\displaystyle t}
   has an instance equivalent to a term 
  
    
      
        u
      
    
    {\displaystyle u}
  , that is, if 
  
    
      
        t
        σ
        ≡
        u
      
    
    {\displaystyle t\sigma \equiv u}
   for some substitution 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
  , then 
  
    
      
        t
      
    
    {\displaystyle t}
   is called more general than 
  
    
      
        u
      
    
    {\displaystyle u}
  , and 
  
    
      
        u
      
    
    {\displaystyle u}
   is called more special than, or subsumed by, 
  
    
      
        t
      
    
    {\displaystyle t}
  . For example, 
  
    
      
        x
        ⊕
        a
      
    
    {\displaystyle x\oplus a}
   is more general than 
  
    
      
        a
        ⊕
        b
      
    
    {\displaystyle a\oplus b}
   if ⊕ is commutative, since then 
  
    
      
        (
        x
        ⊕
        a
        )
        {
        x
        ↦
        b
        }
        =
        b
        ⊕
        a
        ≡
        a
        ⊕
        b
      
    
    {\displaystyle (x\oplus a)\{x\mapsto b\}=b\oplus a\equiv a\oplus b}
  .
If ≡ is literal (syntactic) identity of terms, a term may be both more general and more special than another one only if both terms differ just in their variable names, not in their syntactic structure; such terms are called variants, or renamings of each other.
For example, 

  
    
      
        f
        (
        
          x
          
            1
          
        
        ,
        a
        ,
        g
        (
        
          z
          
            1
          
        
        )
        ,
        
          y
          
            1
          
        
        )
      
    
    {\displaystyle f(x_{1},a,g(z_{1}),y_{1})}
  
is a variant of 

  
    
      
        f
        (
        
          x
          
            2
          
        
        ,
        a
        ,
        g
        (
        
          z
          
            2
          
        
        )
        ,
        
          y
          
            2
          
        
        )
      
    
    {\displaystyle f(x_{2},a,g(z_{2}),y_{2})}
  ,
since

and 

However, 
  
    
      
        f
        (
        
          x
          
            1
          
        
        ,
        a
        ,
        g
        (
        
          z
          
            1
          
        
        )
        ,
        
          y
          
            1
          
        
        )
      
    
    {\displaystyle f(x_{1},a,g(z_{1}),y_{1})}
   is not a variant of  
  
    
      
        f
        (
        
          x
          
            2
          
        
        ,
        a
        ,
        g
        (
        
          x
          
            2
          
        
        )
        ,
        
          x
          
            2
          
        
        )
      
    
    {\displaystyle f(x_{2},a,g(x_{2}),x_{2})}
  , since no substitution can transform the latter term into the former one.
The latter term is therefore properly more special than the former one.
For arbitrary 
  
    
      
        ≡
      
    
    {\displaystyle \equiv }
  , a term may be both more general and more special than a structurally different term.
For example, if ⊕ is idempotent, that is, if always 
  
    
      
        x
        ⊕
        x
        ≡
        x
      
    
    {\displaystyle x\oplus x\equiv x}
  , then the term 
  
    
      
        x
        ⊕
        y
      
    
    {\displaystyle x\oplus y}
   is more general than 
  
    
      
        z
      
    
    {\displaystyle z}
  , and vice versa, although 
  
    
      
        x
        ⊕
        y
      
    
    {\displaystyle x\oplus y}
   and 
  
    
      
        z
      
    
    {\displaystyle z}
   are of different structure.
A substitution 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
   is more special than, or subsumed by, a substitution 
  
    
      
        τ
      
    
    {\displaystyle \tau }
   if 
  
    
      
        t
        σ
      
    
    {\displaystyle t\sigma }
   is more special than 
  
    
      
        t
        τ
      
    
    {\displaystyle t\tau }
   for each term 
  
    
      
        t
      
    
    {\displaystyle t}
  .  We also say that 
  
    
      
        τ
      
    
    {\displaystyle \tau }
   is more general than 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
  .
For instance 
  
    
      
        {
        x
        ↦
        a
        ,
        y
        ↦
        a
        }
      
    
    {\displaystyle \{x\mapsto a,y\mapsto a\}}
   is more special than 
  
    
      
        τ
        =
        {
        x
        ↦
        y
        }
      
    
    {\displaystyle \tau =\{x\mapsto y\}}
  ,
but 

  
    
      
        σ
        =
        {
        x
        ↦
        a
        }
      
    
    {\displaystyle \sigma =\{x\mapsto a\}}
   is not,
as 
  
    
      
        f
        (
        x
        ,
        y
        )
        σ
        =
        f
        (
        a
        ,
        y
        )
      
    
    {\displaystyle f(x,y)\sigma =f(a,y)}
   is not more special than

  
    
      
        f
        (
        x
        ,
        y
        )
        τ
        =
        f
        (
        y
        ,
        y
        )
      
    
    {\displaystyle f(x,y)\tau =f(y,y)}
  .


=== Unification problem, solution set ===
A unification problem is a finite set { l1 ≐ r1, ..., ln ≐ rn } of potential equations, where li, ri ∈ T.
A substitution σ is a solution of that problem if liσ ≡ riσ for 
  
    
      
        i
        =
        1
        ,
        .
        .
        .
        ,
        n
      
    
    {\displaystyle i=1,...,n}
  . Such a substitution is also called a unifier of the unification problem.
For example, if ⊕ is associative, the unification problem { x ⊕ a ≐ a ⊕ x } has the solutions {x ↦ a}, {x ↦ a ⊕ a}, {x ↦ a ⊕ a ⊕ a}, etc., while the problem { x ⊕ a ≐ a } has no solution.
For a given unification problem, a set S of unifiers is called complete if each solution substitution is subsumed by some substitution σ ∈ S; the set S is called minimal if none of its members subsumes another one.


== Syntactic unification of first-order terms ==

Syntactic unification of first-order terms is the most widely used unification framework.
It is based on T being the set of first-order terms (over some given set V of variables, C of constants and Fn of n-ary function symbols) and on ≡ being syntactic equality.
In this framework, each solvable unification problem {l1 ≐ r1, ..., ln ≐ rn} has a complete, and obviously minimal, singleton solution set {σ}.
Its member σ is called the most general unifier (mgu) of the problem.
The terms on the left and the right hand side of each potential equation become syntactically equal when the mgu is applied i.e. l1σ = r1σ ∧ ... ∧ lnσ = rnσ.
Any unifier of the problem is subsumed by the mgu σ.
The mgu is unique up to variants: if S1 and S2 are both complete and minimal solution sets of the same syntactical unification problem, then S1 = { σ1 } and S2 = { σ2 } for some substitutions σ1 and σ2, and xσ1 is a variant of xσ2 for each variable x occurring in the problem.
For example, the unification problem { x ≐ z, y ≐ f(x) } has a unifier { x ↦ z, y ↦ f(z) }, because

This is also the most general unifier.
Other unifiers for the same problem are e.g. { x ↦ f(x1), y ↦ f(f(x1)), z ↦ f(x1) }, { x ↦ f(f(x1)), y ↦ f(f(f(x1))), z ↦ f(f(x1)) }, and so on; there are infinitely many similar unifiers.
As another example, the problem g(x,x) ≐ f(y) has no solution with respect to ≡ being literal identity, since any substitution applied to the left and right hand side will keep the outermost g and f, respectively, and terms with different outermost function symbols are syntactically different.


=== A unification algorithm ===

The first algorithm given by Robinson (1965) was rather inefficient; cf. box.
The following faster algorithm originated from Martelli, Montanari (1982).
This paper also lists preceding attempts to find an efficient syntactical unification algorithm, and states that linear-time algorithms were discovered independently by Martelli, Montanari (1976) and Paterson, Wegman (1976, 1978).Given a finite set 
  
    
      
        G
        =
        {
        
          s
          
            1
          
        
        ≐
        
          t
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          s
          
            n
          
        
        ≐
        
          t
          
            n
          
        
        }
      
    
    {\displaystyle G=\{s_{1}\doteq t_{1},...,s_{n}\doteq t_{n}\}}
   of potential equations,
the algorithm applies rules to transform it to an equivalent set of equations of the form
{ x1 ≐ u1, ..., xm ≐ um }
where x1, ..., xm are distinct variables and u1, ..., um are terms containing none of the xi.
A set of this form can be read as a substitution.
If there is no solution the algorithm terminates with ⊥; other authors use ""Ω"", ""{}"", or ""fail"" in that case.
The operation of substituting all occurrences of variable x in problem G with term t is denoted G {x ↦ t}.
For simplicity, constant symbols are regarded as function symbols having zero arguments.


==== Occurs check ====

An attempt to unify a variable x with a term containing x as a strict subterm x ≐ f(..., x, ...) would lead to an infinite term as solution for x, since x would occur as a subterm of itself.
In the set of (finite) first-order terms as defined above, the equation x ≐ f(..., x, ...) has no solution; hence the eliminate rule may only be applied if x ∉ vars(t).
Since that additional check, called occurs check, slows down the algorithm, it is omitted e.g. in most Prolog systems.
From a theoretical point of view, omitting the check amounts to solving equations over infinite trees, see #Unification of infinite terms below.


==== Proof of termination ====
For the proof of termination of the algorithm consider a triple 
  
    
      
        ⟨
        
          n
          
            v
            a
            r
          
        
        ,
        
          n
          
            l
            h
            s
          
        
        ,
        
          n
          
            e
            q
            n
          
        
        ⟩
      
    
    {\displaystyle \langle n_{var},n_{lhs},n_{eqn}\rangle }
  
where nvar is the number of variables that occur more than once in the equation set, nlhs is the number of function symbols and constants
on the left hand sides of potential equations, and neqn is the number of equations.
When rule eliminate is applied, nvar decreases, since x is eliminated from G and kept only in { x ≐ t }.
Applying any other rule can never increase nvar again.
When rule decompose, conflict, or swap is applied, nlhs decreases, since at least the left hand side's outermost f disappears.
Applying any of the remaining rules delete or check can't increase nlhs, but decreases neqn.
Hence, any rule application decreases the triple 
  
    
      
        ⟨
        
          n
          
            v
            a
            r
          
        
        ,
        
          n
          
            l
            h
            s
          
        
        ,
        
          n
          
            e
            q
            n
          
        
        ⟩
      
    
    {\displaystyle \langle n_{var},n_{lhs},n_{eqn}\rangle }
   with respect to the lexicographical order, which is possible only a finite number of times.
Conor McBride observes that ""by expressing the structure which unification exploits"" in a dependently typed language such as Epigram, Robinson's unification algorithm can be made recursive on the number of variables, in which case a separate termination proof becomes unnecessary.


=== Examples of syntactic unification of first-order terms ===
In the Prolog syntactical convention a symbol starting with an upper case letter is a variable name; a symbol that starts with a lowercase letter is a function symbol; the comma is used as the logical and operator.
For mathematical notation, x,y,z are used as variables, f,g as function symbols, and a,b as constants.

The most general unifier of a syntactic first-order unification problem of size n may have a size of 2n. For example, the problem 
  
    
      
        (
        (
        (
        a
        ∗
        z
        )
        ∗
        y
        )
        ∗
        x
        )
        ∗
        w
        ≐
        w
        ∗
        (
        x
        ∗
        (
        y
        ∗
        (
        z
        ∗
        a
        )
        )
        )
      
    
    {\displaystyle (((a*z)*y)*x)*w\doteq w*(x*(y*(z*a)))}
   has the most general unifier 
  
    
      
        {
        z
        ↦
        a
        ,
        y
        ↦
        a
        ∗
        a
        ,
        x
        ↦
        (
        a
        ∗
        a
        )
        ∗
        (
        a
        ∗
        a
        )
        ,
        w
        ↦
        (
        (
        a
        ∗
        a
        )
        ∗
        (
        a
        ∗
        a
        )
        )
        ∗
        (
        (
        a
        ∗
        a
        )
        ∗
        (
        a
        ∗
        a
        )
        )
        }
      
    
    {\displaystyle \{z\mapsto a,y\mapsto a*a,x\mapsto (a*a)*(a*a),w\mapsto ((a*a)*(a*a))*((a*a)*(a*a))\}}
  , cf. picture. In order to avoid exponential time complexity caused by such blow-up, advanced unification algorithms work on directed acyclic graphs (dags) rather than trees.


=== Application: unification in logic programming ===
The concept of unification is one of the main ideas behind logic programming, best known through the language Prolog. It represents the mechanism of binding the contents of variables and can be viewed as a kind of one-time assignment. In Prolog, this operation is denoted by the equality symbol =, but is also done when instantiating variables (see below). It is also used in other languages by the use of the equality symbol =, but also in conjunction with many operations including +, -, *, /. Type inference algorithms are typically based on unification.
In Prolog:

A variable which is uninstantiated—i.e. no previous unifications were performed on it—can be unified with an atom, a term, or another uninstantiated variable, thus effectively becoming its alias. In many modern Prolog dialects and in first-order logic, a variable cannot be unified with a term that contains it; this is the so-called occurs check.
Two atoms can only be unified if they are identical.
Similarly, a term can be unified with another term if the top function symbols and arities of the terms are identical and if the parameters can be unified simultaneously. Note that this is a recursive behavior.


=== Application: type inference ===
Unification is used during type inference, for instance in the functional programming language Haskell. On one hand, the programmer does not need to provide type information for every function, on the other hand it is used to detect typing errors. The Haskell expression True : ['x', 'y', 'z'] is not correctly typed. The list construction function (:) is of type a -> [a] -> [a], and for the first argument True the polymorphic type variable a has to be unified with True's type, Bool. The second argument, ['x', 'y', 'z'], is of type [Char], but a cannot be both Bool and Char at the same time.
Like for Prolog, an algorithm for type inference can be given:

Any type variable unifies with any type expression, and is instantiated to that expression.  A specific theory might restrict this rule with an occurs check.
Two type constants unify only if they are the same type.
Two type constructions unify only if they are applications of the same type constructor and all of their component types recursively unify.Due to its declarative nature, the order in a sequence of unifications is (usually) unimportant.
Note that in the terminology of first-order logic, an atom is a basic proposition and is unified similarly to a Prolog term.


=== Application: Feature Structure Unification ===
See Feature_structure. 
Unification has been used in different research areas of computational linguistics.


== Order-sorted unification ==
Order-sorted logic allows one to assign a sort, or type, to each term, and to declare a sort s1 a subsort of another sort s2, commonly written as s1 ⊆ s2. For example, when reаsoning about biological creatures, it is useful to declare a sort dog to be a subsort of a sort animal. Wherever a term of some sort s is required, a term of any subsort of s may be supplied instead.
For example, assuming a function declaration mother: animal → animal, and a constant declaration lassie: dog, the term  mother(lassie) is perfectly valid and has the sort animal. In order to supply the information that the mother of a dog is a dog in turn, another declaration mother: dog → dog may be issued; this is called function overloading, similar to overloading in programming languages.
Walther gave a unification algorithm for terms in order-sorted logic, requiring for any two declared sorts s1, s2 their intersection s1 ∩ s2 to be declared, too: if x1 and x2 is a variable of sort s1 and s2, respectively, the equation x1 ≐ x2 has the solution { x1 = x, x2 = x }, where x: s1 ∩ s2.

After incorporating this algorithm into a clause-based automated theorem prover, he could solve a benchmark problem by translating it into order-sorted logic, thereby boiling it down an order of magnitude, as many unary predicates turned into sorts.
Smolka generalized order-sorted logic to allow for parametric polymorphism.

In his framework, subsort declarations are propagated to complex type expressions.
As a programming example, a parametric sort list(X) may be declared (with X being a type parameter as in a C++ template), and from a subsort declaration int ⊆ float the relation list(int) ⊆ list(float) is automatically inferred, meaning that each list of integers is also a list of floats.
Schmidt-Schauß generalized order-sorted logic to allow for term declarations.

As an example, assuming subsort declarations even ⊆ int and odd ⊆ int, a term declaration like ∀ i : int. (i + i) : even allows to declare a property of integer addition that could not be expressed by ordinary overloading.


== Unification of infinite terms ==
Background on infinite trees:

B. Courcelle (1983). ""Fundamental Properties of Infinite Trees"" (PDF). Theoret. Comput. Sci. 25 (2): 95–169. doi:10.1016/0304-3975(83)90059-2. Archived from the original (PDF) on 2014-04-21. Retrieved 2013-06-28.
Michael J. Maher (Jul 1988). ""Complete Axiomatizations of the Algebras of Finite, Rational and Infinite Trees"". Proc. IEEE 3rd Annual Symp. on Logic in Computer Science, Edinburgh. pp. 348–357.
Joxan Jaffar; Peter J. Stuckey (1986). ""Semantics of Infinite Tree Logic Programming"". Theoretical Computer Science. 46: 141–158. doi:10.1016/0304-3975(86)90027-7.Unification algorithm, Prolog II:

A. Colmerauer (1982).  K.L. Clark; S.-A. Tarnlund (eds.). Prolog and Infinite Trees. Academic Press.
Alain Colmerauer (1984). ""Equations and Inequations on Finite and Infinite Trees"".  In ICOT (ed.). Proc. Int. Conf. on Fifth Generation Computer Systems. pp. 85–99.Applications:

Francis Giannesini; Jacques Cohen (1984). ""Parser Generation and Grammar Manipulation using Prolog's Infinite Trees"". Journal of Logic Programming. 1 (3): 253–265. doi:10.1016/0743-1066(84)90013-X.


== E-unification ==
E-unification is the problem of finding solutions to a given set of equations,
taking into account some equational background knowledge E.
The latter is given as a set of universal equalities.
For some particular sets E, equation solving algorithms (a.k.a. E-unification algorithms) have been devised;
for others it has been proven that no such algorithms can exist.
For example, if a and b are distinct constants,
the equation 
  
    
      
        x
        ∗
        a
        ≐
        y
        ∗
        b
      
    
    {\displaystyle x*a\doteq y*b}
   has no solution
with respect to purely syntactic unification,
where nothing is known about the operator 
  
    
      
        ∗
      
    
    {\displaystyle *}
  .
However, if the 
  
    
      
        ∗
      
    
    {\displaystyle *}
   is known to be commutative,
then the substitution {x ↦ b, y ↦ a} solves the above equation,
since

The background knowledge E could state the commutativity of 
  
    
      
        ∗
      
    
    {\displaystyle *}
   by the universal equality
""
  
    
      
        u
        ∗
        v
        =
        v
        ∗
        u
      
    
    {\displaystyle u*v=v*u}
   for all u, v"".


=== Particular background knowledge sets E ===
It is said that unification is decidable for a theory, if a unification algorithm has been devised for it that terminates for any input problem.
It is said that unification is semi-decidable for a theory, if a unification algorithm has been devised for it that terminates for any solvable input problem, but may keep searching forever for solutions of an unsolvable input problem.
Unification is decidable for the following theories:

A
A,C
A,C,I
A,C,Nl
A,I
A,Nl,Nr (monoid)
C
Boolean rings
Abelian groups, even if the signature is expanded by arbitrary additional symbols (but not axioms)
K4 modal algebrasUnification is semi-decidable for the following theories:

A,Dl,Dr
A,C,Dl
Commutative rings


=== One-sided paramodulation ===
If there is a convergent term rewriting system R available for E,
the one-sided paramodulation algorithm
can be used to enumerate all solutions of given equations.

Starting with G being the unification problem to be solved and S being the identity substitution, rules are applied nondeterministically until the empty set appears as the actual G, in which case the actual S is a unifying substitution. Depending on the order the paramodulation rules are applied, on the choice of the actual equation from G, and on the choice of R's rules in mutate, different computations paths are possible. Only some lead to a solution, while others end at a G ≠ {} where no further rule is applicable (e.g. G = { f(...) ≐ g(...) }).

For an example, a term rewrite system R is used defining the append operator of lists built from cons and nil; where cons(x,y) is written in infix notation as x.y for brevity; e.g. app(a.b.nil,c.d.nil) → a.app(b.nil,c.d.nil) → a.b.app(nil,c.d.nil) → a.b.c.d.nil demonstrates the concatenation of the lists a.b.nil and c.d.nil, employing the rewrite rule 2,2, and 1. The equational theory E corresponding to R is the congruence closure of R, both viewed as binary relations on terms.
For example, app(a.b.nil,c.d.nil) ≡ a.b.c.d.nil ≡ app(a.b.c.d.nil,nil). The paramodulation algorithm enumerates solutions to equations with respect to that E when fed with the example R.
A successful example computation path for the unification problem { app(x,app(y,x)) ≐ a.a.nil } is shown below. To avoid variable name clashes, rewrite rules are consistently renamed each time before their use by rule mutate; v2, v3, ... are computer-generated variable names for this purpose. In each line, the chosen equation from G is highlighted in red. Each time the mutate rule is applied, the chosen rewrite rule (1 or 2) is indicated in parentheses. From the last line, the unifying substitution S = { y ↦ nil, x ↦  a.nil } can be obtained. In fact,
app(x,app(y,x)) {y↦nil, x↦ a.nil } = app(a.nil,app(nil,a.nil)) ≡ app(a.nil,a.nil) ≡ a.app(nil,a.nil) ≡ a.a.nil solves the given problem.
A second successful computation path, obtainable by choosing ""mutate(1), mutate(2), mutate(2), mutate(1)"" leads to the substitution S = { y ↦ a.a.nil, x ↦ nil }; it is not shown here. No other path leads to a success.


=== Narrowing ===

If R is a convergent term rewriting system for E,
an approach alternative to the previous section consists in successive application of ""narrowing steps"";
this will eventually enumerate all solutions of a given equation.
A narrowing step (cf. picture) consists in

choosing a nonvariable subterm of the current term,
syntactically unifying it with the left hand side of a rule from R, and
replacing the instantiated rule's right hand side into the instantiated term.Formally, if l → r is a renamed copy of a rewrite rule from R, having no variables in common with a term s, and the subterm s|p is not a variable and is unifiable with l via the mgu σ, then s can be narrowed to the term t = sσ[rσ]p, i.e. to the term sσ, with the subterm at p replaced by rσ. The situation that s can be narrowed to t is commonly denoted as s ↝ t.
Intuitively, a sequence of narrowing steps t1 ↝ t2 ↝ ... ↝ tn can be thought of as a sequence of rewrite steps t1 → t2 → ... → tn, but with the initial term t1 being further and further instantiated, as necessary to make each of the used rules applicable.
The above example paramodulation computation corresponds to the following narrowing sequence (""↓"" indicating instantiation here):

The last term, v2.v2.nil can be syntactically unified with the original right hand side term a.a.nil.
The narrowing lemma ensures that whenever an instance of a term s can be rewritten to a term t by a convergent term rewriting system, then s and t can be narrowed and rewritten to a term s′ and t′, respectively, such that t′ is an instance of s′.
Formally: whenever sσ →∗ t holds for some substitution σ, then there exist terms s′, t′ such that s ↝∗ s′ and t →∗ t′ and s′ τ = t′ for some substitution τ.


== Higher-order unification ==
Many applications require one to consider the unification of typed lambda-terms instead of first-order terms.  Such unification is often called higher-order unification. Higher-order unification is undecidable, and such unification problems do not have most general unifiers. For example, the unification problem { f(a,  b, a) ≐ d(b, a, c) }, where the only variable is f, has the
solutions {f ↦ λx.λy.λz.d(y, x, c)  }, {f ↦ λx.λy.λz.d(y, z, c)  },
{f ↦ λx.λy.λz.d(y, a, c)  }, {f ↦ λx.λy.λz.d(b, x, c)  },
{f ↦ λx.λy.λz.d(b, z, c)  } and {f ↦ λx.λy.λz.d(b, a, c)  }. A well studied branch of higher-order unification is the problem of unifying simply typed lambda terms modulo the equality determined by αβη conversions. Gérard Huet gave a semi-decidable (pre-)unification algorithm that allows a systematic search of the space of unifiers (generalizing the unification algorithm of Martelli-Montanari with rules for terms containing higher-order variables) that seems to work sufficiently well in practice.  Huet and Gilles Dowek have written articles surveying this topic.
Several subsets of higher-order unification are well-behaved, in that they are decidable and have a most-general unifier for solvable problems. One such subset is the previously described first-order terms. Higher-order pattern unification, due to Dale Miller, is another such subset. The higher-order logic programming languages λProlog and Twelf have switched from full higher-order unification to implementing only the pattern fragment; surprisingly pattern unification is sufficient for almost all programs, if each non-pattern unification problem is suspended until a subsequent substitution puts the unification into the pattern fragment. A superset of pattern unification called functions-as-constructors unification is also well-behaved. The Zipperposition theorem prover has an algorithm integrating these well-behaved subsets into a full higher-order unification algorithm.In computational linguistics, one of the most influential theories of elliptical construction is that ellipses are represented by free variables whose values are then determined using Higher-Order Unification (HOU). For instance, the semantic representation of ""Jon likes Mary and Peter does too"" is   like(j, m) ∧ R(p)  and the value of R (the semantic representation of the ellipsis) is determined by the equation  like(j, m) = R(j)  . The process of solving such equations is called Higher-Order Unification.Wayne Snyder gave a generalization of both higher-order unification and E-unification, i.e. an algorithm to unify lambda-terms modulo an equational theory.


== See also ==
Rewriting
Admissible rule
Explicit substitution in lambda calculus
Mathematical equation solving
Dis-unification: solving inequations between symbolic expression
Anti-unification: computing a least general generalization (lgg) of two terms, dual to computing a most general instance (mgu)
Subsumption lattice, a lattice having unification as meet and anti-unification as join
Ontology alignment (use unification with semantic equivalence)


== Notes ==


== References ==


== Further reading ==
Franz Baader and Wayne Snyder (2001). ""Unification Theory"" Archived 2015-06-08 at the Wayback Machine. In John Alan Robinson and Andrei Voronkov, editors, Handbook of Automated Reasoning, volume I, pages 447–533. Elsevier Science Publishers.
Gilles Dowek (2001). ""Higher-order Unification and Matching"". In Handbook of Automated Reasoning.
Franz Baader and Tobias Nipkow (1998). Term Rewriting and All That. Cambridge University Press.
Franz Baader and Jörg H. Siekmann (1993). ""Unification Theory"". In Handbook of Logic in Artificial Intelligence and Logic Programming.
Jean-Pierre Jouannaud and Claude Kirchner (1991). ""Solving Equations in Abstract Algebras: A Rule-Based Survey of Unification"". In Computational Logic: Essays in Honor of Alan Robinson.
Nachum Dershowitz and Jean-Pierre Jouannaud, Rewrite Systems, in: Jan van Leeuwen (ed.), Handbook of Theoretical Computer Science, volume B Formal Models and Semantics, Elsevier, 1990, pp. 243–320
Jörg H. Siekmann (1990). ""Unification Theory"". In Claude Kirchner (editor) Unification. Academic Press.
Kevin Knight (Mar 1989). ""Unification: A Multidisciplinary Survey"" (PDF). ACM Computing Surveys. 21 (1): 93–124. CiteSeerX 10.1.1.64.8967. doi:10.1145/62029.62030. S2CID 14619034.
Gérard Huet and Derek C. Oppen (1980). ""Equations and Rewrite Rules: A Survey"". Technical report. Stanford University.
Raulefs, Peter; Siekmann, Jörg; Szabó, P.; Unvericht, E. (1979). ""A short survey on the state of the art in matching and unification problems"". ACM SIGSAM Bulletin. 13 (2): 14–20. doi:10.1145/1089208.1089210. S2CID 17033087.
Claude Kirchner and Hélène Kirchner. Rewriting, Solving, Proving. In preparation.",299890,539,"All articles to be expanded, All articles with dead external links, Articles to be expanded from December 2021, Articles using small message boxes, Articles with dead external links from March 2021, Articles with permanently dead external links, Automated theorem proving, CS1: long volume value, CS1 maint: multiple names: authors list, Logic in computer science, Logic programming, Rewriting systems, Type theory, Unification (computer science), Webarchive template wayback links",1128929457,cs
https://en.wikipedia.org/wiki/Competition,Competition,"Competition is a rivalry where two or more parties strive for a common goal which cannot be shared: where one's gain is the other's loss (an example of which is a zero-sum game). Competition can arise between entities such as organisms, individuals, economic and social groups, etc. The rivalry can be over attainment of any exclusive goal, including recognition:
Competition occurs in nature, between living organisms which co-exist in the same environment. Animals compete over water supplies, food, mates, and other biological resources. Humans usually compete for food and mates, though when these needs are met deep rivalries often arise over the pursuit of wealth, power, prestige, and fame when in a static, repetitive, or unchanging environment. Competition is a major tenet of market economies and business, often associated with business competition as companies are in competition with at least one other firm over the same group of customers. Competition inside a company is usually stimulated with the larger purpose of meeting and reaching higher quality of services or improved products that the company may produce or develop.
Competition is often considered to be the opposite of cooperation, however in the real world, mixtures of cooperation and competition are the norm. In economies, as the philosopher R. G. Collingwood argued ""the presence of these two opposites together is essential to an economic system. The parties to an economic action co-operate in competing, like two chess players"". Optimal strategies to achieve goals are studied in the branch of mathematics known as game theory.
Competition has been studied in several fields, including psychology, sociology and anthropology. Social psychologists, for instance, study the nature of competition. They investigate the natural urge of competition and its circumstances. They also study group dynamics, to detect how competition emerges and what its effects are. Sociologists, meanwhile, study the effects of competition on society as a whole. Additionally, anthropologists study the history and prehistory of competition in various cultures. They also investigate how competition manifested itself in various cultural settings in the past, and how competition has developed over time.

","Competition is a rivalry where two or more parties strive for a common goal which cannot be shared: where one's gain is the other's loss (an example of which is a zero-sum game). Competition can arise between entities such as organisms, individuals, economic and social groups, etc. The rivalry can be over attainment of any exclusive goal, including recognition:
Competition occurs in nature, between living organisms which co-exist in the same environment. Animals compete over water supplies, food, mates, and other biological resources. Humans usually compete for food and mates, though when these needs are met deep rivalries often arise over the pursuit of wealth, power, prestige, and fame when in a static, repetitive, or unchanging environment. Competition is a major tenet of market economies and business, often associated with business competition as companies are in competition with at least one other firm over the same group of customers. Competition inside a company is usually stimulated with the larger purpose of meeting and reaching higher quality of services or improved products that the company may produce or develop.
Competition is often considered to be the opposite of cooperation, however in the real world, mixtures of cooperation and competition are the norm. In economies, as the philosopher R. G. Collingwood argued ""the presence of these two opposites together is essential to an economic system. The parties to an economic action co-operate in competing, like two chess players"". Optimal strategies to achieve goals are studied in the branch of mathematics known as game theory.
Competition has been studied in several fields, including psychology, sociology and anthropology. Social psychologists, for instance, study the nature of competition. They investigate the natural urge of competition and its circumstances. They also study group dynamics, to detect how competition emerges and what its effects are. Sociologists, meanwhile, study the effects of competition on society as a whole. Additionally, anthropologists study the history and prehistory of competition in various cultures. They also investigate how competition manifested itself in various cultural settings in the past, and how competition has developed over time.


== Biology and ecology ==

Competition within, between, and among species is one of the most important forces in biology, especially in the field of ecology.Competition between members of a species (""intraspecific"") for resources such as food, water, territory, and sunlight may result in an increase in the frequency of a variant of the species best suited for survival and reproduction until its fixation within a population. However, competition among resources also has a strong tendency for diversification between members of the same species, resulting in coexistence of competitive and non-competitive strategies or cycles between low and high competitiveness. Third parties within a species often favour highly competitive strategies leading to species extinction when environmental conditions are harsh (evolutionary suicide).Competition is also present between species (""interspecific""). When resources are limited, several species may depend on these resources. Thus, each of the species competes with the others to gain access to the resources. As a result, species less suited to compete for the resources may die out unless they adapt by character dislocation, for instance. According to evolutionary theory, this competition within and between species for resources plays a significant role in natural selection. At shorter time scales, competition is also one of the most important factors controlling diversity in ecological communities, but at larger scales expansion and contraction of ecological space is a much more larger factor than competition. This is illustrated by living plant communities where asymmetric competition and competitive dominance frequently occur. Multiple examples of symmetric and asymmetric competition also exist for animals.


== Consumer competitions - games of luck or skill ==

In Australia, New Zealand and the United Kingdom, competitions or lotto are the equivalent of what are commonly known as sweepstakes in the United States. The correct technical name for Australian consumer competitions is a trade promotion lottery or lotto.Competition or trade promotion lottery entrants enter to win a prize or prizes, hence many entrants are all in competition, or competing for a limited number of prizes.
A trade promotion lottery or competition is a free entry lottery run to promote goods or services supplied by a business. An example is where you purchase goods or services and then given the chance to enter into the lottery and possibly win a prize. A trade promotion lottery can be called a lotto, competition, contest, sweepstake, or giveaway.
Such competitions can be games of luck (randomly drawn) or skill (judged on an entry question or submission), or possibly a combination of both.
People that enjoy entering competitions are known as compers.
Many compers attend annual national conventions. In 2012 over 100 members of the online competitions community of lottos.com.au from around Australia met on the Gold Coast, Queensland to discuss competitions.


== Competitiveness ==
Many philosophers and psychologists have identified a trait in most living organisms which can drive the particular organism to compete. This trait, called competitiveness, is viewed as having a high adaptive value, which coexists along with the urge for survival. Competitiveness, or the inclination to compete, though, has become synonymous with aggressiveness and ambition in the English language. More advanced civilizations integrate aggressiveness and competitiveness into their interactions, as a way to distribute resources and adapt. Many plants compete with neighboring ones for sunlight.
The term also applies to econometrics. Here, it is a comparative measure of the ability and performance of a firm or sub-sector to sell and produce/supply goods and/or services in a given market. The two academic bodies of thought on the assessment of competitiveness are the Structure Conduct Performance Paradigm and the more contemporary New Empirical Industrial Organisation model. Predicting changes in the competitiveness of business sectors is becoming an integral and explicit step in public policymaking. Within capitalist economic systems, the drive of enterprises is to maintain and improve their own competitiveness.


== Education ==

Competition is a major factor in education. On a global scale, national education systems, intending to bring out the best in the next generation, encourage competitiveness among students through scholarships. Countries such as England and Singapore have special education programmes which cater for specialist students, prompting charges of academic elitism. Upon receipt of their academic results, students tend to compare their grades to see who is better. In severe cases, the pressure to perform in some countries is so high that it can result in stigmatization of intellectually deficient students, or even suicide as a consequence of failing the exams; Japan being a prime example (see Education in Japan). This has resulted in critical re-evaluation of examinations as a whole by educationalists. Critics of competition as a motivating factor in education systems, such as Alfie Kohn, assert that competition actually has a net negative influence on the achievement levels of students, and that it ""turns all of us into losers"". Economist Richard Layard has commented on the harmful effects, stating ""people feel that they are under a great deal of pressure. They feel that their main objective in life is to do better than other people. That is certainly what young people are being taught in school every day. And it's not a good basis for a society.""However, other studies such as the Torrance Tests of Creative Thinking show that the effect of competition on students depends on each individual's level of agency. Students with a high level of agency thrive on competition, are self-motivated, and are willing to risk failure. Compared to their counterparts who are low in agency, these students are more likely to be flexible, adaptable and creative as adults.


== Economics ==

Merriam-Webster gives as one definition of competition (relating to business) as ""[...] rivalry: such as [...] the effort of two or more parties acting independently to secure the business of a third party by offering the most favorable terms"". Adam Smith in his 1776 book The Wealth of Nations and later economists described competition in general as allocating productive resources to their most highly valued uses and encouraging efficiency. Later microeconomic theory distinguished between perfect competition and imperfect competition, concluding that no system of resource allocation is more efficient than perfect competition. Competition, according to the theory, causes commercial firms to develop new products, services and technologies, which would give consumers greater selection and better products. The greater selection typically causes lower prices for the products, compared to what the price would be if there was no competition (monopoly) or little competition (oligopoly).However, competition may also lead to wasted (duplicated) effort and to increased costs (and prices) in some circumstances. For example, the intense competition for the small number of top jobs in music and movie-acting leads many aspiring musicians and actors to make substantial investments in training which are not recouped, because only a fraction become successful. Critics have also argued that competition can be destabilizing, particularly competition between certain financial institutions.
Experts have also questioned the constructiveness of competition in profitability. It has been argued that competition-oriented objectives are counterproductive to raising revenues and profitability because they limit the options of strategies for firms as well as their ability to offer innovative responses to changes in the market. In addition, the strong desire to defeat rival firms with competitive prices has the strong possibility of causing price wars.Another distinction appearing in economics is that between competition as an end-state – as in the case of both perfect and imperfect competition – and competition as a process. That process is typically seen as a process. It is a process of rivalry between firms (or consumers) intensifying selective pressures for improvements. One can restate this as a process of discovery.Three levels of end-state economic competition have been classified:
The most narrow form is direct competition (also called ""category competition"" or ""brand competition""), where products which perform the same function compete against each other. For example, one brand of pick-up trucks competes with several other brands of pick-up trucks. Sometimes, two companies are rivals and one adds new products to their line, which leads to the other company distributing the same new things, and in this manner they compete.
The next form is substitute or indirect competition, where products which are close substitutes for one another compete. For example, butter competes with margarine, with mayonnaise and with other various sauces and spreads.
The broadest form of competition is typically called budget competition. Included in this category is anything on which the consumer might want to spend their available money. For example, a family which has $20,000 available may choose to spend it on many different items, which can all be seen as competing with each other for the family's expenditure. This form of competition is also sometimes described as a competition of ""share of wallet"".In addition, companies compete for financing on the capital markets (equity or debt) in order to generate the necessary cash for their operations. Investor typically consider alternative investment opportunities given their risk profile, and not only look at companies just competing on product (direct competitors). Enlarging the investment universe to include indirect competitors leads to a broader peer universe of comparable, indirectly competing companies.
Competition does not necessarily have to be between companies. For example, business writers sometimes refer to internal competition. This is competition within companies. The idea was first introduced by Alfred Sloan at General Motors in the 1920s. Sloan deliberately created areas of overlap between divisions of the company so that each division would compete with the other divisions. For example, the Chevrolet division would compete with the Pontiac division for some market segments. The competing brands by the same company allowed parts to be designed by one division and shared by several divisions, for example parts designed by Chevrolet would also be used by Pontiac. In 1931 Procter & Gamble initiated a deliberate system of internal brand-versus-brand rivalry. The company was organized around different brands, with each brand allocated resources, including a dedicated group of employees willing to champion the brand. Each brand manager was given responsibility for the success or failure of the brand, and compensated accordingly.
Most businesses also encourage competition between individual employees. An example of this is a contest between sales representatives. The sales representative with the highest sales (or the best improvement in sales) over a period of time would gain benefits from the employer. This is also known as intra-brand competition.
Shalev and Asbjornsen found that success (i.e. the saving resulted) of reverse auctions correlated most closely with competition. The literature widely supported the importance of competition as the primary driver of reverse auctions success. Their findings appear to support that argument, as competition correlated strongly with the reverse auction success, as well as with the number of bidders.Business and economic competition in most countries is often limited or restricted. Competition often is subject to legal restrictions. For example, competition may be legally prohibited, as in the cases of a government monopoly or of a government-granted monopoly. Governments  may institute tariffs, subsidies or other protectionist measures in order to prevent or reduce competition. Depending on the respective economic policy, pure competition is to a greater or lesser extent regulated by competition policy and competition law.  Another component of these activities is the discovery process, with instances of higher government regulations typically leading to less competitive businesses being launched.Nicholas Gruen has referred to The Competition Delusion, in which competition is taken to be unambiguously good, even where that competition leaks into the rules of the game. He claims this drives financialisation (the approximate doubling of proportion of economic resources dedicated to finance and to 'rule making and administering' professions such as law, accountancy and auditing.


=== Interstate ===

Competition between countries is quite subtle to detect, but is quite evident in the world economy. Countries compete to provide the best possible business environment for multinational corporations. Such competition is evident by the policies undertaken by these countries to educate the future workforce. For example, East Asian economies such as Singapore, Japan and South Korea tend to compete by allocating a large portion of the budget to the education sector, including by implementing programmes such as gifted education.


=== Law ===

Competition law, known in the United States as antitrust law, has three main functions:

First, it prohibits agreements aimed to restrict free trading between business entities and their customers. For example, a cartel of sports shops who together fix football-jersey prices higher than normal is illegal.
Second, competition law can ban the existence or abusive behaviour of a firm dominating the market. One case in point could be a software company who through its monopoly on computer platforms makes consumers use its media player.
Third, to preserve competitive markets, the law supervises the mergers and acquisitions of very large corporations. Competition authorities could for instance require that a large packaging company give plastic bottle licenses to competitors before taking over a major PET producer.In all three cases, competition law aims to protect the welfare of consumers by ensuring that each business must compete for its share of the market economy.
In recent decades, competition law has also been sold as good medicine to provide better public services, traditionally funded by tax-payers and administered by democratically accountable governments. Hence competition law is closely connected with the law on deregulation of access to markets, providing state aids and subsidies, the privatisation of state-owned assets and the use of independent sector regulators, such as the United Kingdom telecommunications watchdog Ofcom. Behind the practice lies the theory, which over the last fifty years has been dominated by neo-classical economics. Markets are seen as the most efficient method of allocating resources, although sometimes they fail, and regulation becomes necessary to protect the ideal market model. Behind the theory lies the history, reaching back further than the Roman Empire. The business practices of market traders, guilds and governments have always been subject to scrutiny and sometimes to severe sanctions. Since the twentieth century, competition law has become global. The two largest, most organised and influential systems of competition regulation are United States antitrust law and European Community competition law. The respective national/international authorities, the U.S. Department of Justice (DOJ) and the Federal Trade Commission (FTC) in the United States and the European Commission's Competition Directorate General (DGCOMP) have formed international support- and enforcement-networks. Competition law is growing in importance every day, which warrants for its careful study.


== Game theory ==

Game theory is ""the study of mathematical models of conflict and cooperation between intelligent rational decision-makers."" Game theory is mainly used in economics, political science, and psychology, as well as logic, computer science, biology and poker. Originally, it mainly addressed zero-sum games, in which one person's gains result in losses for the other participants.
Game theory is a major method used in mathematical economics and business for modeling competing behaviors of interacting agents. Applications include a wide array of economic phenomena and approaches, such as auctions, bargaining, mergers & acquisitions pricing, fair division, duopolies, oligopolies, social network formation, agent-based computational economics, general equilibrium, mechanism design, and voting systems; and across such broad areas as experimental economics, behavioral economics, information economics, industrial organization, and political economy.This research usually focuses on particular sets of strategies known as ""solution concepts"" or ""equilibria"". A common assumption is that players act rationally. In non-cooperative games, the most famous of these is the Nash equilibrium. A set of strategies is a Nash equilibrium if each represents a best response to the other strategies. If all the players are playing the strategies in a Nash equilibrium, they have no unilateral incentive to deviate, since their strategy is the best they can do given what others are doing.


== Literature ==
Literary competitions, such as contests sponsored by literary journals, publishing houses and theaters, have increasingly become a means for aspiring writers to gain recognition. Awards for fiction include those sponsored by the Missouri Review, Boston Review, Indiana Review, North American Review and Southwest Review. The Albee Award, sponsored by the Yale Drama Series, is among the most prestigious playwriting awards.


== Philosophy ==
Margaret Heffernan's study, A Bigger Prize,
examines the perils and disadvantages of competition in (for example) biology, families, sport, education, commerce and the Soviet Union.


=== Marx ===
Karl Marx insisted that ""the capitalist system fosters competition and egoism in all its members and thoroughly undermines all genuine forms of community"".
It promotes a ""climate of competitive egoism and individualism"", with competition for jobs and competition between employees; Marx said competition between workers exceeds that demonstrated by company owners. He also points out that competition separates individuals from one another and while concentration of workers and development of better communication alleviate this, they are not a decision.


=== Freud ===
Sigmund Freud explained competition as a primal dilemma in which all infants find themselves. The infant competes with other family members for the attention and affection of the parent of the opposite sex or the primary caregiving parent. During this time, a boy develops a deep fear that the father (the son's prime rival) will punish him for these feelings of desire for the mother, by castrating him. Girls develop penis envy towards all males. The girl's envy is rooted in the biologic fact that, without a penis, she cannot sexually possess mother, as the infantile id demands, resultantly, the girl redirects her desire for sexual union upon father in competitive rivalry with her mother. This constellation of feelings is known as Oedipus Complex (after the Greek Mythology figure who accidentally killed his father and married his mother). This is associated with the phallic stage of childhood development where intense primal emotions of competitive rivalry with (usually) the parent of the same sex are rampant and create a crisis that must be negotiated successfully for healthy psychological development to proceed. Unresolved Oedipus complex competitiveness issues can lead to lifelong neuroses manifesting in various ways related to an overdetermined relationship to competition.


=== Mahatma Gandhi ===
Gandhi speaks of egoistic competition. For him, such qualities glorified and/or left unbridled, can lead to violence, conflict, discord and destructiveness. For Gandhi, competition comes from the ego, and therefore society must be based on mutual love, cooperation and sacrifice for the well-being of humanity. In the society desired by Gandhi, each individual will cooperate and serve for the welfare of others and people will share each other's joys, sorrows and achievements as a norm of a social life. For him, in a non-violent society, competition does not have a place and this should become realized with more people making the personal choice to have fewer tendencies toward egoism and selfishness.


== Politics ==
Competition is also found in politics. In democracies, an election is a competition for an elected office. In other words, two or more candidates strive and compete against one another to attain a position of power. The winner gains the seat of the elected office for a predefined period of time, towards the end of which another election is usually held to determine the next holder of the office.
In addition, there is inevitable competition inside a government. Because several offices are appointed, potential candidates compete against the others in order to gain the particular office. Departments may also compete for a limited amount of resources, such as for funding. Finally, where there are party systems, elected leaders of different parties will ultimately compete against the other parties for laws, funding and power.
Finally, competition also exists between governments. Each country or nationality struggles for world dominance, power, or military strength. For example, the United States competed against the Soviet Union in the Cold War for world power, and the two also struggled over the different types of government (in these cases representative democracy and communism). The result of this type of competition often leads to worldwide tensions, and may sometimes erupt into warfare.


== Sports ==

While some sports and games (such as fishing or hiking) have been viewed as primarily recreational, most sports are considered competitive. The majority involve competition between two or more persons (sometimes using horses or cars). For example, in a game of basketball, two teams compete against one another to determine who can score the most points. When there is no set reward for the winning team, many players gain a sense of pride. In addition, extrinsic rewards may also be given. Athletes, besides competing against other humans, also compete against nature in sports such as whitewater kayaking or mountaineering, where the goal is to reach a destination, with only natural barriers impeding the process. A regularly scheduled (for instance annual) competition meant to determine the ""best"" competitor of that cycle is called a championship.
Competitive sports are governed by codified rules agreed upon by the participants. Violating these rules is considered to be unfair competition. Thus, sports provide artificial (not natural) competition; for example, competing for control of a ball, or defending territory on a playing field is not an innate biological factor in humans. Athletes in sports such as gymnastics and competitive diving compete against each other in order to come closest to a conceptual ideal of a perfect performance, which incorporates measurable criteria and standards which are translated into numerical ratings and scores by appointed judges.
Sports competition is generally broken down into three categories: individual sports, such as archery; dual sports, such as doubles tennis, and team sports competition, such as cricket or football. While most sports competitions are recreation, there exist several major and minor professional sports leagues throughout the world. The Olympic Games, held every four years, is usually regarded as the international pinnacle of sports competition.


== Trade ==
Competition is also found in trade. For nations, as well as firms it is important to understand trade dynamics in order to market their goods and services effectively in international markets. Balance of trade can be considered a crude, but widely used proxy for international competitiveness across levels: country, industry or even firm. Research data hints that exporting firms have a higher survival rate and achieve greater employment growth compared with non-exporters.
Using a simple concept to measure heights that firms can climb may help improve execution of strategies. International competitiveness can be measured on several criteria but few are as flexible and versatile to be applied across levels as Trade Competitiveness Index (TCI) 


=== Hypercompetitiveness ===
The tendency toward extreme, unhealthy competition has been termed hypercompetitiveness. This concept originated in Karen Horney's theories on neurosis; specifically, the highly aggressive personality type which is characterized as ""moving against people"". In her view, some people have a need to compete and win at all costs as a means of maintaining their self-worth. These individuals are likely to turn any activity into a competition, and they will feel threatened if they find themselves losing. Researchers have found that men and women who score high on the trait of hypercompetitiveness are more narcissistic and less psychologically healthy than those who score low on the trait. Hypercompetitive individuals generally believe that winning is the only thing that matters.


== Consequences ==
Competition can have both beneficial and detrimental effects. Many evolutionary biologists view inter-species and intra-species competition as the driving force of adaptation, and ultimately of evolution. However, some biologists disagree, citing competition as a driving force only on a small scale, and citing the larger scale drivers of evolution to be abiotic factors (termed 'Room to Roam'). Richard Dawkins prefers to think of evolution in terms of competition between single genes, which have the welfare of the organism 'in mind' only insofar as that welfare furthers their own selfish drives for replication (termed the 'selfish gene').
Some social Darwinists claim that competition also serves as a mechanism for determining the best-suited group; politically, economically and ecologically. Positively, competition may serve as a form of recreation or a challenge provided that it is non-hostile. On the negative side, competition can cause injury and loss to the organisms involved, and drain valuable resources and energy. In the human species competition can be expensive on many levels, not only in lives lost to war, physical injuries, and damaged psychological well-beings, but also in the health effects from everyday civilian life caused by work stress, long work hours, abusive working relationships, and poor working conditions, that detract from the enjoyment of life, even as such competition results in financial gain for the owners.


== See also ==

Asymmetric competition
Biological interaction
Competition regulator
Competitor analysis
Conflict of interest
Cooperation
Ecological model of competition
Monopolistic competition
Non-zero-sum game
Win-win game
Planned economy
Prisoner's dilemma
Sharing
Student competitions
Zero-profit condition
Zero-sum


== References ==",2205761,1728,"All articles needing additional references, All articles with specifically marked weasel-worded phrases, All articles with unsourced statements, All articles with vague or ambiguous time, Articles needing additional references from September 2022, Articles with GND identifiers, Articles with short description, Articles with specifically marked weasel-worded phrases from March 2020, Articles with unsourced statements from February 2007, Articles with unsourced statements from March 2020, Competition, Short description matches Wikidata, Social events, Vague or ambiguous time from March 2020, Webarchive template wayback links, Wikipedia articles needing clarification from March 2020, Wikipedia articles needing factual verification from March 2020, Wikipedia articles needing page number citations from September 2022",1133962075,cs
https://en.wikipedia.org/wiki/Session_(computer_science),Session (computer science),"In computer science and networking in particular, a session is a time-delimited two-way link, a practical (relatively high) layer in the TCP/IP protocol enabling interactive expression and information exchange between two or more communication devices or ends – be they computers, automated systems, or live active users (see login session). A session is established at a certain point in time, and then ‘torn down’ - brought to an end - at some later point. An established communication session may involve more than one message in each direction. A session is typically stateful, meaning that at least one of the communicating parties needs to hold current state information and save information about the session history to be able to communicate, as opposed to stateless communication, where the communication consists of independent requests with responses.
An established session is the basic requirement to perform a connection-oriented communication. A session also is the basic step to transmit in connectionless communication modes. However, any unidirectional transmission does not define a session.Communication Transport may be implemented as part of protocols and services at the application layer, at the session layer or at the transport layer in the OSI model.

Application layer examples:
HTTP sessions, which allow associating information with individual visitors
A telnet remote login session
Session layer example:
A Session Initiation Protocol (SIP) based Internet phone call
Transport layer example:
A TCP session, which is synonymous to a TCP virtual circuit, a TCP connection, or an established TCP socket.In the case of transport protocols that do not implement a formal session layer (e.g., UDP) or where sessions at the application layer are generally very short-lived (e.g., HTTP), sessions are maintained by a higher level program using a method defined in the data being exchanged.  For example, an HTTP exchange between a browser and a remote host may include an HTTP cookie which identifies state, such as a unique session ID, information about the user's preferences or authorization level.
HTTP/1.0 was thought to only allow a single request and response during one Web/HTTP Session. Protocol version HTTP/1.1 improved this by completing the Common Gateway Interface (CGI), making it easier to maintain the Web Session and supporting HTTP cookies and file uploads.
Most client-server sessions are maintained by the transport layer - a single connection for a single session. However each transaction phase of a Web/HTTP session creates a separate connection. Maintaining session continuity between phases requires a session ID. The session ID is embedded within the <A HREF> or <FORM> links of dynamic web pages so that it is passed back to the CGI. CGI then uses the session ID to ensure session continuity between transaction phases. One advantage of one connection-per-phase is that it works well over low bandwidth (modem) connections.

","In computer science and networking in particular, a session is a time-delimited two-way link, a practical (relatively high) layer in the TCP/IP protocol enabling interactive expression and information exchange between two or more communication devices or ends – be they computers, automated systems, or live active users (see login session). A session is established at a certain point in time, and then ‘torn down’ - brought to an end - at some later point. An established communication session may involve more than one message in each direction. A session is typically stateful, meaning that at least one of the communicating parties needs to hold current state information and save information about the session history to be able to communicate, as opposed to stateless communication, where the communication consists of independent requests with responses.
An established session is the basic requirement to perform a connection-oriented communication. A session also is the basic step to transmit in connectionless communication modes. However, any unidirectional transmission does not define a session.Communication Transport may be implemented as part of protocols and services at the application layer, at the session layer or at the transport layer in the OSI model.

Application layer examples:
HTTP sessions, which allow associating information with individual visitors
A telnet remote login session
Session layer example:
A Session Initiation Protocol (SIP) based Internet phone call
Transport layer example:
A TCP session, which is synonymous to a TCP virtual circuit, a TCP connection, or an established TCP socket.In the case of transport protocols that do not implement a formal session layer (e.g., UDP) or where sessions at the application layer are generally very short-lived (e.g., HTTP), sessions are maintained by a higher level program using a method defined in the data being exchanged.  For example, an HTTP exchange between a browser and a remote host may include an HTTP cookie which identifies state, such as a unique session ID, information about the user's preferences or authorization level.
HTTP/1.0 was thought to only allow a single request and response during one Web/HTTP Session. Protocol version HTTP/1.1 improved this by completing the Common Gateway Interface (CGI), making it easier to maintain the Web Session and supporting HTTP cookies and file uploads.
Most client-server sessions are maintained by the transport layer - a single connection for a single session. However each transaction phase of a Web/HTTP session creates a separate connection. Maintaining session continuity between phases requires a session ID. The session ID is embedded within the <A HREF> or <FORM> links of dynamic web pages so that it is passed back to the CGI. CGI then uses the session ID to ensure session continuity between transaction phases. One advantage of one connection-per-phase is that it works well over low bandwidth (modem) connections.


== Software implementation ==
TCP sessions are typically implemented in software using child processes and/or multithreading, where a new process or thread is created when the computer establishes or joins a session. HTTP sessions are typically not implemented using one thread per session, but by means of a database with information about the state of each session. The advantage with multiple processes or threads is relaxed complexity of the software, since each thread is an instance with its own history and encapsulated variables. The disadvantage is large overhead in terms of system resources, and that the session may be interrupted if the system is restarted.
When a client may connect to any server in a cluster of servers, a special problem is encountered in maintaining consistency when the servers must maintain session state.  The client must either be directed to the same server for the duration of the session, or the servers must transmit server-side session information via a shared file system or database.  Otherwise, the client may reconnect to a different server than the one it started the session with, which will cause problems when the new server does not have access to the stored state of the old one.


== Server-side web sessions ==
Server-side sessions are handy and efficient, but can become difficult to handle in conjunction with load-balancing/high-availability systems and are not usable at all in some embedded systems with no storage. The load-balancing problem can be solved by using shared storage or by applying forced peering between each client and a single server in the cluster, although this can compromise system efficiency and load distribution.
A method of using server-side sessions in systems without mass-storage is to reserve a portion of RAM for storage of session data. This method is applicable for servers with a limited number of clients (e.g. router or access point with infrequent or disallowed access to more than one client at a time).


== Client-side web sessions ==
Client-side sessions use cookies and cryptographic techniques to maintain state without storing as much data on the server. When presenting a dynamic web page, the server sends the current state data to the client (web browser) in the form of a cookie. The client saves the cookie in memory or on disk. With each successive request, the client sends the cookie back to the server, and the server uses the data to ""remember"" the state of the application for that specific client and generate an appropriate response.
This mechanism may work well in some contexts; however, data stored on the client is vulnerable to tampering by the user or by software that has access to the client computer. To use client-side sessions where confidentiality and integrity are required, the following must be guaranteed:

Confidentiality: Nothing apart from the server should be able to interpret session data.
Data integrity: Nothing apart from the server should manipulate session data (accidentally or maliciously).
Authenticity: Nothing apart from the server should be able to initiate valid sessions.To accomplish this, the server needs to encrypt the session data before sending it to the client, and modification of such information by any other party should be prevented via cryptographic means.
Transmitting state back and forth with every request is only practical when the size of the cookie is small. In essence, client-side sessions trade server disk space for the extra bandwidth that each web request will require. Moreover, web browsers limit the number and size of cookies that may be stored by a web site. To improve efficiency and allow for more session data, the server may compress the data before creating the cookie, decompressing it later when the cookie is returned by the client.


== HTTP session token ==
A session token is a unique identifier that is generated and sent from a server to a client to identify the current interaction session. The client usually stores and sends the token as an HTTP cookie and/or sends it as a parameter in GET or POST queries. The reason to use session tokens is that the client only has to handle the identifier—all session data is stored on the server (usually in a database, to which the client does not have direct access) linked to that identifier. Examples of the names that some programming languages use when naming their HTTP cookie include JSESSIONID (JSP), PHPSESSID (PHP), CGISESSID (CGI), and ASPSESSIONID (ASP).


== Session management ==
In human–computer interaction, session management is the process of keeping track of a user's activity across sessions of interaction with the computer system.
Typical session management tasks in a desktop environment include keeping track of which applications are open and which documents each application has opened, so that the same state can be restored when the user logs out and logs in later. For a website, session management might involve requiring the user to re-login if the session has expired (i.e., a certain time limit has passed without user activity). It is also used to store information on the server-side between HTTP requests.


=== Desktop session management ===
A desktop session manager is a program that can save and restore desktop sessions. A desktop session is all the windows currently running and their current content. Session management on Linux-based systems is provided by X session manager. On Microsoft Windows systems, session management is provided by the Session Manager Subsystem (smss.exe); user session functionality can be extended by third-party applications like twinsplay.


=== Browser session management ===
Session management is particularly useful in a web browser where a user can save all open pages and settings and restore them at a later date or on a different  computer (see data portability).
To help recover from a system or application crash, pages and settings can also be restored on next run. Google Chrome, Mozilla Firefox, Internet Explorer, OmniWeb and Opera are examples of web browsers that support session management. Session management is often managed through the application of cookies.


=== Web server session management ===
Hypertext Transfer Protocol (HTTP) is stateless. Session management is the technique used by the web developer to make the stateless HTTP protocol support session state. For example, once a user has been authenticated to the web server, the user's next HTTP request (GET or POST) should not cause the web server to ask for the user's account and password again. For a discussion of the methods used to accomplish this see HTTP cookie and Session ID
In situations where multiple web servers must share knowledge of session state (as is typical in a cluster environment) session information must be shared between the cluster nodes that are running web server software. Methods for sharing session state between nodes in a cluster include: multicasting session information to member nodes (see JGroups for one example of this technique), sharing session information with a partner node using distributed shared memory or memory virtualization, sharing session information between nodes using network sockets, storing session information on a shared file system such as a distributed file system or a global file system, or storing the session information outside the cluster in a database.
If session information is considered transient, volatile data that is not required for non-repudiation of transactions and does not contain data that is subject to compliance auditing (in the U.S. for example, see the Health Insurance Portability and Accountability Act and the Sarbanes–Oxley Act for examples of two laws that necessitate compliance auditing) then any method of storing session information can be used.  However, if session information is subject to audit compliance, consideration should be given to the method used for session storage, replication, and clustering.
In a service-oriented architecture, Simple Object Access Protocol or  SOAP messages constructed with Extensible Markup Language (XML) messages can be used by consumer applications to cause web servers to create sessions.


=== Session management over SMS ===
Just as HTTP is a stateless protocol, so is SMS.  As SMS became interoperable across rival networks in 1999, and text messaging started its ascent towards becoming a ubiquitous global form of communication, various enterprises became interested in using the SMS channel for commercial purposes.  Initial services did not require session management since they were only one-way communications (for example, in 2000, the first mobile news service was delivered via SMS in Finland).  Today, these applications are referred to as application-to-peer (A2P) messaging as distinct from peer-to-peer (P2P) messaging.  The development of interactive enterprise applications required session management, but because SMS is a stateless protocol as defined by the GSM standards, early implementations were controlled client-side by having the end-users enter commands and service identifiers manually.


== See also ==
HTTPS
REST
Session ID
Sessionization
Session fixation
Session poisoning


== References ==


== External links ==
Sessions by Doug Lea",688210,294,"All articles needing additional references, Articles needing additional references from July 2014, Computer networking",1132147435,cs
https://en.wikipedia.org/wiki/Expression_(computer_science),Expression (computer science),"In computer science, an expression is a syntactic entity in a programming language that may be evaluated to determine its value. It is a combination of one or more constants, variables, functions, and operators that the programming language interprets (according to its particular rules of precedence and of association) and computes to produce (""to return"", in a stateful environment) another value. This process, for mathematical expressions, is called evaluation.
In simple settings, the resulting value is usually one of various primitive types, such as numerical, string, boolean, complex data type or other types.
Expression is often contrasted with statement—a syntactic entity that has no value (an instruction).","In computer science, an expression is a syntactic entity in a programming language that may be evaluated to determine its value. It is a combination of one or more constants, variables, functions, and operators that the programming language interprets (according to its particular rules of precedence and of association) and computes to produce (""to return"", in a stateful environment) another value. This process, for mathematical expressions, is called evaluation.
In simple settings, the resulting value is usually one of various primitive types, such as numerical, string, boolean, complex data type or other types.
Expression is often contrasted with statement—a syntactic entity that has no value (an instruction).


== Examples ==
For example, 2 + 3 is both an arithmetic and programming expression, which evaluates to 5. A variable is an expression because it denotes a value in memory, so y + 6 is also an expression. An example of a relational expression is 4 ≠ 4, which evaluates to false.


== Void as a result type ==
In C and most C-derived languages, a call to a function with a void return type is a valid expression, of type void.
Values of type void cannot be used, so the value of such an expression is always thrown away.


== Side effects and elimination ==
In many programming languages a function, and hence an expression containing a function, may have side effects. An expression with side effects does not normally have the property of referential transparency. In many languages (e.g. C++), expressions may be ended with a semicolon (;) to turn the expression into an expression statement.  This asks the implementation to evaluate the expression for its side-effects only and to disregard the result of the expression (e.g. x+1;) unless it is a part of an expression statement that induces side-effects (e.g. y=x+1; or func1(func2());).


=== Caveats ===
Note that the formal notion of a side effect is a change to the abstract state of the running program.
Another class of side effects are changes to the concrete state of the computational system, such as loading data into cache memories.  Languages that are often described as ""side effect–free"" will generally still have concrete side effects that can be exploited, for example, in side-channel attacks.
Furthermore, the elapsed time evaluating an expression (even one with no other apparent side effects), is sometimes essential to the correct operation of a system, as behaviour in time is easily visible from outside the evaluation environment by other parts of the system with which it interacts, and might even be regarded as the primary effect such as when performing benchmark testing.
It depends on the particular programming language specification whether an expression with no abstract side effects can legally be eliminated from the execution path by the processing environment in which the expression is evaluated.


== See also ==
Statement (computer science) (contrast)
Boolean expression
Expression (mathematics)
Evaluation strategy


== References ==


== External links ==
This article is based on material taken from Expression at the Free On-line Dictionary of Computing  prior to 1 November 2008 and incorporated under the ""relicensing"" terms of the GFDL, version 1.3 or later.",424447,130,"Articles with short description, Evaluation strategy, Programming language concepts, Short description is different from Wikidata, Webarchive template wayback links",1122286456,cs
https://en.wikipedia.org/wiki/Circuit_(computer_science),Circuit (computer science),"In theoretical computer science, a circuit is a model of computation in which input values proceed through a sequence of gates, each of which computes a function. Circuits of this kind provide a generalization of Boolean circuits and a mathematical model for digital logic circuits. Circuits are defined by the gates they contain and the values the gates can produce.  For example, the values in a Boolean circuit are boolean values, and the circuit includes conjunction, disjunction, and negation gates.  The values in an integer circuit are sets of integers and the gates compute set union, set intersection, and set complement, as well as the arithmetic operations addition and multiplication.","In theoretical computer science, a circuit is a model of computation in which input values proceed through a sequence of gates, each of which computes a function. Circuits of this kind provide a generalization of Boolean circuits and a mathematical model for digital logic circuits. Circuits are defined by the gates they contain and the values the gates can produce.  For example, the values in a Boolean circuit are boolean values, and the circuit includes conjunction, disjunction, and negation gates.  The values in an integer circuit are sets of integers and the gates compute set union, set intersection, and set complement, as well as the arithmetic operations addition and multiplication.


== Formal definition ==
A circuit is a triple 
  
    
      
        (
        M
        ,
        L
        ,
        G
        )
      
    
    {\displaystyle (M,L,G)}
  , where

  
    
      
        M
      
    
    {\displaystyle M}
   is a set of values,

  
    
      
        L
      
    
    {\displaystyle L}
   is a set of gate labels, each of which is a function from 
  
    
      
        
          M
          
            i
          
        
      
    
    {\displaystyle M^{i}}
   to 
  
    
      
        M
      
    
    {\displaystyle M}
   for some non-negative integer 
  
    
      
        i
      
    
    {\displaystyle i}
   (where 
  
    
      
        i
      
    
    {\displaystyle i}
   represents the number of inputs to the gate), and

  
    
      
        G
      
    
    {\displaystyle G}
   is a labelled directed acyclic graph with labels from 
  
    
      
        L
      
    
    {\displaystyle L}
  .The vertices of the graph are called gates. For each gate 
  
    
      
        g
      
    
    {\displaystyle g}
   of in-degree 
  
    
      
        i
      
    
    {\displaystyle i}
  , the gate 
  
    
      
        g
      
    
    {\displaystyle g}
   can be labeled by an element 
  
    
      
        ℓ
      
    
    {\displaystyle \ell }
   of 
  
    
      
        L
      
    
    {\displaystyle L}
   if and only if 
  
    
      
        ℓ
      
    
    {\displaystyle \ell }
   is defined on 
  
    
      
        
          M
          
            i
          
        
        .
      
    
    {\displaystyle M^{i}.}
  


=== Terminology ===
The gates of in-degree 0 are called inputs or leaves. The gates of out-degree 0 are called outputs. If there is an edge from gate 
  
    
      
        g
      
    
    {\displaystyle g}
   to gate 
  
    
      
        h
      
    
    {\displaystyle h}
   in the graph 
  
    
      
        G
      
    
    {\displaystyle G}
   then 
  
    
      
        h
      
    
    {\displaystyle h}
   is called a child of 
  
    
      
        g
      
    
    {\displaystyle g}
  . We suppose there is an order on the vertices of the graph, so we can speak of the 
  
    
      
        k
      
    
    {\displaystyle k}
  th child of a gate when 
  
    
      
        k
      
    
    {\displaystyle k}
   is less than the out-degree of the gate.
The size of a circuit is the number of nodes of a circuit. The depth of a gate 
  
    
      
        g
      
    
    {\displaystyle g}
   is the length of the longest path in 
  
    
      
        G
      
    
    {\displaystyle G}
   beginning at 
  
    
      
        g
      
    
    {\displaystyle g}
   up to an output gate. In particular, the gates of out-degree 0 are the only gates of depth 1. The depth of a circuit is the maximum depth of any gate. 
Level 
  
    
      
        i
      
    
    {\displaystyle i}
   is the set of all gates of depth 
  
    
      
        i
      
    
    {\displaystyle i}
  . A levelled circuit is a circuit in which the edges to gates of depth 
  
    
      
        i
      
    
    {\displaystyle i}
   comes only from gates of depth 
  
    
      
        i
        +
        1
      
    
    {\displaystyle i+1}
   or from the inputs. In other words, edges only exist between adjacent levels of the circuit. The width of a levelled circuit is the maximum size of any level.


=== Evaluation ===
The exact value 
  
    
      
        V
        (
        g
        )
      
    
    {\displaystyle V(g)}
   of a gate 
  
    
      
        g
      
    
    {\displaystyle g}
   with in-degree 
  
    
      
        i
      
    
    {\displaystyle i}
   and label 
  
    
      
        l
      
    
    {\displaystyle l}
   is defined recursively for all gates 
  
    
      
        g
      
    
    {\displaystyle g}
  .

  
    
      
        V
        (
        g
        )
        =
        
          
            {
            
              
                
                  l
                
                
                  
                    if 
                  
                  g
                  
                     is an input
                  
                
              
              
                
                  l
                  (
                  V
                  (
                  
                    g
                    
                      1
                    
                  
                  )
                  ,
                  …
                  ,
                  V
                  (
                  
                    g
                    
                      i
                    
                  
                  )
                  )
                
                
                  
                    otherwise,
                  
                
              
            
            
          
        
      
    
    {\displaystyle V(g)={\begin{cases}l&{\text{if }}g{\text{ is an input}}\\l(V(g_{1}),\dotsc ,V(g_{i}))&{\text{otherwise,}}\end{cases}}}
  where each 
  
    
      
        
          g
          
            j
          
        
      
    
    {\displaystyle g_{j}}
   is a parent of 
  
    
      
        g
      
    
    {\displaystyle g}
  .
The value of the circuit is the value of each of the output gates.


== Circuits as functions ==
The labels of the leaves can also be variables which take values in 
  
    
      
        M
      
    
    {\displaystyle M}
  . If there are 
  
    
      
        n
      
    
    {\displaystyle n}
   leaves, then the circuit can be seen as a function from  
  
    
      
        
          M
          
            n
          
        
      
    
    {\displaystyle M^{n}}
   to 
  
    
      
        M
      
    
    {\displaystyle M}
  . It is then usual to consider a family of circuits 
  
    
      
        (
        
          C
          
            n
          
        
        
          )
          
            n
            ∈
            
              N
            
          
        
      
    
    {\displaystyle (C_{n})_{n\in \mathbb {N} }}
  , a sequence of circuits indexed by the integers where the circuit 
  
    
      
        
          C
          
            n
          
        
      
    
    {\displaystyle C_{n}}
   has 
  
    
      
        n
      
    
    {\displaystyle n}
   variables. Families of circuits can thus be seen as functions from 
  
    
      
        
          M
          
            ∗
          
        
      
    
    {\displaystyle M^{*}}
   to 
  
    
      
        M
      
    
    {\displaystyle M}
  .
The notions of size, depth and width can be naturally extended to families of functions, becoming functions from 
  
    
      
        
          N
        
      
    
    {\displaystyle \mathbb {N} }
   to 
  
    
      
        
          N
        
      
    
    {\displaystyle \mathbb {N} }
  ; for example, 
  
    
      
        s
        i
        z
        e
        (
        n
        )
      
    
    {\displaystyle size(n)}
   is the size of the  
  
    
      
        n
      
    
    {\displaystyle n}
  th circuit of the family.


== Complexity and algorithmic problems ==
Computing the output of a given Boolean circuit on a specific input is a P-complete problem. If the input is an integer circuit, however, it is unknown whether this problem is decidable.
Circuit complexity attempts to classify Boolean functions with respect to the size or depth of circuits that can compute them.


== See also ==
Arithmetic circuit complexity
Boolean circuit
Circuit complexity
Circuits over sets of natural numbers
The complexity classes NC, AC and TC
Quantum circuit and BQP


== References ==
Vollmer, Heribert (1999). Introduction to Circuit Complexity. Berlin: Springer. ISBN 978-3-540-64310-4.
Yang, Ke (2001). ""Integer Circuit Evaluation Is PSPACE-Complete"". Journal of Computer and System Sciences. 63 (2, September 2001): 288–303. doi:10.1006/jcss.2001.1768. ISSN 0022-0000.",83709,39,"Circuit complexity, Theory of computation",1130988638,cs
https://en.wikipedia.org/wiki/Philosophy_of_computer_science,Philosophy of computer science,"The philosophy of computer science is concerned with the philosophical questions that arise within the study of computer science. There is still no common understanding of the content, aim, focus, or topic of the philosophy of computer science, despite some attempts to develop a philosophy of computer science like the philosophy of physics or the philosophy of mathematics. Due to the abstract nature of computer programs and the technological ambitions of computer science, many of the conceptual questions of the philosophy of computer science are also comparable to the philosophy of science, philosophy of mathematics, and the philosophy of technology.","The philosophy of computer science is concerned with the philosophical questions that arise within the study of computer science. There is still no common understanding of the content, aim, focus, or topic of the philosophy of computer science, despite some attempts to develop a philosophy of computer science like the philosophy of physics or the philosophy of mathematics. Due to the abstract nature of computer programs and the technological ambitions of computer science, many of the conceptual questions of the philosophy of computer science are also comparable to the philosophy of science, philosophy of mathematics, and the philosophy of technology.


== Overview ==
Many of the central philosophical questions of computer science are centered on the logical, ontological and epistemological issues that concern it. Some of these questions may include:

What is computation?
Does the Church–Turing thesis capture the mathematical notion of an effective method in logic and mathematics?
What are the philosophical consequences of the P vs NP problem?
What is information?


== Church–Turing thesis ==
The Church–Turing thesis and its variations are central to the theory of computation. Since, as an informal notion, the concept of effective calculability does not have a formal definition, the thesis, although it has near-universal acceptance, cannot be formally proven. The implications of this thesis is also of philosophical concern. Philosophers have interpreted the Church–Turing thesis as having implications for the philosophy of mind.


== P versus NP problem ==
The P versus NP problem is an unsolved problem in computer science and mathematics. It asks whether every problem whose solution can be verified in polynomial time (and so defined to belong to the class NP) can also be solved in polynomial time (and so defined to belong to the class P). Most computer scientists believe that P ≠ NP. Apart from the reason that after decades of studying these problems no one has been able to find a polynomial-time algorithm for any of more than 3000 important known NP-complete problems, philosophical reasons that concern its implications may have motivated this belief.
For instance, according to Scott Aaronson, the American computer scientist then at MIT:

If P = NP, then the world would be a profoundly different place than we usually assume it to be. There would be no special value in ""creative leaps"", no fundamental gap between solving a problem and recognizing the solution once it's found. Everyone who could appreciate a symphony would be Mozart; everyone who could follow a step-by-step argument would be Gauss.


== See also ==
Computer-assisted proof: Philosophical objections
Artificial philosophy
Philosophy of artificial intelligence
Philosophy of information
Philosophy of mathematics
Philosophy of science
Philosophy of technology


== References ==


== Further reading ==
Matti Tedre (2014). The Science of Computing: Shaping a Discipline. Chapman Hall.
Scott Aaronson. ""Why Philosophers Should Care About Computational Complexity"". In Computability: Gödel, Turing, Church, and beyond.
Timothy Colburn. Philosophy and Computer Science. Explorations in Philosophy. M.E. Sharpe, 1999. ISBN 1-56324-991-X.
A.K. Dewdney. New Turing Omnibus: 66 Excursions in Computer Science
Luciano Floridi (editor). The Blackwell Guide to the Philosophy of Computing and Information, 2004.
Luciano Floridi (editor). Philosophy of Computing and Information: 5 Questions. Automatic Press, 2008.
Luciano Floridi. Philosophy and Computing: An Introduction, Routledge, 1999.
Christian Jongeneel. The informatical worldview, an inquiry into the methodology of computer science.
Jan van Leeuwen. ""Towards a philosophy of the information and computing sciences"", NIAS Newsletter 42, 2009.
Moschovakis, Y. (2001). What is an algorithm? In Enquist, B. and Schmid, W., editors, Mathematics unlimited — 2001 and beyond, pages 919–936. Springer.
Alexander Ollongren, Jaap van den Herik. Filosofie van de informatica. London and New York: Routledge, 1999. ISBN 0-415-19749-X
Tedre, Matti (2014), The Science of Computing: Shaping a Discipline, ISBN 9781482217698 Taylor and Francis.
Ray Turner and Nicola Angius. ""The Philosophy of Computer Science"". Stanford Encyclopedia of Philosophy.
Matti Tedre (2011). Computing as a Science: A Survey of Competing Viewpoints. Minds & Machines 21, 3, 361–387.
Ray Turner. Computational Artefacts-Towards a Philosophy of Computer Science. Springer. [1]


== External links ==
The International Association for Computing and Philosophy
Philosophy of Computing and Information at PhilPapers
Philosophy of Computation at Berkeley
Rapaport, William J. (2020-07-27). ""Philosophy of Computer Science (draft version)"" (PDF). Archived from the original (PDF) on 2021-10-26.",203337,150,"Articles with short description, Philosophy of computer science, Short description is different from Wikidata",1114652614,cs
https://en.wikipedia.org/wiki/AP_Computer_Science,AP Computer Science,"In the United States, Advanced Placement Computer Science (commonly shortened to AP Comp Sci) is a suite of Advanced Placement courses and examinations covering areas of computer science. They are offered by the College Board to high school students as an opportunity to earn college credit for college-level courses.  The suite consists of two current classes and one discontinued class.
AP Computer Science was taught in Pascal for the 1984–1998 exams, in C++ for 1999–2003, and in Java since 2004.","In the United States, Advanced Placement Computer Science (commonly shortened to AP Comp Sci) is a suite of Advanced Placement courses and examinations covering areas of computer science. They are offered by the College Board to high school students as an opportunity to earn college credit for college-level courses.  The suite consists of two current classes and one discontinued class.
AP Computer Science was taught in Pascal for the 1984–1998 exams, in C++ for 1999–2003, and in Java since 2004.


== AP Computer Science A ==
AP Computer Science A is a programming class.  The course emphasizes object-oriented programming methodology, especially problem solving and algorithm development, plus an overview of data structures and abstraction. The AP Computer Science A exam tests students on their knowledge of Java.
It is meant to be the equivalent of a first-semester college course in computer science. 
The Microsoft-sponsored program Technology Education and Literacy in Schools (TEALS) aims to increase the number of students taking AP Computer Science classes.


== AP Computer Science AB (discontinued) ==
AP Computer Science AB included all the topics of AP Computer Science A, as well as a more formal and a more in-depth study of algorithms, data structures, and data abstraction. For example, binary trees were studied in AP Computer Science AB but not in AP Computer Science A. The use of recursive data structures and dynamically allocated structures were fundamental to AP Computer Science AB. 
AP Computer Science AB was equivalent to a full-year college course.Due to low numbers of students taking the exam, AP Computer Science AB was discontinued following the May 2009 exam administration.


== AP Computer Science Principles ==
AP Computer Science Principles is an introductory course to computer science, ""with a focus on how computing powers the world"". It is designed as a complement to AP Computer Science A, to emphasize computational thinking and fluency. It is meant to be the equivalent of a first-semester course in computing.


== See also ==
AP Computer Science A
Computer science
Glossary of computer science
Scope (computer science)
Computer graphics (computer science)


== References ==",278070,292,"Articles with short description, Computer engineering, Computer science education, Short description is different from Wikidata",1128339595,cs
