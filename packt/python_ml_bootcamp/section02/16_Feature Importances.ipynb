{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data= fetch_openml('mnist_784', parser=\"auto\", version=1)#Get data from https://www.openml.org/d/554\n",
    "dfData = pd.DataFrame(np.c_[data[\"data\"],data[\"target\"]],columns = data[\"feature_names\"]+[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a pipeline containing a MinMaxScaler\n",
    "img_pipeline = Pipeline([(\"mm_scaler\", MinMaxScaler())])\n",
    "\n",
    "#define the target values\n",
    "y = dfData[\"target\"]\n",
    "\n",
    "#drop the target values from the dataset X\n",
    "dfData = dfData.drop(\"target\",axis=1)\n",
    "X = dfData.copy()\n",
    "X_transf = img_pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratSplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "\n",
    "#splitting on the transformed dataset (defined before as X_transf)\n",
    "#no need for iloc access because we are using a numpy array and not a pandas dataframe\n",
    "for train_index, test_index in stratSplit.split(X_transf, y):\n",
    "    X_train = X_transf[train_index]\n",
    "    X_test = X_transf[test_index]\n",
    "    \n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing results in a cv function\n",
    "log_reg = LogisticRegression(C=1e5, max_iter=100)\n",
    "def performCV(log_reg, X_train, y_train):\n",
    "    return cross_validate(log_reg,\n",
    "                             X = X_train,\n",
    "                             y = y_train,\n",
    "                            scoring = \"accuracy\",\n",
    "                            cv = 2,\n",
    "                            n_jobs=-1,\n",
    "                            verbose = False,\n",
    "                            return_train_score=True,\n",
    "                            return_estimator=True)\n",
    "results = performCV(log_reg, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.argmax(): is a NumPy function that returns the index of the maximum value in an array.\n",
    "bestEstInd = np.argmax(results[\"test_score\"])\n",
    "best_estimator = results[\"estimator\"][bestEstInd] #our defined index from the line above\n",
    "\n",
    "#accesses the feature importance scores for the first class (assuming a classification problem).\n",
    "#saving the coefficients\n",
    "#coefficients are the weights of each feature we have\n",
    "feature_importaces_zero = best_estimator.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 rows = 10 different classes\n",
    "#784 columns = number of features we have\n",
    "print(best_estimator.coef_.shape)\n",
    "best_estimator.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe to display our features and weights/coefficients \n",
    "#+ absolute value of weights (to avoid negative values)\n",
    "dfImp = pd.DataFrame({\"feature\":dfData.columns,\n",
    "                     \"weight\":feature_importaces_zero,\n",
    "                     \"weightAbs\":np.absolute(feature_importaces_zero)})\n",
    "\n",
    "featureWeights = np.abs(dfImp[\"weight\"].values)\n",
    "# featureWeights[featureWeights<0.1]=0\n",
    "\n",
    "#plotting the feature weights as an image\n",
    "plt.imshow(featureWeights.reshape(28,28)) #28,28 as a size of the matrix: it is also the shape our images have\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.title(\"Feature weight visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cumulative sum of our weights\n",
    "#creating a plot of how much % of our total sum of all the weight values that is contained in a certain number of features\n",
    "#sorting weights by largest to smallest using the absolute values and doing a cumulative sum\n",
    "#cumulative sum: what percentage of a total weight magnitude is contained up to a certain number of features\n",
    "dfImpCumImp = dfImp.sort_values(\"weightAbs\",ascending=False).reset_index(drop=True)\n",
    "dfImpCumImp[\"cumSumWeights\"] = dfImpCumImp[\"weightAbs\"].cumsum()\n",
    "dfImpCumImp[\"cumSumPerc\"] = 100*dfImpCumImp[\"weightAbs\"].cumsum()/dfImpCumImp[\"cumSumWeights\"].iloc[-1]\n",
    "plt.plot(dfImpCumImp.index,dfImpCumImp[\"cumSumPerc\"])\n",
    "plt.title(\"Cumulative weight contained in varying number of features\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Cumulative weight accounted for\")\n",
    "plt.show()\n",
    "xPercTotalWeightsInd = dfImpCumImp.loc[dfImpCumImp[\"cumSumPerc\"]<99].index[-1]\n",
    "\n",
    "#looking at all features that contain 99% of our weight\n",
    "print(\"# features that contain 99% of weight:\",xPercTotalWeightsInd+1)\n",
    "#The weight of the next feature (the one that, when added, would push the cumulative weight above 99%).\n",
    "print(\"Weight of next feature:\",dfImpCumImp.iloc[xPercTotalWeightsInd+1][\"weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the weight\n",
    "#value close to 0 = small, value away from 0 = bigger\n",
    "dfImp.sort_values(\"weight\", ascending=False, inplace=True)\n",
    "print(dfImp.head())\n",
    "print(dfImp.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting: largest weights first\n",
    "sorted(zip(feature_importaces_zero, dfData.columns), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now looking at all feature names where the weight is greater and smaller than -0.04\n",
    "impFeat = dfImp.loc[(dfImp[\"weight\"]>0.04)|(dfImp[\"weight\"]<-0.04)][\"feature\"]\n",
    "impFeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now doing a stratified shuffle split on our 511 features only\n",
    "X_transf_red = img_pipeline.fit_transform(X[impFeat])\n",
    "for train_index, test_index in stratSplit.split(X_transf_red, y):\n",
    "    X_train_red = X_transf_red[train_index]\n",
    "    X_test_red = X_transf_red[test_index]\n",
    "    \n",
    "    y_train_red = y[train_index]\n",
    "    y_test_red = y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_red = performCV(log_reg, X_train_red, y_train_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_red.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing full set to the reduced one\n",
    "def printCVPerformance(results,results_red):\n",
    "    print(\"Number of features all:\",results[\"estimator\"][0].coef_.shape[1])\n",
    "    print(\"Number of features reduced:\",results_red[\"estimator\"][0].coef_.shape[1])\n",
    "    print(\"Test scores with all features:\",round(np.mean(results[\"test_score\"]),4))\n",
    "    print(\"Test scores with reduced feature set:\",round(np.mean(results_red[\"test_score\"]),4))\n",
    "    print()\n",
    "    print(\"Average train time all features:\",round(np.mean(results[\"fit_time\"]),2))\n",
    "    print(\"Average train time all features:\",round(np.mean(results_red[\"fit_time\"]),2))\n",
    "printCVPerformance(results,results_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize model with iterations from 100 to 1000\n",
    "#perform cv on training set and reduced training set\n",
    "#and print out the performance features\n",
    "for max_iter in [100,500,1000]:\n",
    "    log_reg = LogisticRegression(C=1e5, max_iter=max_iter)\n",
    "    results = performCV(log_reg, X_train, y_train)\n",
    "    results_red = performCV(log_reg, X_train_red, y_train_red)\n",
    "    print(\"Performance for max_iter:\",max_iter)\n",
    "    printCVPerformance(results,results_red)\n",
    "    print(\"\\n\")\n",
    "#result on the reduced training set: training time approx. 33% faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Play around with the values e.g. 95% instead of 99% and see if it will improve the results!!! Always try to cut down features (e.g. less important features) to improve your speed and see if it keeps the accuracy on a significant level. Another way (later on) will be dimensionality reduction. If test scores drop it could be overfitting. Validation performance is dropping: also a starting sign for overfitting on the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
