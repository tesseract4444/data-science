{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "t4vZOeetGb3L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "data= fetch_openml('mnist_784', parser=\"auto\", version=1)#Get data from https://www.openml.org/d/554\n",
        "dfData = pd.DataFrame(np.c_[data[\"data\"],data[\"target\"]],columns = data[\"feature_names\"]+[\"target\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fMefcW3WGb3S"
      },
      "outputs": [],
      "source": [
        "#Making about part of our data null\n",
        "fivePer = int(0.05*dfData.shape[0]) #sets 5 percent of our columns to null\n",
        "allInds = np.arange(0,dfData.shape[0],1)\n",
        "for col in dfData:\n",
        "    if col == \"target\":\n",
        "        continue\n",
        "    #get at most 5% unique indeies and set those values to null\n",
        "    indsToNull = np.unique(np.random.choice(allInds,replace=True,size=fivePer))\n",
        "    dfData[col].iloc[indsToNull] = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2O1PvPqXGb3V",
        "outputId": "26822c0a-7ef6-4761-9982-7dd86ca1ddf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of null containing rows: 70000\n",
            "Number of null containing columns: 784\n",
            "Data shape: (70000, 785)\n"
          ]
        }
      ],
      "source": [
        "#High number of rows and columns means it's very likely every row and column contains null values\n",
        "print(\"Number of null containing rows:\",pd.isnull(dfData.astype(float).sum(axis=1,skipna=False)).sum())\n",
        "print(\"Number of null containing columns:\",pd.isnull(dfData.astype(float).sum(axis=0,skipna=False)).sum())\n",
        "print(\"Data shape:\",dfData.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuN2i-m9Gb3Y",
        "outputId": "1f30901f-9662-40ae-a856-a492e8a1e5e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying to train with null values\n",
            "Input X contains NaN.\n",
            "LogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
            "Can't train with null values\n"
          ]
        }
      ],
      "source": [
        "stratSplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
        "\n",
        "for train_index, test_index in stratSplit.split(dfData[data[\"feature_names\"]], dfData[\"target\"]):\n",
        "    X_train = dfData[data[\"feature_names\"]].iloc[train_index].reset_index(drop=True)\n",
        "    X_test = dfData[data[\"feature_names\"]].iloc[test_index].reset_index(drop=True)\n",
        "\n",
        "    y_train = dfData[\"target\"].iloc[train_index].reset_index(drop=True)\n",
        "    y_test = dfData[\"target\"].iloc[test_index].reset_index(drop=True)\n",
        "\n",
        "log_reg = LogisticRegression()\n",
        "print(\"Trying to train with null values\")\n",
        "try:\n",
        "    log_reg.fit(X_train,y_train)\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    print(\"Can't train with null values\")\n",
        "# print(\"Accuracy with null values in data:\",log_reg.score(X_test,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iYiW4bBkGb3b"
      },
      "outputs": [],
      "source": [
        "#Fill in missing data using subject/domain knowledge, or going back to the source to see if missing data\n",
        "#can still be retrieved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoJVtPoqGb3d",
        "outputId": "406fa795-8748-4fd8-8f4b-914cf28e6888"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before: (56000, 784)\n",
            "After: (36559, 784)\n",
            "Before: (56000, 784)\n",
            "After: (55991, 784)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with dropped rows with null values: 0.918411088090305\n"
          ]
        }
      ],
      "source": [
        "#never drop your data randomly, look at it what's better like edges or mid\n",
        "#identify the columns that have the largest amount of missig data and drop these\n",
        "\n",
        "#drop rows with null values\n",
        "#tolerate at most 40 null values in a row\n",
        "numColumns = X_train.shape[1]\n",
        "#X_trainDroppedRows = X_train.dropna(axis=0,how=\"any\",thresh=numColumns-40) #drop rows = axis=0\n",
        "X_trainDroppedRows = X_train.dropna(axis=0,thresh=numColumns-40)\n",
        "print(\"Before:\",X_train.shape)\n",
        "print(\"After:\",X_trainDroppedRows.shape)\n",
        "\n",
        "#any row with at least 8% missing values will be dropped\n",
        "tolPercMissing = 0.08\n",
        "#tolerate at most 8% null values in a row\n",
        "#X_trainDroppedRows = X_train.dropna(axis=0,how=\"any\",thresh=numColumns-numColumns*tolPercMissing)\n",
        "X_trainDroppedRows = X_train.dropna(axis=0, thresh=numColumns-numColumns*tolPercMissing)\n",
        "X_trainDroppedRows = X_trainDroppedRows.fillna(0) #fill missing data with zeroes\n",
        "print(\"Before:\",X_train.shape)\n",
        "print(\"After:\",X_trainDroppedRows.shape)\n",
        "remainingIndeciesTrain = X_trainDroppedRows.index\n",
        "y_trainDroppedRows = y_train.iloc[remainingIndeciesTrain]\n",
        "\n",
        "\n",
        "#X_testDroppedRows = X_test.dropna(axis=0,how=\"any\",thresh=numColumns-numColumns*tolPercMissing)\n",
        "X_testDroppedRows = X_test.dropna(axis=0, thresh=numColumns-numColumns*tolPercMissing)\n",
        "remainingIndeciesTest = X_testDroppedRows.index\n",
        "y_testDroppedRows = y_test.iloc[remainingIndeciesTest]\n",
        "X_testDroppedRows = X_testDroppedRows.fillna(0)\n",
        "\n",
        "\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_trainDroppedRows,y_trainDroppedRows)\n",
        "print(\"Accuracy with dropped rows with null values:\",log_reg.score(X_testDroppedRows,y_testDroppedRows))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1Ye1oVVGb3g",
        "outputId": "0ab3531a-5148-4af8-f079-928c28fd2e94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before: (56000, 784)\n",
            "After: (56000, 0)\n",
            "Before: (56000, 784)\n",
            "After: (56000, 544)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with dropped columns with null values: 0.9139285714285714\n"
          ]
        }
      ],
      "source": [
        "#drop columns with null values\n",
        "#ktolerate at most 1000 null values in a column\n",
        "numRows = X_train.shape[0]\n",
        "#tolerate at most 2000 null values in a column\n",
        "#X_trainDroppedColumns = X_train.dropna(axis=1,how=\"any\",thresh=numRows-2000)\n",
        "X_trainDroppedColumns = X_train.dropna(axis=1,thresh=numRows-2000)\n",
        "print(\"Before:\",X_train.shape)\n",
        "print(\"After:\",X_trainDroppedColumns.shape)\n",
        "\n",
        "tolPercMissing = 0.049\n",
        "#tolerate at most 4.9% null values in a column\n",
        "#(in this case we know that at most 5% of each column is null)\n",
        "#X_trainDroppedColumns = X_train.dropna(axis=1,how=\"any\",thresh=numRows-numRows*tolPercMissing)\n",
        "X_trainDroppedColumns = X_train.dropna(axis=1, thresh=numRows-numRows*tolPercMissing)\n",
        "print(\"Before:\",X_train.shape)\n",
        "print(\"After:\",X_trainDroppedColumns.shape)\n",
        "remainingColumns = X_trainDroppedColumns.columns\n",
        "\n",
        "X_trainDroppedColumns = X_trainDroppedColumns.fillna(0)\n",
        "X_testDroppedColumns = X_test[remainingColumns].fillna(0)\n",
        "\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_trainDroppedColumns,y_train)\n",
        "print(\"Accuracy with dropped columns with null values:\",log_reg.score(X_testDroppedColumns,y_test))\n",
        "\n",
        "#result:\n",
        "#first iteration (2000 rows tolerated) leads to 0 columns which is very bad\n",
        "#2nd iteration (4.9% tolerated) leads to 576 columsn left which seems to be a good choice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySESennoGb3i",
        "outputId": "1fa040a7-d2f5-4c42-c92e-20e39b416ccd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with mean imputed null values: 0.9195714285714286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "#Simple Imputing to fill in data according to a distinct strategy (here: mean)\n",
        "from sklearn.impute import SimpleImputer\n",
        "mean_imputer = SimpleImputer(strategy=\"mean\")#\"mean\",\"median\",\"most_frequent\",\"constant\"\n",
        "#                        fill_value=0.5) #even the fill value can be defined (chossing constant)\n",
        "\n",
        "X_train_mean_imputed = mean_imputer.fit_transform(X_train)\n",
        "X_test_mean_imputed = mean_imputer.transform(X_test)\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train_mean_imputed,y_train)\n",
        "print(\"Accuracy with mean imputed null values:\",log_reg.score(X_test_mean_imputed,y_test))\n",
        "\n",
        "#result: only slightly higher than before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSUhNnVwGb3k"
      },
      "outputs": [],
      "source": [
        "#takes very long!!!\n",
        "\n",
        "#KNN Imputing: use the nearest neigbor method\n",
        "#look for the most similar values to the ones you have used now\n",
        "#filling in the data similar to the ones used in the current observation\n",
        "#for each row: trying to find the n-nearest similar row\n",
        "from sklearn.impute import KNNImputer\n",
        "#Find N nearest samples (if features that neither is missing are close)\n",
        "#Using 1 because large sample size and otherwise finding higher number of nearest neighbours would take long\n",
        "knn_imputer = KNNImputer(n_neighbors=1)#different distance definions (documentation)\n",
        "\n",
        "X_train_knn_imputed = knn_imputer.fit_transform(X_train)\n",
        "X_test_knn_imputed = knn_imputer.transform(X_test)\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train_knn_imputed,y_train)\n",
        "print(\"Accuracy with knn imputed null values:\",log_reg.score(X_test_knn_imputed,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "ZrpY-bxTGb3p",
        "outputId": "f15975e0-3e50-4e2e-8fc1-ff72954d68a9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1802b728dcd2>\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m#Transforming data since each function is return an array, sklearn doesn't like the returned dataformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m#so we put it back into standard nested list/array format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mX_train_custom_impute\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimputeRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mX_test_custom_impute\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimputeRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0my_train_custom_impute\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "#individual algorithm by Max\n",
        "#look at each pixel and transform it according to the average amount of pixels surrounded\n",
        "#custom imputing based on knowledge of problem\n",
        "def getNearestPixels(row,column,maxRow,maxColumn):\n",
        "    nearestpixels = []\n",
        "    #left\n",
        "    if column>0:\n",
        "        nearestpixels.append([row,column-1])\n",
        "        #left diagonal down\n",
        "        if row<maxRow:\n",
        "            nearestpixels.append([row+1,column-1])\n",
        "        #left diagonal up\n",
        "        if row>0:\n",
        "            nearestpixels.append([row-1,column-1])\n",
        "    #right\n",
        "    if column<maxColumn:\n",
        "        nearestpixels.append([row,column+1])\n",
        "        #right diagonal down\n",
        "        if row<maxRow:\n",
        "            nearestpixels.append([row+1,column+1])\n",
        "        #right diagonal up\n",
        "        if row>0:\n",
        "            nearestpixels.append([row-1,column+1])\n",
        "\n",
        "    #up\n",
        "    if row>0:\n",
        "        nearestpixels.append([row-1,column])\n",
        "\n",
        "    #down\n",
        "    if row<maxRow:\n",
        "        nearestpixels.append([row+1,column])\n",
        "    return nearestpixels\n",
        "\n",
        "def imputeRow(pixels):\n",
        "    reshaped = np.reshape(pixels.values,(int(np.sqrt(pixels.size)),-1))\n",
        "    maxRow = reshaped.shape[0]\n",
        "    maxColumn = reshaped.shape[1]\n",
        "    for row in range(maxRow):\n",
        "        for column in range(maxColumn):\n",
        "            if pd.isnull(reshaped[row,column]):\n",
        "                #if a pixel is null find the indecies of the surrounding pixels\n",
        "                nearestpixels = getNearestPixels(row,column,maxRow-1,maxColumn-1)\n",
        "                sur = []\n",
        "                #get values of surrounding pixels\n",
        "                for inds in nearestpixels:\n",
        "                    sur.append(reshaped[inds[0],inds[1]])\n",
        "                #Setting value of null to mean of surrounding pixels\n",
        "                reshaped[row,column] = np.nanmean(sur)\n",
        "\n",
        "    return reshaped.reshape(1,pixels.size)[0]\n",
        "\n",
        "\n",
        "#Applying function to each row\n",
        "#Transforming data since each function is return an array, sklearn doesn't like the returned dataformat\n",
        "#so we put it back into standard nested list/array format\n",
        "X_train_custom_impute = [x for x in X_train.apply(lambda row: imputeRow(row),axis=1).values]\n",
        "X_test_custom_impute = [x for x in X_test.apply(lambda row: imputeRow(row),axis=1).values]\n",
        "y_train_custom_impute = y_train.values\n",
        "y_test_custom_impute = y_test.values\n",
        "\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train_custom_impute,y_train_custom_impute)\n",
        "print(\"Accuracy with custom imputed null values:\",log_reg.score(X_test_custom_impute,y_test_custom_impute))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCTIo9yBGb3r"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}