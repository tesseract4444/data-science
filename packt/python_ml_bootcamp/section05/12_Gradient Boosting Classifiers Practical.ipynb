{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lViw9zbRT4AM","executionInfo":{"status":"ok","timestamp":1708513107694,"user_tz":-60,"elapsed":90665,"user":{"displayName":"Sim Don","userId":"09290070938786203900"}},"outputId":"375b7e5e-8d4b-4391-8412-294b4a982671"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n","  warn(\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.tree import DecisionTreeClassifier\n","\n","data = fetch_openml('mnist_784', version=1)#Get data from https://www.openml.org/d/554\n","dfData = pd.DataFrame(np.c_[data[\"data\"],data[\"target\"]],columns = data[\"feature_names\"]+[\"target\"])"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"xSg8QGrfT4AQ","executionInfo":{"status":"ok","timestamp":1708513159486,"user_tz":-60,"elapsed":6421,"user":{"displayName":"Sim Don","userId":"09290070938786203900"}}},"outputs":[],"source":["stratSplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n","\n","for train_index, test_index in stratSplit.split(dfData[data[\"feature_names\"]], dfData[\"target\"]):\n","    X_train = dfData[data[\"feature_names\"]].iloc[train_index]\n","    X_test = dfData[data[\"feature_names\"]].iloc[test_index]\n","\n","    y_train = dfData[\"target\"].iloc[train_index]\n","    y_test = dfData[\"target\"].iloc[test_index]"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":394},"id":"P9vKYBIrT4AS","executionInfo":{"status":"error","timestamp":1708513674074,"user_tz":-60,"elapsed":502106,"user":{"displayName":"Sim Don","userId":"09290070938786203900"}},"outputId":"f10d90d1-8bc2-4370-d789-0224781518b1"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py:280: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n","  warnings.warn(\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-fe986a75fba8>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                                          )\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mgrad_boost_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         n_stages = self._fit_stages(\n\u001b[0m\u001b[1;32m    539\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0;31m# fit next stage of trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[1;32m    616\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \"\"\"\n\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1248\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    377\u001b[0m             )\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from sklearn.ensemble import GradientBoostingClassifier\n","\n","# For classification, regression trees are still used to perform fitting on residuals\n","#GradientBoosting: use of decision trees\n","grad_boost_clf = GradientBoostingClassifier(n_estimators=30, #default: 100\n","                                          loss='deviance', # 2*neg. log. likelihood\n","                                                           # loss determines how residuals are calculated, residuals\n","                                                           # are the negative gradient of the loss\n","                                          learning_rate=0.1,\n","                                          subsample=0.9,# for the use of bagging\n","                                          criterion='friedman_mse', # decision tree splitting criterion, default friedman because we are using regression trees\n","                                          random_state=0 # random state for Grad. Boost. Ensemble\n","                                          # additional decision tree parameters\n","                                         )\n","\n","grad_boost_clf.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cD80wWu2T4AU","outputId":"faefbc30-131a-4770-fff8-af7dbb04002c"},"outputs":[{"name":"stdout","output_type":"stream","text":["(30, 10)\n","(30,)\n","(30,)\n","(784,)\n"]}],"source":["#looking at the attributes\n","print(grad_boost_clf.estimators_.shape) # (n_estimators, n_classes), shape because we have multiclasses\n","                                        # estimators at each stage for each class (if multiclass problem)\n","print(grad_boost_clf.train_score_.shape) # loss at each estimator in the ensemble for in-bag data, or training date\n","                                         # if subsamples=1\n","print(grad_boost_clf.oob_improvement_.shape) # available if we have subsample<1, loss improvements of oob samples\n","                                             # compared to previous stage\n","print(grad_boost_clf.feature_importances_.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UHj8Fk6wT4AW","outputId":"4fb53f96-dda7-44eb-9a59-39504f3da794"},"outputs":[{"data":{"text/plain":["0.901"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["#score of prediction of our testing features on our testing targets\n","grad_boost_clf.score(X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EaONm79cT4AY","outputId":"a1092c54-55e7-45b2-deb5-9767f8c48e5d"},"outputs":[{"name":"stdout","output_type":"stream","text":["stage:  1 stage predictions: ['0' '0' '9' ... '5' '2' '7'] staged_score: 0.6671 14000\n","stage:  2 stage predictions: ['0' '0' '9' ... '5' '2' '7'] staged_score: 0.7359 14000\n","stage:  3 stage predictions: ['0' '0' '9' ... '5' '2' '7'] staged_score: 0.7588 14000\n","stage:  4 stage predictions: ['0' '0' '9' ... '5' '2' '7'] staged_score: 0.7783 14000\n","stage:  5 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.7949 14000\n","stage:  6 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.8070 14000\n","stage:  7 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.8149 14000\n","stage:  8 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.8250 14000\n","stage:  9 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.8327 14000\n","stage: 10 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.8389 14000\n","stage: 11 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.8436 14000\n","stage: 12 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.8521 14000\n","stage: 13 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.8561 14000\n","stage: 14 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.8601 14000\n","stage: 15 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.8629 14000\n","stage: 16 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.8658 14000\n","stage: 17 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.8689 14000\n","stage: 18 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.8718 14000\n","stage: 19 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.8759 14000\n","stage: 20 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.8789 14000\n","stage: 21 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.8814 14000\n","stage: 22 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.8831 14000\n","stage: 23 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.8869 14000\n","stage: 24 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.8879 14000\n","stage: 25 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.8910 14000\n","stage: 26 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.8929 14000\n","stage: 27 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.8944 14000\n","stage: 28 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.8966 14000\n","stage: 29 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.8990 14000\n","stage: 30 stage predictions: ['0' '0' '4' ... '5' '2' '7'] staged_score: 0.9010 14000\n"]}],"source":["#looking at predictions at each stage (gradient boosting stages)\n","from sklearn.metrics import accuracy_score\n","#resulting final prediction at each stage of the estimator\n","stage = 1\n","for stage_pred in grad_boost_clf.staged_predict(X_test):\n","    print('stage:',\"{0:2d}\".format(stage),\n","          'stage predictions:',stage_pred,\n","          # staged score doesn't exist internally\n","          'staged_score:',\"{0:.4f}\".format(accuracy_score(y_pred=stage_pred,y_true=y_test)),\n","          len(stage_pred))\n","    stage += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lUH7cVTJT4AZ"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}